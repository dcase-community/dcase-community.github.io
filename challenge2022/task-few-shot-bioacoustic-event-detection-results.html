<!DOCTYPE html><html lang="en">
<head>
    <title>Few-shot Bioacoustic Event Detection - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2022/task-few-shot-bioacoustic-event-detection-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description This challenge focuses on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations. Participants will be expected to create a method that can extract information from five exemplar vocalisations (shots) of mammals or birds and detect and classify sounds in field recordings. The â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2022</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2022/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-automatic-audio-captioning-and-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning-and-language-based-audio-retrieval"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Automatic audio-captioning</strong>
    </li>
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning"><i class="fa dc-captioning fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Language-Based Audio Retrieval</strong>
    </li>
            <li class="">
        <a href="/challenge2022/task-language-based-audio-retrieval"><i class="fa fa-file-text fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2022/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2022/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/leafs-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-danger"></i><i class="fa dc-bird fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Bio</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 5</span></span><img src="../images/logos/dcase/dcase2022_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Few-shot Bioacoustic Event Detection</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#dataset-wise-metrics">Dataset wise metrics</a></li>
</ul>
</li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>This challenge focuses on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations. Participants will be expected to create a method that can extract information from five exemplar vocalisations (shots) of mammals or birds and detect and classify sounds in field recordings. The main objective is to find reliable algorithms that are capable of dealing with data sparsity, class imbalance, and noisy/busy environments. </p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2022/task-few-shot-bioacoustic-event-detection" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="label" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="results_evaluation_set_overall_F-score" data-scatter-y="results_validation_set_overall_F-score" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="results_evaluation_set_overall_F-score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="label" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="results_evaluation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_validation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Validation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_TempMatch_task5_1</td>
<td>Baseline Template Matching</td>
<td></td>
<td>12.3 (11.5 - 12.8)</td>
<td>3.4</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_PROTO_task5_1</td>
<td>Baseline Prototypical Network</td>
<td></td>
<td>5.3 ( - )</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wu_SHNU_task5_1</td>
<td>Continual_learning</td>
<td>Wu2022</td>
<td>40.9 (40.5 - 41.3)</td>
<td>53.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang2022</td>
<td>1.2 (0.9 - 1.3)</td>
<td>46.5</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang2022</td>
<td>0.9 (0.0 - 1.0)</td>
<td>45.5</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang2022</td>
<td>1.9 (1.0 - 2.0)</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang2022</td>
<td>4.3 (3.7 - 4.6)</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_1</td>
<td>FewShot_using_good_embedding_model</td>
<td>Kang2022</td>
<td>2.4 (2.4 - 2.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_2</td>
<td>FewShot_using_good_embedding_model</td>
<td>Kang2022</td>
<td>2.8 (2.8 - 2.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_1</td>
<td>ZF_CNN1</td>
<td>Hertkorn2022</td>
<td>43.4 (42.9 - 43.8)</td>
<td>60.6</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_2</td>
<td>ZF_CNN2</td>
<td>Hertkorn2022</td>
<td>44.4 (45.0 - 45.4)</td>
<td>61.8</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_3</td>
<td>ZF_CNN3</td>
<td>Hertkorn2022</td>
<td>41.4 (41.9 - 42.3)</td>
<td>67.9</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_4</td>
<td>ZF_CNN4</td>
<td>Hertkorn2022</td>
<td>33.8 (32.4 - 34.6)</td>
<td>60.5</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_1</td>
<td>TI_1</td>
<td>Yang2022</td>
<td>19.2 (18.9 - 19.5)</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_2</td>
<td>TI_2</td>
<td>Yang2022</td>
<td>18.7 (18.4 - 19.0)</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_3</td>
<td>TI_3</td>
<td>Yang2022</td>
<td>18.9 (18.6 - 19.2)</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_4</td>
<td>TI_4</td>
<td>Yang2022</td>
<td>15.8 (15.4 - 16.1)</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_1</td>
<td>Knowledge trasnfer 75% training 10 iteration adaptive (8)</td>
<td>Tan2022</td>
<td>8.1 (7.3 - 8.5)</td>
<td>52.4</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_2</td>
<td>Knowledge transfer 90% training 15 iteration</td>
<td>Tan2022</td>
<td>16.9 (16.4 - 17.2)</td>
<td>53.9</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_3</td>
<td>Knowledge Transfer 90 training (4)</td>
<td>Tan2022</td>
<td>17.1 (16.7 - 17.4)</td>
<td>54.9</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_4</td>
<td>Knowledge Transfer 90 training adaptive (4)</td>
<td>Tan2022</td>
<td>17.2 (16.8 - 17.6)</td>
<td>54.5</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_1</td>
<td>TI-PN ensemble</td>
<td>Liu2022</td>
<td>44.1 (43.6 - 44.5)</td>
<td>61.2</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_2</td>
<td>TI-PN ensemble_2</td>
<td>Liu2022</td>
<td>41.9 (41.6 - 42.2)</td>
<td>63.3</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_3</td>
<td>TI_scalable</td>
<td>Liu2022</td>
<td>36.8 (36.5 - 37.2)</td>
<td>43.5</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_4</td>
<td>pretrained TI-PN ensemble</td>
<td>Liu2022</td>
<td>44.3 (43.9 - 44.6)</td>
<td>64.8</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_1</td>
<td>willbo_supervised_1</td>
<td>Willbo2022</td>
<td>17.9 (17.6 - 18.2)</td>
<td>51.4</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_2</td>
<td>willbo_supervised_2</td>
<td>Willbo2022</td>
<td>20.4 (20.1 - 20.7)</td>
<td>57.5</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_3</td>
<td>willbo_semi_1</td>
<td>Willbo2022</td>
<td>20.2 (19.9 - 20.5)</td>
<td>50.8</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_4</td>
<td>willbo_semi_2</td>
<td>Willbo2022</td>
<td>21.7 (21.3 - 22.0)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_1</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>28.1 (27.6 - 28.5)</td>
<td>67.3</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_2</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>16.3 (15.1 - 16.9)</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_3</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>29.9 (29.3 - 30.3)</td>
<td>60.0</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_4</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>33.2 (32.7 - 33.7)</td>
<td>57.2</td>
</tr>
<tr>
<td></td>
<td>Huang_SCUT_task5_1</td>
<td>Transductive learning and modified central difference convolution</td>
<td>Huang2022</td>
<td>18.3 (18.0 - 18.6)</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_1</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>48.0 (47.5 - 48.4)</td>
<td>60.0</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_2</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>45.4 (44.9 - 45.9)</td>
<td>30.6</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_3</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>19.4 (18.6 - 20.0)</td>
<td>44.6</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_4</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>32.5 (31.7 - 33.1)</td>
<td>13.3</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_1</td>
<td>Haohe_Liu_S1</td>
<td>Liu2022a</td>
<td>43.1 (42.7 - 43.4)</td>
<td>58.5</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_2</td>
<td>Haohe_Liu_S2</td>
<td>Liu2022a</td>
<td>48.2 (48.5 - 48.9)</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_3</td>
<td>Haohe_Liu_S3</td>
<td>Liu2022a</td>
<td>36.9 (36.5 - 37.2)</td>
<td>40.7</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_4</td>
<td>Haohe_Liu_S4</td>
<td>Liu2022a</td>
<td>45.5 (45.8 - 46.2)</td>
<td>60.2</td>
</tr>
<tr>
<td></td>
<td>Li_QMUL_task5_1</td>
<td>Prototypical Network with ResNet and SpecAugment</td>
<td>Li2022</td>
<td>15.5 (15.2 - 15.8)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Mariajohn_DSPC_task5_1</td>
<td>Prototypical-1</td>
<td>Mariajohn2022</td>
<td>25.7 (25.4 - 25.9)</td>
<td>43.9</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_1</td>
<td>Segment-level embedding learning</td>
<td>Du2022a</td>
<td>36.5 (35.6 - 37.0)</td>
<td>68.2</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_2</td>
<td>Frame-level embedding learning 1</td>
<td>Du2022a</td>
<td>60.2 (59.7 - 61.7)</td>
<td>74.4</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_3</td>
<td>event filtering</td>
<td>Du2022a</td>
<td>42.9 (42.4 - 43.4)</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_4</td>
<td>Frame-level embedding learning 2</td>
<td>Du2022a</td>
<td>60.0 (58.5 - 61.5)</td>
<td>74.4</td>
</tr>
</tbody>
</table>
<h2 id="dataset-wise-metrics">Dataset wise metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="label" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="results_evaluation_set_dataset_wise_CHE_F-score" data-scatter-y="results_evaluation_set_dataset_wise_CT_Fscore" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="results_evaluation_set_overall_F-score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="label" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="results_evaluation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="Event-based F-score (CHE dataset)" data-chartable="true" data-field="results_evaluation_set_dataset_wise_CHE_F-score" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(CHE dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (CT dataset)" data-chartable="true" data-field="results_evaluation_set_dataset_wise_CT_Fscore" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(CT dataset)
            </th>
<th class="text-center" data-axis-label="Event-based F-score (MGE dataset)" data-chartable="true" data-field="results_evaluation_set_dataset_wise_MGE_Fscore" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(MGE dataset)
            </th>
<th class="text-center" data-axis-label="Event-based F-score (MS dataset)" data-chartable="true" data-field="results_evaluation_set_dataset_wise_MS_Fscore" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(MS dataset)
            </th>
<th class="text-center" data-axis-label="Event-based F-score (QU dataset)" data-chartable="true" data-field="results_evaluation_set_dataset_wise_QU_Fscore" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(QU dataset)
            </th>
<th class="text-center" data-axis-label="Event-based F-score (DC dataset)" data-chartable="true" data-field="results_evaluation_set_dataset_wise_DC_Fscore" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(DC dataset)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_TempMatch_task5_1</td>
<td>Baseline Template Matching</td>
<td></td>
<td>12.3 (11.5 - 12.8)</td>
<td>21.1</td>
<td>7.1</td>
<td>44.1</td>
<td>8.0</td>
<td>9.7</td>
<td>35.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_PROTO_task5_1</td>
<td>Baseline Prototypical Network</td>
<td></td>
<td>5.3 ( - )</td>
<td>42.6</td>
<td>8.0</td>
<td>3.8</td>
<td>11.6</td>
<td>1.6</td>
<td>40.1</td>
</tr>
<tr>
<td></td>
<td>Wu_SHNU_task5_1</td>
<td>Continual_learning</td>
<td>Wu2022</td>
<td>40.9 (40.5 - 41.3)</td>
<td>65.0</td>
<td>37.2</td>
<td>38.2</td>
<td>38.9</td>
<td>38.1</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang2022</td>
<td>1.2 (0.9 - 1.3)</td>
<td>30.3</td>
<td>24.6</td>
<td>5.8</td>
<td>1.1</td>
<td>0.3</td>
<td>25.4</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang2022</td>
<td>0.9 (0.0 - 1.0)</td>
<td>26.8</td>
<td>38.3</td>
<td>11.1</td>
<td>0.2</td>
<td>14.6</td>
<td>9.1</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang2022</td>
<td>1.9 (1.0 - 2.0)</td>
<td>29.2</td>
<td>26.0</td>
<td>55.6</td>
<td>0.4</td>
<td>15.8</td>
<td>17.5</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang2022</td>
<td>4.3 (3.7 - 4.6)</td>
<td>29.6</td>
<td>17.6</td>
<td>55.3</td>
<td>0.9</td>
<td>18.2</td>
<td>30.2</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_1</td>
<td>FewShot_using_good_embedding_model</td>
<td>Kang2022</td>
<td>2.4 (2.4 - 2.4)</td>
<td>11.0</td>
<td>0.7</td>
<td>3.3</td>
<td>3.5</td>
<td>4.3</td>
<td>4.7</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_2</td>
<td>FewShot_using_good_embedding_model</td>
<td>Kang2022</td>
<td>2.8 (2.8 - 2.9)</td>
<td>8.7</td>
<td>0.9</td>
<td>3.3</td>
<td>3.9</td>
<td>5.3</td>
<td>4.7</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_1</td>
<td>ZF_CNN1</td>
<td>Hertkorn2022</td>
<td>43.4 (42.9 - 43.8)</td>
<td>70.2</td>
<td>37.8</td>
<td>68.4</td>
<td>64.1</td>
<td>22.5</td>
<td>51.2</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_2</td>
<td>ZF_CNN2</td>
<td>Hertkorn2022</td>
<td>44.4 (45.0 - 45.4)</td>
<td>70.3</td>
<td>37.1</td>
<td>63.8</td>
<td>58.6</td>
<td>25.9</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_3</td>
<td>ZF_CNN3</td>
<td>Hertkorn2022</td>
<td>41.4 (41.9 - 42.3)</td>
<td>66.7</td>
<td>40.0</td>
<td>76.4</td>
<td>74.0</td>
<td>18.2</td>
<td>57.9</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_4</td>
<td>ZF_CNN4</td>
<td>Hertkorn2022</td>
<td>33.8 (32.4 - 34.6)</td>
<td>64.6</td>
<td>15.0</td>
<td>84.9</td>
<td>71.0</td>
<td>21.5</td>
<td>58.8</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_1</td>
<td>TI_1</td>
<td>Yang2022</td>
<td>19.2 (18.9 - 19.5)</td>
<td>33.4</td>
<td>22.8</td>
<td>59.7</td>
<td>44.0</td>
<td>6.8</td>
<td>22.9</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_2</td>
<td>TI_2</td>
<td>Yang2022</td>
<td>18.7 (18.4 - 19.0)</td>
<td>32.9</td>
<td>22.6</td>
<td>60.7</td>
<td>42.7</td>
<td>6.6</td>
<td>22.4</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_3</td>
<td>TI_3</td>
<td>Yang2022</td>
<td>18.9 (18.6 - 19.2)</td>
<td>30.9</td>
<td>24.0</td>
<td>60.9</td>
<td>43.8</td>
<td>6.7</td>
<td>22.1</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_4</td>
<td>TI_4</td>
<td>Yang2022</td>
<td>15.8 (15.4 - 16.1)</td>
<td>43.8</td>
<td>9.3</td>
<td>57.2</td>
<td>30.9</td>
<td>6.3</td>
<td>31.4</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_1</td>
<td>Knowledge trasnfer 75% training 10 iteration adaptive (8)</td>
<td>Tan2022</td>
<td>8.1 (7.3 - 8.5)</td>
<td>39.0</td>
<td>43.9</td>
<td>2.4</td>
<td>10.3</td>
<td>15.0</td>
<td>12.7</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_2</td>
<td>Knowledge transfer 90% training 15 iteration</td>
<td>Tan2022</td>
<td>16.9 (16.4 - 17.2)</td>
<td>31.5</td>
<td>32.8</td>
<td>8.0</td>
<td>15.3</td>
<td>15.4</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_3</td>
<td>Knowledge Transfer 90 training (4)</td>
<td>Tan2022</td>
<td>17.1 (16.7 - 17.4)</td>
<td>25.5</td>
<td>40.3</td>
<td>8.4</td>
<td>15.7</td>
<td>18.0</td>
<td>28.6</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_4</td>
<td>Knowledge Transfer 90 training adaptive (4)</td>
<td>Tan2022</td>
<td>17.2 (16.8 - 17.6)</td>
<td>26.2</td>
<td>40.3</td>
<td>8.4</td>
<td>15.7</td>
<td>18.0</td>
<td>29.6</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_1</td>
<td>TI-PN ensemble</td>
<td>Liu2022</td>
<td>44.1 (43.6 - 44.5)</td>
<td>54.6</td>
<td>45.7</td>
<td>47.3</td>
<td>51.5</td>
<td>32.4</td>
<td>48.5</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_2</td>
<td>TI-PN ensemble_2</td>
<td>Liu2022</td>
<td>41.9 (41.6 - 42.2)</td>
<td>54.6</td>
<td>56.3</td>
<td>47.3</td>
<td>51.5</td>
<td>24.0</td>
<td>48.5</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_3</td>
<td>TI_scalable</td>
<td>Liu2022</td>
<td>36.8 (36.5 - 37.2)</td>
<td>52.2</td>
<td>41.0</td>
<td>51.6</td>
<td>49.3</td>
<td>22.2</td>
<td>33.6</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_4</td>
<td>pretrained TI-PN ensemble</td>
<td>Liu2022</td>
<td>44.3 (43.9 - 44.6)</td>
<td>54.6</td>
<td>45.0</td>
<td>48.0</td>
<td>53.9</td>
<td>32.5</td>
<td>47.7</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_1</td>
<td>willbo_supervised_1</td>
<td>Willbo2022</td>
<td>17.9 (17.6 - 18.2)</td>
<td>43.8</td>
<td>19.1</td>
<td>24.6</td>
<td>20.9</td>
<td>12.2</td>
<td>12.8</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_2</td>
<td>willbo_supervised_2</td>
<td>Willbo2022</td>
<td>20.4 (20.1 - 20.7)</td>
<td>47.1</td>
<td>17.4</td>
<td>31.1</td>
<td>21.4</td>
<td>12.2</td>
<td>21.9</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_3</td>
<td>willbo_semi_1</td>
<td>Willbo2022</td>
<td>20.2 (19.9 - 20.5)</td>
<td>44.0</td>
<td>14.8</td>
<td>24.8</td>
<td>24.9</td>
<td>13.9</td>
<td>22.1</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_4</td>
<td>willbo_semi_2</td>
<td>Willbo2022</td>
<td>21.7 (21.3 - 22.0)</td>
<td>48.8</td>
<td>14.9</td>
<td>31.1</td>
<td>25.9</td>
<td>13.9</td>
<td>25.5</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_1</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>28.1 (27.6 - 28.5)</td>
<td>51.0</td>
<td>52.9</td>
<td>13.9</td>
<td>33.4</td>
<td>27.4</td>
<td>33.7</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_2</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>16.3 (15.1 - 16.9)</td>
<td>51.2</td>
<td>39.8</td>
<td>4.2</td>
<td>48.4</td>
<td>34.7</td>
<td>46.3</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_3</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>29.9 (29.3 - 30.3)</td>
<td>49.7</td>
<td>23.7</td>
<td>15.5</td>
<td>60.9</td>
<td>35.9</td>
<td>41.7</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_4</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>33.2 (32.7 - 33.7)</td>
<td>58.8</td>
<td>31.1</td>
<td>19.7</td>
<td>41.1</td>
<td>38.4</td>
<td>40.4</td>
</tr>
<tr>
<td></td>
<td>Huang_SCUT_task5_1</td>
<td>Transductive learning and modified central difference convolution</td>
<td>Huang2022</td>
<td>18.3 (18.0 - 18.6)</td>
<td>17.9</td>
<td>20.6</td>
<td>65.6</td>
<td>56.0</td>
<td>7.4</td>
<td>22.1</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_1</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>48.0 (47.5 - 48.4)</td>
<td>71.7</td>
<td>48.4</td>
<td>77.6</td>
<td>70.6</td>
<td>24.6</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_2</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>45.4 (44.9 - 45.9)</td>
<td>56.3</td>
<td>37.6</td>
<td>61.5</td>
<td>70.7</td>
<td>29.5</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_3</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>19.4 (18.6 - 20.0)</td>
<td>67.1</td>
<td>4.7</td>
<td>65.5</td>
<td>73.3</td>
<td>34.7</td>
<td>45.0</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_4</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>32.5 (31.7 - 33.1)</td>
<td>50.9</td>
<td>13.4</td>
<td>47.8</td>
<td>71.2</td>
<td>34.1</td>
<td>42.5</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_1</td>
<td>Haohe_Liu_S1</td>
<td>Liu2022a</td>
<td>43.1 (42.7 - 43.4)</td>
<td>81.9</td>
<td>58.4</td>
<td>46.4</td>
<td>48.4</td>
<td>22.8</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_2</td>
<td>Haohe_Liu_S2</td>
<td>Liu2022a</td>
<td>48.2 (48.5 - 48.9)</td>
<td>76.9</td>
<td>57.4</td>
<td>48.0</td>
<td>60.7</td>
<td>28.9</td>
<td>56.8</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_3</td>
<td>Haohe_Liu_S3</td>
<td>Liu2022a</td>
<td>36.9 (36.5 - 37.2)</td>
<td>83.0</td>
<td>52.2</td>
<td>29.1</td>
<td>53.5</td>
<td>18.5</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_4</td>
<td>Haohe_Liu_S4</td>
<td>Liu2022a</td>
<td>45.5 (45.8 - 46.2)</td>
<td>80.5</td>
<td>61.8</td>
<td>38.8</td>
<td>47.7</td>
<td>30.3</td>
<td>53.8</td>
</tr>
<tr>
<td></td>
<td>Li_QMUL_task5_1</td>
<td>Prototypical Network with ResNet and SpecAugment</td>
<td>Li2022</td>
<td>15.5 (15.2 - 15.8)</td>
<td>39.5</td>
<td>35.0</td>
<td>11.9</td>
<td>17.9</td>
<td>6.9</td>
<td>30.7</td>
</tr>
<tr>
<td></td>
<td>Mariajohn_DSPC_task5_1</td>
<td>Prototypical-1</td>
<td>Mariajohn2022</td>
<td>25.7 (25.4 - 25.9)</td>
<td>27.4</td>
<td>23.6</td>
<td>55.4</td>
<td>65.5</td>
<td>19.4</td>
<td>14.9</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_1</td>
<td>Segment-level embedding learning</td>
<td>Du2022a</td>
<td>36.5 (35.6 - 37.0)</td>
<td>53.6</td>
<td>43.9</td>
<td>43.0</td>
<td>57.5</td>
<td>17.7</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_2</td>
<td>Frame-level embedding learning 1</td>
<td>Du2022a</td>
<td>60.2 (59.7 - 61.7)</td>
<td>71.7</td>
<td>48.4</td>
<td>89.1</td>
<td>66.3</td>
<td>48.7</td>
<td>57.3</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_3</td>
<td>event filtering</td>
<td>Du2022a</td>
<td>42.9 (42.4 - 43.4)</td>
<td>57.4</td>
<td>48.6</td>
<td>62.3</td>
<td>42.4</td>
<td>23.5</td>
<td>52.2</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_4</td>
<td>Frame-level embedding learning 2</td>
<td>Du2022a</td>
<td>60.0 (58.5 - 61.5)</td>
<td>73.3</td>
<td>49.6</td>
<td>91.3</td>
<td>64.4</td>
<td>46.3</td>
<td>57.7</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="label" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="results_evaluation_set_overall_F-score" data-scatter-y="results_validation_set_overall_F-score" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="results_evaluation_set_overall_F-score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="label" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="results_evaluation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_validation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_TempMatch_task5_1</td>
<td>Baseline Template Matching</td>
<td></td>
<td>12.3 (11.5 - 12.8)</td>
<td>3.4</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_PROTO_task5_1</td>
<td>Baseline Prototypical Network</td>
<td></td>
<td>5.3 ( - )</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wu_SHNU_task5_1</td>
<td>Continual_learning</td>
<td>Wu2022</td>
<td>40.9 (40.5 - 41.3)</td>
<td>53.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang2022</td>
<td>4.3 (3.7 - 4.6)</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_2</td>
<td>FewShot_using_good_embedding_model</td>
<td>Kang2022</td>
<td>2.8 (2.8 - 2.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_2</td>
<td>ZF_CNN2</td>
<td>Hertkorn2022</td>
<td>44.4 (45.0 - 45.4)</td>
<td>61.8</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_1</td>
<td>TI_1</td>
<td>Yang2022</td>
<td>19.2 (18.9 - 19.5)</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_4</td>
<td>Knowledge Transfer 90 training adaptive (4)</td>
<td>Tan2022</td>
<td>17.2 (16.8 - 17.6)</td>
<td>54.5</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_4</td>
<td>pretrained TI-PN ensemble</td>
<td>Liu2022</td>
<td>44.3 (43.9 - 44.6)</td>
<td>64.8</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_4</td>
<td>willbo_semi_2</td>
<td>Willbo2022</td>
<td>21.7 (21.3 - 22.0)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_4</td>
<td>Siamese Network with fully connected head</td>
<td>Zgorzynski2022</td>
<td>33.2 (32.7 - 33.7)</td>
<td>57.2</td>
</tr>
<tr>
<td></td>
<td>Huang_SCUT_task5_1</td>
<td>Transductive learning and modified central difference convolution</td>
<td>Huang2022</td>
<td>18.3 (18.0 - 18.6)</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_1</td>
<td>Adaptive prototypical ensemble</td>
<td>Martinsson2022</td>
<td>48.0 (47.5 - 48.4)</td>
<td>60.0</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_2</td>
<td>Haohe_Liu_S2</td>
<td>Liu2022a</td>
<td>48.2 (48.5 - 48.9)</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Li_QMUL_task5_1</td>
<td>Prototypical Network with ResNet and SpecAugment</td>
<td>Li2022</td>
<td>15.5 (15.2 - 15.8)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Mariajohn_DSPC_task5_1</td>
<td>Prototypical-1</td>
<td>Mariajohn2022</td>
<td>25.7 (25.4 - 25.9)</td>
<td>43.9</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_2</td>
<td>Frame-level embedding learning 1</td>
<td>Du2022a</td>
<td>60.2 (59.7 - 61.7)</td>
<td>74.4</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="label" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="results_evaluation_set_overall_F-score" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="label" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_evaluation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_input_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_acoustic_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_TempMatch_task5_1</td>
<td></td>
<td>12.3 (11.5 - 12.8)</td>
<td>any</td>
<td></td>
<td>spectrogram</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_PROTO_task5_1</td>
<td></td>
<td>5.3 ( - )</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Wu_SHNU_task5_1</td>
<td>Wu2022</td>
<td>40.9 (40.5 - 41.3)</td>
<td>any</td>
<td>Time masking, Frequency masking</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang2022</td>
<td>1.2 (0.9 - 1.3)</td>
<td>22.05 KHz</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang2022</td>
<td>0.9 (0.0 - 1.0)</td>
<td>22.05 KHz</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang2022</td>
<td>1.9 (1.0 - 2.0)</td>
<td>22.05 KHz</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang2022</td>
<td>4.3 (3.7 - 4.6)</td>
<td>22.05 KHz</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_1</td>
<td>Kang2022</td>
<td>2.4 (2.4 - 2.4)</td>
<td>16 KHz</td>
<td>specaugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_2</td>
<td>Kang2022</td>
<td>2.8 (2.8 - 2.9)</td>
<td>16 KHz</td>
<td>Specaugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_1</td>
<td>Hertkorn2022</td>
<td>43.4 (42.9 - 43.8)</td>
<td>any</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_2</td>
<td>Hertkorn2022</td>
<td>44.4 (45.0 - 45.4)</td>
<td>any</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_3</td>
<td>Hertkorn2022</td>
<td>41.4 (41.9 - 42.3)</td>
<td>any</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_4</td>
<td>Hertkorn2022</td>
<td>33.8 (32.4 - 34.6)</td>
<td>any</td>
<td></td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_1</td>
<td>Yang2022</td>
<td>19.2 (18.9 - 19.5)</td>
<td>22.05 KHz</td>
<td>time and frequency masking, mixup</td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_2</td>
<td>Yang2022</td>
<td>18.7 (18.4 - 19.0)</td>
<td>22.05 KHz</td>
<td>time and frequency masking, mixup</td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_3</td>
<td>Yang2022</td>
<td>18.9 (18.6 - 19.2)</td>
<td>22.05 KHz</td>
<td>time and frequency masking, mixup</td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_4</td>
<td>Yang2022</td>
<td>15.8 (15.4 - 16.1)</td>
<td>22.05 KHz</td>
<td>time masking, frequency masking, mixup</td>
<td>Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_1</td>
<td>Tan2022</td>
<td>8.1 (7.3 - 8.5)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_2</td>
<td>Tan2022</td>
<td>16.9 (16.4 - 17.2)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_3</td>
<td>Tan2022</td>
<td>17.1 (16.7 - 17.4)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_4</td>
<td>Tan2022</td>
<td>17.2 (16.8 - 17.6)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_1</td>
<td>Liu2022</td>
<td>44.1 (43.6 - 44.5)</td>
<td>22.05 KHz</td>
<td>Specaugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_2</td>
<td>Liu2022</td>
<td>41.9 (41.6 - 42.2)</td>
<td>22.05 KHz</td>
<td>Specaugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_3</td>
<td>Liu2022</td>
<td>36.8 (36.5 - 37.2)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_4</td>
<td>Liu2022</td>
<td>44.3 (43.9 - 44.6)</td>
<td>22.05 KHz</td>
<td>Specaugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_1</td>
<td>Willbo2022</td>
<td>17.9 (17.6 - 18.2)</td>
<td>any</td>
<td></td>
<td>Mel-spectrogram, PCEN</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_2</td>
<td>Willbo2022</td>
<td>20.4 (20.1 - 20.7)</td>
<td>any</td>
<td></td>
<td>Mel-spectrogram, PCEN</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_3</td>
<td>Willbo2022</td>
<td>20.2 (19.9 - 20.5)</td>
<td>any</td>
<td></td>
<td>Mel-spectrogram, PCEN</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_4</td>
<td>Willbo2022</td>
<td>21.7 (21.3 - 22.0)</td>
<td>any</td>
<td></td>
<td>Mel-spectrogram, PCEN</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_1</td>
<td>Zgorzynski2022</td>
<td>28.1 (27.6 - 28.5)</td>
<td>48 KHz</td>
<td>Noise mixing, Random Crop</td>
<td>Mel-spectrogram, PCEN</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_2</td>
<td>Zgorzynski2022</td>
<td>16.3 (15.1 - 16.9)</td>
<td>48 KHz</td>
<td>Noise mixing</td>
<td>Mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_3</td>
<td>Zgorzynski2022</td>
<td>29.9 (29.3 - 30.3)</td>
<td>48 KHz</td>
<td>Noise mixing</td>
<td>Mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_4</td>
<td>Zgorzynski2022</td>
<td>33.2 (32.7 - 33.7)</td>
<td>48 KHz</td>
<td>Noise mixing</td>
<td>Mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>Huang_SCUT_task5_1</td>
<td>Huang2022</td>
<td>18.3 (18.0 - 18.6)</td>
<td>22.05 KHz</td>
<td>Specaugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_1</td>
<td>Martinsson2022</td>
<td>48.0 (47.5 - 48.4)</td>
<td>22.05 KHz</td>
<td></td>
<td>Log-Mel energies, PCEN</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_2</td>
<td>Martinsson2022</td>
<td>45.4 (44.9 - 45.9)</td>
<td>22.05 KHz</td>
<td></td>
<td>Log-Mel energies, PCEN</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_3</td>
<td>Martinsson2022</td>
<td>19.4 (18.6 - 20.0)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_4</td>
<td>Martinsson2022</td>
<td>32.5 (31.7 - 33.1)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_1</td>
<td>Liu2022a</td>
<td>43.1 (42.7 - 43.4)</td>
<td>22.05 KHz</td>
<td>Dynamic dataloader</td>
<td>PCEN, Delta-MFCC</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_2</td>
<td>Liu2022a</td>
<td>48.2 (48.5 - 48.9)</td>
<td>22.05 KHz</td>
<td>Dynamic dataloader</td>
<td>PCEN, Delta-MFCC</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_3</td>
<td>Liu2022a</td>
<td>36.9 (36.5 - 37.2)</td>
<td>22.05 KHz</td>
<td>Dynamic dataloader</td>
<td>PCEN, Delta-MFCC</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_4</td>
<td>Liu2022a</td>
<td>45.5 (45.8 - 46.2)</td>
<td>22.05 KHz</td>
<td>Dynamic dataloader</td>
<td>PCEN, Delta-MFCC</td>
</tr>
<tr>
<td></td>
<td>Li_QMUL_task5_1</td>
<td>Li2022</td>
<td>15.5 (15.2 - 15.8)</td>
<td>any</td>
<td>time masking, frequency masking, time warping</td>
<td>PCEN, Spectrogram</td>
</tr>
<tr>
<td></td>
<td>Mariajohn_DSPC_task5_1</td>
<td>Mariajohn2022</td>
<td>25.7 (25.4 - 25.9)</td>
<td>any</td>
<td>time shifting, segment level mirroring</td>
<td>Log-Mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_1</td>
<td>Du2022a</td>
<td>36.5 (35.6 - 37.0)</td>
<td>22.05 KHz</td>
<td>SpecAugment</td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_2</td>
<td>Du2022a</td>
<td>60.2 (59.7 - 61.7)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_3</td>
<td>Du2022a</td>
<td>42.9 (42.4 - 43.4)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_4</td>
<td>Du2022a</td>
<td>60.0 (58.5 - 61.5)</td>
<td>22.05 KHz</td>
<td></td>
<td>PCEN</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="label" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="results_evaluation_set_overall_F-score" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="label" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_evaluation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="text-center narrow-col" data-field="system_machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-field="system_few_shot_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Few-shot approach
            </th>
<th class="text-center narrow-col" data-field="system_post-processing" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Post-processing
            </th>
</tr>
</thead>
<tbody>
<tr data-hline="true">
<td></td>
<td>Baseline_TempMatch_task5_1</td>
<td></td>
<td>12.3 (11.5 - 12.8)</td>
<td>template matching</td>
<td>template matching</td>
<td>peak picking, threshold</td>
</tr>
<tr data-hline="true">
<td></td>
<td>Baseline_PROTO_task5_1</td>
<td></td>
<td>5.3 ( - )</td>
<td>ResNet</td>
<td>prototypical</td>
<td>threshold</td>
</tr>
<tr>
<td></td>
<td>Wu_SHNU_task5_1</td>
<td>Wu2022</td>
<td>40.9 (40.5 - 41.3)</td>
<td>Continual Learning</td>
<td>prototypical, weight generator</td>
<td>threshold</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang2022</td>
<td>1.2 (0.9 - 1.3)</td>
<td>CNN</td>
<td>prototypical</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang2022</td>
<td>0.9 (0.0 - 1.0)</td>
<td>CNN</td>
<td>prototypical</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang2022</td>
<td>1.9 (1.0 - 2.0)</td>
<td>CNN</td>
<td>prototypical</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang2022</td>
<td>4.3 (3.7 - 4.6)</td>
<td>CNN</td>
<td>prototypical</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_1</td>
<td>Kang2022</td>
<td>2.4 (2.4 - 2.4)</td>
<td>TDNN</td>
<td>Fine tuning</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_2</td>
<td>Kang2022</td>
<td>2.8 (2.8 - 2.9)</td>
<td>TDNN</td>
<td>Fine tuning</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_1</td>
<td>Hertkorn2022</td>
<td>43.4 (42.9 - 43.8)</td>
<td>CNN</td>
<td></td>
<td>threshold, duration threshold, event stitching</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_2</td>
<td>Hertkorn2022</td>
<td>44.4 (45.0 - 45.4)</td>
<td>CNN</td>
<td></td>
<td>threshold, duration threshold, event stitching</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_3</td>
<td>Hertkorn2022</td>
<td>41.4 (41.9 - 42.3)</td>
<td>CNN</td>
<td></td>
<td>threshold, duration threshold, event stitching</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_4</td>
<td>Hertkorn2022</td>
<td>33.8 (32.4 - 34.6)</td>
<td>CNN</td>
<td></td>
<td>threshold, duration threshold, event stitching</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_1</td>
<td>Yang2022</td>
<td>19.2 (18.9 - 19.5)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, peak picking</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_2</td>
<td>Yang2022</td>
<td>18.7 (18.4 - 19.0)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, peak picking</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_3</td>
<td>Yang2022</td>
<td>18.9 (18.6 - 19.2)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, peak picking</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_4</td>
<td>Yang2022</td>
<td>15.8 (15.4 - 16.1)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, peak picking</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_1</td>
<td>Tan2022</td>
<td>8.1 (7.3 - 8.5)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>threshold, minimum event length</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_2</td>
<td>Tan2022</td>
<td>16.9 (16.4 - 17.2)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>threshold</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_3</td>
<td>Tan2022</td>
<td>17.1 (16.7 - 17.4)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>threshold</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_4</td>
<td>Tan2022</td>
<td>17.2 (16.8 - 17.6)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>threshold, minimum event length</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_1</td>
<td>Liu2022</td>
<td>44.1 (43.6 - 44.5)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>peak picking, threshold, VAD</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_2</td>
<td>Liu2022</td>
<td>41.9 (41.6 - 42.2)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>peak picking, threshold, VAD</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_3</td>
<td>Liu2022</td>
<td>36.8 (36.5 - 37.2)</td>
<td>CNN</td>
<td>Transductive inference</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_4</td>
<td>Liu2022</td>
<td>44.3 (43.9 - 44.6)</td>
<td>CNN</td>
<td>prototypical, transductive inference</td>
<td>peak picking, threshold, VAD</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_1</td>
<td>Willbo2022</td>
<td>17.9 (17.6 - 18.2)</td>
<td>ResNet</td>
<td>prototypical</td>
<td>median filtering, minimum event length, threshold</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_2</td>
<td>Willbo2022</td>
<td>20.4 (20.1 - 20.7)</td>
<td>ResNet</td>
<td>prototypical, threshold fitting</td>
<td>median filtering, minimum event length, threshold</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_3</td>
<td>Willbo2022</td>
<td>20.2 (19.9 - 20.5)</td>
<td>ResNet</td>
<td>prototypical</td>
<td>median filtering, minimum event length, threshold</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_4</td>
<td>Willbo2022</td>
<td>21.7 (21.3 - 22.0)</td>
<td>ResNet</td>
<td>prototypical, threshold fitting</td>
<td>median filtering, minimum event length, threshold</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_1</td>
<td>Zgorzynski2022</td>
<td>28.1 (27.6 - 28.5)</td>
<td>CNN</td>
<td>Siamese network with fully connected head, fine tuning</td>
<td>peak picking, threshold, Gaussian filter</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_2</td>
<td>Zgorzynski2022</td>
<td>16.3 (15.1 - 16.9)</td>
<td>CNN</td>
<td>Siamese network with fully connected head, fine tuning</td>
<td>threshold, Gaussian filter</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_3</td>
<td>Zgorzynski2022</td>
<td>29.9 (29.3 - 30.3)</td>
<td>CNN</td>
<td>Siamese network with fully connected head, fine tuning</td>
<td>threshold, Gaussian filter</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_4</td>
<td>Zgorzynski2022</td>
<td>33.2 (32.7 - 33.7)</td>
<td>CNN</td>
<td>Siamese network with fully connected head, fine tuning</td>
<td>threshold, Gaussian filter</td>
</tr>
<tr>
<td></td>
<td>Huang_SCUT_task5_1</td>
<td>Huang2022</td>
<td>18.3 (18.0 - 18.6)</td>
<td>transductive learning</td>
<td>transductive learning</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_1</td>
<td>Martinsson2022</td>
<td>48.0 (47.5 - 48.4)</td>
<td>Ensemble, CNN</td>
<td>prototypical, input size</td>
<td>threshold, merging, filter too small, filter too big</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_2</td>
<td>Martinsson2022</td>
<td>45.4 (44.9 - 45.9)</td>
<td>Ensemble, CNN</td>
<td>prototypical, input size</td>
<td>threshold, merging, filter too small, filter too big</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_3</td>
<td>Martinsson2022</td>
<td>19.4 (18.6 - 20.0)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, merging, filter too small, filter too big</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_4</td>
<td>Martinsson2022</td>
<td>32.5 (31.7 - 33.1)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, merging, filter too small, filter too big</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_1</td>
<td>Liu2022a</td>
<td>43.1 (42.7 - 43.4)</td>
<td>CNN, ensemble</td>
<td>prototypical</td>
<td>threshold, filter by length, split long, remove long</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_2</td>
<td>Liu2022a</td>
<td>48.2 (48.5 - 48.9)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, filter by length, remove long, padding</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_3</td>
<td>Liu2022a</td>
<td>36.9 (36.5 - 37.2)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, filter by length, split long, remove long, merge short, padding</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_4</td>
<td>Liu2022a</td>
<td>45.5 (45.8 - 46.2)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold, filter by length, remove long</td>
</tr>
<tr>
<td></td>
<td>Li_QMUL_task5_1</td>
<td>Li2022</td>
<td>15.5 (15.2 - 15.8)</td>
<td>CNN</td>
<td>prototypical</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Mariajohn_DSPC_task5_1</td>
<td>Mariajohn2022</td>
<td>25.7 (25.4 - 25.9)</td>
<td>CNN</td>
<td>prototypical</td>
<td>threshold</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_1</td>
<td>Du2022a</td>
<td>36.5 (35.6 - 37.0)</td>
<td>CNN</td>
<td>fine tuning</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_2</td>
<td>Du2022a</td>
<td>60.2 (59.7 - 61.7)</td>
<td>CNN</td>
<td>fine tuning</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_3</td>
<td>Du2022a</td>
<td>42.9 (42.4 - 43.4)</td>
<td>CNN</td>
<td>fine tuning</td>
<td>peak picking, threshold</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_4</td>
<td>Du2022a</td>
<td>60.0 (58.5 - 61.5)</td>
<td>CNN</td>
<td>fine tuning</td>
<td>peak picking, threshold</td>
</tr>
</tbody>
</table>
<h2 id="complexity">Complexity</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="label" data-filter-control="true" data-filter-show-clear="true" data-id-field="label" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="results_evaluation_set_overall_F-score" data-scatter-y="system_complexity_total_parameters" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="system_complexity_total_parameters" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="label" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_evaluation_set_overall_F-score" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total_parameters" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-field="system_complexity_trainining_time" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Training time
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_TempMatch_task5_1</td>
<td></td>
<td>12.3 (11.5 - 12.8)</td>
<td></td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_PROTO_task5_1</td>
<td></td>
<td>5.3 ( - )</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wu_SHNU_task5_1</td>
<td>Wu2022</td>
<td>40.9 (40.5 - 41.3)</td>
<td>443520</td>
<td>2.5h</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_1</td>
<td>Zhang2022</td>
<td>1.2 (0.9 - 1.3)</td>
<td></td>
<td>90min</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_2</td>
<td>Zhang2022</td>
<td>0.9 (0.0 - 1.0)</td>
<td></td>
<td>90min</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_3</td>
<td>Zhang2022</td>
<td>1.9 (1.0 - 2.0)</td>
<td></td>
<td>90min</td>
</tr>
<tr>
<td></td>
<td>Zhang_CQU_task5_4</td>
<td>Zhang2022</td>
<td>4.3 (3.7 - 4.6)</td>
<td></td>
<td>90min</td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_1</td>
<td>Kang2022</td>
<td>2.4 (2.4 - 2.4)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kang_ET_task5_2</td>
<td>Kang2022</td>
<td>2.8 (2.8 - 2.9)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_1</td>
<td>Hertkorn2022</td>
<td>43.4 (42.9 - 43.8)</td>
<td>54979</td>
<td>6 min/ wav file</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_2</td>
<td>Hertkorn2022</td>
<td>44.4 (45.0 - 45.4)</td>
<td>54979</td>
<td>6 min/ wav file</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_3</td>
<td>Hertkorn2022</td>
<td>41.4 (41.9 - 42.3)</td>
<td>54979</td>
<td>6 min/ wav file</td>
</tr>
<tr>
<td></td>
<td>Hertkorn_ZF_task5_4</td>
<td>Hertkorn2022</td>
<td>33.8 (32.4 - 34.6)</td>
<td>54979</td>
<td>6 min/ wav file</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_1</td>
<td>Yang2022</td>
<td>19.2 (18.9 - 19.5)</td>
<td>468627</td>
<td>30 min</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_2</td>
<td>Yang2022</td>
<td>18.7 (18.4 - 19.0)</td>
<td>468627</td>
<td>30 min</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_3</td>
<td>Yang2022</td>
<td>18.9 (18.6 - 19.2)</td>
<td>468627</td>
<td>30 min</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task5_4</td>
<td>Yang2022</td>
<td>15.8 (15.4 - 16.1)</td>
<td>468627</td>
<td>30 min</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_1</td>
<td>Tan2022</td>
<td>8.1 (7.3 - 8.5)</td>
<td>700k</td>
<td>1h</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_2</td>
<td>Tan2022</td>
<td>16.9 (16.4 - 17.2)</td>
<td>700k</td>
<td>1h</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_3</td>
<td>Tan2022</td>
<td>17.1 (16.7 - 17.4)</td>
<td>700k</td>
<td>1h</td>
</tr>
<tr>
<td></td>
<td>Tan_WHU_task5_4</td>
<td>Tan2022</td>
<td>17.2 (16.8 - 17.6)</td>
<td>700k</td>
<td>1h</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_1</td>
<td>Liu2022</td>
<td>44.1 (43.6 - 44.5)</td>
<td>9627177</td>
<td>1.5h</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_2</td>
<td>Liu2022</td>
<td>41.9 (41.6 - 42.2)</td>
<td>9627177</td>
<td>1.5h</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_3</td>
<td>Liu2022</td>
<td>36.8 (36.5 - 37.2)</td>
<td>8757077</td>
<td>1.5h</td>
</tr>
<tr>
<td></td>
<td>Liu_BIT-SRCB_task5_4</td>
<td>Liu2022</td>
<td>44.3 (43.9 - 44.6)</td>
<td>9914068</td>
<td>1.5h</td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_1</td>
<td>Willbo2022</td>
<td>17.9 (17.6 - 18.2)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_2</td>
<td>Willbo2022</td>
<td>20.4 (20.1 - 20.7)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_3</td>
<td>Willbo2022</td>
<td>20.2 (19.9 - 20.5)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Willbo_RISE_task5_4</td>
<td>Willbo2022</td>
<td>21.7 (21.3 - 22.0)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_1</td>
<td>Zgorzynski2022</td>
<td>28.1 (27.6 - 28.5)</td>
<td>76700357</td>
<td>9h</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_2</td>
<td>Zgorzynski2022</td>
<td>16.3 (15.1 - 16.9)</td>
<td>76700357</td>
<td>9h</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_3</td>
<td>Zgorzynski2022</td>
<td>29.9 (29.3 - 30.3)</td>
<td>76700357</td>
<td>9h</td>
</tr>
<tr>
<td></td>
<td>ZGORZYNSKI_SRPOL_task5_4</td>
<td>Zgorzynski2022</td>
<td>33.2 (32.7 - 33.7)</td>
<td>76700357</td>
<td>9h</td>
</tr>
<tr>
<td></td>
<td>Huang_SCUT_task5_1</td>
<td>Huang2022</td>
<td>18.3 (18.0 - 18.6)</td>
<td>492206</td>
<td>50min, RTX3090</td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_1</td>
<td>Martinsson2022</td>
<td>48.0 (47.5 - 48.4)</td>
<td>25994880</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_2</td>
<td>Martinsson2022</td>
<td>45.4 (44.9 - 45.9)</td>
<td>25994880</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_3</td>
<td>Martinsson2022</td>
<td>19.4 (18.6 - 20.0)</td>
<td>1732992</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Martinsson_RISE_task5_4</td>
<td>Martinsson2022</td>
<td>32.5 (31.7 - 33.1)</td>
<td>1732992</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_1</td>
<td>Liu2022a</td>
<td>43.1 (42.7 - 43.4)</td>
<td>724096</td>
<td>91 min, NVIDIA GeForce 3070</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_2</td>
<td>Liu2022a</td>
<td>48.2 (48.5 - 48.9)</td>
<td>724096</td>
<td>91 min, NVIDIA GeForce 3070</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_3</td>
<td>Liu2022a</td>
<td>36.9 (36.5 - 37.2)</td>
<td>724096</td>
<td>91 min, NVIDIA GeForce 3070</td>
</tr>
<tr>
<td></td>
<td>Liu_Surrey_task5_4</td>
<td>Liu2022a</td>
<td>45.5 (45.8 - 46.2)</td>
<td>724096</td>
<td>91 min, NVIDIA GeForce 3070</td>
</tr>
<tr>
<td></td>
<td>Li_QMUL_task5_1</td>
<td>Li2022</td>
<td>15.5 (15.2 - 15.8)</td>
<td></td>
<td>40 min, Colab pro Tesla p100</td>
</tr>
<tr>
<td></td>
<td>Mariajohn_DSPC_task5_1</td>
<td>Mariajohn2022</td>
<td>25.7 (25.4 - 25.9)</td>
<td></td>
<td>2h</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_1</td>
<td>Du2022a</td>
<td>36.5 (35.6 - 37.0)</td>
<td>464531</td>
<td>5 minutes, TeslaP40-24GB</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_2</td>
<td>Du2022a</td>
<td>60.2 (59.7 - 61.7)</td>
<td>469654</td>
<td>1 hour, TeslaV100-32GB</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_3</td>
<td>Du2022a</td>
<td>42.9 (42.4 - 43.4)</td>
<td>12091947</td>
<td>1 hour, TeslaV100-32GB</td>
</tr>
<tr>
<td></td>
<td>Du_NERCSLIP_task5_4</td>
<td>Du2022a</td>
<td>60.0 (58.5 - 61.5)</td>
<td>12091947</td>
<td>1 hour, TeslaV100-32GB</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2022/technical_reports_task5.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Aaquila2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Aaquila2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        BIOACOUSTIC FEW SHOT LEARNING WITH CLASS AUGMENTATION Technical Report
       </h4>
<p style="text-align:left">
        Mariajohn, Aaquila
       </p>
<p style="text-align:left">
<em>
</em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mariajohn_DSPC_task5_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Aaquila2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Aaquila2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Aaquila2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Mariajohn_104_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Aaquila2022" class="panel-collapse collapse" id="collapse-Aaquila2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       BIOACOUSTIC FEW SHOT LEARNING WITH CLASS AUGMENTATION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Mariajohn, Aaquila
       </small>
<br/>
<small>
<em>
</em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This document details the results and techniques used for the submission for the DCASE 2022 Task 5 challenge. The goal is to identify positive shots of the required sample throughout the audio clip using few-shot learning. Prototypical networks are used for the few-shot learning training and inference models. The lack of data was compensated with augmentations.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time shifting, segment level mirroring
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         directly as additional training data
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Aaquila2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Mariajohn_104_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Aaquila2022label" class="modal fade" id="bibtex-Aaquila2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAaquila2022label">
        BIOACOUSTIC FEW SHOT LEARNING WITH CLASS AUGMENTATION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Aaquila2022,
    Author = "Mariajohn, Aaquila",
    title = "BIOACOUSTIC FEW SHOT LEARNING WITH CLASS AUGMENTATION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This document details the results and techniques used for the submission for the DCASE 2022 Task 5 challenge. The goal is to identify positive shots of the required sample throughout the audio clip using few-shot learning. Prototypical networks are used for the few-shot learning training and inference models. The lack of data was compensated with augmentations."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Du2022a" style="box-shadow: none">
<div class="panel-heading" id="heading-Du2022a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT EMBEDDING LEARNING AND EVENT FILTERING FOR BIOACOUSTIC EVENT DETECTION Technical Report
       </h4>
<p style="text-align:left">
        Tang,Jigang and Xueyang,Zhang and Gao,Tian and Liu,Diyuan and Fang,Xin and Pan,Jia and Wang,Qing and Du,Jan and Xu,Kele and Pan,Qinghua
       </p>
<p style="text-align:left">
<em>
         iFLYTEK Research Institute
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> Du_NERCSLIP_task5_1</span> <span class="label label-primary"> Du_NERCSLIP_task5_2</span> <span class="label label-primary"> Du_NERCSLIP_task5_3</span> <span class="label label-primary"> Du_NERCSLIP_task5_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
<span class="label label-success">
         Judgesâ€™ award
        </span>
</p>
<button aria-controls="collapse-Du2022a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Du2022a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Du2022a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Du_122_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Du2022a" class="panel-collapse collapse" id="collapse-Du2022a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT EMBEDDING LEARNING AND EVENT FILTERING FOR BIOACOUSTIC EVENT DETECTION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Tang,Jigang and Xueyang,Zhang and Gao,Tian and Liu,Diyuan and Fang,Xin and Pan,Jia and Wang,Qing and Du,Jan and Xu,Kele and Pan,Qinghua
       </small>
<br/>
<small>
<em>
         iFLYTEK Research Institute
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE2022 Task5: few-shot bioacoustic event detection.We propose several methods to improve the representational ability of embedding under limited positive samples. Including the segmentlevel and frame-level embedding learning strategy, model adaptation technology and embedding-guided event filtering approach. The event filtering task is independently trained on each test file to improve the discrimination of embeddings between similar events. The proposed system is evaluated on the official validation set, and the best overall F-measure score is 74.4%.
      </p>
<p>
<strong>
        Awards:
       </strong>
       Judgesâ€™ award
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Du2022a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Du_122_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Du2022alabel" class="modal fade" id="bibtex-Du2022a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDu2022alabel">
        FEW-SHOT EMBEDDING LEARNING AND EVENT FILTERING FOR BIOACOUSTIC EVENT DETECTION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Du2022a,
    Author = "Tang, Jigang and Xueyang, Zhang and Gao, Tian and Liu, Diyuan and Fang, Xin and Pan, Jia and Wang, Qing and Du, Jan and Xu, Kele and Pan, Qinghua",
    title = "FEW-SHOT EMBEDDING LEARNING AND EVENT FILTERING FOR BIOACOUSTIC EVENT DETECTION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE2022 Task5: few-shot bioacoustic event detection.We propose several methods to improve the representational ability of embedding under limited positive samples. Including the segmentlevel and frame-level embedding learning strategy, model adaptation technology and embedding-guided event filtering approach. The event filtering task is independently trained on each test file to improve the discrimination of embeddings between similar events. The proposed system is evaluated on the official validation set, and the best overall F-measure score is 74.4\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hertkorn2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Hertkorn2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT BIOACOUSTIC EVENT DETECTION : DON ' T WASTE INFORMATION Technical Report
       </h4>
<p style="text-align:left">
        Hertkorn, Michael
       </p>
<p style="text-align:left">
<em>
         ZF Friedrichshafen AG
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hertkorn_ZF_task5_1</span> <span class="label label-primary">Hertkorn_ZF_task5_2</span> <span class="label label-primary">Hertkorn_ZF_task5_3</span> <span class="label label-primary">Hertkorn_ZF_task5_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hertkorn2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hertkorn2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hertkorn2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Hertkorn_28_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hertkorn2022" class="panel-collapse collapse" id="collapse-Hertkorn2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT BIOACOUSTIC EVENT DETECTION : DON ' T WASTE INFORMATION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Hertkorn, Michael
       </small>
<br/>
<small>
<em>
         ZF Friedrichshafen AG
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In the past a lot of attention has been dedicated into finding a good neural network architecture, mainly adopting large NN architectures found in image processing.[1] The parameters in the fixed preprocess, which usually consists of a short-time Fourier transform (STFT) and optionally adding a Mel or Mel frequency cepstral coefficient (MFCC) transformation, can be made trainable[2], however some major parameters stay fixed, like the window size and the fact that the absolute of the complex output of the Fourier transformation is calculated. Also, a learnable frontend is not desirable for a few-shot training setting. This investigation shall demonstrate the importance of choosing suitable parameters for the acoustic preprocess. In order to do this, a standard CNN with a minor tweak is used and the pretraining with training data has been skipped which means that the model is only trained on the 5 shots provided in the validation and evaluation datasets, similar to the pattern matching baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hertkorn2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Hertkorn_28_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hertkorn2022label" class="modal fade" id="bibtex-Hertkorn2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHertkorn2022label">
        FEW-SHOT BIOACOUSTIC EVENT DETECTION : DON ' T WASTE INFORMATION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hertkorn2022,
    Author = "Hertkorn, Michael",
    title = "FEW-SHOT BIOACOUSTIC EVENT DETECTION : DON ' T WASTE INFORMATION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In the past a lot of attention has been dedicated into finding a good neural network architecture, mainly adopting large NN architectures found in image processing.[1] The parameters in the fixed preprocess, which usually consists of a short-time Fourier transform (STFT) and optionally adding a Mel or Mel frequency cepstral coefficient (MFCC) transformation, can be made trainable[2], however some major parameters stay fixed, like the window size and the fact that the absolute of the complex output of the Fourier transformation is calculated. Also, a learnable frontend is not desirable for a few-shot training setting. This investigation shall demonstrate the importance of choosing suitable parameters for the acoustic preprocess. In order to do this, a standard CNN with a minor tweak is used and the pretraining with training data has been skipped which means that the model is only trained on the 5 shots provided in the validation and evaluation datasets, similar to the pattern matching baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT BIO-ACOUSTIC EVENT DETECTION BASED ON TRANSDUCTIVE LEARNING AND ADAPTED CENTRAL DIFFERENCE CONVOLUTION Technical Report
       </h4>
<p style="text-align:left">
        Huang, Qisheng and Li, Yanxiong and Cao, Wenchang and Chen, Hao
       </p>
<p style="text-align:left">
<em>
         School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> Huang_SCUT_task5_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Huang_59_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2022" class="panel-collapse collapse" id="collapse-Huang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT BIO-ACOUSTIC EVENT DETECTION BASED ON TRANSDUCTIVE LEARNING AND ADAPTED CENTRAL DIFFERENCE CONVOLUTION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Huang, Qisheng and Li, Yanxiong and Cao, Wenchang and Chen, Hao
       </small>
<br/>
<small>
<em>
         School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our submitted system for DCASE2022 Task5: few-shot bio-acoustic event detection. Our system employs the transductive learning strategy, data augmentation and an adapted version of central difference convolution (CDC). Evaluated on the validation set, our method achieves the overall F-measure score of 41.1%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Huang_59_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2022label" class="modal fade" id="bibtex-Huang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2022label">
        FEW-SHOT BIO-ACOUSTIC EVENT DETECTION BASED ON TRANSDUCTIVE LEARNING AND ADAPTED CENTRAL DIFFERENCE CONVOLUTION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang2022,
    Author = "Huang, Qisheng and Li, Yanxiong and Cao, Wenchang and Chen, Hao",
    title = "FEW-SHOT BIO-ACOUSTIC EVENT DETECTION BASED ON TRANSDUCTIVE LEARNING AND ADAPTED CENTRAL DIFFERENCE CONVOLUTION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we present our submitted system for DCASE2022 Task5: few-shot bio-acoustic event detection. Our system employs the transductive learning strategy, data augmentation and an adapted version of central difference convolution (CDC). Evaluated on the validation set, our method achieves the overall F-measure score of 41.1\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kang2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Kang2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT BIOACOUSTIC EVENT DETECTION USING GOOD EMBEDDING MODEL Technical report
       </h4>
<p style="text-align:left">
        Kang, Taein
       </p>
<p style="text-align:left">
<em>
         Chung-Ang University, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kang_ET_task5_1</span> <span class="label label-primary">Kang_ET_task5_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kang2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kang_10_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kang2022" class="panel-collapse collapse" id="collapse-Kang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT BIOACOUSTIC EVENT DETECTION USING GOOD EMBEDDING MODEL Technical report
      </h4>
<p style="text-align:left">
<small>
        Kang, Taein
       </small>
<br/>
<small>
<em>
         Chung-Ang University, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Few-shot learning is widely used as benchmarks for meta-learning. Few-shot learning is a learning algorithm that attempts to show how quickly it adapts to test tasks with limited data. Unlike general image new-shot learning, DCASE 2022 Task 5 [1] examines whether it can detect the corresponding sound at the back of audio data when five annotations are given in audio data. In this paper, we would like to demonstrate whether an embedding model well-learned bioacoustic information can perform few-shot learning well even with a simple classifier.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Specaugment, inference-time augmentation
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         5
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         AudioSet
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kang_10_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kang2022label" class="modal fade" id="bibtex-Kang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKang2022label">
        FEW-SHOT BIOACOUSTIC EVENT DETECTION USING GOOD EMBEDDING MODEL Technical report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kang2022,
    Author = "Kang, Taein",
    title = "FEW-SHOT BIOACOUSTIC EVENT DETECTION USING GOOD EMBEDDING MODEL Technical report",
    institution = "DCASE2021 Challenge",
    year = "2022",
    month = "June",
    abstract = "Few-shot learning is widely used as benchmarks for meta-learning. Few-shot learning is a learning algorithm that attempts to show how quickly it adapts to test tasks with limited data. Unlike general image new-shot learning, DCASE 2022 Task 5 [1] examines whether it can detect the corresponding sound at the back of audio data when five annotations are given in audio data. In this paper, we would like to demonstrate whether an embedding model well-learned bioacoustic information can perform few-shot learning well even with a simple classifier."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT BIOACOUSTIC EVENT DETECTION USING PROTOTYPICAL NETWORKS WITH RESNET CLASSIFIER Technical Report
       </h4>
<p style="text-align:left">
        Li, Ren and Liang, Jinhua and Phan, Huy
       </p>
<p style="text-align:left">
<em>
         Queen Mary University of London, United Kingdom
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_QMUL_task5_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_90_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022" class="panel-collapse collapse" id="collapse-Li2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT BIOACOUSTIC EVENT DETECTION USING PROTOTYPICAL NETWORKS WITH RESNET CLASSIFIER Technical Report
      </h4>
<p style="text-align:left">
<small>
        Li, Ren and Liang, Jinhua and Phan, Huy
       </small>
<br/>
<small>
<em>
         Queen Mary University of London, United Kingdom
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for the few-shot bioacoustic event detection in the DCASE2022 task5. Participants are expected to develop a few-shot learning system for detecting mammal and birds sounds from audio recordings. In our system, Prototypical Networks are used to embed spectrograms into an embedding space and learn a non-linear mapping between data samples. We leverage various data augmentation techniques on Mel-spectrograms and introduce a ResNet variant as the classifier. Our experiments demonstrate that the system can achieve the F1-score of 47.88% on the vali-dation data.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time warping, time masking, frequency masking
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_90_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022label" class="modal fade" id="bibtex-Li2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022label">
        FEW-SHOT BIOACOUSTIC EVENT DETECTION USING PROTOTYPICAL NETWORKS WITH RESNET CLASSIFIER Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2022,
    Author = "Li, Ren and Liang, Jinhua and Phan, Huy",
    title = "FEW-SHOT BIOACOUSTIC EVENT DETECTION USING PROTOTYPICAL NETWORKS WITH RESNET CLASSIFIER Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our submission system for the few-shot bioacoustic event detection in the DCASE2022 task5. Participants are expected to develop a few-shot learning system for detecting mammal and birds sounds from audio recordings. In our system, Prototypical Networks are used to embed spectrograms into an embedding space and learn a non-linear mapping between data samples. We leverage various data augmentation techniques on Mel-spectrograms and introduce a ResNet variant as the classifier. Our experiments demonstrate that the system can achieve the F1-score of 47.88\% on the vali-dation data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        BIT SRCB TEAM ' S SUBMISSION FOR DCASE2022 TASK5 - FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical Report
       </h4>
<p style="text-align:left">
        Liu, Miao and Zhang, Jianqian and Wang, Lizhong and Peng, Jiawei and Hu, Chenguang and Li, Kaige and Wang, Jing and Ma, Qiuyue
       </p>
<p style="text-align:left">
<em>
         Beijing Institute of Technology, Beijing, China,Samsung Research China-Beijing (SRC-B), Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_BIT-SRCB_task5_1</span> <span class="label label-primary">Liu_BIT-SRCB_task5_2</span> <span class="label label-primary">Liu_BIT-SRCB_task5_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Liu_43_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2022" class="panel-collapse collapse" id="collapse-Liu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       BIT SRCB TEAM ' S SUBMISSION FOR DCASE2022 TASK5 - FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Liu, Miao and Zhang, Jianqian and Wang, Lizhong and Peng, Jiawei and Hu, Chenguang and Li, Kaige and Wang, Jing and Ma, Qiuyue
       </small>
<br/>
<small>
<em>
         Beijing Institute of Technology, Beijing, China,Samsung Research China-Beijing (SRC-B), Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our system for the task 5 of Detection and Classification of Acoustic Scenes and Events 2022 (DCASE2022) challenge, i.e. few-shot bioacoustic event detection. First, per-channel energy normalization (PCEN) is extracted as features. In order to improve the diversity of original audio, some data augmentation methods are adopted, for example, specaugment. Then, the prototypical network with convolutional neural networks (CNN) and the transductive inference method are used for few-shot detection in our systems. Finally, we use aforementioned features as inputs to train our CNN model. Moreover, we merge the prediction results of improved prototypical network and transductive inference method for better performance. We evaluate the proposed systems with overall F-measure for the whole of the evaluation set, and our best F-measure score on the validation set is 64.77%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Liu_43_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2022label" class="modal fade" id="bibtex-Liu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2022label">
        BIT SRCB TEAM ' S SUBMISSION FOR DCASE2022 TASK5 - FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2022,
    Author = "Liu, Miao and Zhang, Jianqian and Wang, Lizhong and Peng, Jiawei and Hu, Chenguang and Li, Kaige and Wang, Jing and Ma, Qiuyue",
    title = "BIT SRCB TEAM ' S SUBMISSION FOR DCASE2022 TASK5 - FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we present our system for the task 5 of Detection and Classification of Acoustic Scenes and Events 2022 (DCASE2022) challenge, i.e. few-shot bioacoustic event detection. First, per-channel energy normalization (PCEN) is extracted as features. In order to improve the diversity of original audio, some data augmentation methods are adopted, for example, specaugment. Then, the prototypical network with convolutional neural networks (CNN) and the transductive inference method are used for few-shot detection in our systems. Finally, we use aforementioned features as inputs to train our CNN model. Moreover, we merge the prediction results of improved prototypical network and transductive inference method for better performance. We evaluate the proposed systems with overall F-measure for the whole of the evaluation set, and our best F-measure score on the validation set is 64.77\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2022a" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2022a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SURREY SYSTEM FOR DCASE 2022 TASK 5 : FEW-SHOT BIOACOUSTIC EVENT DETECTION WITH SEGMENT-LEVEL METRIC LEARNING Technical Report
       </h4>
<p style="text-align:left">
        Liu, Haohe and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D
       </p>
<p style="text-align:left">
<em>
         University of Surrey
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> Liu_Surrey_task5_1</span> <span class="label label-primary"> Liu_Surrey_task5_2</span> <span class="label label-primary"> Liu_Surrey_task5_3</span> <span class="label label-primary"> Liu_Surrey_task5_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2022a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2022a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2022a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Haohe_85_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Liu2022a').collapse('show');window.location.hash='#Liu2022a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2022a" class="panel-collapse collapse" id="collapse-Liu2022a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SURREY SYSTEM FOR DCASE 2022 TASK 5 : FEW-SHOT BIOACOUSTIC EVENT DETECTION WITH SEGMENT-LEVEL METRIC LEARNING Technical Report
      </h4>
<p style="text-align:left">
<small>
        Liu, Haohe and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D
       </small>
<br/>
<small>
<em>
         University of Surrey
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Few-shot audio event detection is a task that detects the occurrence time of a novel sound class given a few examples. In this work, we propose a system based on segment-level metric learning for DCASE 2022 challenge few-shot bioacoustic event detection (task 5). We make better utilization of the negative data within each sound class to build the loss function, and use transductive inference to gain better adaptation on the evaluation set. For the input feature, we find the per-channel energy normalization concatenated with delta melfrequency cepstral coefficients to be the most effective combination. We also introduce new data augmentation and post-processing procedures for this task. Our final system achieves an f-measure of 68.74 on the DCASE task 5 validation set, outperforming the baseline performance of 29.5 by a large margin. Our system is fully open-sourced1
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2022a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Haohe_85_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/haoheliu/DCASE_2022_Task_5" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2022alabel" class="modal fade" id="bibtex-Liu2022a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2022alabel">
        SURREY SYSTEM FOR DCASE 2022 TASK 5 : FEW-SHOT BIOACOUSTIC EVENT DETECTION WITH SEGMENT-LEVEL METRIC LEARNING Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2022a,
    Author = "Liu, Haohe and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D",
    title = "SURREY SYSTEM FOR DCASE 2022 TASK 5 : FEW-SHOT BIOACOUSTIC EVENT DETECTION WITH SEGMENT-LEVEL METRIC LEARNING Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "Few-shot audio event detection is a task that detects the occurrence time of a novel sound class given a few examples. In this work, we propose a system based on segment-level metric learning for DCASE 2022 challenge few-shot bioacoustic event detection (task 5). We make better utilization of the negative data within each sound class to build the loss function, and use transductive inference to gain better adaptation on the evaluation set. For the input feature, we find the per-channel energy normalization concatenated with delta melfrequency cepstral coefficients to be the most effective combination. We also introduce new data augmentation and post-processing procedures for this task. Our final system achieves an f-measure of 68.74 on the DCASE task 5 validation set, outperforming the baseline performance of 29.5 by a large margin. Our system is fully open-sourced1"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Martinsson2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Martinsson2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT BIOACOUSTIC EVENT DETECTION USING A PROTOTYPICAL NETWORK ENSEMBLE WITH ADAPTIVE EMBEDDING FUNCTIONS Technical Report
       </h4>
<p style="text-align:left">
        Martinsson, John and Willbo, Martin and Pirinen, Aleksis and Mogren, Olof and Sandsten, Maria
       </p>
<p style="text-align:left">
<em>
         Computer Science, RISE Research Institutes of Sweden, Sweden, Centre for Mathematical Sciences, Lund University, Sweden
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> Martinsson_RISE_task5_1</span> <span class="label label-primary"> Martinsson_RISE_task5_2</span> <span class="label label-primary"> Martinsson_RISE_task5_3</span> <span class="label label-primary"> Martinsson_RISE_task5_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Martinsson2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Martinsson2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Martinsson2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Martinsson_78_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Martinsson2022" class="panel-collapse collapse" id="collapse-Martinsson2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT BIOACOUSTIC EVENT DETECTION USING A PROTOTYPICAL NETWORK ENSEMBLE WITH ADAPTIVE EMBEDDING FUNCTIONS Technical Report
      </h4>
<p style="text-align:left">
<small>
        Martinsson, John and Willbo, Martin and Pirinen, Aleksis and Mogren, Olof and Sandsten, Maria
       </small>
<br/>
<small>
<em>
         Computer Science, RISE Research Institutes of Sweden, Sweden, Centre for Mathematical Sciences, Lund University, Sweden
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report we present our method for the DCASE 2022 challenge on few-shot bioacoustic event detection. We use an ensemble of prototypical neural networks with adaptive embedding functions and show that both ensemble and adaptive embedding functions can be used to improve results from an average F-score of 41.3% to an average F-score of 60.0% on the validation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Martinsson2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Martinsson_78_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Martinsson2022label" class="modal fade" id="bibtex-Martinsson2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMartinsson2022label">
        FEW-SHOT BIOACOUSTIC EVENT DETECTION USING A PROTOTYPICAL NETWORK ENSEMBLE WITH ADAPTIVE EMBEDDING FUNCTIONS Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Martinsson2022,
    Author = "Martinsson, John and Willbo, Martin and Pirinen, Aleksis and Mogren, Olof and Sandsten, Maria",
    title = "FEW-SHOT BIOACOUSTIC EVENT DETECTION USING A PROTOTYPICAL NETWORK ENSEMBLE WITH ADAPTIVE EMBEDDING FUNCTIONS Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this report we present our method for the DCASE 2022 challenge on few-shot bioacoustic event detection. We use an ensemble of prototypical neural networks with adaptive embedding functions and show that both ensemble and adaptive embedding functions can be used to improve results from an average F-score of 41.3\% to an average F-score of 60.0\% on the validation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tan2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Tan2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A NEW TRANSDUCTIVE FRAMEWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION TASK Technical Report
       </h4>
<p style="text-align:left">
        Tan, Yizhou and Xu, Lifan and Zhu, Chenyang and Li, Shengchen and Ai, Haojun and Shao, Xi
       </p>
<p style="text-align:left">
<em>
         Wuhan University, School of Cyber Science and Engineering, Wuhan, China,Xiâ€™an Jiaotong-Liverpool University, Department of Intelligent Science School of Advanced Engineering, Suzhou, China,Jiangnan University, School of Artificial Intelligence and Computer Science,Wuxi, China, Nanjing University of Posts and Telecommunications,School of Communication and Information Engineering, Nanjing, China,
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Tan_WHU_task5_1</span> <span class="label label-primary">Tan_WHU_task5_2</span> <span class="label label-primary">Tan_WHU_task5_3</span> <span class="label label-primary">Tan_WHU_task5_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Tan2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tan2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tan2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Tan_39_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tan2022" class="panel-collapse collapse" id="collapse-Tan2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A NEW TRANSDUCTIVE FRAMEWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION TASK Technical Report
      </h4>
<p style="text-align:left">
<small>
        Tan, Yizhou and Xu, Lifan and Zhu, Chenyang and Li, Shengchen and Ai, Haojun and Shao, Xi
       </small>
<br/>
<small>
<em>
         Wuhan University, School of Cyber Science and Engineering, Wuhan, China,Xiâ€™an Jiaotong-Liverpool University, Department of Intelligent Science School of Advanced Engineering, Suzhou, China,Jiangnan University, School of Artificial Intelligence and Computer Science,Wuxi, China, Nanjing University of Posts and Telecommunications,School of Communication and Information Engineering, Nanjing, China,
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Few-shot learning is introduced to reduce the requirements of data availability in machine learning, especially when the labelling is labour expensive. Few-shot learning algorithms usually suffer from the extraordinary feature distribution of the query class, especially in few-shot bioacoustic event detection task. In this work, Knowledge transfer technique is introduced into the transductive inference process to restrict the feature distribution of newly appeared class to a dedicated sub-space, while adapts the feature distribution for existing classes. The proposed system outperforms the traditional few-shot learning system according to the development dataset provided by bioacoustics event detection (Task 5) in DCASE data challenge 2022. The f-measure score of the validation in development dataset successfully reaches 57.40.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tan2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Tan_39_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tan2022label" class="modal fade" id="bibtex-Tan2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTan2022label">
        A NEW TRANSDUCTIVE FRAMEWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION TASK Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Tan2022,
    Author = "Tan, Yizhou and Xu, Lifan and Zhu, Chenyang and Li, Shengchen and Ai, Haojun and Shao, Xi",
    title = "A NEW TRANSDUCTIVE FRAMEWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION TASK Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "Few-shot learning is introduced to reduce the requirements of data availability in machine learning, especially when the labelling is labour expensive. Few-shot learning algorithms usually suffer from the extraordinary feature distribution of the query class, especially in few-shot bioacoustic event detection task. In this work, Knowledge transfer technique is introduced into the transductive inference process to restrict the feature distribution of newly appeared class to a dedicated sub-space, while adapts the feature distribution for existing classes. The proposed system outperforms the traditional few-shot learning system according to the development dataset provided by bioacoustics event detection (Task 5) in DCASE data challenge 2022. The f-measure score of the validation in development dataset successfully reaches 57.40."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Willbo2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Willbo2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        WIDE RESNET MODELS FOR FEW-SHOT SOUND EVENT DETECTION Technical report
       </h4>
<p style="text-align:left">
        Willbo, Martin and Martinsson, John and Pirinen, Aleksis and Mogren, Olof
       </p>
<p style="text-align:left">
<em>
         Computer Science, RISE Research Institutes of Sweden, Sweden, Centre for Mathematical Sciences, Lund University, Sweden
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> Willbo_RISE_task5_1</span> <span class="label label-primary"> Willbo_RISE_task5_2</span> <span class="label label-primary"> Willbo_RISE_task5_3</span> <span class="label label-primary"> Willbo_RISE_task5_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Willbo2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Willbo2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Willbo2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Willbo_53_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Willbo2022" class="panel-collapse collapse" id="collapse-Willbo2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       WIDE RESNET MODELS FOR FEW-SHOT SOUND EVENT DETECTION Technical report
      </h4>
<p style="text-align:left">
<small>
        Willbo, Martin and Martinsson, John and Pirinen, Aleksis and Mogren, Olof
       </small>
<br/>
<small>
<em>
         Computer Science, RISE Research Institutes of Sweden, Sweden, Centre for Mathematical Sciences, Lund University, Sweden
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report we describe our few-shot sound event detection (SED) systems used to generate predictions for the DCASE 2022 task 5 challenge. At the core of the SED systems is a wider variant of ResNet-18, i.e., each block throughout the depth of the network have more convolutional filters. In addition to this, for one of the submissions we include what we believe to be a novel approach to semi-supervised learning for prototypical networks. For both the fully supervised and semi-supervised methods we showcase the importance of calibrating the probability thresholds in the few-shot learning tasks, and provide a simple implementation of how to find these.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Willbo2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Willbo_53_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Willbo2022label" class="modal fade" id="bibtex-Willbo2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWillbo2022label">
        WIDE RESNET MODELS FOR FEW-SHOT SOUND EVENT DETECTION Technical report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Willbo2022,
    Author = "Willbo, Martin and Martinsson, John and Pirinen, Aleksis and Mogren, Olof",
    title = "WIDE RESNET MODELS FOR FEW-SHOT SOUND EVENT DETECTION Technical report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report we describe our few-shot sound event detection (SED) systems used to generate predictions for the DCASE 2022 task 5 challenge. At the core of the SED systems is a wider variant of ResNet-18, i.e., each block throughout the depth of the network have more convolutional filters. In addition to this, for one of the submissions we include what we believe to be a novel approach to semi-supervised learning for prototypical networks. For both the fully supervised and semi-supervised methods we showcase the importance of calibrating the probability thresholds in the few-shot learning tasks, and provide a simple implementation of how to find these."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wu2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Wu2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FEW-SHOT CONTINUAL LEARNING FOR BIOACOUSTIC EVENT DETECTION Technical Report
       </h4>
<p style="text-align:left">
        Wu, Xiaoxiao and Long, Yanhua
       </p>
<p style="text-align:left">
<em>
         Shanghai Normal University, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> Wu_SHNU_task5_1</span> <span class="label&lt;span class=" clearfix"=""></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wu2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Wu_4_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wu2022" class="panel-collapse collapse" id="collapse-Wu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FEW-SHOT CONTINUAL LEARNING FOR BIOACOUSTIC EVENT DETECTION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Wu, Xiaoxiao and Long, Yanhua
       </small>
<br/>
<small>
<em>
         Shanghai Normal University, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE2022 Task5: few-shot bioacoustic event detection. In this submission, a few-shot continual learning framework is used for our bioacoustic event detection, where we can continuously expand a trained base classifier to detect novel classes with only few labeled data at inference time. On the official validation set, the proposed continual learning achieves the overall F-measure score of 53.876%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Time masking,Frequency masking
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Wu_4_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wu2022label" class="modal fade" id="bibtex-Wu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWu2022label">
        FEW-SHOT CONTINUAL LEARNING FOR BIOACOUSTIC EVENT DETECTION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wu2022,
    Author = "Wu, Xiaoxiao and Long, Yanhua",
    title = "FEW-SHOT CONTINUAL LEARNING FOR BIOACOUSTIC EVENT DETECTION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE2022 Task5: few-shot bioacoustic event detection. In this submission, a few-shot continual learning framework is used for our bioacoustic event detection, where we can continuously expand a trained base classifier to detect novel classes with only few labeled data at inference time. On the official validation set, the proposed continual learning achieves the overall F-measure score of 53.876\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yang2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Yang2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        IMPROVED PROTOTYPICAL NETWORK WITH DATA AUGMENTATION Technical Report
       </h4>
<p style="text-align:left">
        Dongchao Yang and Helin Wang and Zhongjie Ye and Yuexian Zou
       </p>
<p style="text-align:left">
<em>
         Peking University, Shcool of ECE, Shenzhen,China, Xiaomi Corporation, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zou_PKU_task5_1</span> <span class="label label-primary">Zou_PKU_task5_2</span> <span class="label label-primary">Zou_PKU_task5_3</span> <span class="label label-primary">Zou_PKU_task5_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yang2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zou_36_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yang2022" class="panel-collapse collapse" id="collapse-Yang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       IMPROVED PROTOTYPICAL NETWORK WITH DATA AUGMENTATION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Dongchao Yang and Helin Wang and Zhongjie Ye and Yuexian Zou
       </small>
<br/>
<small>
<em>
         Peking University, Shcool of ECE, Shenzhen,China, Xiaomi Corporation, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our few-shot bioacoustic event detection methods submitted to Detection and Classification of Acoustic Scenes and Events Challenge 2022 Task 5. We follow our previous work, and further improve our model through data augmentation strategy. Specifically, we analyze the reason why Prototypical networks cannot perform well, and propose to use transductive inference for few shot learning. Our method maximizes the mutual information between the query features and their label predictions for a given few-shot task, in conjunction with a supervision loss based on the support set. Furthermore, we use multiple data augmentation strategies to improve the feature extractor, including time and frequency masking, mixup, and so on. Experimental results indicate our model gets better performance than baseline, and F1 score is about 51.9% on evaluation set
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zou_36_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yang2022label" class="modal fade" id="bibtex-Yang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYang2022label">
        IMPROVED PROTOTYPICAL NETWORK WITH DATA AUGMENTATION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yang2022,
    Author = "Yang, Dongchao and Zou, Yuexian and Cui, Fan and Wang, Yujun",
    title = "IMPROVED PROTOTYPICAL NETWORK WITH DATA AUGMENTATION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our few-shot bioacoustic event detection methods submitted to Detection and Classification of Acoustic Scenes and Events Challenge 2022 Task 5. We follow our previous work, and further improve our model through data augmentation strategy. Specifically, we analyze the reason why Prototypical networks cannot perform well, and propose to use transductive inference for few shot learning. Our method maximizes the mutual information between the query features and their label predictions for a given few-shot task, in conjunction with a supervision loss based on the support set. Furthermore, we use multiple data augmentation strategies to improve the feature extractor, including time and frequency masking, mixup, and so on. Experimental results indicate our model gets better performance than baseline, and F1 score is about 51.9\% on evaluation set"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zgorzynski" style="box-shadow: none">
<div class="panel-heading" id="heading-Zgorzynski" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SIAMESE NETWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical report
       </h4>
<p style="text-align:left">
        Zgorzynski, Bartlomiej and Matuszewski, Mateusz
       </p>
<p style="text-align:left">
<em>
         Samsung R&amp;D Institute, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary"> ZGORZYNSKI_SRPOL_task5_1</span> <span class="label label-primary"> ZGORZYNSKI_SRPOL_task5_2</span> <span class="label label-primary"> ZGORZYNSKI_SRPOL_task5_3</span> <span class="label label-primary"> ZGORZYNSKI_SRPOL_task5_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zgorzynski" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zgorzynski" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zgorzynski" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zgorzynski_55_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zgorzynski" class="panel-collapse collapse" id="collapse-Zgorzynski" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SIAMESE NETWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical report
      </h4>
<p style="text-align:left">
<small>
        Zgorzynski, Bartlomiej and Matuszewski, Mateusz
       </small>
<br/>
<small>
<em>
         Samsung R&amp;D Institute, Poland
        </em>
</small>
</p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zgorzynski" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zgorzynski_55_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zgorzynskilabel" class="modal fade" id="bibtex-Zgorzynski" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZgorzynskilabel">
        SIAMESE NETWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zgorzynski,
    Author = "Zgorzynski, Bartlomiej and Matuszewski, Mateusz",
    title = "SIAMESE NETWORK FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION Technical report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = ""
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhang2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhang2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A META-LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION Technical Report
       </h4>
<p style="text-align:left">
        Zhang, Tianyang and Wang, Yuyang and Wang, Ying
       </p>
<p style="text-align:left">
<em>
         Chongqing University, Shapingba
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_CQU_task5_1</span> <span class="label label-primary">Zhang_CQU_task5_2</span> <span class="label label-primary">Zhang_CQU_task5_3</span><span class="label label-primary">Zhang_CQU_task5_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhang2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zhang_6_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhang2022" class="panel-collapse collapse" id="collapse-Zhang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A META-LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION Technical Report
      </h4>
<p style="text-align:left">
<small>
        Zhang, Tianyang and Wang, Yuyang and Wang, Ying
       </small>
<br/>
<small>
<em>
         Chongqing University, Shapingba
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The report presents our submission to Detection and Classification of Acoustic Scenes and Events challenges 2022 (DCASE2022) task 5. This task focuses on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations. Main issue of this task is that only five exemplar vocalisations (shots) of mammals or birds are available. In this paper, we propose a metalearning framework for few-shot bioacoustic event detection challenge. Maximizing inter-class distance and minimizing intra-class distance (MIMI) are used as a criteria to fine-tune embedded network for few-shot tasks. Experimental results indicate our framework get better performance than baseline, and F1 score is about 46.51% on evaluation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         System embeddings
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         Subsystem count
        </td>
<td>
         False
        </td>
</tr>
<tr>
<td class="col-md-3">
         External data usage
        </td>
<td>
         False
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zhang_6_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhang2022label" class="modal fade" id="bibtex-Zhang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhang2022label">
        A META-LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhang2022,
    Author = "Zhang, Tianyang and Wang, Yuyang and Wang, Ying",
    title = "A META-LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "The report presents our submission to Detection and Classification of Acoustic Scenes and Events challenges 2022 (DCASE2022) task 5. This task focuses on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations. Main issue of this task is that only five exemplar vocalisations (shots) of mammals or birds are available. In this paper, we propose a metalearning framework for few-shot bioacoustic event detection challenge. Maximizing inter-class distance and minimizing intra-class distance (MIMI) are used as a criteria to fine-tune embedded network for few-shot tasks. Experimental results indicate our framework get better performance than baseline, and F1 score is about 46.51\% on evaluation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>