<!DOCTYPE html><html lang="en">
<head>
    <title>Large-scale weakly labeled semi-supervised sound event detection in domestic environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The task evaluates systems for the large-scale detection of sound events using weakly labeled data. The challenge is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance. Note to participants (September 20th …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2018/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-general-purpose-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-bird-audio-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-monitoring-domestic-activities" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-home text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2018/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2018/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/grid-toledo-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-large-scale fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Large-scale</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2018_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Large-scale weakly labeled semi-supervised sound event detection in domestic environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Romain Serizel" class="img img-circle" src="/images/person/romain_serizel.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Romain Serizel</strong>
<a class="icon" href="mailto:romain.serizel@loria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.loria.fr/en/">
                                University of Lorraine
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Hamid Eghbal-zadeh" class="img img-circle" src="/images/person/hamid_eghbal_zadeh.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Hamid Eghbal-zadeh</strong>
<a class="icon" href="mailto:hamid.eghbal-zadeh@jku.at"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.cp.jku.at/">
                                Johannes Kepler University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Nicolas Turpault" class="img img-circle" src="/images/person/nicolas_turpault.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Nicolas Turpault</strong>
<a class="icon" href="mailto:nicolas.turpault@inria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.inria.fr/">
                                Inria Nancy Grand-Est
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Ankit Parag Shah" class="img img-circle" src="/images/person/ankit_parag_shah.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Ankit Parag Shah</strong>
<a class="icon" href="mailto:aps1@andrew.cmu.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.cmu.edu/">
                                Carnegie Mellon University
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#recording-and-annotation-procedure">Recording and annotation procedure</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
<li><a href="#submission">Submission</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#system-description">System description</a></li>
<li><a href="#script-description">Script description</a></li>
<li><a href="#python-implementation">Python implementation</a></li>
<li><a href="#system-performance">System performance</a></li>
</ul>
</li>
<li><a href="#results">Results</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The task evaluates systems for the large-scale detection of sound events using weakly labeled data. The challenge is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance.</p>
<div class="bg-danger">
<p><strong>Note to participants (September 20th): Minor bug in the evaluation script.</strong></p>
<p>We found a minor bug in the evaluation script. Some of the files were not taken into account correctly. This has been fixed and the evaluation scores have been updated (resulting in at most 0.3% decrease of the F-score). This had no impact on the <a href="task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#teams-ranking">teams ranking</a>. However, the <a href="task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#systems-ranking">systems ranking</a> can have varied very slightly (around ranks 15~25).</p>
</div>
<h1 id="description">Description</h1>
<p>The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong> given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong> together with a small weakly annotated training set to improve system performance. <strong>The labels in the annotated subset are verified and can be considered as reliable.</strong> The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events...) and potential industrial applications.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-6 col-centered">
<img class="img img-responsive" src="../images/tasks/challenge2018/task4_sound_event_detection.png"/>
<figcaption>Figure 1: Overview of a sound event detection system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<h1 id="audio-dataset">Audio dataset</h1>
<p>The task employs a subset of “Audioset: An Ontology And Human-Labeled Dataset For Audio Events” by Google. <a href="https://research.google.com/audioset/">Audioset</a> consists of an expanding ontology of 632 sound event classes and a collection of 2 million human-labeled 10-second sound clips (less than 21% are shorter than 10-seconds) drawn from 2 million Youtube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.
We will focus on a subset of Audioset that consists of 10 classes of sound events:</p>
<ul>
<li>Speech <code>Speech</code></li>
<li>Dog <code>Dog</code></li>
<li>Cat <code>Cat</code></li>
<li>Alarm/bell/ringing <code>Alarm_bell_ringing</code></li>
<li>Dishes <code>Dishes</code></li>
<li>Frying <code>Frying</code></li>
<li>Blender <code>Blender</code></li>
<li>Running water <code>Running_water</code></li>
<li>Vacuum cleaner <code>Vacuum_cleaner</code></li>
<li>Electric shaver/toothbrush <code>Electric_shaver_toothbrush</code></li>
</ul>
<h2 id="recording-and-annotation-procedure">Recording and annotation procedure</h2>
<p>Audioset provides annotations at clip level (without time boundaries for the events). Therefore, the annotations are considered as weak labels.</p>
<p>Audio clips are collected from Youtube videos uploaded by independent users so the number of clips per class vary dramatically and the dataset is not balanced (see also <a href="https://research.google.com/Audioset//dataset/index.html">https://research.google.com/Audioset//dataset/index.html</a>).</p>
<p>Google researchers conducted a quality assessment task where experts were exposed to 10 randomly selected clips for each class and discovered that a in most of the cases not all the clips contains the event related to the given annotation. More information about the initial annotation process can be found in:</p>
<div class="btex-item" data-item="Gemmeke2017" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Gemmeke2017"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter.
<em>Audio set: an ontology and human-labeled dataset for audio events.</em>
In Proc. IEEE ICASSP 2017. New Orleans, LA, 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexGemmeke20171b7a2d9eaec94813847764940a88f720" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://research.google.com/pubs/archive/45857.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseGemmeke20171b7a2d9eaec94813847764940a88f720" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseGemmeke20171b7a2d9eaec94813847764940a88f720" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingGemmeke20171b7a2d9eaec94813847764940a88f720" class="panel-collapse collapse" id="collapseGemmeke20171b7a2d9eaec94813847764940a88f720" role="tabpanel">
<h4>Audio Set: An ontology and human-labeled dataset for audio events</h4>
<h5>Abstract</h5>
<p class="text-justify">Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets -- principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 635 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexGemmeke20171b7a2d9eaec94813847764940a88f720" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://research.google.com/pubs/archive/45857.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexGemmeke20171b7a2d9eaec94813847764940a88f720label" class="modal fade" id="bibtexGemmeke20171b7a2d9eaec94813847764940a88f720" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexGemmeke20171b7a2d9eaec94813847764940a88f720label">Audio Set: An ontology and human-labeled dataset for audio events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gemmeke2017,
    Author = "Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin",
    title = "Audio Set: An ontology and human-labeled dataset for audio events",
    booktitle = "Proc. IEEE ICASSP 2017",
    year = "2017",
    address = "New Orleans, LA",
    abstract = "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets -- principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 635 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>The weak annotations have been verified manually for a small subset of the training set. The weak annotations are provided in a tab separated csv file under the following format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">class_label (strings)</span><span class="o">]</span>
</code></pre></div>
<p>For example:</p>
<div class="highlight"><pre><span></span><code>Y-BJNMHMZDcU_50.000_60.000.wav  Alarm_bell_ringing;Dog
</code></pre></div>
<p>The first column, <code>Y-BJNMHMZDcU_50.000_60.000.wav</code>, is the name of the audio file downloaded from Youtube (<code>Y-BJNMHMZDcU</code> is Youtube ID of the video from where the 10-second clips was extracted t=50 sec to t=60 sec, correspond to the clip boundaries within the full video) and the last column, <code>Alarm_bell_ringing;Dog</code> corresponds to the sound classes present in the clip separated by a semi-colon.</p>
<p>Another subset of the development has been annotated manually with strong annotations, to be used as the test set (see also below for a detailed explanation about the development set). The minimum length for an event is 250ms. The minimum duration of the pause between two events from the same class is 150ms. When the silence between two consecutive events from the same class was less than 150ms the events have been merged to a single event. The strong annotations are provided in a tab separated csv file under the following format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event onset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event offset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">class_label (strings)</span><span class="o">]</span>
</code></pre></div>
<p>For example:</p>
<div class="highlight"><pre><span></span><code>YOTsn73eqbfc_10.000_20.000.wav  0.163   0.665   Alarm_bell_ringing
</code></pre></div>
<p>The first column, <code>YOTsn73eqbfc_10.000_20.000.wav</code>, is the name of the audio file downloaded from Youtube, the second column <code>0.163</code> is the onset time in seconds, the third column <code>0.665</code> is the offset time in seconds and the last column, <code>Alarm_bell_ringing</code> corresponds to the class of the sound event.</p>
<h2 id="download">Download</h2>
<p>The annotations files and the script to download the audio files is available on the git repository for task 4. <strong>The final dataset is 82Gb, the download/extraction process can take approximately 12 hours.</strong> (In case you are using the provided baseline system, there is no need to download the dataset as the system will automatically download needed dataset for you.)</p>
<p class="bg-danger">
If you experience problems during the download of the dataset please contact the task organizers.
(Nicolas Turpault and Romain Serizel in priority)
</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/dataset" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/dataset" target="_blank">
<span style="font-size:20px;">DCASE2018 Task 4 <strong>development dataset</strong>, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p>The content of the development set is structured in the following manner:</p>
<div class="highlight"><pre><span></span><code><span class="n">dataset</span><span class="w"> </span><span class="n">root</span>
<span class="err">│</span><span class="w">   </span><span class="n">readme</span><span class="o">.</span><span class="n">md</span><span class="w">                         </span><span class="p">(</span><span class="n">instructions</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">downloads</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">audio</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">annotations</span><span class="w"> </span><span class="n">files</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="n">download_data</span><span class="o">.</span><span class="n">py</span><span class="w">                  </span><span class="p">(</span><span class="n">script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">download</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">files</span><span class="p">)</span>
<span class="err">│</span>
<span class="err">└───</span><span class="n">metadata</span><span class="w">                          </span><span class="p">(</span><span class="n">directories</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">annotations</span><span class="w"> </span><span class="n">files</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span>
<span class="err">│</span><span class="w">   </span><span class="err">└───</span><span class="n">train</span><span class="w">                         </span><span class="p">(</span><span class="n">annotations</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="n">sets</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span><span class="w">     </span><span class="n">weak</span><span class="o">.</span><span class="n">csv</span><span class="w">                    </span><span class="p">(</span><span class="n">weakly</span><span class="w"> </span><span class="n">labeled</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">list</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span><span class="w">     </span><span class="n">unlabel_in_domain</span><span class="o">.</span><span class="n">csv</span><span class="w">       </span><span class="p">(</span><span class="n">unlabeled</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">list</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span><span class="w">     </span><span class="n">unlabel_out_of_domain</span><span class="o">.</span><span class="n">csv</span><span class="w">   </span><span class="p">(</span><span class="n">unlabeled</span><span class="w"> </span><span class="n">out</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">domain</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">list</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span>
<span class="err">│</span><span class="w">   </span><span class="err">└───</span><span class="n">test</span><span class="w">                          </span><span class="p">(</span><span class="n">annotations</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="p">)</span>
<span class="err">│</span><span class="w">         </span><span class="n">test</span><span class="o">.</span><span class="n">csv</span><span class="w">                    </span><span class="p">(</span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">strong</span><span class="w"> </span><span class="n">labels</span><span class="p">)</span>
<span class="err">│</span><span class="w">    </span>
<span class="err">└───</span><span class="n">audio</span><span class="w">                             </span><span class="p">(</span><span class="n">directories</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">audio</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">downloaded</span><span class="p">)</span>
<span class="w">    </span><span class="err">└───</span><span class="n">train</span><span class="w">                         </span><span class="p">(</span><span class="n">annotations</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="n">sets</span><span class="p">)</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="err">└───</span><span class="n">weak</span><span class="w">                      </span><span class="p">(</span><span class="n">weakly</span><span class="w"> </span><span class="n">labeled</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">set</span><span class="p">)</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="err">└───</span><span class="n">unlabel_in_domain</span><span class="w">         </span><span class="p">(</span><span class="n">unlabeled</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">set</span><span class="p">)</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="err">└───</span><span class="n">unlabel_out_of_domain</span><span class="w">     </span><span class="p">(</span><span class="n">unlabeled</span><span class="w"> </span><span class="n">out</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">domain</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">set</span><span class="p">)</span>
<span class="w">    </span><span class="err">│</span>
<span class="w">    </span><span class="err">└───</span><span class="n">test</span><span class="w">                          </span><span class="p">(</span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="p">)</span><span class="w">       </span>
</code></pre></div>
<h1 id="task-setup">Task setup</h1>
<p>The challenge consists of detecting sound events within web videos using weakly labeled training data. The detection within a 10-seconds clip should be performed with start and end timestamps.</p>
<h2 id="development-dataset">Development dataset</h2>
<p>The development set is divided into two main partitions: training and validation.</p>
<h3>Training set</h3>
<p>To motivate the participants to come up with innovative solutions, we provide 3 different splits of training data in our training set: Labeled training set, Unlabeled in domain training set and Unlabeled out of domain training set.</p>
<p><strong>Labeled training set</strong>:<br/>
This set contains <strong>1578 clips</strong> (2244 class occurrences) for which weak annotations have been verified and cross-checked. The amount of clips per class is the following:</p>
<table class="table table-striped">
<thead>
<tr>
<th>Class</th>
<th class="col-md-4"># 10s clips containing the event</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speech</td>
<td>550</td>
</tr>
<tr>
<td>Dog</td>
<td>214</td>
</tr>
<tr>
<td>Cat</td>
<td>173</td>
</tr>
<tr>
<td>Alarm/bell/ringing</td>
<td>205</td>
</tr>
<tr>
<td>Dishes</td>
<td>184</td>
</tr>
<tr>
<td>Frying</td>
<td>171</td>
</tr>
<tr>
<td>Blender</td>
<td>134</td>
</tr>
<tr>
<td>Running water</td>
<td>343</td>
</tr>
<tr>
<td>Vacuum cleaner</td>
<td>167</td>
</tr>
<tr>
<td>Electric shaver/toothbrush</td>
<td>103</td>
</tr>
</tbody>
<tfoot>
<tr>
<td><strong>Total</strong></td>
<td><strong>2244</strong></td>
</tr>
</tfoot>
</table>
<p><strong>Unlabeled in domain training set</strong>:<br/>
This set is considerably larger than the previous one. It contains <strong>14412 clips</strong>. The clips are selected such that the distribution per class (based on Audioset annotations) is close to the distribution in the labeled set. Note however that given the uncertainty on Audioset labels this distribution might not be exactly similar.</p>
<p><strong>Unlabeled out of domain training set</strong>:<br/>
This set is composed of <strong>39999 clips</strong> extracted from classes that are not considered in the task. Note that these clips are chosen based on the Audioset labels which are not verified and therefore might be noisy. Additionally, as speech is present in half of the segments of Audioset this set also contain almost 20000 clips with speech. This was the only way to have an unlabel_out_of_domain set which is somehow representative of Audioset. Indeed, discarding speech would have also meant discarding many other classes and the variability of the set would have been penalized.</p>
<h3>Test set</h3>
<p>The test set is designed such that the distribution in term of clips per class is similar to that of the weakly labeled training set. The size of the validation set is such that it represent about 20% of the size of the labeled training set, it contains <strong>288 clips</strong> (906 events). The evaluation set is annotated with strong labels, with timestamps (obtained by human annotators).  Note that a 10-seconds clip may correspond to more than one sound event. The amount of events per class is the following:</p>
<table class="table table-striped">
<thead>
<tr>
<th>Class</th>
<th class="col-md-4"># events</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speech</td>
<td>261</td>
</tr>
<tr>
<td>Dog</td>
<td>127</td>
</tr>
<tr>
<td>Cat</td>
<td>97</td>
</tr>
<tr>
<td>Alarm/bell/ringing</td>
<td>112</td>
</tr>
<tr>
<td>Dishes</td>
<td>122</td>
</tr>
<tr>
<td>Frying</td>
<td>24</td>
</tr>
<tr>
<td>Blender</td>
<td>40</td>
</tr>
<tr>
<td>Running water</td>
<td>76</td>
</tr>
<tr>
<td>Vacuum cleaner</td>
<td>36</td>
</tr>
<tr>
<td>Electric shaver/toothbrush</td>
<td>28</td>
</tr>
</tbody>
<tfoot>
<tr>
<td><strong>Total</strong></td>
<td><strong>906</strong></td>
</tr>
</tfoot>
</table>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>Evaluation set is a subset of Audioset including the classes mentioned above. Labels with timestamps (obtained by human annotations) will be released after the DCASE 2018 challenge is concluded.The eval dataset can be downloaded from the following location:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/dataset/metadata/eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/dataset/metadata/eval" target="_blank">
<span style="font-size:20px;">DCASE2018 Task 4 <strong>evaluation dataset</strong>, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<h2 id="submission">Submission</h2>
<p>Detailed information for the challenge submission can found from in the <a href="submission">submission page</a>.</p>
<p>System output should be presented as a single text-file (in CSV format) containing predictions for each audio file in the evaluation set. Result items can be in any order. Format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event onset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event offset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event label (string)</span><span class="o">]</span>
</code></pre></div>
<p>Multiple system outputs can be submitted (maximum 4 per participant). If submitting multiple systems, the individual text-files should be packaged into a zip file for submission. Please carefully mark the connection between the submitted files and the corresponding system or system parameters (for example by naming the text file appropriately).</p>
<p>If no event is detected for the particular audio signal, the system should still output a row containing only the file name, to indicate that the file was processed. This is used to verify that participants processed all evaluation files.</p>
<h1 id="task-rules">Task rules</h1>
<ul>
<li>Participants <strong>are not allowed</strong> to use external data for system development. Data from other task is considered external data.</li>
<li>Another example of external data is other materials related to the video such as the rest of audio from where the 10-sec clip was extracted, the video frames and metadata.</li>
<li>Participants <strong>are not allowed</strong> to use the embeddings provided by Audioset or any other features that indirectly use external data.</li>
<li><strong>Only weak labels and none of the strong labels (timestamps) or original (Audioset) labels can be used for the training of the submitted system.</strong></li>
<li>Manipulation of provided training data <strong>is allowed</strong>.</li>
<li>The development dataset can be augmented without the use of external data (e.g. by mixing data sampled from a PDF or using techniques such as pitch shifting or time stretching).</li>
<li>Participants <strong>are not allowed</strong> to make subjective judgments of the evaluation data, nor to annotate it (this includes the use of statistics about the evaluation dataset in the decision making). The evaluation dataset cannot be used to train the submitted system.</li>
</ul>
<h1 id="evaluation">Evaluation</h1>
<p>Submissions will be evaluated with event-based measures with a 200ms collar on onsets and a 200ms / 20% of the events length collar on offsets. Submissions will be ranked according to the event-based F1-score computed over the whole evaluation set. Additionally, event-based error rate will be provided as a secondary measure. Evaluation is done using sed_eval toolbox:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span style="font-size:20px;">sed_eval - Evaluation toolbox for Sound Event Detection <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>Detailed information on metrics calculation is available in:</p>
<div class="btex-item" data-item="Mesaros2016_MDPI" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_MDPI"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>Metrics for polyphonic sound event detection.</em>
<em>Applied Sciences</em>, 6(6):162, 2016.
URL: <a href="http://www.mdpi.com/2076-3417/6/6/162">http://www.mdpi.com/2076-3417/6/6/162</a>, <a href="https://doi.org/10.3390/app6060162">doi:10.3390/app6060162</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" class="panel-collapse collapse" id="collapseMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" role="tabpanel">
<h4>Metrics for Polyphonic Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6label" class="modal fade" id="bibtexMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_MDPIdd60a99e0c9b485192e0e0b34ce4f4c6label">Metrics for Polyphonic Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2016_MDPI,
    Author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "Metrics for Polyphonic Sound Event Detection",
    journal = "Applied Sciences",
    year = "2016",
    number = "6",
    pages = "162",
    volume = "6",
    abstract = "This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.",
    doi = "10.3390/app6060162",
    issn = "2076-3417",
    url = "http://www.mdpi.com/2076-3417/6/6/162"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h1 id="baseline-system">Baseline system</h1>
<h2 id="system-description">System description</h2>
<p>The baseline system is based on two conolutional recurrent neural network (CRNN) using 64 log mel-band magnitudes as features.
10 seconds audio files are divided in 500 frames.</p>
<p>Using these features, we train a first CRNN with three convolution layers (64 filters (3x3), max pooling (4) along the frequency axis
 and 30% dropout), one recurrent layer (64 Gated Recurrent Units <em>GRU</em> with 30% dropout on the input), a dense layer (10 units sigmoid activation)
 and global average pooling across frames.
 The system is trained for 100 epochs (early stopping after 15 epochs patience) on weak labels (1578 clips, 20% is used for validation).
 This model is trained at clip level (file containing the event or not), inputs are 500 frames long (10 sec audio file) for a single output frame.
 This first model is used to predict labels of unlabeled files (unlabel_in_domain, 14412 clips).</p>
<p>A second model based on the same architecture (3 convolutional layers and 1 recurrent layer) is trained on
predictions of the first model (unlabel_in_domain, 14412 clips; the weak files, 1578 clips are used to validate the model).
The main difference with the first pass model is that the output is the dense layer in order to be able to predict event
at frame level.
Inputs are 500 frames long, each of them labeled identically following clip labels.
The model outputs a decision for each frame.
Preprocessing (median filtering) is used to obtain events onset and offset for each file.
The baseline system includes evaluations of results using <strong>event-based F-score</strong> as metric.</p>
<h2 id="script-description">Script description</h2>
<p>The baseline system is a semi supervised approach:</p>
<ul>
<li>Download the data (only the first time)</li>
<li>First pass at clip level:<ul>
<li>Train a CRNN on weak data (<code>train/weak</code>) - <em>20% of data used for validation</em></li>
<li>Predict unlabel (in domain) data (<code>train/unlabel_in_domain</code>)</li>
</ul>
</li>
<li>Second pass at frame level:<ul>
<li>Train a CRNN on predicted unlabel data from the first pass (<code>train/unlabel_in_domain</code>) - <em>weak data (<code>train/weak</code>)
is used for validation</em>
<em>Note: labels are used at frames level but annotations are at clip level, so if an event is present in the 10 sec,
all frames contain this label during training</em></li>
<li>Predict strong test labels (<code>test/</code>) <em>Note: predict an event with an onset and offset</em></li>
</ul>
</li>
<li>Evaluate the model between test annotations and second pass predictions (metric is <a href="http://tut-arg.github.io/sed_eval/sound_event.html#event-based">macro-averaged event based F-measure</a>)</li>
</ul>
<h2 id="python-implementation">Python implementation</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/" target="_blank">
<span style="font-size:20px;">DCASE2018 Task 4 <strong>Baseline</strong> (python implementation) <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="system-performance">System performance</h2>
<p>Event-based metrics with a 200ms collar on onsets and a 200ms / 20% of the events length collar on offsets:</p>
<div class="alert alert-success">
Performance on the latest test set version (<strong>July 6th</strong>)</div>
<div>
<div class="table-responsive col-md-4">
<table class="table table-striped">
<thead>
<tr>
<td colspan="2">Event-based overall metrics (macro-average)</td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>F-score</strong></td>
<td>14.06 %</td>
</tr>
<tr>
<td>ER</td>
<td>1.54</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<br/>
<strong>Note:</strong> This performance was obtained on a CPU based system (Intel® Xeon E5-1630 -- 8 cores, 128Gb RAM). The total runtime was approximately 24h.
<br/>
<strong>Note:</strong> The performance might not be exactly reproducible on a GPU based system. However, it runs in around 8 hours on a single Nvidia Geforce 1080 Ti GPU.

<br/>
<br/>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="10" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="1"></th>
</tr>
<tr>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_1</td>
<td>Anastasia Avdeeva</td>
<td>Speech Information Systems Department, University of Information Technology Mechanics and Optics, Saint-Petersburg, Russia</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Avdveeva2018</td>
<td>20.1</td>
</tr>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_2</td>
<td>Anastasia Avdeeva</td>
<td>Speech Information Systems Department, University of Information Technology Mechanics and Optics, Saint-Petersburg, Russia</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Avdveeva2018</td>
<td>19.5</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>Dezhi Wang</td>
<td>School of Computer, National University of Defense Technology, Changsha, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#WangD2018</td>
<td>12.4</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>Dezhi Wang</td>
<td>School of Computer, National University of Defense Technology, Changsha, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#WangD2018</td>
<td>12.6</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>Dezhi Wang</td>
<td>School of Computer, National University of Defense Technology, Changsha, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#WangD2018</td>
<td>12.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>Dezhi Wang</td>
<td>School of Computer, National University of Defense Technology, Changsha, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#WangD2018</td>
<td>12.2</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_1</td>
<td>Heinrich Dinkel</td>
<td>Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Dinkel2018</td>
<td>10.4</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_2</td>
<td>Heinrich Dinkel</td>
<td>Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Dinkel2018</td>
<td>10.7</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_3</td>
<td>Heinrich Dinkel</td>
<td>Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Dinkel2018</td>
<td>13.4</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_4</td>
<td>Heinrich Dinkel</td>
<td>Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Dinkel2018</td>
<td>11.2</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_1</td>
<td>Yingmei Guo</td>
<td>Department of Computer Science and Technology, Tsinghua University, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Guo2018</td>
<td>21.3</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_2</td>
<td>Yingmei Guo</td>
<td>Department of Computer Science and Technology, Tsinghua University, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Guo2018</td>
<td>20.6</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_3</td>
<td>Yingmei Guo</td>
<td>Department of Computer Science and Technology, Tsinghua University, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Guo2018</td>
<td>19.1</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_4</td>
<td>Yingmei Guo</td>
<td>Department of Computer Science and Technology, Tsinghua University, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Guo2018</td>
<td>19.0</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_1</td>
<td>Robert Harb</td>
<td>Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Harb2018</td>
<td>19.4</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_2</td>
<td>Robert Harb</td>
<td>Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Harb2018</td>
<td>15.7</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_3</td>
<td>Robert Harb</td>
<td>Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Harb2018</td>
<td>21.6</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_1</td>
<td>Yuanbo Hou</td>
<td>Institute of Information Photonics and Optical,  Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Hou2018</td>
<td>19.6</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_2</td>
<td>Yuanbo Hou</td>
<td>Institute of Information Photonics and Optical,  Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Hou2018</td>
<td>18.9</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_3</td>
<td>Yuanbo Hou</td>
<td>Institute of Information Photonics and Optical,  Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Hou2018</td>
<td>20.9</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_4</td>
<td>Yuanbo Hou</td>
<td>Institute of Information Photonics and Optical,  Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Hou2018</td>
<td>21.1</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_1</td>
<td>Cances Leo</td>
<td>Institut de Recherche en Informatique de Toulouse, Université Paul Sabatier Toulouse III, Toulouse, France</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Cances2018</td>
<td>8.4</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_2</td>
<td>Thomas Pellegrini</td>
<td>Institut de Recherche en Informatique de Toulouse, Université Paul Sabatier Toulouse III, Toulouse, France</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Cances2018</td>
<td>16.6</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>Sandeep Kothinti</td>
<td>Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kothinti2018</td>
<td>20.6</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>Sandeep Kothinti</td>
<td>Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kothinti2018</td>
<td>20.9</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>Sandeep Kothinti</td>
<td>Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kothinti2018</td>
<td>20.9</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>Sandeep Kothinti</td>
<td>Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kothinti2018</td>
<td>22.4</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_1</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University, Linz, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Koutini2018</td>
<td>21.5</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_2</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University, Linz, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Koutini2018</td>
<td>21.1</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_3</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University, Linz, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Koutini2018</td>
<td>20.6</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_4</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University, Linz, Austria</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Koutini2018</td>
<td>18.8</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_1</td>
<td>Yaming Liu</td>
<td>National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Liu2018</td>
<td>27.3</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_2</td>
<td>Yaming Liu</td>
<td>National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Liu2018</td>
<td>28.8</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_3</td>
<td>Yaming Liu</td>
<td>National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Liu2018</td>
<td>28.1</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_4</td>
<td>Yaming Liu</td>
<td>National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Liu2018</td>
<td>29.9</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_1</td>
<td>JaiKai Lu</td>
<td>1T5K, PFU SHANGHAI Co., LTD, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lu2018</td>
<td>24.1</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_2</td>
<td>JaiKai Lu</td>
<td>1T5K, PFU SHANGHAI Co., LTD, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lu2018</td>
<td>26.3</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_3</td>
<td>JaiKai Lu</td>
<td>1T5K, PFU SHANGHAI Co., LTD, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lu2018</td>
<td>29.5</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_4</td>
<td>JaiKai Lu</td>
<td>1T5K, PFU SHANGHAI Co., LTD, Shanghai, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lu2018</td>
<td>32.4</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_1</td>
<td>Moon Hyeongi</td>
<td>School of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of korea</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Moon2018</td>
<td>15.9</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_2</td>
<td>Moon Hyeongi</td>
<td>School of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of korea</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Moon2018</td>
<td>14.3</td>
</tr>
<tr>
<td></td>
<td>Raj_IITKGP_task4_1</td>
<td>Rojin Raj</td>
<td>Electronics and Electrical Communication Engineering Department, Indian Institute of Technology Kharagpur, Kharagpur, India</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Raj2018</td>
<td>9.4</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Wootaek Lim</td>
<td>Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lim2018</td>
<td>17.1</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Wootaek Lim</td>
<td>Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lim2018</td>
<td>18.0</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Wootaek Lim</td>
<td>Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lim2018</td>
<td>19.6</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Wootaek Lim</td>
<td>Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Lim2018</td>
<td>20.4</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task4_2</td>
<td>Wang Jun</td>
<td>Laboratory of Signal Processing &amp; Knowledge Discovery, Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#WangJ2018</td>
<td>17.9</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Romain Serizel</td>
<td>Department of Natural Language Processing &amp; Knowledge Discovery, University of Lorraine, Loria, Nancy, France</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#SerizelJ2018</td>
<td>10.8</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_1</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kong2018</td>
<td>18.6</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_2</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kong2018</td>
<td>16.7</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_3</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK</td>
<td>task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results#Kong2018</td>
<td>24.0</td>
</tr>
</tbody>
</table>
<br/>

Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results">Task 4 results page</a>
<h1 id="citation">Citation</h1>

If you are using the <strong>dataset</strong> or <strong>baseline</strong> code, or want to refer <strong>challenge task</strong> please cite the following paper:

<div class="btex-item" data-item="Serizel2018" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Serizel2018"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Romain Serizel, Nicolas Turpault, Hamid Eghbal-Zadeh, and Ankit Parag Shah.
<em>Large-scale weakly labeled semi-supervised sound event detection in domestic environments.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018), 19–23. November 2018.
URL: <a href="https://hal.inria.fr/hal-01850270">https://hal.inria.fr/hal-01850270</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexSerizel20186d857b16859247069a3c35fe28889ddc" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-01850270/file/task4.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseSerizel20186d857b16859247069a3c35fe28889ddc" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseSerizel20186d857b16859247069a3c35fe28889ddc" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingSerizel20186d857b16859247069a3c35fe28889ddc" class="panel-collapse collapse" id="collapseSerizel20186d857b16859247069a3c35fe28889ddc" role="tabpanel">
<h4>Large-scale weakly labeled semi-supervised sound event detection in domestic environments</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events. . . ) and potential applications.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection, Large scale, Weakly labeled data, Semi-supervised learning</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexSerizel20186d857b16859247069a3c35fe28889ddc" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-01850270/file/task4.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexSerizel20186d857b16859247069a3c35fe28889ddclabel" class="modal fade" id="bibtexSerizel20186d857b16859247069a3c35fe28889ddc" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexSerizel20186d857b16859247069a3c35fe28889ddclabel">Large-scale weakly labeled semi-supervised sound event detection in domestic environments</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Serizel2018,
    author = "Serizel, Romain and Turpault, Nicolas and Eghbal-Zadeh, Hamid and Shah, Ankit Parag",
    title = "Large-scale weakly labeled semi-supervised sound event detection in domestic environments",
    abstract = "This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events. . . ) and potential applications.",
    month = "November",
    pages = "19--23",
    year = "2018",
    keywords = "Sound event detection, Large scale, Weakly labeled data, Semi-supervised learning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    hal_id = "hal-01850270",
    hal_version = "v1",
    url = "https://hal.inria.fr/hal-01850270"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<br/>
<br/></div>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>