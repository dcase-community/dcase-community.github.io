<!DOCTYPE html><html lang="en">
<head>
    <title>Large-scale weakly supervised sound event detection for smart cars - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2017/task-large-scale-sound-event-detection">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="Challenge has ended. Full results for this task can be found here Description The task evaluates systems for the large-scale detection of sound events using weakly labeled training data. The data are YouTube video excerpts focusing on transportation and warnings due to their industry relevance and to the underuse of …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2017</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2017/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-rare-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-alarm text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-rare-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-rare-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-events text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2017/task-large-scale-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2017/task-large-scale-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-large-scale-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Download data">
        <a href="/challenge2017/download"><i class="fa fa-download"></i>&nbsp;Download</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2017/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2017/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge organizers">
        <a href="/challenge2017/organizers"><i class="fa fa-users"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/marina-bay-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-large-scale fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Large-scale</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2017_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Large-scale weakly supervised <br>sound event detection for smart cars</h1><hr class="small right bold"><span class="subheading">Task 4</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Benjamin Elizalde" class="img img-circle" src="/images/person/benjamin_elizalde.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Benjamin Elizalde</strong>
<a class="icon" href="mailto:bmartin1@andrew.cmu.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">       
                                                                
                                            
                                
                                Carnegie Mellon University
                                
                            
                            </p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Bhiksha Raj" class="img img-circle" src="/images/person/bhiksha_raj.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Bhiksha Raj</strong>
<a class="icon" href="mailto:bhiksha@cs.cmu.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">       
                                                                
                                            
                                
                                Carnegie Mellon University
                                
                            
                            </p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Emmanuel Vincent" class="img img-circle" src="/images/person/emmanuel_vincent.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Emmanuel Vincent</strong>
<a class="icon" href="mailto:emmanuel.vincent@inria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">       
                                                                
                                            
                                
                                INRIA
                                
                            
                            </p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a>
<ul>
<li><a href="#audio-dataset">Audio dataset</a></li>
<li><a href="#annotations">Annotations</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
<li><a href="#submission">Submission</a></li>
<li><a href="#task-rules">Task rules</a></li>
</ul>
</li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#python-implementation">Python implementation</a></li>
<li><a href="#results-for-development-dataset">Results for development dataset</a></li>
</ul>
</li>
<li><a href="#contributors">Contributors</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found <a class="btn btn-default btn-xs" href="/challenge2017/task-large-scale-sound-event-detection-results">here <i class="fa fa-caret-right"></i></a>
</p>
<h1 id="description">Description</h1>
<p>The task evaluates systems for the large-scale detection of sound events using weakly labeled training data. The data are YouTube video excerpts focusing on transportation and warnings due to their industry relevance and to the underuse of audio in this context. The results will help define new grounds for large-scale sound event detection and show the benefit of audio for self-driving cars, smart cities and related areas.</p>
<h2 id="audio-dataset">Audio dataset</h2>
<p>The task employs a subset of “AudioSet: An Ontology And Human-Labeled Dataset For Audio Events” by Google. <a href="https://research.google.com/audioset/"><strong>AudioSet</strong></a> consists of an expanding ontology of 632 sound event classes and a collection of 2 million human-labeled 10-second sound clips (less than 21% are shorter than 10-seconds) drawn from 2 million YouTube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.</p>
<p>The subset consists of 17 sound events divided into two categories: “Warning” and “Vehicle”. The list below shows each class and next to it the approximate number of 10-second clips. Note that each clip may correspond to more than one sound event.</p>
<p><strong>Warning sounds:</strong></p>
<ul>
<li>Train horn (441)</li>
<li>Air horn, truck horn (407)</li>
<li>Car alarm (273)</li>
<li>Reversing beeps (337)</li>
<li>Ambulance (siren) (624)</li>
<li>Police car (siren) (2,399)</li>
<li>Fire engine, fire truck (siren) (2,399)</li>
<li>Civil defense siren (1,506)</li>
<li>Screaming (744)</li>
</ul>
<p><strong>Vehicle sounds:</strong></p>
<ul>
<li>Bicycle (2,020)</li>
<li>Skateboard (1,617)</li>
<li>Car (25,744)</li>
<li>Car passing by (3,724)</li>
<li>Bus (3,745)</li>
<li>Truck (7,090)</li>
<li>Motorcycle (3,291)</li>
<li>Train (2,301)</li>
</ul>
<h2 id="annotations">Annotations</h2>
<p>To collect all the data Google worked with human annotators who listen, analyze, and verify the sounds they hear within YouTube clips. To facilitate faster accumulation of examples for all classes, Google rely on available YouTube metadata and content-based search to nominate candidate video segments that are likely to contain the target sound.</p>
<p>A detailed description of the data annotation procedure is available in:</p>
<div class="btex-item" data-item="Gemmeke2017" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Gemmeke2017"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter.
<em>Audio set: an ontology and human-labeled dataset for audio events.</em>
In Proc. IEEE ICASSP 2017. New Orleans, LA, 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" class="panel-collapse collapse" id="collapseGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" role="tabpanel">
<h4>Audio Set: An ontology and human-labeled dataset for audio events</h4>
<h5>Abstract</h5>
<p class="text-justify">Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets -- principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 635 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6alabel" class="modal fade" id="bibtexGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexGemmeke201736ce9d0aa6aa4b20a08c3a7d6df75b6alabel">Audio Set: An ontology and human-labeled dataset for audio events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gemmeke2017,
    author = "Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin",
    title = "Audio Set: An ontology and human-labeled dataset for audio events",
    year = "2017",
    abstract = "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets -- principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 635 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.",
    booktitle = "Proc. IEEE ICASSP 2017",
    address = "New Orleans, LA"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>The subset consists of two partitions: <strong>development</strong> and <strong>evaluation</strong>. The format for each partition list is the following:</p>
<div class="highlight"><pre><span></span><code>CID, start_seconds, end_seconds, positive_labels
</code></pre></div>
<p>For example:</p>
<div class="highlight"><pre><span></span><code>-jj2tyuf6-A,  80.000,  90.000,  "Train horn,Air horn, truck horn", "/m/0284vy3,/m/05x_td"
</code></pre></div>
<p>The first column, <code>-jj2tyuf6-A</code>, is the YouTube ID of the video from where the 10-second clips was extracted; The second and third columns, t=80 sec to t=90 sec, correspond to the clip boundaries within the full video; last column, <code>/m/0284vy3</code> ("Train horn") and <code>/m/05x_td</code> ("Air horn, truck horn"), corresponds to the sound classes present in the clip.</p>
<p>Note that AudioSet does not come with time boundaries for each sound class within the 10-second clip and thus annotations are considered <strong>weak labels</strong>.</p>
<h2 id="download">Download</h2>
<p>** Development dataset **</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/ankitshah009/Task-4-Large-scale-weakly-supervised-sound-event-detection-for-smart-cars" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/ankitshah009/Task-4-Large-scale-weakly-supervised-sound-event-detection-for-smart-cars" target="_blank">
<span style="font-size:20px;">Large-scale weakly supervised sound event detection for smart cars, Development dataset <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>** Evaluation dataset **</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://dl.dropboxusercontent.com/s/bbgqfd47cudwe9y/DCASE_2017_evaluation_set_audio_files.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://dl.dropboxusercontent.com/s/bbgqfd47cudwe9y/DCASE_2017_evaluation_set_audio_files.zip" target="_blank">
<span style="font-size:20px;">Large-scale weakly supervised sound event detection for smart cars, Evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(863 MB)</span>
<br/>
<span class="text-muted">
                
                
                (.zip)
                
                </span>
<br/>
<strong>
                password "DCASE_2017_evaluation_set"
                </strong>
</div>
</div>
<p><br/></p>
<h1 id="task-setup">Task setup</h1>
<p>The challenge consists of detecting sound events within web videos using weakly labeled training data. The detection within a 10-second clip should be performed at two levels:</p>
<ul>
<li>Without timestamps (similar to audio tagging).</li>
<li>With timestamps (start-end time).</li>
</ul>
<p>The training set has <strong>weak labels</strong> denoting the presence of a given sound event in the video’s soundtrack and no timestamps are provided. For testing and evaluation, <strong>strong labels</strong> with timestamps are provided for the purpose of evaluating performance.</p>
<h2 id="development-dataset">Development dataset</h2>
<p>The development is divided in two partitions: training and testing. Training is unbalanced and has at least 30 clips per sound event, whereas testing has about 30 clips per sound event. Note that a 10-second clip may correspond to more than one sound event.</p>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>Evaluation set without ground truth will be released one month before the submission deadline. Full ground truth will be published after the DCASE 2017 challenge and workshop are concluded. The evaluation set has at least 60 files per sound event. A fraction of these clips will have strong labels for the purpose of evaluation.</p>
<h2 id="submission">Submission</h2>
<p>Detailed information for the challenge submission can found from <a href="submission">submission page</a>.</p>
<p>System output should be presented as a single text-file (in CSV format) containing classification result for each audio file in the evaluation set. Result items can be in any order. Format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event onset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event offset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event label (string)</span><span class="o">]</span>
</code></pre></div>
<p>Detection for both subtasks--with and without timestamps will be evaluated using the same output format, but the latter will ignore the timestamps. The system output file can be the same for both subtasks or two different versions can be provided.</p>
<p>Multiple system outputs can be submitted (maximum 4 per participant). If submitting multiple systems, the individual text-files should be packaged into a zip file for submission. Please carefully mark the connection between the submitted files and the corresponding system or system parameters (for example by naming the text file appropriately).</p>
<p>If no event is detected for the particular audio signal, the system should still output a row containing only the file name, to indicate that the file was processed. This is used to verify that participants processed all evaluation files.</p>
<h2 id="task-rules">Task rules</h2>
<p>These are the general rules valid for all tasks. The same rules and additional information on technical report and submission requirements can be found <a href="rules">here</a>. Task specific rules are highlighted with green.</p>
<ul>
<li>Participants are <strong>not allowed</strong> to use external data for system development. Data from another task is considered external data.
    <li><p class="bg-success"><strong>Another example of external data are other elements related to the video such as the rest of audio from where the 10-sec clip was extracted, the video frames and metadata. </strong></p></li>
<li>The system outputs that do not respect the challenge rules will be evaluated on request, but they will not be officially included in the challenge rankings.
    <li>Participants are <strong>not allowed</strong> to use the embeddings provided by AudioSet or any other features that indirectly use external data.
    <li><p class="bg-success"><strong>Only weak labels and none of the strong labels</strong> (timestamps) can be used for training the submitted system.</p></li>
<li>We strongly suggest to use an approach that addresses the use of weak labels.</li>
<li>Manipulation of provided training and development data <strong>is allowed</strong>.
    <p class="bg-success">
    The development dataset can be augmented without use of external data (e.g. by mixing data sampled from a pdf or using techniques such as pitch shifting or time stretching).
    </p>
</li>
<li>Participants are <strong>not allowed</strong> to make subjective judgments of the evaluation data, nor to annotate it. The evaluation dataset cannot be used to train the submitted system.</li>
</li></li></li></ul>
<h1 id="evaluation">Evaluation</h1>
<p><strong>A - Sound event detection without timestamps (or Audio Tagging).</strong><br/>
<strong>F-score (Precision and Recall)</strong> will be calculated for the detection of sound events within the 10-second clip. Ranking of submitted systems will be based on F-score.</p>
<p><strong>B - Sound event detection with timestamps.</strong><br/>
<strong>Segment-based error rate</strong> will be calculated in one-second segments over the entire test set. Additionally, segment-based F-score will be calculated. Ranking of submitted systems will be based on segment-based error rate.</p>
<p>Ranking of submitted systems will be done independently for each of the two metrics. Detailed information on metrics calculation is available in:</p>
<div class="btex-item" data-item="Mesaros2016_MDPI" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_MDPI"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>Metrics for polyphonic sound event detection.</em>
<em>Applied Sciences</em>, 6(6):162, 2016.
URL: <a href="http://www.mdpi.com/2076-3417/6/6/162">http://www.mdpi.com/2076-3417/6/6/162</a>, <a href="https://doi.org/10.3390/app6060162">doi:10.3390/app6060162</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://github.com/TUT-ARG/sed_eval" rel="tooltip" title="Toolbox"><i class="fa fa-file-code-o"></i></a>
<button aria-controls="collapseMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" class="panel-collapse collapse" id="collapseMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" role="tabpanel">
<h4>Metrics for Polyphonic Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" href="http://www.mdpi.com/2076-3417/6/6/162" title="Journal page"><i class="fa fa-book"></i> Web publication</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://github.com/TUT-ARG/sed_eval" rel="tooltip" title="Toolbox"><i class="fa fa-file-code-o"></i> Toolbox</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781label" class="modal fade" id="bibtexMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_MDPI15c4e77dc7454c50b9cba10830a18781label">Metrics for Polyphonic Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2016_MDPI,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "Metrics for Polyphonic Sound Event Detection",
    journal = "Applied Sciences",
    volume = "6",
    year = "2016",
    number = "6",
    pages = "162",
    url = "http://www.mdpi.com/2076-3417/6/6/162",
    issn = "2076-3417",
    abstract = "This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.",
    doi = "10.3390/app6060162"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>A short description of metrics can be found <a href="metrics">here</a>.</p>
<p>Evaluation for Subtask A is done using a script available in the data repository link in the "Download" section inside the folder name "evaluation". Evaluation for Subtask B is done using <strong>sed_eval toolbox</strong> and we provided a wrapper to facilitate its usage also in the data repository link in the "Download" section inside the folder name "TaskB_evaluation".</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span style="font-size:20px;">sed_eval - Evaluation toolbox for Sound Event Detection <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>In case of using the toolbox directly, use the following parameters for <code>sed_eval.sound_event.SegmentBasedMetrics</code> evaluator to align it with the baseline system:</p>
<ul>
<li>For Subtask B: one second segment size <code>time_resolution=1.0</code></li>
</ul>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="B_segment_based_ER_overall_eval" data-scatter-y="B_segment_based_F1_overall_eval" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="A_micro_F1_overall_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="1">Subtask A,<br/> Audio tagging</th>
<th class="sep-left-cell" colspan="2">Subtask B,<br/> Sound event detection</th>
</tr>
<tr>
<th class="sm-cell" data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="A_micro_F1_overall_eval" data-sortable="true" data-value-type="float1-percentage">
                F1 <small class="hidden">(subtask A / evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="B_segment_based_ER_overall_eval" data-reversed="true" data-sortable="true" data-value-type="float4">
                ER <small class="hidden">(subtask B / evaluation dataset)
            </small></th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="B_segment_based_F1_overall_eval" data-sortable="true" data-value-type="float1-percentage">
                F1 <small class="hidden">(subtask B / evaluation dataset)
            </small></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Adavanne_TUT_task4_1</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-large-scale-sound-event-detection-results#Adavanne2017</td>
<td>45.5</td>
<td>0.8100</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Adavanne_TUT_task4_2</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-large-scale-sound-event-detection-results#Adavanne2017</td>
<td>46.6</td>
<td>0.8000</td>
<td>48.3</td>
</tr>
<tr>
<td></td>
<td>Adavanne_TUT_task4_3</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-large-scale-sound-event-detection-results#Adavanne2017</td>
<td>44.5</td>
<td>0.8200</td>
<td>48.9</td>
</tr>
<tr>
<td></td>
<td>Adavanne_TUT_task4_4</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-large-scale-sound-event-detection-results#Adavanne2017</td>
<td>26.3</td>
<td>0.7900</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task4_1</td>
<td>Szu-Yu Chou</td>
<td>Research Center for IT innovation, Academia Sinica, Taipei, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Chou2017</td>
<td>47.6</td>
<td>0.8300</td>
<td>42.4</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task4_2</td>
<td>Szu-Yu Chou</td>
<td>Research Center for IT innovation, Academia Sinica, Taipei, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Chou2017</td>
<td>49.0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task4_3</td>
<td>Szu-Yu Chou</td>
<td>Research Center for IT innovation, Academia Sinica, Taipei, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Chou2017</td>
<td>47.9</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task4_4</td>
<td>Szu-Yu Chou</td>
<td>Research Center for IT innovation, Academia Sinica, Taipei, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Chou2017</td>
<td>49.0</td>
<td></td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2017 baseline</td>
<td>Benjamin Elizalde</td>
<td>Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA</td>
<td>task-large-scale-sound-event-detection-results#Badlani2017</td>
<td>18.2</td>
<td>0.9300</td>
<td>28.4</td>
</tr>
<tr>
<td></td>
<td>Kukanov_UEF_task4_1</td>
<td>Ivan Kukanov</td>
<td>School of Computing, University of Eastern Finland, Joensuu, Finland</td>
<td>task-large-scale-sound-event-detection-results#Kukanov2017</td>
<td>39.6</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_KAIST_task4_1</td>
<td>Jongpil Lee</td>
<td>Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, Daejeon, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017</td>
<td>40.3</td>
<td>0.8200</td>
<td>39.4</td>
</tr>
<tr>
<td></td>
<td>Lee_KAIST_task4_2</td>
<td>Jongpil Lee</td>
<td>Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, Daejeon, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017</td>
<td>47.3</td>
<td>0.7800</td>
<td>42.6</td>
</tr>
<tr>
<td></td>
<td>Lee_KAIST_task4_3</td>
<td>Jongpil Lee</td>
<td>Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, Daejeon, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017</td>
<td>47.2</td>
<td>0.7800</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>Lee_KAIST_task4_4</td>
<td>Jongpil Lee</td>
<td>Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, Daejeon, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017</td>
<td>47.1</td>
<td>0.7500</td>
<td>47.1</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task4_1</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017a</td>
<td>52.3</td>
<td>0.6700</td>
<td>54.4</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task4_2</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017a</td>
<td>52.3</td>
<td>0.6700</td>
<td>54.4</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task4_3</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017a</td>
<td>52.6</td>
<td>0.6700</td>
<td>55.4</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task4_4</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-large-scale-sound-event-detection-results#Lee2017a</td>
<td>52.1</td>
<td>0.6600</td>
<td>55.5</td>
</tr>
<tr>
<td></td>
<td>Salamon_NYU_task4_1</td>
<td>Justin Salamon</td>
<td>Music and Audio Research Laboratory, New York University, New York City, USA; Center of Urban Science and Progress, New York University, New York City, USA</td>
<td>task-large-scale-sound-event-detection-results#Salamon2017</td>
<td>46.0</td>
<td>0.8200</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>Salamon_NYU_task4_2</td>
<td>Justin Salamon</td>
<td>Music and Audio Research Laboratory, New York University, New York City, USA; Center of Urban Science and Progress, New York University, New York City, USA</td>
<td>task-large-scale-sound-event-detection-results#Salamon2017</td>
<td>45.3</td>
<td>0.8500</td>
<td>45.6</td>
</tr>
<tr>
<td></td>
<td>Salamon_NYU_task4_3</td>
<td>Justin Salamon</td>
<td>Music and Audio Research Laboratory, New York University, New York City, USA; Center of Urban Science and Progress, New York University, New York City, USA</td>
<td>task-large-scale-sound-event-detection-results#Salamon2017</td>
<td>44.9</td>
<td>0.7700</td>
<td>45.9</td>
</tr>
<tr>
<td></td>
<td>Salamon_NYU_task4_4</td>
<td>Justin Salamon</td>
<td>Music and Audio Research Laboratory, New York University, New York City, USA; Center of Urban Science and Progress, New York University, New York City, USA</td>
<td>task-large-scale-sound-event-detection-results#Salamon2017</td>
<td>38.1</td>
<td>0.7700</td>
<td>45.9</td>
</tr>
<tr>
<td></td>
<td>Toan_NCU_task4_1</td>
<td>Toan Vu</td>
<td>Department of Computer Science and Information Engineering, National Central University, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Vu2017</td>
<td>48.5</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Toan_NCU_task4_2</td>
<td>Toan Vu</td>
<td>Department of Computer Science and Information Engineering, National Central University, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Vu2017</td>
<td>46.5</td>
<td>0.9400</td>
<td>43.0</td>
</tr>
<tr>
<td></td>
<td>Toan_NCU_task4_3</td>
<td>Toan Vu</td>
<td>Department of Computer Science and Information Engineering, National Central University, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Vu2017</td>
<td></td>
<td>0.9000</td>
<td>42.7</td>
</tr>
<tr>
<td></td>
<td>Toan_NCU_task4_4</td>
<td>Toan Vu</td>
<td>Department of Computer Science and Information Engineering, National Central University, Taiwan</td>
<td>task-large-scale-sound-event-detection-results#Vu2017</td>
<td></td>
<td>0.8700</td>
<td>41.6</td>
</tr>
<tr>
<td></td>
<td>Tseng_Bosch_task4_1</td>
<td>Juncheng Billy Li</td>
<td>Research and Technology Center, Robert Bosch LLC, Pittsburgh, USA</td>
<td>task-large-scale-sound-event-detection-results#Tseng2017</td>
<td>35.0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tseng_Bosch_task4_2</td>
<td>Juncheng Billy Li</td>
<td>Research and Technology Center, Robert Bosch LLC, Pittsburgh, USA</td>
<td>task-large-scale-sound-event-detection-results#Tseng2017</td>
<td>35.1</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tseng_Bosch_task4_3</td>
<td>Juncheng Billy Li</td>
<td>Research and Technology Center, Robert Bosch LLC, Pittsburgh, USA</td>
<td>task-large-scale-sound-event-detection-results#Tseng2017</td>
<td>35.2</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tseng_Bosch_task4_4</td>
<td>Juncheng Billy Li</td>
<td>Research and Technology Center, Robert Bosch LLC, Pittsburgh, USA</td>
<td>task-large-scale-sound-event-detection-results#Tseng2017</td>
<td>35.2</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Xu_CVSSP_task4_1</td>
<td>Yong Xu</td>
<td>Center for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-large-scale-sound-event-detection-results#Xu2017</td>
<td>54.4</td>
<td>0.7300</td>
<td>51.8</td>
</tr>
<tr>
<td></td>
<td>Xu_CVSSP_task4_2</td>
<td>Yong Xu</td>
<td>Center for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-large-scale-sound-event-detection-results#Xu2017</td>
<td>55.6</td>
<td>0.7800</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td>Xu_CVSSP_task4_3</td>
<td>Yong Xu</td>
<td>Center for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-large-scale-sound-event-detection-results#Xu2017</td>
<td>54.2</td>
<td>1.0100</td>
<td>52.1</td>
</tr>
<tr>
<td></td>
<td>Xu_CVSSP_task4_4</td>
<td>Yong Xu</td>
<td>Center for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-large-scale-sound-event-detection-results#Xu2017</td>
<td>52.8</td>
<td>0.8000</td>
<td>50.4</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found <a href="task-large-scale-sound-event-detection-results">here</a>.</p>
<h1 id="baseline-system">Baseline system</h1>
<p>The baseline system for the task is provided. The system is meant to implement a basic approach for acoustic scene classification, and provide some comparison point for the participants while developing their systems. The baseline systems for all tasks share the code base, implementing quite similar approach for all tasks. The baseline system will download the needed datasets and produces the results below when ran with the default parameters.</p>
<p>The baseline system is based on a multilayer perceptron architecture using log mel-band energies as features. A 5-frame context is used, resulting in a feature vector length of 200. Using these features, a neural network containing two dense layers of 50 hidden units per layer and 20% dropout is trained for 200 epochs for each class.  Detection decision is based on the network output layer containing sigmoid units that can be active at the same time. A detailed description is available in the baseline system documentation. The baseline system includes evaluation of results using for for Subtask A: <strong>F-score</strong>, and for Subtask B: <strong>segment-based error rate</strong> and <strong>segment-based F-score</strong> as metrics.</p>
<p>The baseline system is implemented using Python (version 2.7 and 3.6). Participants are allowed to build their system on top of the given baseline system. The system has all needed functionality for dataset handling, storing / accessing features and models, and evaluating the results, making the adaptation for one's needs rather easy. The baseline system is also a good starting point for entry level researchers.</p>
<h2 id="python-implementation">Python implementation</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/ankitshah009/Task-4-Large-scale-weakly-supervised-sound-event-detection-for-smart-cars" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/ankitshah009/Task-4-Large-scale-weakly-supervised-sound-event-detection-for-smart-cars" target="_blank">
<span style="font-size:20px;">Large-scale weakly supervised sound event detection for smart cars, Development dataset <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="results-for-development-dataset">Results for development dataset</h2>
<p><em>Evaluation setup</em></p>
<ul>
<li>System is trained using the Training set and tested using the Testing set (488 clips).</li>
<li>Python 2.7.13 used</li>
</ul>
<p><em>System parameters</em></p>
<ul>
<li>Frame size: 40 ms (with 50% hop size)</li>
<li>Feature vector: 40 log mel-band energies in 5 consecutive frames = 200 values</li>
<li>MLP: 2 layers x 50 hidden units, 20% dropout, 200 epochs, learning rate 0.001, sigmoid output layer</li>
<li>Trained and tested on full audio</li>
</ul>
<h3>Subtask A, Audio tagging (Ranked based on Micro-averaging or Instance-Based)</h3>
<div class="table-responsive col-md-4">
<table class="table table-striped">
<tbody>
<tr>
<td class="col-md-6"><strong>F-score</strong></td>
<td>10.9 %</td>
</tr>
<tr>
<td>Precision</td>
<td>7.8 %</td>
</tr>
<tr>
<td>Recall</td>
<td>17.5 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<h3>Subtask A, Audio tagging (Macro-averaging or Class-Based)</h3>
<div class="table-responsive col-md-4">
<table class="table table-striped">
<tbody>
<tr>
<td class="col-md-6"><strong>F-score</strong></td>
<td>13.1 %</td>
</tr>
<tr>
<td>Precision</td>
<td>12.2 %</td>
</tr>
<tr>
<td>Recall</td>
<td>14.1 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<h3>Subtask B, Sound event detection (Ranked based on Micro-averaging or Instanced-Based)</h3>
<div class="table-responsive col-md-4">
<table class="table table-striped">
<caption>Segment-based overall metrics</caption>
<tbody>
<tr>
<td class="col-md-6"><strong>ER</strong></td>
<td>1.02</td>
</tr>
<tr>
<td><strong>F-score</strong></td>
<td>13.8 %</td>
</tr>
<tr>
<td><strong>Precision</strong></td>
<td>19.2 %</td>
</tr>
<tr>
<td><strong>Recall</strong></td>
<td>10.9 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<h1 id="contributors">Contributors</h1>
<div class="panel panel-default">
<table class="table bpersonnel-container">
<tr>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Rohan Badlani</strong>
<a class="icon" href="mailto:rohan.badlani@gmail.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">       
                                                                
                                            
                                
                                Birla Institute of Technology &amp; Science
                                
                            
                            </p>
</div>
</div>
</td>
</tr>
<tr>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Ankit Shah</strong>
<a class="icon" href="mailto:ankit.tronix@gmail.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">       
                                                                
                                            
                                
                                Carnegie Mellon University
                                
                            
                            </p>
</div>
</div>
</td>
</tr>
</table>
</div>
<h1 id="citation">Citation</h1>
<p>If you are using the <strong>dataset</strong> or <strong>baseline</strong> code, or want to refer <strong>challenge task</strong> please cite the following paper:</p>
<div class="btex-item" data-item="DCASE2017challenge" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="DCASE2017challenge"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            A. Mesaros, T. Heittola, A. Diment, B. Elizalde, A. Shah, E. Vincent, B. Raj, and T. Virtanen.
<em>DCASE 2017 challenge setup: tasks, datasets and baseline system.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), 85–92. November 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDCASE2017challengea8462af412274fd899a0ca9c141039f8" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDCASE2017challengea8462af412274fd899a0ca9c141039f8" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDCASE2017challengea8462af412274fd899a0ca9c141039f8" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDCASE2017challengea8462af412274fd899a0ca9c141039f8" class="panel-collapse collapse" id="collapseDCASE2017challengea8462af412274fd899a0ca9c141039f8" role="tabpanel">
<h4>DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System</h4>
<h5>Abstract</h5>
<p class="text-justify">DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDCASE2017challengea8462af412274fd899a0ca9c141039f8" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDCASE2017challengea8462af412274fd899a0ca9c141039f8label" class="modal fade" id="bibtexDCASE2017challengea8462af412274fd899a0ca9c141039f8" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDCASE2017challengea8462af412274fd899a0ca9c141039f8label">DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{DCASE2017challenge,
    Author = "Mesaros, A. and Heittola, T. and Diment, A. and Elizalde, B. and Shah, A. and Vincent, E. and Raj, B. and Virtanen, T.",
    title = "{DCASE} 2017 Challenge Setup: Tasks, Datasets and Baseline System",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "85--92",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.",
    keywords = "Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<p>When citing <strong>challenge task</strong> and <strong>results</strong> please cite the following paper:</p>
<div class="btex-item" data-item="Mesaros2019_TASLP" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2019_TASLP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            A. Mesaros, A. Diment, B. Elizalde, T. Heittola, E. Vincent, B. Raj, and T. Virtanen.
<em>Sound event detection in the DCASE 2017 challenge.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2019.
In press.
<a href="https://doi.org/10.1109/TASLP.2019.2907016">doi:10.1109/TASLP.2019.2907016</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02067935/document" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" class="panel-collapse collapse" id="collapseMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" role="tabpanel">
<h4>Sound event detection in the DCASE 2017 Challenge</h4>
<h5>Abstract</h5>
<p class="text-justify">Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly-labeled data was available for training. In this paper, we present the three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure, to perform statistical analysis of the challenge results. The analysis indicates that while the 95% confidence intervals for many systems overlap, there are significant difference in performance between the top systems and the baseline for all tasks.</p>
<h5>Keywords</h5>
<p class="text-justify">Event detection;Task analysis;Training;Acoustics;Speech processing;Glass;Hidden Markov models;Sound event detection;weak labels;pattern recognition;jackknife estimates;confidence intervals</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02067935/document" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97clabel" class="modal fade" id="bibtexMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97c" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2019_TASLP633ecdd3ebf04958924e6af6633aa97clabel">Sound event detection in the DCASE 2017 Challenge</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2019_TASLP,
    author = "Mesaros, A. and Diment, A. and Elizalde, B. and Heittola, T. and Vincent, E. and Raj, B. and Virtanen, T.",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    title = "Sound event detection in the {DCASE} 2017 Challenge",
    year = "2019",
    abstract = "Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly-labeled data was available for training. In this paper, we present the three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure, to perform statistical analysis of the challenge results. The analysis indicates that while the 95\% confidence intervals for many systems overlap, there are significant difference in performance between the top systems and the baseline for all tasks.",
    keywords = "Event detection;Task analysis;Training;Acoustics;Speech processing;Glass;Hidden Markov models;Sound event detection;weak labels;pattern recognition;jackknife estimates;confidence intervals",
    doi = "10.1109/TASLP.2019.2907016",
    issn = "2329-9290",
    note = "In press"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>