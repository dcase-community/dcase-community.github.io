<!DOCTYPE html><html lang="en">
<head>
    <title>Submission - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2023/submission">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="The submission deadline is May 15th 2023 23:59 Anywhere on Earth (AoE) Introduction Challenge submission consists of a submission package (one zip package) containing system outputs, system meta information, and technical report (pdf file). Submission process shortly: Participants run their system with an evaluation dataset, and produce the system â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2023</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2023/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-sound-event-detection-with-weak-and-soft-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-weak-and-soft-labels"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>A: Sound Event Detection with Weak Labels and Synthetic Soundscapes</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-weak-labels-and-synthetic-soundscapes"><i class="fa fa-random fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-weak-labels-and-synthetic-soundscapes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>B: Sound Event Detection with Soft Labels</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-soft-labels"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-soft-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-automated-audio-captioning-and-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-automated-audio-captioning-and-language-based-audio-retrieval"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>A: Automated Audio-Captioning</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-automated-audio-captioning"><i class="fa dc-captioning fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>B: Language-Based Audio Retrieval</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-language-based-audio-retrieval"><i class="fa fa-file-text fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-foley-sound-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-task2"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-foley-sound-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-foley-sound-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2023/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2023/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/tiles-11.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><h1 class="bold">Submission</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Instructions</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#submission-system">Submission system</a></li>
<li><a href="#submission-package">Submission package</a>
<ul>
<li><a href="#submission-label">Submission label</a></li>
<li><a href="#package-structure">Package structure</a></li>
</ul>
</li>
<li><a href="#system-outputs">System outputs</a></li>
<li><a href="#meta-information">Meta information</a></li>
<li><a href="#technical-report">Technical report</a>
<ul>
<li><a href="#template">Template</a></li>
</ul>
</li></ul></div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="bg-danger">
<strong>The submission deadline is May 15th 2023 23:59 Anywhere on Earth (AoE)</strong>
</p>
<h1 id="introduction">Introduction</h1>
<p>Challenge submission consists of a <strong>submission package (one zip package)</strong> containing system outputs, system meta information, and <strong>technical report (pdf file)</strong>.</p>
<p>Submission process shortly:</p>
<ol>
<li>Participants run their system with an evaluation dataset, and produce the <strong>system output</strong> in the specified format. Participants are allowed to submit 4 different system outputs per task or subtask.</li>
<li>Participants create a <strong>meta-information file</strong> to go along the system output to describe the system used to produce this particular output. Meta information file has a predefined format to help the automatic handling of the challenge submissions. Information provided in the meta file will be later used to produce challenge results. Participants should fill in all meta information and make sure meta information file follows defined formatting.</li>
<li>Participants describe their system in a <strong>technical report</strong> in sufficient detail. There is a template provided for the technical report.</li>
<li>Participants prepare the <strong>submission package</strong> (zip-file). The submission package contains system outputs, a maximum of 4 per task, systems meta information, and the technical report.</li>
<li>Participants submit the submission package and the technical report to DCASE2023 Challenge.</li>
</ol>
<p class="bg-danger">
Please read carefully the requirements for the files included in the submission package!
</p>
<h1 id="submission-system">Submission system</h1>
<p>The submission system is now available:</p>
<p><a class="btn btn-success" href="https://cmt3.research.microsoft.com/DCASECHALLENGE2023/" style="" target="_blank">Submission system</a></p>
<ul>
<li>Create a user account and login</li>
<li>Go to the "All Conferences" tab in the system and type DCASE to filter the list</li>
<li>Select "2023 Challenge on Detection and Classification of Acoustic Scenes and Events"</li>
<li>Create a new submission</li>
</ul>
<p><strong>The technical report in the submission package must contain at least the title, authors, and abstract. An updated camera-ready version of the technical report can be submitted separately until 22 May 2023 (AOE).</strong></p>
<div class="bg-danger">
<p><strong>Note:</strong> the submission system does not any send a confirmation email. You can check that your submission has been taken into account in <a href="https://cmt3.research.microsoft.com/DCASECHALLENGE2023/Submission/Index">your author console</a>. A confirmation email will be sent to all participants <strong>once the submissions are closed.</strong></p>
</div>
<p>By submitting to the challenge, participants agree for the system output to be evaluated and to be published together with the results and the technical report on the DCASE Challenge website under <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY license</a>.</p>
<h1 id="submission-package">Submission package</h1>
<p>Participants are instructed to pack their system output(s), system meta information, and technical report into one zip-package. Example package:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2023/dcase2023_challenge_submission_package_example.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2023/dcase2023_challenge_submission_package_example.zip" target="_blank">
<span style="font-size:20px;">DCASE2023 challenge submission example package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(8.9 MB)</span>
<br/>
<span class="text-muted">
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/></p>
<p>Please prepare your submission zip-file as the provided example. Follow the same file structure and fill meta information with a similar structure as the one in <code>*.meta.yaml</code> -files. The zip-file should contain system outputs for all tasks/subtasks, maximum of 4 submissions per task/subtask, separate meta information for each system, and technical report(s) covering all submitted systems.</p>
<p>If you submit similar systems for multiple tasks, you can describe everything in one technical report. If your approaches for different tasks are significantly different, prepare one technical report for each and include it in the corresponding task folder.</p>
<p>More detailed instructions for constructing the package can be found in the following sections.
The technical report template is available <a href="/challenge2023/submission#template">here</a>.</p>
<p>Scripts for checking the content of the submission package is provided for selected tasks, please validate your submission package accordingly.   </p>
<p>For <strong>task 1</strong>, use validator code from repository  <div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/toni-heittola/dcase2023_task1_submission_validator" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/toni-heittola/dcase2023_task1_submission_validator" target="_blank">
<span style="font-size:20px;">DCASE2023 Task 1 <strong>submission validator</strong> <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
</p>
<p>For <strong>task 4</strong>, use validator script <code>task4/validate_submissions.py</code> from the example submission package  <div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2023/dcase2023_challenge_submission_package_example.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2023/dcase2023_challenge_submission_package_example.zip" target="_blank">
<span style="font-size:20px;">DCASE2023 challenge submission example package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(8.9 MB)</span>
<br/>
<span class="text-muted">
                
                
                (.zip)
                
                </span>
</div>
</div>
</p>
<p>For <strong>task 3</strong>, you can submit up to 4 systems per one of the two task tracks, up to 4 systems for models using audio-only input, and up to 4 systems for models using audio and video input. To make easier the distinction between the two tracks, please use <em>task_3a</em> for audio-only systems and <em>task_3b</em> for audiovisual systems. If you submit systems of both types, you can describe them in a single report, or even better a separate on systems of each type.</p>
<h2 id="submission-label">Submission label</h2>
<p>A submission label is used to index all your submissions (systems per tasks). To avoid overlapping labels among all submitted systems, use the following way to form your label:</p>
<p><code>[Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number][subtask letter (optional)]_[index number of your submission (1-4)]</code></p>
<p>For example, the baseline systems would have the following labels:</p>
<ul>
<li><code>Martin_TAU_task1_1</code></li>
<li><code>Dohi_HIT_task2_1</code></li>
<li><code>Politis_TAU_task3a_1</code></li>
<li><code>Shimada_SONY_task3b_1</code></li>
<li><code>Ronchini_INR_task4a_1</code></li>
<li><code>Martin_TAU_task4b_1</code></li>
<li><code>Morfi_QMUL_task5_1</code></li>
<li><code>Gontier_INR_task6a_1</code></li>
<li><code>Xie_TAU_task6b_1</code></li>
<li><code>Choi_GLI_task7a_1</code></li>
<li><code>Choi_GLI_task7b_1</code></li>
</ul>
<p>A script for checking the content of the submission package will be provided for selected tasks. In that case, please validate your submission package accordingly.   </p>
<h2 id="package-structure">Package structure</h2>
<p>Make sure your zip-package follows provided file naming convention and directory structure:</p>
<pre>
Zip-package root
â”‚  
â””â”€â”€â”€task1                                           Task 1 submissions
â”‚   â”‚   Martin_TAU_task1.technical_report.pdf       Technical report covering all subtasks
â”‚   â”‚
â”‚   â””â”€â”€â”€Martin_TAU_task1_1                          System 1 submission files
â”‚   â”‚       Martin_TAU_task1_1.meta.yaml            System 1 meta information
â”‚   â”‚       Martin_TAU_task1_1.output.csv           System 1 output
â”‚   :
â”‚   â””â”€â”€â”€Martin_TAU_task1_4                          System 4 submission files
â”‚           Martin_TAU_task1_4.meta.yaml            System 4 meta information
â”‚           Martin_TAU_task1_4.output.csv           System 4 output
â”‚                    
â””â”€â”€â”€task2                                           Task 2 submissions
â”‚   â”‚   Dohi_HIT_task2_1.technical_report.pdf       Technical report                       
â”‚   â”‚
â”‚   â””â”€â”€â”€Dohi_HIT_task2_1                            System 1 submission files
â”‚   â”‚     Dohi_HIT_task2_1.meta.yaml                System 1 meta information
â”‚   â”‚     anomaly_score_bandsaw_section_00_test.csv System 1 output for each section and domain in the evaluation dataset   
â”‚   â”‚     anomaly_score_grinder_section_00_test.csv        
â”‚   â”‚     anomaly_score_shaker_section_00_test.csv         
â”‚   :     :
â”‚   â”‚     anomaly_score_Vacuum_section_00_test.csv           
â”‚   â”‚     decision_result_bandsaw_section_00_test.csv           
â”‚   â”‚     decision_result_grinder_section_00_test.csv           
â”‚   â”‚     decision_result_shaker_section_00_test.csv            
â”‚   :     :
â”‚   â”‚     decision_result_Vacuum_section_00_test.csv           
â”‚   â”‚
â”‚   â””â”€â”€â”€Dohi_HIT_task2_4                            System 4 submission files
â”‚         Dohi_HIT_task2_4.meta.yaml                System 4 meta information
â”‚         anomaly_score_bandsaw_section_00_test.csv System 1 output for each section and domain in the evaluation dataset   
â”‚         anomaly_score_grinder_section_00_test.csv        
â”‚         anomaly_score_shaker_section_00_test.csv         
â”‚         :
â”‚         anomaly_score_Vacuum_section_00_test.csv           
â”‚         decision_result_bandsaw_section_00_test.csv           
â”‚         decision_result_grinder_section_00_test.csv           
â”‚         decision_result_shaker_section_00_test.csv            
â”‚         :
â”‚         decision_result_Vacuum_section_00_test.csv     
â”‚   
â””â”€â”€â”€task3                                           Task 3 submissions
â”‚   â”‚   Politis-Shimada_TAU-SONY_task3.technical_report.pdf     Technical report
â”‚   â”‚   Politis_TAU_task3a.technical_report.pdf                  (Optional) Technical report only for audio-only system (Track A)
â”‚   â”‚   Shimada_SONY_task3b.technical_report.pdf                 (Optional) Technical report only for audiovisual system (Track B)
â”‚   â”‚
â”‚   â””â”€â”€â”€Politis_TAU_task3a_1                        Track A (audio-only) System 1 submission files 
â”‚   â”‚     Politis_TAU_task3_1.meta.yaml             Track A (audio-only) System 1 meta information
â”‚   â””â”€â”€â”€â”€â”€Politis_TAU_task3_1                       Track A (audio-only) System 1 output files in a folder
|   |       mix001.csv
|   |       ...
â”‚   :
â”‚   â”‚
â”‚   â””â”€â”€â”€Politis_TAU_task3a_4                        Track A (audio-only) System 4 submission files
â”‚   |     Politis_TAU_task3_4.meta.yaml             Track A (audio-only) System 4 meta information
â”‚   â””â”€â”€â”€â”€â”€Politis_TAU_task3_4                       Track A (audio-only) System 4 output files in a folder
|   |       mix001.csv
|   |       ...
|   |
â”‚   â””â”€â”€â”€Shimada_SONY_task3b_1                       Track B (audiovisual) System 1 submission files
â”‚   â”‚     Shimada_SONY_task3b_1.meta.yaml           Track B (audiovisual) System 1 meta information
â”‚   â””â”€â”€â”€â”€â”€Shimada_SONY_task3b_1                     Track B (audiovisual) System 1 output files in a folder
|   |       mix001.csv
|   |       ...
â”‚   :
â”‚   â”‚
â”‚   â””â”€â”€â”€Shimada_SONY_task3b_4                       Track B (audiovisual) System 4 submission files (audiovisual)
â”‚   |     Shimada_SONY_task3b_4.meta.yaml           Track B (audiovisual) System 4 meta information (audiovisual)
â”‚   â””â”€â”€â”€â”€â”€Shimada_SONY_task3b_4                     Track B (audiovisual) System 4 output files in a folder (audiovisual)
|   |       mix001.csv
|   |       ...
â”‚
â””â”€â”€â”€task4                                           Task 4 submissions
â”‚   â”‚   Ronchini-Martin_PM-TAU_task4.technical_report.pdf  Technical report (joint report for subtasks A and B) 
â”‚   â”‚   Ronchini_PM_task4a.technical_report.pdf     (optional) Technical report for subtask A only
â”‚   â”‚   Martin_TAU_task4b.technical_report.pdf      (optional) Technical report for subtask B only
â”‚   â”‚   validate_submissions.py                     Submission validation code           
â”‚   â”‚   readme.md                                   Instructions how to use the submission validation code
â”‚   â”‚
â”‚   â””â”€â”€â”€Ronchini_PM_task4a_1                        Subtask A System 1 submission files
â”‚   â”‚     Ronchini_PM_task4a_1.meta.yaml            Subtask A System 1 meta information
â”‚   â”‚     Ronchini_PM_task4a_1.output.csv           Subtask A System 1 output
â”‚   :
â”‚   â”‚
â”‚   â””â”€â”€â”€Ronchini_PM_task4a_4                        Subtask A System 4 submission files
â”‚   |     Ronchini_PM_task4a_4.meta.yaml            Subtask A System 4 meta information
â”‚   |     Ronchini_PM_task4a_4.output.csv           Subtask A System 4 output
|   | 
â”‚   â””â”€â”€â”€Martin_TAU_task4b_1                         Subtask B System 1 submission files
â”‚   |     Martin_TAU_task4b_1.meta.yaml             Subtask B System 1 meta information
â”‚   |     Martin_TAU_task4b_1.output.csv            Subtask B System 1 output
â”‚   :
â”‚   â””â”€â”€â”€Martin_TAU_task4b_4                         Subtask B System 4 submission files
â”‚         Martin_TAU_task4b_4.meta.yaml             Subtask B System 4 meta information
â”‚         Martin_TAU_task4b_4.output.csv            Subtask B System 4 output
â”‚    
â””â”€â”€â”€task5                                           Task 5 submissions
â”‚   â”‚   Morfi_QMUL_task5.technical_report.pdf       Technical report
â”‚   â”‚
â”‚   â””â”€â”€â”€Morfi_QMUL_task5_1                          System 1 submission files
â”‚   â”‚     Morfi_QMUL_task5_1.meta.yaml              System 1 meta information
â”‚   â”‚     Morfi_QMUL_task5_1.output.csv             System 1 output
â”‚   :
â”‚   â”‚
â”‚   â””â”€â”€â”€Morfi_QMUL_task5_4                          System 4 submission files
â”‚         Morfi_QMUL_task5_4.meta.yaml              System 4 meta information
â”‚         Morfi_QMUL_task5_4.output.csv             System 4 output
â”‚
â””â”€â”€â”€task6                                           Task 6 submissions
â”‚   â”‚   Gontier_INR_task6_1.technical_report.pdf    Technical report (joint report for subtasks A and B) 
â”‚   â”‚   Gontier_INR_task6a_1.technical_report.pdf   (optional) Technical report for subtask A system only
â”‚   â”‚   Xie_TAU_task6b_1.technical_report.pdf       (optional) Technical report for subtask B system only
â”‚   â”‚
â”‚   â””â”€â”€â”€Gontier_INR_task6a_1                        Subtask A System 1 submission files
â”‚   â”‚     Gontier_INR_task6a_1.meta.yaml            Subtask A System 1 meta information
â”‚   â”‚     Gontier_INR_task6a_1.output.csv           Subtask A System 1 output
â”‚   :
â”‚   â”‚
â”‚   â””â”€â”€â”€Gontier_INR_task6a_4                        Subtask A System 4 submission files
â”‚   â”‚     Gontier_INR_task6a_4.meta.yaml            Subtask A System 4 meta information
â”‚   â”‚     Gontier_INR_task6a_4.output.csv           Subtask A System 4 output
â”‚   â”‚
â”‚   â””â”€â”€â”€Xie_TAU_task6b_1                            Subtask B System 1 submission files
â”‚   â”‚     Xie_TAU_task6b_1.meta.yaml                Subtask B System 1 meta information
â”‚   â”‚     Xie_TAU_task6b_1.output.csv               Subtask B System 1 output
â”‚   :
â”‚   â”‚
â”‚   â””â”€â”€â”€Xie_TAU_task6b_4                            Subtask B System 4 submission files
â”‚         Xie_TAU_task6b_4.meta.yaml                Subtask B System 4 meta information
â”‚         Xie_TAU_task6b_4.output.csv               Subtask B System 4 output
â”‚ 
â””â”€â”€â”€task7                                           Task 7 submissions
    â”‚   Choi_GLI_task7.technical_report.pdf         Technical report 
    â”‚
    â””â”€â”€â”€Choi_GLI_task7a_1                           Track A (with external resources) System 1 submission files
    â”‚     Choi_GLI_task7a_1.meta.yaml               Track A (with external resources) System 1 meta information
    :
    â””â”€â”€â”€Choi_GLI_task7a_4                           Track A (with external resources) System 4 submission files
    â”‚     Choi_GLI_task7a_4.meta.yaml               Track A (with external resources) System 4 meta information
    â”‚
    â””â”€â”€â”€Choi_GLI_task7b_1                           Track B (without external resources) System 1 submission files
    â”‚     Choi_GLI_task7b_1.meta.yaml               Track B (without external resources) System 1 meta information

    â””â”€â”€â”€Choi_GLI_task7b_4                           Track B (without external resources) System 4 submission files
          Choi_GLI_task7b_4.meta.yaml               Track B (without external resources) System 4 meta information

</pre>
<h1 id="system-outputs">System outputs</h1>
<p>Participants must submit the results for the provided evaluation datasets.</p>
<ul>
<li>
<p><strong>Follow the system output format specified in the task description.</strong></p>
</li>
<li>
<p><strong>Tasks are independent.</strong> You can participate in a single task or multiple tasks.</p>
</li>
<li>
<p><strong>Multiple submissions for the same task</strong> are allowed (maximum 4 per task). Use a running index in the submission label, and give more detailed names for the submitted systems in the system meta information files. Please mark carefully the connection between the submitted systems and system parameters description in the technical report (for example by referring to the systems by using the submission label or system name given in the system meta information file).</p>
</li>
<li>
<p><strong>Submitted system outputs will be published</strong> online on the DCASE2023 website later to allow future evaluations.</p>
</li>
</ul>
<h1 id="meta-information">Meta information</h1>
<p>In order to enable fast processing of the submissions and meta analysis of submitted systems, participants should provide meta information presented in a structured and correctly formatted <a href="https://en.wikipedia.org/wiki/YAML">YAML-file</a>. Participants are advised to fill in the meta information carefully while making sure all asked information is correctly provided.  </p>
<p>A complete meta file will help us notice possible errors before officially publishing the results (for example unexpectedly large difference in performance between development and evaluation set) and allow contacting the authors in case we consider it necessary. Please note that task organizers may ask you to update the meta file after the challenge submission deadline.   </p>
<p>See the example meta files below for each baseline system. These examples are also available in the <a href="../documents/challenge2023/dcase2023_challenge_submission_package_example.zip">example submission package</a>. Meta file structure is mostly the same for all tasks, only the metrics collected in <code>results-&gt;development_dataset</code>-section differ per challenge task.</p>
<div aria-multiselectable="true" class="panel-group" id="submission-package-example-accordion" role="tablist">
<div class="panel panel-primary">
<div class="panel-heading" id="task1-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task1-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-primary"></i>
<i class="fa dc-scene fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 1 - Low-Complexity Acoustic Scene Classification
                </a>
</h4>
</div>
<div aria-labelledby="task1-example-header" class="panel-collapse collapse" id="task1-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 1 baseline system <code>task1/Martin_TAU_task1_1/Martin_TAU_task1_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Martin_TAU_task1_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2022 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: MartÃ­n MoratÃ³
      firstname: Irene
      email: irene.martinmorato@tuni.fi           # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Second author
    - lastname: Heittola
      firstname: Toni
      email: toni.heittola@tuni.fi                # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Third author
    - lastname: Mesaros
      firstname: Annamaria
      email: annamaria.mesaros@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

    # Fourth author
    - lastname: Virtanen
      firstname: Tuomas
      email: tuomas.virtanen@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input / sampling rate
    # e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 44.1kHz

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: log-mel energies

    # Embeddings
    # e.g. VGGish, OpenL3, ...
    embeddings: !!null

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    # one or multiple, e.g. GMM, HMM, SVM, MLP, CNN, RNN, CRNN, ResNet, ensemble, ...
    machine_learning_method: CNN

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

    # Decision making methods
    # e.g. "average", "majority vote", "maximum likelihood", ...
    decision_making: !!null

    # External data usage method
    # e.g. "directly", "embeddings", "pre-trained model", ...
    external_data_usage: embeddings

    # Method for handling the complexity restrictions
    # e.g. "weight quantization", "sparsity", "pruning", ...
    complexity_management: weight quantization

    # System training/processing pipeline stages
    # e.g. "pretraining", "training" (from scratch), "pruning", "weight quantization", ...
    pipeline: pretraining, training, adaptation, pruning, weight quantization

    # Machine learning framework
    # e.g. keras/tensorflow, pytorch, matlab, ...
    framework: keras/tensorflow

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total model size in bytes. Calculated as [parameter count]*[bit per parameter]/8
    total_model_size: 46512 # B

    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value.
    total_parameters: 46512

    # Total amount of non-zero parameters in the acoustic model.
    # Calculated with same principles as "total_parameters".
    # Use numerical value.
    total_parameters_non_zero: 46512

    # Model size calculated using NeSsi, as instructed in task description page.
    # Use numerical value
    memory_use: 65280 # B

    # MACS
    # Required for the submission ranking!
    macs: 29234920

  energy_consumption:
    # Energy consumption while training the model. Unit is kWh.
    training: 0.302 #kWh

    # Energy consumption while producing output for all files in the evaluation dataset. Unit is kWh.
    inference: 0.292 #kWh

    # Baseline system's energy consumption while producing output for all files in the evaluation dataset. Unit is kWh.
    # Run baseline code to get this value. Value is used to normalize training and inference values from your system.
    baseline_inference: 0.292 #kWh

  # List of external datasets used in the submission.
  # Development dataset is used here only as example, list only external datasets
  external_datasets:
    # Below an example how to fill the field as a list of datasets
    # Dataset name
    #- name: TAU Urban Acoustic Scenes 2022 Mobile, Development dataset
    #  # Dataset access url
    #  url: https://zenodo.org/record/6337421
    #  # Total audio length in minutes
    #  total_audio_length: 2400            # minutes

  # URL to the source code of the system [optional]
  source_code: https://github.com/marmoi/dcase2022_task1_baseline

# System results
results:
  development_dataset:
    # System results for development dataset with provided the cross-validation setup.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.

    # Overall metrics
    overall:
      logloss: 1.575
      accuracy: 42.9    # mean of class-wise accuracies

    # Class-wise metrics
    class_wise:
      airport:
        logloss: 1.534
        accuracy: 39.4
      bus:
        logloss: 1.758
        accuracy: 29.3
      metro:
        logloss: 1.382
        accuracy: 47.9
      metro_station:
        logloss: 1.672
        accuracy: 36.0
      park:
        logloss: 1.448
        accuracy: 58.9
      public_square:
        logloss: 2.265
        accuracy: 20.8
      shopping_mall:
        logloss: 1.385
        accuracy: 51.4
      street_pedestrian:
        logloss: 1.822
        accuracy: 30.1
      street_traffic:
        logloss: 1.025
        accuracy: 70.6
      tram:
        logloss: 1.462
        accuracy: 44.6

    # Device-wise
    device_wise:
      a:
        logloss: 1.109
        accuracy: !!null
      b:
        logloss: 1.439
        accuracy: !!null
      c:
        logloss: 1.374
        accuracy: !!null
      s1:
        logloss: 1.621
        accuracy: !!null
      s2:
        logloss: 1.559
        accuracy: !!null
      s3:
        logloss: 1.531
        accuracy: !!null
      s4:
        logloss: 1.813
        accuracy: !!null
      s5:
        logloss: 1.800
        accuracy: !!null
      s6:
        logloss: 1.931
        accuracy: !!null
</code></pre>
</div>
</div>
</div>
<div class="panel panel-success">
<div class="panel-heading" id="task2-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task2-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa dc-large-scale fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 2 - First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring
                </a>
</h4>
</div>
<div aria-labelledby="task2-example-header" class="panel-collapse collapse" id="task2-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 2 baseline system <code>task2/Dohi_HIT_task2_1/Dohi_HIT_task2_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Dohi_HIT_task2_1

  # Submission name
  # This name will be used in the results tables when space permits.
  name: DCASE2023 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use a maximum of 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system.
  # Mark authors in the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author, this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Dohi
      firstname: Kota
      email: kota.dohi.gr@hitachi.com # Contact email address
      corresponding: true # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        institution: Hitachi, Ltd.
        department: Research and Development Group # Optional
        location: Tokyo, Japan

    # Second author
    - lastname: Imoto
      firstname: Keisuke
      email: keisuke.imoto@ieee.org

      # Affiliation information for the author
      affiliation:
        institution: Doshisha University
        location: Kyoto, Japan

    # Third author
    - lastname: Koizumi
      firstname: Yuma
      email: koizumi.yuma@ieee.org

      # Affiliation information for the author
      affiliation:
        institution: Google LLC
        location: Tokyo, Japan

# System information
system:
  # System description, metadata provided here will be used to do a meta-analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
    # Audio input
    # Please specify all sampling rates (comma-separated list).
    # e.g. 16kHz, 22.05kHz, 44.1kHz
    input_sampling_rate: 16kHz

    # Data augmentation methods
    # Please specify all methods used (comma-separated list).
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

    # Front-end (preprocessing) methods
    # Please specify all methods used (comma-separated list).
    # e.g. HPSS, WPE, NMF, NN filter, RPCA, ...
    front_end: !!null

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: log-mel energies

    # Embeddings
    # Please specify all pre-trained embedings used (comma-separated list).
    # one or multiple, e.g. VGGish, OpenL3, ...
    embeddings: !!null

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma-separated list).
    # e.g. AE, VAE, GAN, GMM, k-means, OCSVM, normalizing flow, CNN, LSTM, random forest, ensemble, ...
    machine_learning_method: AE

    # Method for aggregating predictions over time
    # Please specify all methods used (comma-separated list).
    # e.g. average, median, maximum, minimum, ...
    aggregation_method: average

    # Method for domain generalizatoin and domain adaptation
    # Please specify all methods used (comma-separated list).
    # e.g. fine-tuning, invariant feature extraction, ...
    domain_adaptation_method: !!null
    domain_generalization_method: !!null

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

    # Decision making in ensemble
    # e.g. average, median, maximum, minimum, ...
    decision_making: !!null

    # Usage of the attribute information in the file names and attribute csv files
    # Please specify all usages (comma-separated list).
    # e.g. interpolation, extrapolation, condition ...
    attribute_usage: !!null

    # External data usage method
    # Please specify all usages (comma-separated list).
    # e.g. simulation of anomalous samples, embeddings, pre-trained model, ...
    external_data_usage: !!null

    # Usage of the development dataset
    # Please specify all usages (comma-separated list).
    # e.g. development, pre-training, fine-tuning
    development_data_usage: development

  # System complexity, metadata provided here may be used to evaluate submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding extraction networks and classification network.
    # Use numerical value.
    total_parameters: 269992

  # List of external datasets used in the submission.
  # Development dataset is used here only as an example, list only external datasets
  external_datasets:
    # Dataset name
    - name: DCASE 2023 Challenge Task 2 Development Dataset

      # Dataset access URL
      url: https://zenodo.org/record/7690157

  # URL to the source code of the system [optional, highly recommended]
  # Reproducibility will be used to evaluate submitted systems.
  source_code: https://github.com/nttcslab/dcase2023_task2_baseline_ae

# System results
results:
  development_dataset:
    # System results for development dataset.
    # Full results are not mandatory, however, they are highly recommended as they are needed for a thorough analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete results can be reported.

    # AUC for all domains [%]
    # No need to round numbers
    ToyCar:
      auc_source: 70.10
      auc_target: 46.89
      pauc: 52.47

    ToyTrain:
      auc_source: 57.93
      auc_target: 57.02
      pauc: 48.57

    fan:
      auc_source: 80.19
      auc_target: 36.18
      pauc: 59.04

    gearbox:
      auc_source: 60.31
      auc_target: 60.69
      pauc: 53.22

    bearing:
      auc_source: 65.92
      auc_target: 55.75
      pauc: 50.42

    slider:
      auc_source: 70.31
      auc_target: 48.77
      pauc: 56.37

    valve:
      auc_source: 55.35
      auc_target: 50.69
      pauc: 51.18
</code></pre>
</div>
</div>
</div>
<div class="panel panel-warning">
<div class="panel-heading" id="task3-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task3-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-warning"></i>
<i class="fa dc-localization fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 3 - Sound Event Localization and Detection Evaluated in Real Spatial Sound Scenes (2 tracks)
                </a>
</h4>
</div>
<div aria-labelledby="task3-example-header" class="panel-collapse collapse" id="task3-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 3 baseline system <code>task3/Politis_TAU_task3a_1/Politis_TAU_task3a_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions, to avoid overlapping codes among submissions
  # use following way to form your label:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Politis_TAU_task3a_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2023 Audio-only Ambisonic baseline

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight, maximum 10 characters
  abbreviation: FOA_AO_base

  # Submission authors in order, mark one of the authors as corresponding author.
  authors:
    # First author
    - lastname: Politis
      firstname: Archontis
      email: archontis.politis@tuni.fi                  # Contact email address
      corresponding: true                             	# Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Audio Research Group
        location: Tampere, Finland

    # Second author
    - lastname: Shimada
      firstname: Kazuki
      email: kazuki.shimada@sony.com                   # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: SONY
        institute: SONY
        department:
        location: Tokyo, Japan


# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system. Use general level tags, if possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
  
    # Model type (audio-only or audiovisual track)
    model_type: Audio                       # Audio or Audiovisual

    # Audio input
    input_format: Ambisonic                 # Ambisonic or Microphone Array or both
    input_sampling_rate: 24kHz

    # Acoustic representation
    acoustic_features: mel spectra, intensity vector   # e.g one or multiple [phase and magnitude spectra, mel spectra, GCC-PHAT, TDOA, intensity vector ...]
    visual_features: !!null

    # Data augmentation methods
    data_augmentation: !!null             	# [time stretching, block mixing, pitch shifting, ...]

    # Machine learning
    # In case of using ensemble methods, please specify all methods used (comma separated list).
    machine_learning_method: CRNN, MHSA     # e.g one or multiple [GMM, HMM, SVM, kNN, MLP, CNN, RNN, CRNN, NMF, MHSA, random forest, ensemble, ...]
    
    #List external datasets in case of use for training
    external_datasets:  !!null              #AudioSet, ImageNet...

    #List here pre-trained models in case of use
    pre_trained_models: !!null              #AST, PANNs...


  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:

    # Total amount of parameters used in the acoustic model. For neural networks, this
    # information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available,
    # try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    total_parameters: 500000

  # URL to the source code of the system [optional]
  source_code: https://github.com/sharathadavanne/seld-dcase2023

# System results
results:

  development_dataset:
    # System result for development dataset on the provided testing split.

    # Overall score 
    overall:
      ER_20: 0.71
      F_20: 21.0
      LE_CD: 29.3
      LR_CD: 46.0
</code></pre>
</div>
</div>
</div>
<div class="panel panel-info">
<div class="panel-heading" id="task4a-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task4a-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-info"></i>
<i class="fa dc-domestic fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 4A - Sound Event Detection with Weak Labels and Synthetic Soundscapes
                </a>
</h4>
</div>
<div aria-labelledby="task4a-example-header" class="panel-collapse collapse" id="task4a-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 4 baseline system <code>task4/Ronchini_PM_task4a_1/Ronchini_PM_task4a_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions, to avoid overlapping codes among submissions
  # use following way to form your label:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Turpault_INR_task4_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2022 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight, maximum 10 characters
  abbreviation: Baseline

  # Submission authors in order, mark one of the authors as corresponding author.
  authors:
    # First author
    - lastname: Turpault
      firstname: Nicolas
      email: nicolas.turpault@inria.fr                # Contact email address
      corresponding: true                             # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: INR
        institute: Inria Nancy Grand-Est
        department: Department of Natural Language Processing &amp; Knowledge Discovery
        location: Nancy, France

    # Second author
    - lastname: Serizel
      firstname: Romain
      email: romain.serizel@loria.fr                  # Contact email address


      # Affiliation information for the author
      affiliation:
        abbreviation: ULO
        institute: University of Lorraine, Loria
        department: Department of Natural Language Processing &amp; Knowledge Discovery
        location: Nancy, France

        #...



# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system. Use general level tags, if possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input
    input_channels: mono                  # e.g. one or multiple [mono, binaural, left, right, mixed, ...]
    input_sampling_rate: 16               # In kHz

    # Acoustic representation
    acoustic_features: log-mel energies   # e.g one or multiple [MFCC, log-mel energies, spectrogram, CQT, ...]

    # Data augmentation methods
    data_augmentation: !!null             # [time stretching, block mixing, pitch shifting, ...]

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    machine_learning_method: CRNN      # e.g one or multiple [GMM, HMM, SVM, kNN, MLP, CNN, RNN, CRNN, NMF, random forest, ensemble, ...]

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    ensemble_method_subsystem_count: !!null # [2, 3, 4, 5, ... ]

    # Decision making methods
    decision_making: !!null                 # [majority vote, ...]

    # Semi-supervised method used to exploit both labelled and unlabelled data
    machine_learning_semi_supervised: mean-teacher student         # e.g one or multiple [pseudo-labelling, mean-teacher student...]

    # Segmentation method
    segmentation_method: !!null					            # E.g. [RBM, attention layers...]

    # Post-processing, followed by the time span (in ms) in case of smoothing
    post-processing: median filtering (93ms)				# [median filtering, time aggregation...]

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:

    # Total amount of parameters used in the acoustic model. For neural networks, this
    # information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available,
    # try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    total_parameters: 1112420
    MACS: 44.683 G
    # Approximate training time followed by the hardware used
    trainining_time: 3h (1 GTX 1080 Ti)
    # Model size in MB
    model_size: 4.5
  #Report here the energy consumption measured with e.g. codecarbon
  energy_consumption:
    training: 1.717
    test: 0.030
    #Energy consumption of the baseline (10 epochs) on your hardware
    baseline: 0.02


  # The training subsets used to train the model. Followed the amount of data (number of clips) used per subset.
  subsets: 				# [weak (xx), unlabel_in_domain (xx), synthetic (xx), FUSS (xx)...]

  #List here the external datasets you used for training
  external_datasets:  #AudioSet, ImageNet...

  #List here the pre-trained models you used
  pre_trained_models: #AST, PANNs...

  # URL to the source code of the system [optional, highly recommended]
  source_code: https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline

# System results
results:
  # Full results are not mandatory, but for through analysis of the challenge submissions recommended.
  # If you cannot provide all results, also incomplete results can be reported.

  development_dataset:
    # System result for development dataset with provided the cross-validation setup.
    overall:
      PSDS1: 0.420
      PSDS2: 0.610
</code></pre>
</div>
</div>
</div>
<div class="panel panel-info">
<div class="panel-heading" id="task4b-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task4b-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-info"></i>
<i class="fa dc-domestic fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 4B - Sound Event Detection with Soft Labels
                </a>
</h4>
</div>
<div aria-labelledby="task4b-example-header" class="panel-collapse collapse" id="task4b-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 4 baseline system <code>task4/Martin_TAU_task4b_1/Martin_TAU_task4b_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Martin_TAU_task4b_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2023 task4b baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: Baseline_task4b

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: MartÃ­n MoratÃ³
      firstname: Irene
      email: irene.martinmorato@tuni.fi           # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Second author
    - lastname: Mesaros
      firstname: Annamaria
      email: annamaria.mesaros@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

    # Third author
    - lastname: Heittola
      firstname: Toni
      email: toni.heittola@tuni.fi                # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland


# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input / sampling rate
    input_channels: mono
    # e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 44.1kHz

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: mel energies

    # Data augmentation methods
    # time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null


    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    # one or multiple, e.g. GMM, HMM, SVM, MLP, CNN, RNN, CRNN, ResNet, ensemble, ...
    machine_learning_method: CRNN

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

    # Decision making methods
    # e.g. average, majority vote, maximum likelihood, ...
    decision_making: !!null

    # Semi-supervised method used to exploit both labelled and unlabelled data
    # e.g one or multiple [pseudo-labelling, mean-teacher student...]
    machine_learning_semi_supervised: !!null

    # Segmentation method
    # E.g. [RBM, attention layers...]
    segmentation_method: !!null

    # Post-processing, followed by the time span (in ms) in case of smoothing
    # [median filtering, time aggregation...]
    post-processing: !!null



  # The training subsets used to train the model. Followed the amount of data (number of clips) used per subset.
  # [weak (xx), unlabel_in_domain (xx), synthetic (xx), FUSS (xx)...]
  subsets: !!null

  #List here the external datasets you used for training
  #AudioSet, ImageNet...
  external_datasets: !!null

  #List here the pre-trained models you used
  #AST, PANNs..
  pre_trained_models: !!null.

  # URL to the source code of the system [optional]
  source_code: https://github.com/marmoi/dcase2023_task4b_baseline

# System results
results:
  development_dataset:
    # System results for development dataset with provided the cross-validation setup.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.

    # Overall metrics
    overall:
      ER_m: 0.487
      F1_m: 70.34    # segment-based 1 second for all the test folds
      F1_M: 35.83
      F1_MO: 42.87
</code></pre>
</div>
</div>
</div>
<div class="panel panel-danger">
<div class="panel-heading" id="task5-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task5-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-danger"></i>
<i class="fa dc-bird fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 5 - Few-shot Bioacoustic Event Detection
                </a>
</h4>
</div>
<div aria-labelledby="task5-example-header" class="panel-collapse collapse" id="task5-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 5 baseline system <code>task5/Morfi_QMUOL_task5_1/Morfi_QMUOL_task5_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions, to avoid overlapping codes among submissions
  # use the following way to form your label:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Morfi_QMUL_task5_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: Cross-correlation baseline

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight, maximum 10 characters
  abbreviation: xcorr_base

  # Submission authors in order, mark one of the authors as corresponding author.
  authors:
    # First author
    - lastname: Morfi
      firstname: Veronica
      email: g.v.morfi@qmul.ac.uk                     # Contact email address
      corresponding: true                             # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: QMUL
        institute: Queen Mary University of London
        department: Centre for Digital Music
        location: London, UK

    # Second author
    - lastname: Stowell
      firstname: Dan
      email: dan.stowell@qmul.ac.uk                  # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: QMUL
        institute: Queen Mary University of London
        department: Centre for Digital Music
        location: London, UK

        #...


# System information
system:
  # SED system description, meta data provided here will be used to do
  # meta analysis of the submitted system. Use general level tags, if possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input
    input_sampling_rate: any               # In kHz

    # Acoustic representation
    acoustic_features: spectrogram   # e.g one or multiple [MFCC, log-mel energies, spectrogram, CQT, PCEN, ...]

    # Data augmentation methods
    data_augmentation: !!null             # [time stretching, block mixing, pitch shifting, ...]

    # Embeddings
    # e.g. VGGish, OpenL3, ...
    embeddings: !!null

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    machine_learning_method: template matching         # e.g one or multiple [GMM, HMM, SVM, kNN, MLP, CNN, RNN, CRNN, NMF, random forest, ensemble, transformer, ...]
    # the system adaptation for "few shot" scenario.
    # For example, if machine_learning_method is "CNN", the few_shot_method might use one of [fine tuning, prototypical, MAML] in addition to the standard CNN architecture.
    few_shot_method: template matching         # e.g [fine tuning, prototypical, MAML, nearest neighbours...]

    # External data usage method
    # e.g. directly, embeddings, pre-trained model, ...
    external_data_usage: !!null

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    ensemble_method_subsystem_count: !!null # [2, 3, 4, 5, ... ]

    # Decision making methods (for ensemble)
    decision_making: !!null                 # [majority vote, ...]

    # Post-processing, followed by the time span (in ms) in case of smoothing
    post-processing: peak picking, threshold				# [median filtering, time aggregation...]

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:

    # Total amount of parameters used in the acoustic model. For neural networks, this
    # information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available,
    # try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    total_parameters: !!null    # note that for simple template matching, the "parameters"==the pixel count of the templates, plus 1 for each param such as thresholding. 
    # Approximate training time followed by the hardware used
    trainining_time: !!null
    # Model size in MB
    model_size: !!null


  # URL to the source code of the system [optional, highly recommended]
  source_code:   

  # List of external datasets used in the submission.
  # A previous DCASE development dataset is used here only as example! List only external datasets
  external_datasets:
    # Dataset name
    - name: !!null
      # Dataset access url
      url: !!null
      # Total audio length in minutes
      total_audio_length: !!null            # minutes

# System results 
results:
  # Full results are not mandatory, but for through analysis of the challenge submissions recommended.
  # If you cannot provide all result details, also incomplete results can be reported.
  validation_set:
    overall:
      F-score: 2.01 # percentile

    # Per-dataset
    dataset_wise:
      HV:
        F-score: 1.22 #percentile
      PB:
        F-score: 5.84 #percentile

</code></pre>
</div>
</div>
</div>
<div class="panel panel-task1">
<div class="panel-heading" id="task6a-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task6a-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-task1"></i>
<i class="fa dc-captioning fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 6A - Automated Audio Captioning
                </a>
</h4>
</div>
<div aria-labelledby="task6a-example-header" class="panel-collapse collapse" id="task6a-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 6 baseline system <code>task6/Gontier_INR_task6a_1/Gontier_INR_task6a_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information for task 6, subtask A
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: gontier_inr_task6a_1
  #
  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2022 baseline system
  #
  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
abbreviation: Baseline

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Gontier
      firstname: Felix
      email: felix.gontier@inria.fr               # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: INRIA
        institute: INRIA
        department: Multispeech            # Optional
        location: Nancy, France

    # Second author...

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input / sampling rate
    # e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 16kHz

    # Acoustic representation
    # Here you should indicate what can or audio representation
    # you used. If your system used hand-crafted features (e.g.
    # mel band energies), then you can do:
    #
    # `acoustic_features: mel energies`
    #
    # Else, if you used some pre-trained audio feature extractor, 
    # you can indicate the name of the system, for example:
    #
    # `acoustic_features: audioset`
    acoustic_features: VGGish

    # Word embeddings
    # Here you can indicate how you treated word embeddings.
    # If your method learned its own word embeddings (i.e. you
    # did not used any pre-trained word embeddings) then you can
    # do:
    #
    # `word_embeddings: learned`
    #  
    # Else, specify the pre-trained word embeddings that you used
    # (e.g. Word2Vec, BERT, etc).
    word_embeddings: BART

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

    # Method scheme
    # Here you should indicate the scheme of the method that you
    # used. For example:
    machine_learning_method: encoder-decoder

    # Learning scheme
    # Here you should indicate the learning scheme. 
    # For example, you could specify either
    # supervised, self-supervised, or even 
    # reinforcement learning. 
    learning_scheme: supervised

    # Ensemble
    # Here you should indicate if you used ensemble
    # of systems or not.
    ensemble: No

    # Audio modelling
    # Here you should indicate the type of system used for
    # audio modelling. For example, if you used some stacked CNNs, then
    # you could do:
    #
    # audio_modelling: cnn
    #
    # If you used some pre-trained system for audio modelling,
    # then you should indicate the system used (e.g. COALA, COLA,
    # transfomer).
    audio_modelling: transformer

    # Word modelling
    # Similarly, here you should indicate the type of system used
    # for word modelling. For example, if you used some RNNs,
    # then you could do: 
    #
    # word_modelling: rnn
    #
    # If you used some pre-trained system for word modelling,
    # then you should indicate the system used (e.g. transfomer).
    word_modelling: transformer

    # Loss function
    # Here you should indicate the loss fuction that you employed.
    loss_function: crossentropy

    # Optimizer
    # Here you should indicate the name of the optimizer that you
    # used. 
    optimizer: adamw

    # Learning rate
    # Here you should indicate the learning rate of the optimizer
    # that you used.
    leasrning_rate: 1e-5

    # Gradient clipping
    # Here you should indicate if you used any gradient clipping. 
    # You do this by indicating the value used for clipping. Use
    # 0 for no clipping.
    gradient_clipping: 0

    # Gradient norm
    # Here you should indicate the norm of the gradient that you
    # used for gradient clipping. This field is used only when 
    # gradient clipping has been employed.
    gradient_norm: !!null

    # Metric monitored
    # Here you should report the monitored metric
    # for optimizing your method. For example, did you
    # monitored the loss on the validation data (i.e. validation
    # loss)? Or you monitored the SPIDEr metric? Maybe the training
    # loss?
    metric_monitored: validation_loss

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value (do not use comma for thousands-separator).
    total_parameters: 140000000

  # List of external datasets used in the submission.
  # Development dataset is used here only as example, list only external datasets
  external_datasets:
    # Dataset name
    - name: Clotho

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.3490683

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: Yes

      # Number of captions per audio
      nb_captions_per_audio: 5

      # Total amount of examples used
      total_audio_length: 24430

      # Used for (e.g. audio_modelling, word_modelling, audio_and_word_modelling)
      used_for: audio_and_word_modelling

  # URL to the source code of the system [optional]
      source_code: https://github.com/audio-captioning/dcase-2021-baseline

# System results
results:
  development_evaluation:
    # System results for development evaluation split.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.
    bleu1: 0.555
    bleu2: 0.358
    bleu3: 0.239
    bleu4: 0.156
    rougel: 0.364
    meteor: 0.164
    cider: 0.358
    spice: 0.109
    spider: 0.233
</code></pre>
</div>
</div>
</div>
<div class="panel panel-task1">
<div class="panel-heading" id="task6b-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task6b-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-task1"></i>
<i class="fa dc-captioning fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 6B - Language-Based Audio Retrieval
                </a>
</h4>
</div>
<div aria-labelledby="task6b-example-header" class="panel-collapse collapse" id="task6b-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 6 baseline system <code>task6/Drossos_TAU_task6b_1/Xie_TAU_task6b_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information for task 6 - subtask B
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: xie_tau_task6b_1
  #
  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2022 baseline system
  #
  # Submission name abbreviated
  # This abbreviated name will be used in the result table when space is tight.
  # Use maximum 10 characters.
abbreviation: Baseline

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Xie
      firstname: Huang
      email: huang.xie@tuni.fi                    # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Second author
    - lastname: Lipping
      firstname: Samuel
      email: samuel.lipping@tuni.fi                # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Third author
    - lastname: Virtanen
      firstname: Tuomas
      email: tuomas.virtanen@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

# System information
system:
  # System description, meta-data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input / sampling rate
    # e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 44.1kHz

    # Acoustic representation
    # Here you should indicate what can or audio representation
    # you used. If your system used hand-crafted features (e.g.
    # mel band energies), then you can do:
    #
    # `acoustic_features: mel energies`
    #
    # Else, if you used some pre-trained audio feature extractor, 
    # you can indicate the name of the system, for example:
    #
    # `acoustic_features: audioset`
    acoustic_features: log-mel energies

    # Word embeddings
    # Here you can indicate how you treated word embeddings.
    # If your method learned its own word embeddings (i.e. you
    # did not use any pre-trained word embeddings) then you can
    # do:
    #
    # `word_embeddings: learned`
    #  
    # Else, specify the pre-trained word embeddings that you used
    # (e.g. Word2Vec, BERT, etc).
    word_embeddings: Word2Vec

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

    # Method scheme
    # Here you should indicate the scheme of the method that you
    # used. For example:
    machine_learning_method: cross-modal alignment

    # Learning scheme
    # Here you should indicate the learning scheme. 
    # For example, you could specify either
    # supervised, self-supervised, or even 
    # reinforcement learning. 
    learning_scheme: self-supervised

    # Ensemble
    # Here you should indicate if you used ensemble
    # of systems or not.
    ensemble: No

    # Audio modelling
    # Here you should indicate the type of system used for
    # audio modelling. For example, if you used some stacked CNNs, then
    # you could do:
    #
    # audio_modelling: cnn
    #
    # If you used some pre-trained system for audio modelling,
    # then you should indicate the system used (e.g. COALA, COLA,
    # transformer).
    audio_modelling: crnn

    # Word modelling
    # Similarly, here you should indicate the type of system used
    # for word modelling. For example, if you used some RNNs,
    # then you could do: 
    #
    # word_modelling: rnn
    #
    # If you used some pre-trained system for word modelling,
    # then you should indicate the system used (e.g. transformer).
    word_modelling: word2vec

    # Loss function
    # Here you should indicate the loss function that you employed.
    loss_function: triplet loss

    # Optimizer
    # Here you should indicate the name of the optimizer that you
    # used. 
    optimizer: adam

    # Learning rate
    # Here you should indicate the learning rate of the optimizer
    # that you used.
    learning_rate: 1e-3

    # Gradient clipping
    # Here you should indicate if you used any gradient clipping. 
    # You do this by indicating the value used for clipping. Use
    # 0 for no clipping.
    gradient_clipping: 0

    # Gradient norm
    # Here you should indicate the norm of the gradient that you
    # used for gradient clipping. This field is used only when 
    # gradient clipping has been employed.
    gradient_norm: !!null

    # Metric monitored
    # Here you should report the monitored metric
    # for optimizing your method. For example, did you
    # monitored the loss on the validation data (i.e. validation
    # loss)? Or you monitored the SPIDEr metric? Maybe the training
    # loss?
    metric_monitored: validation_loss

  # System complexity, meta-data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value (do not use comma for thousands-separator).
    total_parameters: 732354

  # List of datasets used for training the system.
  # Development-training data is used here only as example.
  training_datasets:
    - name: Clotho-development

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.4783391

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: Yes

      # Number of captions per audio
      nb_captions_per_audio: 5

      # Number of audio clips per caption
      nb_clips_per_caption: 1

      # Total amount durations (in seconds) of audio used
      total_audio_length: 86353

      # Total amount of captions used
      total_captions: 3839

  # List of datasets used for validating the system, for example, optimizing hyperparameter.
  # Development-validation data is used here only as example.
  validation_datasets:
    - name: Clotho-validation

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.4783391

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: Yes

      # Number of captions per audio
      nb_captions_per_audio: 5

      # Number of audio clips per caption
      nb_clips_per_caption: 1

      # Total amount durations (in seconds) of audio used
      total_audio_length: 23636

      # Total amount of captions used
      total_captions: 1045

  # List of external datasets used in the submission.
  # Development dataset is used here only as example, list only external datasets
  external_datasets:
    # Dataset name
    - name: Clotho

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.4783391

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: Yes

      # Number of captions per audio
      nb_captions_per_audio: 5

      # Number of audio clips per caption
      nb_clips_per_caption: 1

      # Total amount durations (in seconds) of audio used
      total_audio_length: 133442

      # Total amount of captions used
      total_captions: 29645

      # Used for (e.g. audio_modelling, word_modelling, audio_and_word_modelling)
      used_for: audio_and_word_modelling

      # URL to the source code of the system [optional]
      source_code: https://github.com/xieh97/dcase2022-audio-retrieval

# System results
results:
  development_testing:
    # System results for development testing split.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.
    R@1: 0.03
    R@5: 0.11
    R@10: 0.19
    mAP@10: 0.07
</code></pre>
</div>
</div>
</div>
<div class="panel panel-task2">
<div class="panel-heading" id="task7-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#submission-package-example-accordion" data-toggle="collapse" href="#task7-example-collapse" role="button">
<span class="fa-stack fa-1x">
<i class="fa fa-square fa-stack-2x text-task2"></i>
<i class="fa dc-synthesis fa-stack-1x fa-inverse"></i>
</span>                   
                    Task 7 - Foley Sound Synthesis (2 tracks)
                </a>
</h4>
</div>
<div aria-labelledby="task7-example-header" class="panel-collapse collapse" id="task7-example-collapse" role="tabpanel">
<div class="panel-body">
<p>Example meta information file for Task 7 baseline system <code>task7/Choi_GLI_task7a_1/Choi_GLI_task7a_1.meta.yaml</code>:
                </p>
<pre class="font110"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task7_track[track]_[index number of your submission (1-4)]
  label: Choi_GLI_task7_trackA_1

  # Submission name
  # This name will be used in the results tables when space permits.
  name: DCASE2023 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use a maximum of 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system.
  # Mark authors in the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author, this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Keunwoo
      firstname: Choi
      email: keunwoo@gaudiolab.com # Contact email address
      corresponding: true # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        institution: Gaudio Lab, Inc.
        department: AI Research # Optional
        location: Seoul, Korea

    # Second author
    - lastname: Jaekwon
      firstname: Im
      email: jaekwon@gaudiolab.com

      # Affiliation information for the author
      affiliation:
        institution: Gaudio Lab, Inc./Korea Advanced Institute of Science &amp; Technology (KAIST)
        department: AI Research/Graduate School of Culture Technology # Optional
        location: Seoul, Korea/Daejeon, Korea

    # Third author
    - lastname: Laurie
      firstname: Heller
      email: laurieheller@cmu.edu

      # Affiliation information for the author
      affiliation:
        institution: Carnegie Mellon University
        department: Psychology # Optional
        location: Pittsburgh, US

# System results
results:
  # Google Colab URL to generate sounds for evaluation [mandatory]
  # The sounds must be unique and must be generated by the code supplied in the colab.
  colab_url: https://colab.research.google.com/drive/1FzbBf_FqWKu59i97ITibJbdPAqSzeMD4?usp=sharing

  development_dataset:
    # System results for development dataset
    # Full results are not mandatory, however, they are highly recommended as they are needed for a thorough analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete results can be reported.
    # Average FAD
    average:
      FAD: 9.702

    # Class-wise FAD
    class_wise:
      DogBark:
        FAD: 13.411
      Footstep:
        FAD: 8.109
      GunShot:
        FAD: 7.951
      Keyboard:
        FAD: 5.230
      MovingMotorVehicle:
        FAD: 16.108
      Rain:
        FAD: 13.337
      Sneeze/Cough:
        FAD: 3.770

  # URL to the source code of the system [optional]
  source_code: https://github.com/DCASE2023-Task7-Foley-Sound-Synthesis/dcase2023_task7_baseline


# System information
system:
  # System description, metadata provided here will be used to do a meta-analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
    # System input
    # Please specify all system input used (comma-separated list).
    input: sound event label

    # Machine learning methods
    # In case using ensemble methods, please specify all methods used (comma-separated list).
    # e.g. AE, VAE, GAN, Transformer, diffusion model, ensemble...
    machine_learning_method: VQ-VAE, PixelSNAIL
    phase_reconstruction: HiFi-GAN

    # Generated acoustic feature input to phase reconstructor
    # One or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, ...
    acoustic_feature: spectrogram

    # System training/processing pipeline stages
    # e.g. "pretraining", "encoding" (from scratch), ,"weight quantization", "decoding", "phase reconstruction", ...
    pipeline: pretraining, encoding, weight quantization, decoding, phase reconstruction

    # Data augmentation methods
    # Please specify all methods used (comma-separated list).
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null    

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

  # System complexity
  complexity:
    # Total amount of parameters used in the acoustic model(s) and phase reconstruction method(s).
    # For neural networks, this information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding extraction networks and phase reconstruction methods.
    # Use numerical value.
    total_parameters: 269992

  # List of ALL external audio datasets used in the submission. (only for track A)
  # Development dataset is used here only as an example, list only external datasets
  # If multiple external audio datasets are used, please copy the lines after [# Dataset name] and list information on all the audio datasets.
  # e.g. AudioSet, ESC-50, URBAN-SED, Clotho, ...
  external_audio_datasets:
    # Dataset name
    - name: DCASE2023 Challenge Task 7 Development Dataset

      # Dataset access URL
      url: https://drive.google.com/drive/folders/1GzfZvYVdbgDXnykOR93C3LCchPYBPh5I

      # Total audio length in minutes
      total_audio_length: 100

  # List of ALL external pre-trained models used in the submission. (only for track A)
  # If multiple external pre-trained models are used, please copy the lines after [# Model name] and list information on all the pre-trained models.
  # e.g. PANNs, VGGish, AST, BYOL-A, ...
  external_models:
    # Model name
    - name: HiFi-GAN

      # Access URL for pre-trained model
      url: https://drive.google.com/drive/folders/1-eEYTB5Av9jNql0WGBlRoi-WH2J7bp5Y

      # How to use pre-trained model
      # e.g. encoder, decoder, weight quantization, vocoder, ... (comma-separated list)
      usage: vocoder

  # URL to the source code of the system [optional, highly recommended]
  # Reproducibility will be used to evaluate submitted systems.
  source_code: https://github.com/DCASE2023-Task7-Foley-Sound-Synthesis/dcase2023_task7_baseline

# Questionnaire
questionnaire:
  # Do you agree to allow the DCASE distribution of 700 audio samples (100 samples * 7 audio categories) to evaluator(s) for the subjective evaluation? [mandatory]
  # The audio samples will not be distributed for any purpose other than subjective evaluation without other explicit permissions.
  distribute_audio_samples: Yes

  # Do you give permission to publish 700 audio samples (100 samples * 7 audio categories) used in the evaluation on the challenge result page?
  # This is very important from the perspective of reproducible research, and we strongly encourage you to allow it.
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 7 organizers.
  publish_audio_samples: Yes

  # Do you agree to allow the DCASE use of 100 audio samples per category in a future version of this DCASE competition? (not required for competition entry, optional).
  # This may be used in future baseline comparisons or classification challenges related to this Foley challenge.
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 7 organizers.
  use_audio_samples: Yes


</code></pre>
</div>
</div>
</div>
</div>
<p><br/>
<br/></p>
<h1 id="technical-report">Technical report</h1>
<p>All participants are expected to submit a technical report about the submitted system, to help the DCASE community better understand how the algorithm works.</p>
<p><strong>Technical reports are not peer-reviewed.</strong> The technical reports will be published on the challenge website together with all other information about the submitted system. For the technical report, it is not necessary to follow closely the scientific publication structure (for example there is no need for extensive literature review). The report should however contain a sufficient description of the system.</p>
<p>Please report the system performance using the provided cross-validation setup or development set, according to the task. For participants taking part in multiple tasks, one technical report covering all tasks is sufficient, if the systems have only small differences. Describe the task-specific parameters in the report.</p>
<p class="bg-success">
Participants can also submit the same report as a <strong>scientific paper</strong> to DCASE2023 Workshop. In this case, the paper must respect the structure of a scientific publication, and be prepared according to the provided Workshop paper instructions and template. Please note that the template is slightly different, and you will have to create a separate submission to the DCASE2023 Workshop track in the submission system. Please refer to the workshop webpage for more details.
  <strong>DCASE2023 Workshop papers will be peer-reviewed.</strong>
</p>
<h2 id="template">Template</h2>
<p>Reports are in format <strong>4+1</strong> pages. Papers are maximum 5 pages, including all text, figures, and references, with the 5th page containing only references. The templates for technical report are available here:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2023/dcase2023_challenge_tech_report_template.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2023/dcase2023_challenge_tech_report_template.zip" target="_blank">
<span style="font-size:20px;">Latex template <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(133 KB)</span>
<br/>
<span class="text-muted">
                
                version 1.0
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2023/dcase2023_challenge_word_template.docx" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-word-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2023/dcase2023_challenge_word_template.docx" target="_blank">
<span style="font-size:20px;">Word template <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(36 KB)</span>
<br/>
<span class="text-muted">
                
                version 1.0
                
                
                (.docx)
                
                </span>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2023/dcase2023_challenge_tech_report_template.pdf" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2023/dcase2023_challenge_tech_report_template.pdf" target="_blank">
<span style="font-size:20px;">Sample PDF produced with Latex template <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(158 KB)</span>
<br/>
<span class="text-muted">
                
                version 1.0
                
                
                (.pdf)
                
                </span>
</div>
</div>
<p><br/>
<br/>
<br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>