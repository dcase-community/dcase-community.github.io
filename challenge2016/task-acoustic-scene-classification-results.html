<!DOCTYPE html><html lang="en">
<head>
    <title>Acoustic scene classification - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2016/task-acoustic-scene-classification-results">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="Task description The goal of acoustic scene classification task was to classify a test recordings into one of predefined classes (15) that characterizes the environment in which it was recorded — for example &#34;park&#34;, &#34;home&#34;, &#34;office&#34;. More detailed task description can be found in the task description page Challenge results Here …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2016</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2016/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2016/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2016/task-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2016/task-sound-event-detection-in-synthetic-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthetic text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-synthetic-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-synthetic-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2016/task-sound-event-detection-in-real-life-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-events text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-real-life-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-real-life-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2016/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Download data">
        <a href="/challenge2016/download"><i class="fa fa-download"></i>&nbsp;Download</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2016/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2016/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge organizers">
        <a href="/challenge2016/organizers"><i class="fa fa-users"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/dunes-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-primary"></i><i class="fa dc-scene fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Scenes</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2016_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Acoustic<br> scene classification</h1><hr class="small right bold"><span class="subheading">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#challenge-results">Challenge results</a>
<ul>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a></li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h2 id="task-description">Task description</h2>
<p>The goal of acoustic scene classification task was to classify a test recordings into one of predefined classes (15) that characterizes the environment in which it was recorded — for example "park", "home", "office".</p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2016/task-acoustic-scene-classification">task description page</a></p>
<h2 id="challenge-results">Challenge results</h2>
<p>Here you can find complete information on the submissions for Task 1: results on evaluation and development set (when reported by authors), class-wise results, technical reports and bibtex citations.</p>
<p>System outputs:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/926660" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/926660" target="_blank">
<span style="font-size:20px;">DCASE2016 Challenge Submissions Package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(28.7 MB)</span>
<br/>
<a href="http://dx.doi.org/10.5281/zenodo.926660">
<img alt="10.5281/zenodo.926660" src="https://zenodo.org/badge/doi/10.5281/zenodo.926660.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h3 id="systems-ranking">Systems ranking</h3>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Aggarwal_task1_1</td>
<td></td>
<td>Vij2016</td>
<td>74.4</td>
<td>74.1</td>
</tr>
<tr>
<td></td>
<td>Bae_task1_1</td>
<td>CLC</td>
<td>Bae2016</td>
<td>84.1</td>
<td>79.2</td>
</tr>
<tr>
<td></td>
<td>Bao_task1_1</td>
<td></td>
<td>Bao2016</td>
<td>83.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Battaglino_task1_1</td>
<td></td>
<td>Battaglino2016</td>
<td>80.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Bisot_task1_1</td>
<td></td>
<td>Bisot2016</td>
<td>87.7</td>
<td>86.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2016 baseline</td>
<td>DCASE2016_baseline</td>
<td>Heittola2016</td>
<td>77.2</td>
<td>72.5</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_1</td>
<td>Tec_SVM_A</td>
<td>Sena_Mafra2016</td>
<td>76.4</td>
<td>80.0</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_2</td>
<td>Tec_SVM_V</td>
<td>Sena_Mafra2016</td>
<td>80.5</td>
<td>78.0</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_3</td>
<td>Tec_MLP</td>
<td>Sena_Mafra2016</td>
<td>73.1</td>
<td>75.0</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_4</td>
<td>Tec_CNN</td>
<td>Sena_Mafra2016</td>
<td>62.8</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_1</td>
<td>CPJKU16_BMBI</td>
<td>Eghbal-Zadeh2016</td>
<td>86.4</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_2</td>
<td>CPJKU16_CBMBI</td>
<td>Eghbal-Zadeh2016</td>
<td>88.7</td>
<td>83.9</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_3</td>
<td>CPJKU16_DCNN</td>
<td>Eghbal-Zadeh2016</td>
<td>83.3</td>
<td>79.5</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_4</td>
<td>CPJKU16_LFCBI</td>
<td>Eghbal-Zadeh2016</td>
<td>89.7</td>
<td>89.9</td>
</tr>
<tr>
<td></td>
<td>Foleiss_task1_1</td>
<td>JFTT</td>
<td>Foleiss2016</td>
<td>76.2</td>
<td>71.8</td>
</tr>
<tr>
<td></td>
<td>Hertel_task1_1</td>
<td>All-ConvNet</td>
<td>Hertel2016</td>
<td>79.5</td>
<td>84.5</td>
</tr>
<tr>
<td></td>
<td>Kim_task1_1</td>
<td>QRK</td>
<td>Yun2016</td>
<td>82.1</td>
<td>84.0</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_1</td>
<td>KU_ISPL1_2016</td>
<td>Park2016</td>
<td>87.2</td>
<td>76.3</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_2</td>
<td>KU_ISPL2_2016</td>
<td>Mun2016</td>
<td>82.3</td>
<td>72.7</td>
</tr>
<tr>
<td></td>
<td>Kong_task1_1</td>
<td>QK</td>
<td>Kong2016</td>
<td>81.0</td>
<td>76.4</td>
</tr>
<tr>
<td></td>
<td>Kumar_task1_1</td>
<td>Gauss</td>
<td>Elizalde2016</td>
<td>85.9</td>
<td>78.9</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_1</td>
<td>MARGNet_MWFD</td>
<td>Han2016</td>
<td>84.6</td>
<td>83.1</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_2</td>
<td>MARGNet_ZENS</td>
<td>Kim2016</td>
<td>85.4</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_1</td>
<td>liu-re</td>
<td>Liu2016</td>
<td>83.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_task1_2</td>
<td>liu-pre</td>
<td>Liu2016</td>
<td>83.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lostanlen_task1_1</td>
<td>LostanlenAnden_2016</td>
<td>Lostanlen2016</td>
<td>80.8</td>
<td>79.4</td>
</tr>
<tr>
<td></td>
<td>Marchi_task1_1</td>
<td>Marchi_2016</td>
<td>Marchi2016</td>
<td>86.4</td>
<td>81.4</td>
</tr>
<tr>
<td></td>
<td>Marques_task1_1</td>
<td>DRKNN_2016</td>
<td>Marques2016</td>
<td>83.1</td>
<td>78.2</td>
</tr>
<tr>
<td></td>
<td>Moritz_task1_1</td>
<td></td>
<td>Moritz2016</td>
<td>79.0</td>
<td>76.5</td>
</tr>
<tr>
<td></td>
<td>Mulimani_task1_1</td>
<td></td>
<td>Mulimani2016</td>
<td>65.6</td>
<td>66.8</td>
</tr>
<tr>
<td></td>
<td>Nogueira_task1_1</td>
<td></td>
<td>Nogueira2016</td>
<td>81.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Patiyal_task1_1</td>
<td>IITMandi_2016</td>
<td>Patiyal2016</td>
<td>78.5</td>
<td>97.6</td>
</tr>
<tr>
<td></td>
<td>Phan_task1_1</td>
<td>CNN-LTE</td>
<td>Phan2016</td>
<td>83.3</td>
<td>81.2</td>
</tr>
<tr>
<td></td>
<td>Pugachev_task1_1</td>
<td></td>
<td>Pugachev2016</td>
<td>73.1</td>
<td>82.9</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_1</td>
<td></td>
<td>Dai2016</td>
<td>80.5</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Qu_task1_2</td>
<td></td>
<td>Dai2016</td>
<td>84.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Qu_task1_3</td>
<td></td>
<td>Dai2016</td>
<td>82.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Qu_task1_4</td>
<td></td>
<td>Dai2016</td>
<td>80.5</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_1</td>
<td>RAK_2016_1</td>
<td>Rakotomamonjy2016</td>
<td>82.1</td>
<td>81.2</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_2</td>
<td>RAK_2016_2</td>
<td>Rakotomamonjy2016</td>
<td>79.2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Santoso_task1_1</td>
<td>SWW</td>
<td>Santoso2016</td>
<td>80.8</td>
<td>78.8</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_1</td>
<td>CQTCNN_1</td>
<td>Lidy2016</td>
<td>81.8</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_2</td>
<td>CQTCNN_2</td>
<td>Lidy2016</td>
<td>83.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Takahashi_task1_1</td>
<td>UTNII_2016</td>
<td>Takahashi2016</td>
<td>85.6</td>
<td>77.5</td>
</tr>
<tr>
<td></td>
<td>Valenti_task1_1</td>
<td></td>
<td>Valenti2016</td>
<td>86.2</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Vikaskumar_task1_1</td>
<td>ABSP_IITKGP_2016</td>
<td>Vikaskumar2016</td>
<td>81.3</td>
<td>80.4</td>
</tr>
<tr>
<td></td>
<td>Vu_task1_1</td>
<td></td>
<td>Vu2016</td>
<td>80.0</td>
<td>82.1</td>
</tr>
<tr>
<td></td>
<td>Xu_task1_1</td>
<td>HL-DNN-ASC_2016</td>
<td>Xu2016</td>
<td>73.3</td>
<td>81.4</td>
</tr>
<tr>
<td></td>
<td>Zoehrer_task1_1</td>
<td></td>
<td>Zoehrer2016</td>
<td>73.1</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="teams-ranking">Teams ranking</h3>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Aggarwal_task1_1</td>
<td></td>
<td>Vij2016</td>
<td>74.4</td>
<td>74.1</td>
</tr>
<tr>
<td></td>
<td>Bae_task1_1</td>
<td>CLC</td>
<td>Bae2016</td>
<td>84.1</td>
<td>79.2</td>
</tr>
<tr>
<td></td>
<td>Bao_task1_1</td>
<td></td>
<td>Bao2016</td>
<td>83.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Battaglino_task1_1</td>
<td></td>
<td>Battaglino2016</td>
<td>80.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Bisot_task1_1</td>
<td></td>
<td>Bisot2016</td>
<td>87.7</td>
<td>86.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2016 baseline</td>
<td>DCASE2016_baseline</td>
<td>Heittola2016</td>
<td>77.2</td>
<td>72.5</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_2</td>
<td>Tec_SVM_V</td>
<td>Sena_Mafra2016</td>
<td>80.5</td>
<td>78.0</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_4</td>
<td>CPJKU16_LFCBI</td>
<td>Eghbal-Zadeh2016</td>
<td>89.7</td>
<td>89.9</td>
</tr>
<tr>
<td></td>
<td>Foleiss_task1_1</td>
<td>JFTT</td>
<td>Foleiss2016</td>
<td>76.2</td>
<td>71.8</td>
</tr>
<tr>
<td></td>
<td>Hertel_task1_1</td>
<td>All-ConvNet</td>
<td>Hertel2016</td>
<td>79.5</td>
<td>84.5</td>
</tr>
<tr>
<td></td>
<td>Kim_task1_1</td>
<td>QRK</td>
<td>Yun2016</td>
<td>82.1</td>
<td>84.0</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_1</td>
<td>KU_ISPL1_2016</td>
<td>Park2016</td>
<td>87.2</td>
<td>76.3</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_2</td>
<td>KU_ISPL2_2016</td>
<td>Mun2016</td>
<td>82.3</td>
<td>72.7</td>
</tr>
<tr>
<td></td>
<td>Kong_task1_1</td>
<td>QK</td>
<td>Kong2016</td>
<td>81.0</td>
<td>76.4</td>
</tr>
<tr>
<td></td>
<td>Kumar_task1_1</td>
<td>Gauss</td>
<td>Elizalde2016</td>
<td>85.9</td>
<td>78.9</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_1</td>
<td>MARGNet_MWFD</td>
<td>Han2016</td>
<td>84.6</td>
<td>83.1</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_2</td>
<td>MARGNet_ZENS</td>
<td>Kim2016</td>
<td>85.4</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_1</td>
<td>liu-re</td>
<td>Liu2016</td>
<td>83.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lostanlen_task1_1</td>
<td>LostanlenAnden_2016</td>
<td>Lostanlen2016</td>
<td>80.8</td>
<td>79.4</td>
</tr>
<tr>
<td></td>
<td>Marchi_task1_1</td>
<td>Marchi_2016</td>
<td>Marchi2016</td>
<td>86.4</td>
<td>81.4</td>
</tr>
<tr>
<td></td>
<td>Marques_task1_1</td>
<td>DRKNN_2016</td>
<td>Marques2016</td>
<td>83.1</td>
<td>78.2</td>
</tr>
<tr>
<td></td>
<td>Moritz_task1_1</td>
<td></td>
<td>Moritz2016</td>
<td>79.0</td>
<td>76.5</td>
</tr>
<tr>
<td></td>
<td>Mulimani_task1_1</td>
<td></td>
<td>Mulimani2016</td>
<td>65.6</td>
<td>66.8</td>
</tr>
<tr>
<td></td>
<td>Nogueira_task1_1</td>
<td></td>
<td>Nogueira2016</td>
<td>81.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Patiyal_task1_1</td>
<td>IITMandi_2016</td>
<td>Patiyal2016</td>
<td>78.5</td>
<td>97.6</td>
</tr>
<tr>
<td></td>
<td>Phan_task1_1</td>
<td>CNN-LTE</td>
<td>Phan2016</td>
<td>83.3</td>
<td>81.2</td>
</tr>
<tr>
<td></td>
<td>Pugachev_task1_1</td>
<td></td>
<td>Pugachev2016</td>
<td>73.1</td>
<td>82.9</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_2</td>
<td></td>
<td>Dai2016</td>
<td>84.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_1</td>
<td>RAK_2016_1</td>
<td>Rakotomamonjy2016</td>
<td>82.1</td>
<td>81.2</td>
</tr>
<tr>
<td></td>
<td>Santoso_task1_1</td>
<td>SWW</td>
<td>Santoso2016</td>
<td>80.8</td>
<td>78.8</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_2</td>
<td>CQTCNN_2</td>
<td>Lidy2016</td>
<td>83.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Takahashi_task1_1</td>
<td>UTNII_2016</td>
<td>Takahashi2016</td>
<td>85.6</td>
<td>77.5</td>
</tr>
<tr>
<td></td>
<td>Valenti_task1_1</td>
<td></td>
<td>Valenti2016</td>
<td>86.2</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Vikaskumar_task1_1</td>
<td>ABSP_IITKGP_2016</td>
<td>Vikaskumar2016</td>
<td>81.3</td>
<td>80.4</td>
</tr>
<tr>
<td></td>
<td>Vu_task1_1</td>
<td></td>
<td>Vu2016</td>
<td>80.0</td>
<td>82.1</td>
</tr>
<tr>
<td></td>
<td>Xu_task1_1</td>
<td>HL-DNN-ASC_2016</td>
<td>Xu2016</td>
<td>73.3</td>
<td>81.4</td>
</tr>
<tr>
<td></td>
<td>Zoehrer_task1_1</td>
<td></td>
<td>Zoehrer2016</td>
<td>73.1</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="class-wise-performance">Class-wise performance</h3>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2016 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Eghbal-Zadeh_task1_4" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["class_accuracy_eval_beach", "class_accuracy_eval_bus", "class_accuracy_eval_cafe_restaurant", "class_accuracy_eval_car", "class_accuracy_eval_city_center", "class_accuracy_eval_forest_path", "class_accuracy_eval_grocery_store", "class_accuracy_eval_home", "class_accuracy_eval_library", "class_accuracy_eval_metro_station", "class_accuracy_eval_office", "class_accuracy_eval_park", "class_accuracy_eval_residential_area", "class_accuracy_eval_train", "class_accuracy_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_cafe_restaurant", "class_accuracy_eval_grocery_store", "class_accuracy_eval_home", "class_accuracy_eval_library", "class_accuracy_eval_office"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_beach", "class_accuracy_eval_city_center", "class_accuracy_eval_forest_path", "class_accuracy_eval_park", "class_accuracy_eval_residential_area"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_bus","class_accuracy_eval_car","class_accuracy_eval_metro_station","class_accuracy_eval_train","class_accuracy_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission<br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Evaluation dataset)
            </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="class_accuracy_eval_beach" data-sortable="true" data-value-type="float1-percentage">
                Beach
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_bus" data-sortable="true" data-value-type="float1-percentage">
                Bus
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_cafe_restaurant" data-sortable="true" data-value-type="float1-percentage">
                Cafe / <br/>Restaurant
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_car" data-sortable="true" data-value-type="float1-percentage">
                Car
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_city_center" data-sortable="true" data-value-type="float1-percentage">
                City <br/>center
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_forest_path" data-sortable="true" data-value-type="float1-percentage">
                Forest <br/>path
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_grocery_store" data-sortable="true" data-value-type="float1-percentage">
                Grocery <br/>store
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_home" data-sortable="true" data-value-type="float1-percentage">
                Home
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_library" data-sortable="true" data-value-type="float1-percentage">
                Library
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro_station" data-sortable="true" data-value-type="float1-percentage">
                Metro <br/>station
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_office" data-sortable="true" data-value-type="float1-percentage">
                Office
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_park" data-sortable="true" data-value-type="float1-percentage">
                Park
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_residential_area" data-sortable="true" data-value-type="float1-percentage">
                Residential<br/>area
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_train" data-sortable="true" data-value-type="float1-percentage">
                Train
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_tram" data-sortable="true" data-value-type="float1-percentage">
                Tram
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Aggarwal_task1_1</td>
<td></td>
<td>Vij2016</td>
<td>74.4</td>
<td>80.8</td>
<td>84.6</td>
<td>69.2</td>
<td>88.5</td>
<td>80.8</td>
<td>84.6</td>
<td>84.6</td>
<td>92.3</td>
<td>38.5</td>
<td>96.2</td>
<td>92.3</td>
<td>65.4</td>
<td>42.3</td>
<td>34.6</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Bae_task1_1</td>
<td>CLC</td>
<td>Bae2016</td>
<td>84.1</td>
<td>84.6</td>
<td>100.0</td>
<td>61.5</td>
<td>88.5</td>
<td>92.3</td>
<td>100.0</td>
<td>96.2</td>
<td>88.5</td>
<td>46.2</td>
<td>88.5</td>
<td>100.0</td>
<td>96.2</td>
<td>65.4</td>
<td>53.8</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Bao_task1_1</td>
<td></td>
<td>Bao2016</td>
<td>83.1</td>
<td>84.6</td>
<td>96.2</td>
<td>57.7</td>
<td>100.0</td>
<td>76.9</td>
<td>92.3</td>
<td>84.6</td>
<td>88.5</td>
<td>46.2</td>
<td>96.2</td>
<td>100.0</td>
<td>96.2</td>
<td>76.9</td>
<td>50.0</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Battaglino_task1_1</td>
<td></td>
<td>Battaglino2016</td>
<td>80.0</td>
<td>84.6</td>
<td>73.1</td>
<td>76.9</td>
<td>84.6</td>
<td>96.2</td>
<td>100.0</td>
<td>96.2</td>
<td>84.6</td>
<td>34.6</td>
<td>80.8</td>
<td>84.6</td>
<td>96.2</td>
<td>65.4</td>
<td>53.8</td>
<td>88.5</td>
</tr>
<tr>
<td></td>
<td>Bisot_task1_1</td>
<td></td>
<td>Bisot2016</td>
<td>87.7</td>
<td>88.5</td>
<td>100.0</td>
<td>76.9</td>
<td>100.0</td>
<td>100.0</td>
<td>88.5</td>
<td>88.5</td>
<td>96.2</td>
<td>50.0</td>
<td>100.0</td>
<td>96.2</td>
<td>80.8</td>
<td>76.9</td>
<td>73.1</td>
<td>100.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2016 baseline</td>
<td>DCASE2016_baseline</td>
<td>Heittola2016</td>
<td>77.2</td>
<td>84.6</td>
<td>88.5</td>
<td>69.2</td>
<td>96.2</td>
<td>80.8</td>
<td>65.4</td>
<td>88.5</td>
<td>92.3</td>
<td>26.9</td>
<td>100.0</td>
<td>96.2</td>
<td>53.8</td>
<td>88.5</td>
<td>30.8</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_1</td>
<td>Tec_SVM_A</td>
<td>Sena_Mafra2016</td>
<td>76.4</td>
<td>88.5</td>
<td>100.0</td>
<td>69.2</td>
<td>88.5</td>
<td>84.6</td>
<td>100.0</td>
<td>96.2</td>
<td>38.5</td>
<td>46.2</td>
<td>80.8</td>
<td>100.0</td>
<td>61.5</td>
<td>34.6</td>
<td>57.7</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_2</td>
<td>Tec_SVM_V</td>
<td>Sena_Mafra2016</td>
<td>80.5</td>
<td>80.8</td>
<td>100.0</td>
<td>84.6</td>
<td>92.3</td>
<td>92.3</td>
<td>100.0</td>
<td>96.2</td>
<td>57.7</td>
<td>46.2</td>
<td>96.2</td>
<td>100.0</td>
<td>50.0</td>
<td>53.8</td>
<td>57.7</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_3</td>
<td>Tec_MLP</td>
<td>Sena_Mafra2016</td>
<td>73.1</td>
<td>73.1</td>
<td>92.3</td>
<td>50.0</td>
<td>84.6</td>
<td>88.5</td>
<td>100.0</td>
<td>80.8</td>
<td>34.6</td>
<td>26.9</td>
<td>92.3</td>
<td>100.0</td>
<td>84.6</td>
<td>46.2</td>
<td>50.0</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_4</td>
<td>Tec_CNN</td>
<td>Sena_Mafra2016</td>
<td>62.8</td>
<td>80.8</td>
<td>88.5</td>
<td>53.8</td>
<td>80.8</td>
<td>69.2</td>
<td>96.2</td>
<td>76.9</td>
<td>50.0</td>
<td>15.4</td>
<td>46.2</td>
<td>92.3</td>
<td>42.3</td>
<td>34.6</td>
<td>19.2</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_1</td>
<td>CPJKU16_BMBI</td>
<td>Eghbal-Zadeh2016</td>
<td>86.4</td>
<td>92.3</td>
<td>92.3</td>
<td>76.9</td>
<td>96.2</td>
<td>92.3</td>
<td>96.2</td>
<td>100.0</td>
<td>88.5</td>
<td>69.2</td>
<td>73.1</td>
<td>100.0</td>
<td>96.2</td>
<td>76.9</td>
<td>46.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_2</td>
<td>CPJKU16_CBMBI</td>
<td>Eghbal-Zadeh2016</td>
<td>88.7</td>
<td>96.2</td>
<td>100.0</td>
<td>84.6</td>
<td>100.0</td>
<td>92.3</td>
<td>96.2</td>
<td>100.0</td>
<td>92.3</td>
<td>69.2</td>
<td>69.2</td>
<td>100.0</td>
<td>96.2</td>
<td>84.6</td>
<td>50.0</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_3</td>
<td>CPJKU16_DCNN</td>
<td>Eghbal-Zadeh2016</td>
<td>83.3</td>
<td>92.3</td>
<td>96.2</td>
<td>42.3</td>
<td>88.5</td>
<td>84.6</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>53.8</td>
<td>100.0</td>
<td>96.2</td>
<td>46.2</td>
<td>80.8</td>
<td>69.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_4</td>
<td>CPJKU16_LFCBI</td>
<td>Eghbal-Zadeh2016</td>
<td>89.7</td>
<td>96.2</td>
<td>100.0</td>
<td>61.5</td>
<td>96.2</td>
<td>96.2</td>
<td>96.2</td>
<td>100.0</td>
<td>96.2</td>
<td>69.2</td>
<td>100.0</td>
<td>96.2</td>
<td>88.5</td>
<td>88.5</td>
<td>61.5</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Foleiss_task1_1</td>
<td>JFTT</td>
<td>Foleiss2016</td>
<td>76.2</td>
<td>84.6</td>
<td>84.6</td>
<td>61.5</td>
<td>80.8</td>
<td>96.2</td>
<td>84.6</td>
<td>96.2</td>
<td>88.5</td>
<td>46.2</td>
<td>57.7</td>
<td>84.6</td>
<td>65.4</td>
<td>42.3</td>
<td>80.8</td>
<td>88.5</td>
</tr>
<tr>
<td></td>
<td>Hertel_task1_1</td>
<td>All-ConvNet</td>
<td>Hertel2016</td>
<td>79.5</td>
<td>84.6</td>
<td>92.3</td>
<td>53.8</td>
<td>100.0</td>
<td>80.8</td>
<td>80.8</td>
<td>76.9</td>
<td>76.9</td>
<td>69.2</td>
<td>100.0</td>
<td>100.0</td>
<td>84.6</td>
<td>46.2</td>
<td>53.8</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Kim_task1_1</td>
<td>QRK</td>
<td>Yun2016</td>
<td>82.1</td>
<td>76.9</td>
<td>100.0</td>
<td>76.9</td>
<td>100.0</td>
<td>84.6</td>
<td>100.0</td>
<td>88.5</td>
<td>100.0</td>
<td>0.0</td>
<td>92.3</td>
<td>96.2</td>
<td>76.9</td>
<td>69.2</td>
<td>69.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_1</td>
<td>KU_ISPL1_2016</td>
<td>Park2016</td>
<td>87.2</td>
<td>88.5</td>
<td>96.2</td>
<td>84.6</td>
<td>96.2</td>
<td>100.0</td>
<td>96.2</td>
<td>96.2</td>
<td>88.5</td>
<td>53.8</td>
<td>80.8</td>
<td>100.0</td>
<td>57.7</td>
<td>80.8</td>
<td>88.5</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_2</td>
<td>KU_ISPL2_2016</td>
<td>Mun2016</td>
<td>82.3</td>
<td>92.3</td>
<td>84.6</td>
<td>65.4</td>
<td>92.3</td>
<td>100.0</td>
<td>84.6</td>
<td>96.2</td>
<td>92.3</td>
<td>53.8</td>
<td>65.4</td>
<td>84.6</td>
<td>92.3</td>
<td>84.6</td>
<td>53.8</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Kong_task1_1</td>
<td>QK</td>
<td>Kong2016</td>
<td>81.0</td>
<td>84.6</td>
<td>100.0</td>
<td>57.7</td>
<td>92.3</td>
<td>88.5</td>
<td>96.2</td>
<td>92.3</td>
<td>76.9</td>
<td>34.6</td>
<td>80.8</td>
<td>100.0</td>
<td>96.2</td>
<td>69.2</td>
<td>46.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Kumar_task1_1</td>
<td>Gauss</td>
<td>Elizalde2016</td>
<td>85.9</td>
<td>84.6</td>
<td>92.3</td>
<td>73.1</td>
<td>88.5</td>
<td>92.3</td>
<td>96.2</td>
<td>96.2</td>
<td>92.3</td>
<td>50.0</td>
<td>96.2</td>
<td>96.2</td>
<td>80.8</td>
<td>88.5</td>
<td>73.1</td>
<td>88.5</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_1</td>
<td>MARGNet_MWFD</td>
<td>Han2016</td>
<td>84.6</td>
<td>84.6</td>
<td>96.2</td>
<td>61.5</td>
<td>100.0</td>
<td>88.5</td>
<td>96.2</td>
<td>92.3</td>
<td>96.2</td>
<td>42.3</td>
<td>84.6</td>
<td>96.2</td>
<td>84.6</td>
<td>76.9</td>
<td>69.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_2</td>
<td>MARGNet_ZENS</td>
<td>Kim2016</td>
<td>85.4</td>
<td>84.6</td>
<td>92.3</td>
<td>61.5</td>
<td>100.0</td>
<td>96.2</td>
<td>100.0</td>
<td>96.2</td>
<td>96.2</td>
<td>46.2</td>
<td>84.6</td>
<td>100.0</td>
<td>92.3</td>
<td>69.2</td>
<td>61.5</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_1</td>
<td>liu-re</td>
<td>Liu2016</td>
<td>83.8</td>
<td>84.6</td>
<td>96.2</td>
<td>69.2</td>
<td>84.6</td>
<td>92.3</td>
<td>96.2</td>
<td>88.5</td>
<td>92.3</td>
<td>46.2</td>
<td>92.3</td>
<td>96.2</td>
<td>88.5</td>
<td>76.9</td>
<td>53.8</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_2</td>
<td>liu-pre</td>
<td>Liu2016</td>
<td>83.6</td>
<td>88.5</td>
<td>92.3</td>
<td>69.2</td>
<td>84.6</td>
<td>96.2</td>
<td>92.3</td>
<td>92.3</td>
<td>88.5</td>
<td>46.2</td>
<td>88.5</td>
<td>96.2</td>
<td>92.3</td>
<td>76.9</td>
<td>50.0</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Lostanlen_task1_1</td>
<td>LostanlenAnden_2016</td>
<td>Lostanlen2016</td>
<td>80.8</td>
<td>80.8</td>
<td>92.3</td>
<td>50.0</td>
<td>96.2</td>
<td>84.6</td>
<td>96.2</td>
<td>84.6</td>
<td>80.8</td>
<td>65.4</td>
<td>96.2</td>
<td>100.0</td>
<td>65.4</td>
<td>69.2</td>
<td>53.8</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Marchi_task1_1</td>
<td>Marchi_2016</td>
<td>Marchi2016</td>
<td>86.4</td>
<td>88.5</td>
<td>92.3</td>
<td>80.8</td>
<td>100.0</td>
<td>96.2</td>
<td>100.0</td>
<td>100.0</td>
<td>76.9</td>
<td>50.0</td>
<td>96.2</td>
<td>100.0</td>
<td>92.3</td>
<td>84.6</td>
<td>42.3</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Marques_task1_1</td>
<td>DRKNN_2016</td>
<td>Marques2016</td>
<td>83.1</td>
<td>88.5</td>
<td>96.2</td>
<td>65.4</td>
<td>84.6</td>
<td>84.6</td>
<td>96.2</td>
<td>80.8</td>
<td>84.6</td>
<td>69.2</td>
<td>84.6</td>
<td>92.3</td>
<td>96.2</td>
<td>65.4</td>
<td>57.7</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Moritz_task1_1</td>
<td></td>
<td>Moritz2016</td>
<td>79.0</td>
<td>88.5</td>
<td>100.0</td>
<td>19.2</td>
<td>100.0</td>
<td>92.3</td>
<td>100.0</td>
<td>88.5</td>
<td>92.3</td>
<td>38.5</td>
<td>80.8</td>
<td>100.0</td>
<td>61.5</td>
<td>76.9</td>
<td>46.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Mulimani_task1_1</td>
<td></td>
<td>Mulimani2016</td>
<td>65.6</td>
<td>73.1</td>
<td>96.2</td>
<td>69.2</td>
<td>100.0</td>
<td>73.1</td>
<td>50.0</td>
<td>65.4</td>
<td>76.9</td>
<td>7.7</td>
<td>76.9</td>
<td>96.2</td>
<td>96.2</td>
<td>23.1</td>
<td>15.4</td>
<td>65.4</td>
</tr>
<tr>
<td></td>
<td>Nogueira_task1_1</td>
<td></td>
<td>Nogueira2016</td>
<td>81.0</td>
<td>88.5</td>
<td>88.5</td>
<td>65.4</td>
<td>92.3</td>
<td>73.1</td>
<td>96.2</td>
<td>84.6</td>
<td>92.3</td>
<td>38.5</td>
<td>96.2</td>
<td>100.0</td>
<td>73.1</td>
<td>80.8</td>
<td>53.8</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Patiyal_task1_1</td>
<td>IITMandi_2016</td>
<td>Patiyal2016</td>
<td>78.5</td>
<td>84.6</td>
<td>96.2</td>
<td>61.5</td>
<td>92.3</td>
<td>92.3</td>
<td>92.3</td>
<td>80.8</td>
<td>92.3</td>
<td>34.6</td>
<td>96.2</td>
<td>96.2</td>
<td>92.3</td>
<td>69.2</td>
<td>11.5</td>
<td>84.6</td>
</tr>
<tr>
<td></td>
<td>Phan_task1_1</td>
<td>CNN-LTE</td>
<td>Phan2016</td>
<td>83.3</td>
<td>84.6</td>
<td>96.2</td>
<td>53.8</td>
<td>100.0</td>
<td>100.0</td>
<td>96.2</td>
<td>84.6</td>
<td>88.5</td>
<td>46.2</td>
<td>84.6</td>
<td>100.0</td>
<td>88.5</td>
<td>84.6</td>
<td>46.2</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Pugachev_task1_1</td>
<td></td>
<td>Pugachev2016</td>
<td>73.1</td>
<td>84.6</td>
<td>69.2</td>
<td>61.5</td>
<td>92.3</td>
<td>80.8</td>
<td>96.2</td>
<td>92.3</td>
<td>80.8</td>
<td>26.9</td>
<td>96.2</td>
<td>88.5</td>
<td>57.7</td>
<td>42.3</td>
<td>34.6</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_1</td>
<td></td>
<td>Dai2016</td>
<td>80.5</td>
<td>84.6</td>
<td>100.0</td>
<td>73.1</td>
<td>88.5</td>
<td>96.2</td>
<td>84.6</td>
<td>100.0</td>
<td>88.5</td>
<td>23.1</td>
<td>76.9</td>
<td>96.2</td>
<td>73.1</td>
<td>76.9</td>
<td>46.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_2</td>
<td></td>
<td>Dai2016</td>
<td>84.1</td>
<td>88.5</td>
<td>100.0</td>
<td>80.8</td>
<td>92.3</td>
<td>96.2</td>
<td>84.6</td>
<td>100.0</td>
<td>88.5</td>
<td>42.3</td>
<td>76.9</td>
<td>96.2</td>
<td>76.9</td>
<td>80.8</td>
<td>57.7</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_3</td>
<td></td>
<td>Dai2016</td>
<td>82.3</td>
<td>88.5</td>
<td>100.0</td>
<td>76.9</td>
<td>92.3</td>
<td>96.2</td>
<td>84.6</td>
<td>92.3</td>
<td>88.5</td>
<td>30.8</td>
<td>88.5</td>
<td>96.2</td>
<td>76.9</td>
<td>76.9</td>
<td>46.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_4</td>
<td></td>
<td>Dai2016</td>
<td>80.5</td>
<td>80.8</td>
<td>100.0</td>
<td>84.6</td>
<td>88.5</td>
<td>92.3</td>
<td>84.6</td>
<td>92.3</td>
<td>92.3</td>
<td>42.3</td>
<td>76.9</td>
<td>96.2</td>
<td>76.9</td>
<td>76.9</td>
<td>23.1</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_1</td>
<td>RAK_2016_1</td>
<td>Rakotomamonjy2016</td>
<td>82.1</td>
<td>80.8</td>
<td>96.2</td>
<td>46.2</td>
<td>92.3</td>
<td>84.6</td>
<td>100.0</td>
<td>96.2</td>
<td>88.5</td>
<td>42.3</td>
<td>80.8</td>
<td>96.2</td>
<td>88.5</td>
<td>73.1</td>
<td>65.4</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_2</td>
<td>RAK_2016_2</td>
<td>Rakotomamonjy2016</td>
<td>79.2</td>
<td>92.3</td>
<td>92.3</td>
<td>69.2</td>
<td>84.6</td>
<td>80.8</td>
<td>96.2</td>
<td>84.6</td>
<td>88.5</td>
<td>38.5</td>
<td>96.2</td>
<td>100.0</td>
<td>73.1</td>
<td>57.7</td>
<td>34.6</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Santoso_task1_1</td>
<td>SWW</td>
<td>Santoso2016</td>
<td>80.8</td>
<td>84.6</td>
<td>84.6</td>
<td>61.5</td>
<td>96.2</td>
<td>84.6</td>
<td>100.0</td>
<td>80.8</td>
<td>100.0</td>
<td>42.3</td>
<td>92.3</td>
<td>100.0</td>
<td>80.8</td>
<td>65.4</td>
<td>42.3</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_1</td>
<td>CQTCNN_1</td>
<td>Lidy2016</td>
<td>81.8</td>
<td>88.5</td>
<td>100.0</td>
<td>34.6</td>
<td>92.3</td>
<td>96.2</td>
<td>100.0</td>
<td>92.3</td>
<td>88.5</td>
<td>46.2</td>
<td>96.2</td>
<td>100.0</td>
<td>65.4</td>
<td>73.1</td>
<td>53.8</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_2</td>
<td>CQTCNN_2</td>
<td>Lidy2016</td>
<td>83.3</td>
<td>88.5</td>
<td>100.0</td>
<td>34.6</td>
<td>92.3</td>
<td>96.2</td>
<td>100.0</td>
<td>92.3</td>
<td>92.3</td>
<td>46.2</td>
<td>96.2</td>
<td>100.0</td>
<td>65.4</td>
<td>76.9</td>
<td>69.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Takahashi_task1_1</td>
<td>UTNII_2016</td>
<td>Takahashi2016</td>
<td>85.6</td>
<td>92.3</td>
<td>100.0</td>
<td>61.5</td>
<td>100.0</td>
<td>88.5</td>
<td>88.5</td>
<td>96.2</td>
<td>84.6</td>
<td>57.7</td>
<td>80.8</td>
<td>100.0</td>
<td>92.3</td>
<td>80.8</td>
<td>61.5</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Valenti_task1_1</td>
<td></td>
<td>Valenti2016</td>
<td>86.2</td>
<td>84.6</td>
<td>100.0</td>
<td>76.9</td>
<td>100.0</td>
<td>96.2</td>
<td>100.0</td>
<td>92.3</td>
<td>92.3</td>
<td>42.3</td>
<td>96.2</td>
<td>96.2</td>
<td>76.9</td>
<td>76.9</td>
<td>65.4</td>
<td>96.2</td>
</tr>
<tr>
<td></td>
<td>Vikaskumar_task1_1</td>
<td>ABSP_IITKGP_2016</td>
<td>Vikaskumar2016</td>
<td>81.3</td>
<td>84.6</td>
<td>92.3</td>
<td>61.5</td>
<td>100.0</td>
<td>84.6</td>
<td>84.6</td>
<td>80.8</td>
<td>88.5</td>
<td>65.4</td>
<td>92.3</td>
<td>69.2</td>
<td>80.8</td>
<td>73.1</td>
<td>73.1</td>
<td>88.5</td>
</tr>
<tr>
<td></td>
<td>Vu_task1_1</td>
<td></td>
<td>Vu2016</td>
<td>80.0</td>
<td>88.5</td>
<td>76.9</td>
<td>61.5</td>
<td>100.0</td>
<td>92.3</td>
<td>100.0</td>
<td>80.8</td>
<td>73.1</td>
<td>46.2</td>
<td>92.3</td>
<td>100.0</td>
<td>92.3</td>
<td>50.0</td>
<td>46.2</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Xu_task1_1</td>
<td>HL-DNN-ASC_2016</td>
<td>Xu2016</td>
<td>73.3</td>
<td>84.6</td>
<td>96.2</td>
<td>23.1</td>
<td>96.2</td>
<td>84.6</td>
<td>100.0</td>
<td>84.6</td>
<td>69.2</td>
<td>23.1</td>
<td>57.7</td>
<td>100.0</td>
<td>73.1</td>
<td>69.2</td>
<td>38.5</td>
<td>100.0</td>
</tr>
<tr>
<td></td>
<td>Zoehrer_task1_1</td>
<td></td>
<td>Zoehrer2016</td>
<td>73.1</td>
<td>80.8</td>
<td>92.3</td>
<td>38.5</td>
<td>92.3</td>
<td>65.4</td>
<td>96.2</td>
<td>84.6</td>
<td>65.4</td>
<td>23.1</td>
<td>84.6</td>
<td>100.0</td>
<td>61.5</td>
<td>69.2</td>
<td>42.3</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<h2 id="system-characteristics">System characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-fields="code:Submission&lt;br&gt;&lt;/br&gt;code:str:visible;sortable;sep-left,
                 name:Submission&lt;br&gt;&lt;/br&gt;name:str:visible;sortable;small,
                 anchor:Tech.&lt;br&gt;&lt;/br&gt;Report:anchor:visible;sep-left;text-center,
                 accuracy_eval:Accuracy &lt;br&gt;&lt;/br&gt;(Eval):float:visible;sortable;chartable;sep-left;text-center:percentage,
                 system_input:Input:tag:visible;sortable;filterable;sep-left,
                 system_features:Features:tag:visible;sortable;filterable,
                 system_classifier:Classifier:tag:visible;sortable;filterable" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_eval" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_input" data-filter-control="select" data-sortable="true" data-tag="true">
                Input
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-sortable="true" data-tag="true">
                Features
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-sortable="true" data-tag="true">
                Classifier
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Aggarwal_task1_1</td>
<td></td>
<td>Vij2016</td>
<td>74.4</td>
<td>binaural</td>
<td>various</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Bae_task1_1</td>
<td>CLC</td>
<td>Bae2016</td>
<td>84.1</td>
<td>monophonic</td>
<td>spectrogram</td>
<td>CNN-RNN</td>
</tr>
<tr>
<td></td>
<td>Bao_task1_1</td>
<td></td>
<td>Bao2016</td>
<td>83.1</td>
<td>monophonic</td>
<td>MFCC+mel energy</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Battaglino_task1_1</td>
<td></td>
<td>Battaglino2016</td>
<td>80.0</td>
<td>binaural</td>
<td>mel energy</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Bisot_task1_1</td>
<td></td>
<td>Bisot2016</td>
<td>87.7</td>
<td>monophonic</td>
<td>spectrogram</td>
<td>NMF</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2016 baseline</td>
<td>DCASE2016_baseline</td>
<td>Heittola2016</td>
<td>77.2</td>
<td>monophonic</td>
<td>MFCC</td>
<td>GMM</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_1</td>
<td>Tec_SVM_A</td>
<td>Sena_Mafra2016</td>
<td>76.4</td>
<td>monophonic</td>
<td>mel energy</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_2</td>
<td>Tec_SVM_V</td>
<td>Sena_Mafra2016</td>
<td>80.5</td>
<td>monophonic</td>
<td>mel energy</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_3</td>
<td>Tec_MLP</td>
<td>Sena_Mafra2016</td>
<td>73.1</td>
<td>monophonic</td>
<td>mel energy</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_4</td>
<td>Tec_CNN</td>
<td>Sena_Mafra2016</td>
<td>62.8</td>
<td>monophonic</td>
<td>mel energy</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_1</td>
<td>CPJKU16_BMBI</td>
<td>Eghbal-Zadeh2016</td>
<td>86.4</td>
<td>binaural</td>
<td>MFCC</td>
<td>I-vector</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_2</td>
<td>CPJKU16_CBMBI</td>
<td>Eghbal-Zadeh2016</td>
<td>88.7</td>
<td>binaural</td>
<td>MFCC</td>
<td>I-vector</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_3</td>
<td>CPJKU16_DCNN</td>
<td>Eghbal-Zadeh2016</td>
<td>83.3</td>
<td>monophonic</td>
<td>spectrogram</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_4</td>
<td>CPJKU16_LFCBI</td>
<td>Eghbal-Zadeh2016</td>
<td>89.7</td>
<td>mono+binaural</td>
<td>MFCC+spectrograms</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Foleiss_task1_1</td>
<td>JFTT</td>
<td>Foleiss2016</td>
<td>76.2</td>
<td>monophonic</td>
<td>various</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Hertel_task1_1</td>
<td>All-ConvNet</td>
<td>Hertel2016</td>
<td>79.5</td>
<td>left</td>
<td>spectrogram</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Kim_task1_1</td>
<td>QRK</td>
<td>Yun2016</td>
<td>82.1</td>
<td>mono</td>
<td>MFCC</td>
<td>GMM</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_1</td>
<td>KU_ISPL1_2016</td>
<td>Park2016</td>
<td>87.2</td>
<td>mono</td>
<td>various</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_2</td>
<td>KU_ISPL2_2016</td>
<td>Mun2016</td>
<td>82.3</td>
<td>left+right+mono</td>
<td>various</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Kong_task1_1</td>
<td>QK</td>
<td>Kong2016</td>
<td>81.0</td>
<td>mono</td>
<td>mel energy</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Kumar_task1_1</td>
<td>Gauss</td>
<td>Elizalde2016</td>
<td>85.9</td>
<td>mono</td>
<td>MFCC distribution</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_1</td>
<td>MARGNet_MWFD</td>
<td>Han2016</td>
<td>84.6</td>
<td>mono</td>
<td>mel energy</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_2</td>
<td>MARGNet_ZENS</td>
<td>Kim2016</td>
<td>85.4</td>
<td>mono</td>
<td>unsupervised</td>
<td>CNN ensemble</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_1</td>
<td>liu-re</td>
<td>Liu2016</td>
<td>83.8</td>
<td>mono</td>
<td>MFCC+mel energy</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_2</td>
<td>liu-pre</td>
<td>Liu2016</td>
<td>83.6</td>
<td>mono</td>
<td>MFCC+mel energy</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Lostanlen_task1_1</td>
<td>LostanlenAnden_2016</td>
<td>Lostanlen2016</td>
<td>80.8</td>
<td>mixed</td>
<td>gammatone scattering</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Marchi_task1_1</td>
<td>Marchi_2016</td>
<td>Marchi2016</td>
<td>86.4</td>
<td>mono</td>
<td>various</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Marques_task1_1</td>
<td>DRKNN_2016</td>
<td>Marques2016</td>
<td>83.1</td>
<td>mono</td>
<td>MFCC</td>
<td>kNN</td>
</tr>
<tr>
<td></td>
<td>Moritz_task1_1</td>
<td></td>
<td>Moritz2016</td>
<td>79.0</td>
<td>left+right+mono</td>
<td>amplitude modulation filter bank</td>
<td>TDNN</td>
</tr>
<tr>
<td></td>
<td>Mulimani_task1_1</td>
<td></td>
<td>Mulimani2016</td>
<td>65.6</td>
<td>mono</td>
<td>MFCC+matching pursuit</td>
<td>GMM</td>
</tr>
<tr>
<td></td>
<td>Nogueira_task1_1</td>
<td></td>
<td>Nogueira2016</td>
<td>81.0</td>
<td>binaural</td>
<td>various</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Patiyal_task1_1</td>
<td>IITMandi_2016</td>
<td>Patiyal2016</td>
<td>78.5</td>
<td>mono</td>
<td>MFCC</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Phan_task1_1</td>
<td>CNN-LTE</td>
<td>Phan2016</td>
<td>83.3</td>
<td>mono</td>
<td>label tree embedding</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Pugachev_task1_1</td>
<td></td>
<td>Pugachev2016</td>
<td>73.1</td>
<td>mono</td>
<td>MFCC</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_1</td>
<td></td>
<td>Dai2016</td>
<td>80.5</td>
<td>mono</td>
<td>various</td>
<td>ensemble</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_2</td>
<td></td>
<td>Dai2016</td>
<td>84.1</td>
<td>mono</td>
<td>various</td>
<td>ensemble</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_3</td>
<td></td>
<td>Dai2016</td>
<td>82.3</td>
<td>mono</td>
<td>various</td>
<td>ensemble</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_4</td>
<td></td>
<td>Dai2016</td>
<td>80.5</td>
<td>mono</td>
<td>various</td>
<td>ensemble</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_1</td>
<td>RAK_2016_1</td>
<td>Rakotomamonjy2016</td>
<td>82.1</td>
<td>mono</td>
<td>various</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_2</td>
<td>RAK_2016_2</td>
<td>Rakotomamonjy2016</td>
<td>79.2</td>
<td>mono</td>
<td>various</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Santoso_task1_1</td>
<td>SWW</td>
<td>Santoso2016</td>
<td>80.8</td>
<td>mono</td>
<td>MFCC</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_1</td>
<td>CQTCNN_1</td>
<td>Lidy2016</td>
<td>81.8</td>
<td>mono</td>
<td>CQT</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_2</td>
<td>CQTCNN_2</td>
<td>Lidy2016</td>
<td>83.3</td>
<td>mono</td>
<td>CQT</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Takahashi_task1_1</td>
<td>UTNII_2016</td>
<td>Takahashi2016</td>
<td>85.6</td>
<td>mono</td>
<td>MFCC</td>
<td>DNN-GMM</td>
</tr>
<tr>
<td></td>
<td>Valenti_task1_1</td>
<td></td>
<td>Valenti2016</td>
<td>86.2</td>
<td>mono</td>
<td>mel energy</td>
<td>CNN</td>
</tr>
<tr>
<td></td>
<td>Vikaskumar_task1_1</td>
<td>ABSP_IITKGP_2016</td>
<td>Vikaskumar2016</td>
<td>81.3</td>
<td>mono</td>
<td>MFCC</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Vu_task1_1</td>
<td></td>
<td>Vu2016</td>
<td>80.0</td>
<td>mono</td>
<td>MFCC</td>
<td>RNN</td>
</tr>
<tr>
<td></td>
<td>Xu_task1_1</td>
<td>HL-DNN-ASC_2016</td>
<td>Xu2016</td>
<td>73.3</td>
<td>mono</td>
<td>mel energy</td>
<td>DNN</td>
</tr>
<tr>
<td></td>
<td>Zoehrer_task1_1</td>
<td></td>
<td>Zoehrer2016</td>
<td>73.1</td>
<td>mono</td>
<td>spectrogram</td>
<td>GRNN</td>
</tr>
</tbody>
</table>
<h2 id="technical-reports">Technical reports</h2>
<div class="btex" data-source="content/data/challenge2016/technical_reports_task1.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Bae2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Bae2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Parallel Combination of LSTM and CNN
       </h4>
<p style="text-align:left">
        Soo Hyun Bae, Inkyu Choi and Nam Soo Kim
       </p>
<p style="text-align:left">
<em>
         Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bae_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bae2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bae2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bae2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Bae_1025.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bae2016" class="panel-collapse collapse" id="collapse-Bae2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Parallel Combination of LSTM and CNN
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deep neural networks(DNNs) have recently achieved a great success in various learning task, and have also been used for classification of environmental sounds. While DNNs are showing their potential in the classification task, they cannot fully utilize the temporal information. In this paper, we propose a neural network architecture for the purpose of using sequential information. The proposed structure is composed of two separated lower networks and one upper network. We refer to these as LSTM layers, CNN layers and connected layers, respectively. The LSTM layers extract the sequential information from consecutive audio features. The CNN layers learn the spectro-temporal locality from spectrogram images. Finally, the connected layers summarize the outputs of two networks to take advantage of the complementary features of the LSTM and CNN by combining them. To compare the proposed method with other neural networks, we conducted a number of experiments on the TUT acoustic scenes 2016 dataset which consists of recordings from various acoustic scenes. By using the proposed combination structure, we achieved higher performance compared to the conventional DNN, CNN and LSTM architecture.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         monophonic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN-RNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bae2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Bae_1025.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bae2016label" class="modal fade" id="bibtex-Bae2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBae2016label">
        Acoustic Scene Classification Using Parallel Combination of LSTM and CNN
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bae2016,
    Author = "Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo",
    title = "Acoustic Scene Classification Using Parallel Combination of {LSTM} and {CNN}",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "Deep neural networks(DNNs) have recently achieved a great success in various learning task, and have also been used for classification of environmental sounds. While DNNs are showing their potential in the classification task, they cannot fully utilize the temporal information. In this paper, we propose a neural network architecture for the purpose of using sequential information. The proposed structure is composed of two separated lower networks and one upper network. We refer to these as LSTM layers, CNN layers and connected layers, respectively. The LSTM layers extract the sequential information from consecutive audio features. The CNN layers learn the spectro-temporal locality from spectrogram images. Finally, the connected layers summarize the outputs of two networks to take advantage of the complementary features of the LSTM and CNN by combining them. To compare the proposed method with other neural networks, we conducted a number of experiments on the TUT acoustic scenes 2016 dataset which consists of recordings from various acoustic scenes. By using the proposed combination structure, we achieved higher performance compared to the conventional DNN, CNN and LSTM architecture."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bao2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Bao2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Technical Report of USTC System for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Xiao Bao, Tian Gao and Jun Du
       </p>
<p style="text-align:left">
<em>
         National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, Anhui, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bao_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bao2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bao2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bao2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Bao_1013.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bao2016" class="panel-collapse collapse" id="collapse-Bao2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Technical Report of USTC System for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This technical report describes our submission for acoustic scene classification task of DCASE 2016. We first explore the use of and Gaussian mixture models (GMM) and ergodic hidden Markov models (HMM). Next, we combine neural network based discriminative models (DNN, CNN) with generative models to build hybrid systems, including DNN-GMM, CNN-GMM, DNN-HMM and CNNHMM. Finally, a system combination method is used to obtain the best overall performance from the multiple systems.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         monophonic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC+mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bao2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Bao_1013.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bao2016label" class="modal fade" id="bibtex-Bao2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBao2016label">
        Technical Report of USTC System for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bao2016,
    Author = "Bao, Xiao and Gao, Tian and Du, Jun",
    title = "Technical Report of {USTC} System for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This technical report describes our submission for acoustic scene classification task of DCASE 2016. We first explore the use of and Gaussian mixture models (GMM) and ergodic hidden Markov models (HMM). Next, we combine neural network based discriminative models (DNN, CNN) with generative models to build hybrid systems, including DNN-GMM, CNN-GMM, DNN-HMM and CNNHMM. Finally, a system combination method is used to obtain the best overall performance from the multiple systems."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Battaglino2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Battaglino2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Daniele Battaglino<sup>1,2</sup>, Ludovick Lepauloux<sup>1</sup> and Nicholas Evans<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>NXP Software, France, <sup>2</sup>EURECOM, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Battaglino_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Battaglino2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Battaglino2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Battaglino2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Battaglino_1032.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Battaglino2016" class="panel-collapse collapse" id="collapse-Battaglino2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Convolutional Neural Networks
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic scene classification (ASC) aims to distinguish between different acoustic environments and is a technology which can be used by smart devices for contextualization and personalization. Standard algorithms exploit hand-crafted features which are unlikely to offer the best potential for reliable classification. This paper reports the first application of convolutional neural networks (CNNs) to ASC, an approach which learns discriminant features automatically from spectral representations of raw acoustic data. A principal influence on performance comes from the specific convolutional filters which can be adjusted to capture different spectrotemporal, recurrent acoustic structure. The proposed CNN approach is shown to outperform a Gaussian mixture model baseline for the DCASE 2016 database even though training data is sparse.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Battaglino2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Battaglino_1032.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Battaglino2016label" class="modal fade" id="bibtex-Battaglino2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBattaglino2016label">
        Acoustic Scene Classification Using Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Battaglino2016,
    Author = "Battaglino, Daniele and Lepauloux, Ludovick and Evans, Nicholas",
    title = "Acoustic Scene Classification Using Convolutional Neural Networks",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "Acoustic scene classification (ASC) aims to distinguish between different acoustic environments and is a technology which can be used by smart devices for contextualization and personalization. Standard algorithms exploit hand-crafted features which are unlikely to offer the best potential for reliable classification. This paper reports the first application of convolutional neural networks (CNNs) to ASC, an approach which learns discriminant features automatically from spectral representations of raw acoustic data. A principal influence on performance comes from the specific convolutional filters which can be adjusted to capture different spectrotemporal, recurrent acoustic structure. The proposed CNN approach is shown to outperform a Gaussian mixture model baseline for the DCASE 2016 database even though training data is sparse."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bisot2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Bisot2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Supervised Nonnegative Matrix Factorization for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Victor Bisot, Romain Serizel, Slim Essid and Gaël Richard
       </p>
<p style="text-align:left">
<em>
         Telecom ParisTech, Paris, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bisot_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bisot2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bisot2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bisot2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Bisot_1010.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bisot2016" class="panel-collapse collapse" id="collapse-Bisot2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Supervised Nonnegative Matrix Factorization for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes our contribution to the 2016 IEEE AASP DCASE challenge for the acoustic scene classification task. We propose a feature learning approach following the idea of decomposing time-frequency representations with nonnegative matrix factorization. We aim at learning a common dictionary representing the data and use projections on this dictionary as features for classification. Our system is based on a novel supervised extension of nonnegative matrix factorization. In the approach we propose, the dictionary and the classifier are optimized jointly in order to find a suited representation to minimize the classification cost. The proposed method significantly outperforms the baseline and provides improved results compared to unsupervised nonnegative matrix factorization.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         monophonic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         NMF
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bisot2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Bisot_1010.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bisot2016label" class="modal fade" id="bibtex-Bisot2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBisot2016label">
        Supervised Nonnegative Matrix Factorization for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bisot2016,
    Author = "Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gaël",
    title = "Supervised Nonnegative Matrix Factorization for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This report describes our contribution to the 2016 IEEE AASP DCASE challenge for the acoustic scene classification task. We propose a feature learning approach following the idea of decomposing time-frequency representations with nonnegative matrix factorization. We aim at learning a common dictionary representing the data and use projections on this dictionary as features for classification. Our system is based on a novel supervised extension of nonnegative matrix factorization. In the approach we propose, the dictionary and the classifier are optimized jointly in order to find a suited representation to minimize the classification cost. The proposed method significantly outperforms the baseline and provides improved results compared to unsupervised nonnegative matrix factorization."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dai2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Dai2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Recognition with Deep Neural Networks (DCASE Challenge 2016)
       </h4>
<p style="text-align:left">
        Wei Dai<sup>1</sup>, Juncheng Li<sup>2</sup>, Phuong Pham<sup>3</sup>, Samarjit Das<sup>2</sup> and Shuhui Qu<sup>4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Carnegie Mellon University, Pittsburgh, USA, <sup>2</sup>Robert Bosch Research and Technology Center, USA, <sup>3</sup>University of Pittsburgh, Pittsburgh, USA, <sup>4</sup>Stanford University, Stanford, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Qu_task1_1</span> <span class="label label-primary">Qu_task1_2</span> <span class="label label-primary">Qu_task1_3</span> <span class="label label-primary">Qu_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dai2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dai2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dai2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Qu_1017.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dai2016" class="panel-collapse collapse" id="collapse-Dai2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Recognition with Deep Neural Networks (DCASE Challenge 2016)
      </h4>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dai2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Qu_1017.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dai2016label" class="modal fade" id="bibtex-Dai2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDai2016label">
        Acoustic Scene Recognition with Deep Neural Networks (DCASE Challenge 2016)
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dai2016,
    Author = "Dai, Wei and Li, Juncheng and Pham, Phuong and Das, Samarjit and Qu, Shuhui",
    title = "Acoustic Scene Recognition with Deep Neural Networks ({DCASE} Challenge 2016)",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Eghbal-Zadeh2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Eghbal-Zadeh2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CP-JKU Submissions for DCASE-2016: a Hybrid Approach Using Binaural I-Vectors and Deep Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Hamid Eghbal-Zadeh, Bernhard Lehner, Matthias Dorfer and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Department of Computational Perception, Johannes Kepler University of Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Eghbal-Zadeh_task1_1</span> <span class="label label-primary">Eghbal-Zadeh_task1_2</span> <span class="label label-primary">Eghbal-Zadeh_task1_3</span> <span class="label label-primary">Eghbal-Zadeh_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Eghbal-Zadeh2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Eghbal-Zadeh2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Eghbal-Zadeh2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Eghbal-Zadeh_1028.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Eghbal-Zadeh2016" class="panel-collapse collapse" id="collapse-Eghbal-Zadeh2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CP-JKU Submissions for DCASE-2016: a Hybrid Approach Using Binaural I-Vectors and Deep Convolutional Neural Networks
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes the 4 submissions for Task 1 (Audio scene classification) of the DCASE-2016 challenge of the CP-JKU team. We propose 4 different approaches for Audio Scene Classification (ASC). First, we propose a novel i-vector extraction scheme for ASC using both left and right audio channels. Second, we propose a Deep Convolutional Neural Network (DCNN) architecture trained on spectrograms of audio excerpts in end-to-end fashion. Third, we use a calibration transformation to improve the performance of our binaural i-vector system. Finally, we propose a late-fusion of our binaural i-vector and the DCNN. We report the performance of our proposed methods on the provided cross-validation setup for the DCASE-2016 challenge. Using the late-fusion approach, we improve the performance of the baseline by 17% in accuracy.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural; monophonic; mono+binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC; spectrogram; MFCC+spectrograms
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         I-vector; CNN; fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Eghbal-Zadeh2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Eghbal-Zadeh_1028.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Eghbal-Zadeh2016label" class="modal fade" id="bibtex-Eghbal-Zadeh2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEghbal-Zadeh2016label">
        CP-JKU Submissions for DCASE-2016: a Hybrid Approach Using Binaural I-Vectors and Deep Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Eghbal-Zadeh2016,
    Author = "Eghbal-Zadeh, Hamid and Lehner, Bernhard and Dorfer, Matthias and Widmer, Gerhard",
    title = "{CP-JKU} Submissions for {DCASE}-2016: a Hybrid Approach Using Binaural I-Vectors and Deep Convolutional Neural Networks",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This report describes the 4 submissions for Task 1 (Audio scene classification) of the DCASE-2016 challenge of the CP-JKU team. We propose 4 different approaches for Audio Scene Classification (ASC). First, we propose a novel i-vector extraction scheme for ASC using both left and right audio channels. Second, we propose a Deep Convolutional Neural Network (DCNN) architecture trained on spectrograms of audio excerpts in end-to-end fashion. Third, we use a calibration transformation to improve the performance of our binaural i-vector system. Finally, we propose a late-fusion of our binaural i-vector and the DCNN. We report the performance of our proposed methods on the provided cross-validation setup for the DCASE-2016 challenge. Using the late-fusion approach, we improve the performance of the baseline by 17\% in accuracy."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Elizalde2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Elizalde2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Experiments on The DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording
       </h4>
<p style="text-align:left">
        Benjamin Elizalde<sup>1</sup>, Anurag Kumar<sup>1</sup>, Ankit Shah<sup>2</sup>, Rohan Badlani<sup>3</sup>, Emmanuel Vincent<sup>4</sup>, Bhiksha Raj<sup>1</sup> and Ian Lane<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Carnegie Mellon University, Pittsburgh, USA, <sup>2</sup>NIT Surathkal, India, <sup>3</sup>BITS, Pilani, India, <sup>4</sup>Inria, Villers-les-Nancy, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kumar_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Elizalde2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Elizalde2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Elizalde2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Kumar_1000.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Elizalde2016" class="panel-collapse collapse" id="collapse-Elizalde2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Experiments on The DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper we present our work on Task 1 Acoustic Scene Classification and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9% compared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based Error Rate of 0.48 compared to the baseline of 0.91
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC distribution
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Elizalde2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Kumar_1000.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Elizalde2016label" class="modal fade" id="bibtex-Elizalde2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexElizalde2016label">
        Experiments on The DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Elizalde2016,
    Author = "Elizalde, Benjamin and Kumar, Anurag and Shah, Ankit and Badlani, Rohan and Vincent, Emmanuel and Raj, Bhiksha and Lane, Ian",
    title = "Experiments on The {DCASE} Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "In this paper we present our work on Task 1 Acoustic Scene Classification and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9\% compared to the baseline of 72.6\% and for Task 3 we achieved a Segment-Based Error Rate of 0.48 compared to the baseline of 0.91"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Foleiss2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Foleiss2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Mel-Band Features for DCASE 2016 Acoustic Scene Classification Task
       </h4>
<p style="text-align:left">
        Juliano Henrique Foleiss<sup>1</sup> and Tiago Fernandes Tavares<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Universidade Tecnologica Federal do Parana, Campo Mourao, Brazil, <sup>2</sup>Universidade Estadual de Campinas, Campinas, Brazil
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Foleiss_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Foleiss2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Foleiss2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Foleiss2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Foleiss_1004.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Foleiss2016" class="panel-collapse collapse" id="collapse-Foleiss2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Mel-Band Features for DCASE 2016 Acoustic Scene Classification Task
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this work we propose to separately calculate spectral low-level features in each frequency band, as it is commonly done in the problem of beat tracking and tempo estimation [1]. We based this assumption in the same auditory models that inspired the use of Mel- Frequency Cepstral Coefficients (MFCCs) [2] or energy through a filter bank [3] for audio genre classification. They rely on a model for the cochlea in which similar regions of the inner ear are stimulated by similar frequencies, and are processed independently. Both the MFCCs and the energy through filterbank approaches only generate an energy spectrum. In our approach, we expand this idea to incorporate other perceptually-inspired features.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         monophonic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Foleiss2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Foleiss_1004.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Foleiss2016label" class="modal fade" id="bibtex-Foleiss2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFoleiss2016label">
        Mel-Band Features for DCASE 2016 Acoustic Scene Classification Task
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Foleiss2016,
    Author = "Foleiss, Juliano Henrique and Fernandes Tavares, Tiago",
    title = "Mel-Band Features for {DCASE} 2016 Acoustic Scene Classification Task",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "In this work we propose to separately calculate spectral low-level features in each frequency band, as it is commonly done in the problem of beat tracking and tempo estimation [1]. We based this assumption in the same auditory models that inspired the use of Mel- Frequency Cepstral Coefficients (MFCCs) [2] or energy through a filter bank [3] for audio genre classification. They rely on a model for the cochlea in which similar regions of the inner ear are stimulated by similar frequencies, and are processed independently. Both the MFCCs and the energy through filterbank approaches only generate an energy spectrum. In our approach, we expand this idea to incorporate other perceptually-inspired features."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Han2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Han2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolutional Neural Network with Multiple-Width Frequency-Delta Data Augmentation for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Yoonchang Han and Kyogu Lee
       </p>
<p style="text-align:left">
<em>
         Music and Audio Research Group, Seoul National University, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lee_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Han2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Han2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Han2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Lee_1034.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Han2016" class="panel-collapse collapse" id="collapse-Han2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolutional Neural Network with Multiple-Width Frequency-Delta Data Augmentation for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we apply convolutional neural network on acoustic scene classification task of DCASE 2016. We propose multiwidth frequency-delta data augmentation which uses static melspectrogram as well as frequency-delta features as individual examples with same labels for the network input, and the experimental result shows that this method significantly improves the performance compare to the case of using static mel-spectrogram input only. In addition, we propose folded mean aggregation, which first multiplies output probabilities of static and delta augmentation data from the same window first prior to audio clip-wise aggregation, and we found that this method reduces the error rate further. The system exhibited a classification accuracy of 0.831 when classifying 15 acoustic scenes.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Han2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Lee_1034.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Han2016label" class="modal fade" id="bibtex-Han2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHan2016label">
        Convolutional Neural Network with Multiple-Width Frequency-Delta Data Augmentation for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Han2016,
    Author = "Han, Yoonchang and Lee, Kyogu",
    title = "Convolutional Neural Network with Multiple-Width Frequency-Delta Data Augmentation for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "In this paper, we apply convolutional neural network on acoustic scene classification task of DCASE 2016. We propose multiwidth frequency-delta data augmentation which uses static melspectrogram as well as frequency-delta features as individual examples with same labels for the network input, and the experimental result shows that this method significantly improves the performance compare to the case of using static mel-spectrogram input only. In addition, we propose folded mean aggregation, which first multiplies output probabilities of static and delta augmentation data from the same window first prior to audio clip-wise aggregation, and we found that this method reduces the error rate further. The system exhibited a classification accuracy of 0.831 when classifying 15 acoustic scenes."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Heittola2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Heittola2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE2016 Baseline System
       </h4>
<p style="text-align:left">
        Toni Heittola, Annamaria Mesaros and Tuomas Virtanen
       </p>
<p style="text-align:left">
<em>
         Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2016_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Heittola2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Heittola2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Heittola2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="documents/mesaros_eusipco2016-dcase.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Heittola2016').collapse('show');window.location.hash='#Heittola2016';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Heittola2016" class="panel-collapse collapse" id="collapse-Heittola2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE2016 Baseline System
      </h4>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         monophonic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Heittola2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="documents/mesaros_eusipco2016-dcase.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Heittola2016label" class="modal fade" id="bibtex-Heittola2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHeittola2016label">
        DCASE2016 Baseline System
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Heittola2016,
    Author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas",
    title = "{DCASE}2016 Baseline System",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hertel2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Hertel2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Classifying Variable-Length Audio Files with All-Convolutional Networks and Masked Global Pooling
       </h4>
<p style="text-align:left">
        Lars Hertel<sup>1</sup>, Huy Phan<sup>1,2</sup> and Alfred Mertins<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute for Signal Processing, University of Luebeck, Luebeck, Germany, <sup>2</sup>Graduate School for Computing in Medicine and Life Sciences, University of Luebeck, Luebeck, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hertel_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hertel2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hertel2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hertel2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Hertel_1016.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hertel2016" class="panel-collapse collapse" id="collapse-Hertel2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Classifying Variable-Length Audio Files with All-Convolutional Networks and Masked Global Pooling
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We trained a deep all-convolutional neural network with masked global pooling to perform single-label classification for acoustic scene classification and multi-label classification for domestic audio tagging in the DCASE-2016 contest. Our network achieved an average accuracy of 84:5% on the four-fold cross-validation for acoustic scene recognition, compared to the provided baseline of 72:5%, and an average equal error rate of 0:17 for domestic audio tagging, compared to the baseline of 0:21. The network therefore improves the baselines by a relative amount of 17% and 19%, respectively. The network only consists of convolutional layers to extract features from the short-time Fourier transform and one global pooling layer to combine those features. It particularly possesses neither fully-connected layers, besides the fully-connected output layer, nor dropout layers.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hertel2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Hertel_1016.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hertel2016label" class="modal fade" id="bibtex-Hertel2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHertel2016label">
        Classifying Variable-Length Audio Files with All-Convolutional Networks and Masked Global Pooling
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hertel2016,
    Author = "Hertel, Lars and Phan, Huy and Mertins, Alfred",
    title = "Classifying Variable-Length Audio Files with All-Convolutional Networks and Masked Global Pooling",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "We trained a deep all-convolutional neural network with masked global pooling to perform single-label classification for acoustic scene classification and multi-label classification for domestic audio tagging in the DCASE-2016 contest. Our network achieved an average accuracy of 84:5\% on the four-fold cross-validation for acoustic scene recognition, compared to the provided baseline of 72:5\%, and an average equal error rate of 0:17 for domestic audio tagging, compared to the baseline of 0:21. The network therefore improves the baselines by a relative amount of 17\% and 19\%, respectively. The network only consists of convolutional layers to extract features from the short-time Fourier transform and one global pooling layer to combine those features. It particularly possesses neither fully-connected layers, besides the fully-connected output layer, nor dropout layers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Empirical Study on Ensemble Method of Deep Neural Networks for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Jaehun Kim and Kyogu Lee
       </p>
<p style="text-align:left">
<em>
         Music and Audio Research Group, Seoul National University, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lee_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Lee_1029.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2016" class="panel-collapse collapse" id="collapse-Kim2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Empirical Study on Ensemble Method of Deep Neural Networks for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The deep neural network has shown superior classification or regression performances in wide range of applications. In particular, the ensemble of deep machines was reported to effectively decrease test errors in many studies. In this work, we extend the scale of deep machines to include hundreds of networks, and apply it to acoustic scene classification. In so doing, several recent learning techniques are employed to accelerate the training process, and a novel stochastic feature diversification method is proposed to allow different contributions from each constituent network. Experimental results with the DCASE2016 dataset indicate that an ensemble of deep machines leads to better performances on the acoustic scene classification.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         unsupervised
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN ensemble
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Lee_1029.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2016label" class="modal fade" id="bibtex-Kim2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2016label">
        Empirical Study on Ensemble Method of Deep Neural Networks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2016,
    Author = "Kim, Jaehun and Lee, Kyogu",
    title = "Empirical Study on Ensemble Method of Deep Neural Networks for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "The deep neural network has shown superior classification or regression performances in wide range of applications. In particular, the ensemble of deep machines was reported to effectively decrease test errors in many studies. In this work, we extend the scale of deep machines to include hundreds of networks, and apply it to acoustic scene classification. In so doing, several recent learning techniques are employed to accelerate the training process, and a novel stochastic feature diversification method is proposed to allow different contributions from each constituent network. Experimental results with the DCASE2016 dataset indicate that an ensemble of deep machines leads to better performances on the acoustic scene classification."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Deep Neural Network Baseline for DCASE Challenge 2016
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Iwona Sobieraj, Wenwu Wang and Mark Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vision, Speech and Signal Processing, University of Surrey, Surrey, United Kingdom
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kong_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Kong_1021.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2016').collapse('show');window.location.hash='#Kong2016';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2016" class="panel-collapse collapse" id="collapse-Kong2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Deep Neural Network Baseline for DCASE Challenge 2016
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The DCASE Challenge 2016 contains tasks for Acoustic Acene Classification (ASC), Acoustic Event Detection (AED), and audio tagging. Since 2006, Deep Neural Networks (DNNs) have been widely applied to computer visions, speech recognition and natural language processing tasks. In this paper, we provide DNN baselines for the DCASE Challenge 2016. For feature extraction, 40 Melfilter bank features are used. Two kinds of Mel banks, same area bank and same height bank are discussed. Experimental results show that the same height bank is better than the same area bank. DNNs with the same structure are applied to all four tasks in the DCASE Challenge 2016. In Task 1 we obtained accuracy of 76.4% using Mel + DNN against 72.5% by using Mel Frequency Ceptral Coefficient (MFCC) + Gaussian Mixture Model (GMM). In Task 2 we obtained F value of 17.4% using Mel + DNN against 41.6% by using Constant Q Transform (CQT) + Nonnegative Matrix Factorization (NMF). In Task 3 we obtained F value of 38.1% using Mel + DNN against 26.6% by using MFCC + GMM. In task 4 we obtained Equal Error Rate (ERR) of 20.9% using Mel + DNN against 21.0% by using MFCC + GMM. Therefore the DNN improves the baseline in Task 1 and Task 3, and is similar to the baseline in Task 4, although is worse than the baseline in Task 2. This indicates that DNNs can be successful in many of these tasks, but may not always work.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Kong_1021.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/DCASE2016_Task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2016label" class="modal fade" id="bibtex-Kong2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2016label">
        Deep Neural Network Baseline for DCASE Challenge 2016
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2016,
    Author = "Kong, Qiuqiang and Sobieraj, Iwona and Wang, Wenwu and Plumbley, Mark",
    title = "Deep Neural Network Baseline for {DCASE} Challenge 2016",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "The DCASE Challenge 2016 contains tasks for Acoustic Acene Classification (ASC), Acoustic Event Detection (AED), and audio tagging. Since 2006, Deep Neural Networks (DNNs) have been widely applied to computer visions, speech recognition and natural language processing tasks. In this paper, we provide DNN baselines for the DCASE Challenge 2016. For feature extraction, 40 Melfilter bank features are used. Two kinds of Mel banks, same area bank and same height bank are discussed. Experimental results show that the same height bank is better than the same area bank. DNNs with the same structure are applied to all four tasks in the DCASE Challenge 2016. In Task 1 we obtained accuracy of 76.4\% using Mel + DNN against 72.5\% by using Mel Frequency Ceptral Coefficient (MFCC) + Gaussian Mixture Model (GMM). In Task 2 we obtained F value of 17.4\% using Mel + DNN against 41.6\% by using Constant Q Transform (CQT) + Nonnegative Matrix Factorization (NMF). In Task 3 we obtained F value of 38.1\% using Mel + DNN against 26.6\% by using MFCC + GMM. In task 4 we obtained Equal Error Rate (ERR) of 20.9\% using Mel + DNN against 21.0\% by using MFCC + GMM. Therefore the DNN improves the baseline in Task 1 and Task 3, and is similar to the baseline in Task 4, although is worse than the baseline in Task 2. This indicates that DNNs can be successful in many of these tasks, but may not always work."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lidy2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Lidy2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CQT-Based Convolutional Neural Networks for Audio Scene Classification and Domestic Audio Tagging
       </h4>
<p style="text-align:left">
        Thomas Lidy<sup>1</sup> and Alexander Schindler<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute of Software Technology, Vienna University of Technology, Vienna, Austria, <sup>2</sup>Digital Safety and Security, Austrian Institute of Technology, Vienna, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Schindler_task1_1</span> <span class="label label-primary">Schindler_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lidy2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lidy2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lidy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Schindler_1035.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lidy2016" class="panel-collapse collapse" id="collapse-Lidy2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CQT-Based Convolutional Neural Networks for Audio Scene Classification and Domestic Audio Tagging
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       For the DCASE 2016 audio benchmarking contest, we submitted a parallel Convolutional Neural Network architecture for the tasks of 1) classifying acoustic scenes (task 1) and urban sound scapes and 2) domestic audio tagging (task 4). A popular choice for input to a Convolutional Neural Network in audio classification problems are Mel-transformed spectrograms. We, however, found that a Constant-Q-transformed input improves results. Furthermore, we evaluated critical parameters such as the number of necessary bands and filter sizes in a Convolutional Neural Network. Finally, we propose a parallel (graph-based) neural network architecture which captures relevant audio characteristics both in time and in frequency, and submitted it to the DCASE 2016 tasks 1 and 4, with some slight alterations described in this paper. Our approach shows a 10.7 % relative improvement of the baseline system of the Acoustic Scenes Classification task on the development set of task 1[1].
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lidy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Schindler_1035.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lidy2016label" class="modal fade" id="bibtex-Lidy2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLidy2016label">
        CQT-Based Convolutional Neural Networks for Audio Scene Classification and Domestic Audio Tagging
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lidy2016,
    Author = "Lidy, Thomas and Schindler, Alexander",
    title = "{CQT}-Based Convolutional Neural Networks for Audio Scene Classification and Domestic Audio Tagging",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "For the DCASE 2016 audio benchmarking contest, we submitted a parallel Convolutional Neural Network architecture for the tasks of 1) classifying acoustic scenes (task 1) and urban sound scapes and 2) domestic audio tagging (task 4). A popular choice for input to a Convolutional Neural Network in audio classification problems are Mel-transformed spectrograms. We, however, found that a Constant-Q-transformed input improves results. Furthermore, we evaluated critical parameters such as the number of necessary bands and filter sizes in a Convolutional Neural Network. Finally, we propose a parallel (graph-based) neural network architecture which captures relevant audio characteristics both in time and in frequency, and submitted it to the DCASE 2016 tasks 1 and 4, with some slight alterations described in this paper. Our approach shows a 10.7 \% relative improvement of the baseline system of the Acoustic Scenes Classification task on the development set of task 1[1]."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification by Feed Forward Neural Network with Class Dependent Attention Mechanism
       </h4>
<p style="text-align:left">
        Jiaming Liu<sup>1</sup>, Hihui Wang<sup>1</sup>, Mingyu You<sup>1</sup>, Ruiwei Zhao<sup>1</sup> and Guozheng Li<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Control Science and Engineering, Tongji University, Shanghai, China, <sup>2</sup>Data Center of Traditional Chinese Medicine, China Academy of Chinese Medical Science, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_task1_1</span> <span class="label label-primary">Liu_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Liu_1001.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2016" class="panel-collapse collapse" id="collapse-Liu2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification by Feed Forward Neural Network with Class Dependent Attention Mechanism
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In the acoustic scene classification task, we proposed a novel attention mechanism embedded to feed forward networks. On top of a shared input layer, 15 separated attention modules are calculated for each class, and output 15 class dependent feature vectors. Then the feature vectors are mapped to class labels by 15 subnetworks. A softmax layer is employed on the very top of the network. In our experiments, the default feature, MFCC and mel filterbank with delta and acceleration, is used to represent each segment. We split each 30s audio recording into 1s segments and calculate label for the segment, then output the most frequent label for the 30s recording. The best single neural network could get 77.4% cross validation accuracy without further feature engineering and any data augmentation. We train 5 models with MFCC features and 5 models with mel filterbank features, then make an ensemble with majority vote, getting a 78.6% final cross validation result. For submission, the 10 models are retrained with full dataset. And, the final submission is a majority vote ensemble of the 10 models' outputs.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC+mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Liu_1001.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2016label" class="modal fade" id="bibtex-Liu2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2016label">
        Acoustic Scene Classification by Feed Forward Neural Network with Class Dependent Attention Mechanism
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2016,
    Author = "Liu, Jiaming and Wang, Hihui and You, Mingyu and Zhao, Ruiwei and Li, Guozheng",
    title = "Acoustic Scene Classification by Feed Forward Neural Network with Class Dependent Attention Mechanism",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "In the acoustic scene classification task, we proposed a novel attention mechanism embedded to feed forward networks. On top of a shared input layer, 15 separated attention modules are calculated for each class, and output 15 class dependent feature vectors. Then the feature vectors are mapped to class labels by 15 subnetworks. A softmax layer is employed on the very top of the network. In our experiments, the default feature, MFCC and mel filterbank with delta and acceleration, is used to represent each segment. We split each 30s audio recording into 1s segments and calculate label for the segment, then output the most frequent label for the 30s recording. The best single neural network could get 77.4\% cross validation accuracy without further feature engineering and any data augmentation. We train 5 models with MFCC features and 5 models with mel filterbank features, then make an ensemble with majority vote, getting a 78.6\% final cross validation result. For submission, the 10 models are retrained with full dataset. And, the final submission is a majority vote ensemble of the 10 models' outputs."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lostanlen2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Lostanlen2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Binaural Scene Classification with Wavelet Scattering
       </h4>
<p style="text-align:left">
        Vincent Lostanlen<sup>1</sup> and Joakim Andén<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Departement d’Informatique, Ecole normale superieure, Paris, France, <sup>2</sup>Program in Applied and Computational Mathematics, Princeton University, Princeton, NJ, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lostanlen_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lostanlen2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lostanlen2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lostanlen2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Lostanlen_1003.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lostanlen2016" class="panel-collapse collapse" id="collapse-Lostanlen2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Binaural Scene Classification with Wavelet Scattering
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This technical report describes our contribution to the scene classification task of the 2016 edition of the IEEE AASP Challenge for Detection and Classification of Acoustic Scenes and Events (DCASE). Our computational pipeline consists of a gammatone scattering transform, logarithmically compressed and coupled with a per-frame linear support vector machine. At test time, frame-level labels are aggregated over the whole recording by majority vote. During the training phase, we propose a novel data augmentation technique, where left and right channels are mixed at different proportions to introduce invariance to sound direction in the training data.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         gammatone scattering
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lostanlen2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Lostanlen_1003.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lostanlen2016label" class="modal fade" id="bibtex-Lostanlen2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLostanlen2016label">
        Binaural Scene Classification with Wavelet Scattering
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lostanlen2016,
    Author = "Lostanlen, Vincent and Andén, Joakim",
    title = "Binaural Scene Classification with Wavelet Scattering",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This technical report describes our contribution to the scene classification task of the 2016 edition of the IEEE AASP Challenge for Detection and Classification of Acoustic Scenes and Events (DCASE). Our computational pipeline consists of a gammatone scattering transform, logarithmically compressed and coupled with a per-frame linear support vector machine. At test time, frame-level labels are aggregated over the whole recording by majority vote. During the training phase, we propose a novel data augmentation technique, where left and right channels are mixed at different proportions to introduce invariance to sound direction in the training data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Marchi2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Marchi2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The Up System for The 2016 DCASE Challenge Using Deep Recurrent Neural Network and Multiscale Kernel Subspace Learning
       </h4>
<p style="text-align:left">
        Erik Marchi<sup>1,2</sup>, Dario Tonelli<sup>3</sup>, Xinzhou Xu<sup>1</sup>, Fabien Ringeval<sup>1,2</sup>, Jun Deng<sup>1</sup>, Stefano Squartini<sup>3</sup> and Björn Schuller<sup>1,2,4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>2</sup>audEERING GmbH, Gilching, Germany, <sup>3</sup>A3LAB, Department of Information Engineering, Universita Politecnica delle Marche, Italy, <sup>4</sup>Department of Computing, Imperial College London, London, United Kingdom
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Marchi_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Marchi2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Marchi2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Marchi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Marchi_1019.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Marchi2016" class="panel-collapse collapse" id="collapse-Marchi2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The Up System for The 2016 DCASE Challenge Using Deep Recurrent Neural Network and Multiscale Kernel Subspace Learning
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a system for acoustic scene classification using pairwise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the one-against-all paradigm to perform a pairwise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.5% is achieved. In this technical report, we provide a description of the actual system submitted to the challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Marchi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Marchi_1019.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Marchi2016label" class="modal fade" id="bibtex-Marchi2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMarchi2016label">
        The Up System for The 2016 DCASE Challenge Using Deep Recurrent Neural Network and Multiscale Kernel Subspace Learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Marchi2016,
    Author = "Marchi, Erik and Tonelli, Dario and Xu, Xinzhou and Ringeval, Fabien and Deng, Jun and Squartini, Stefano and Schuller, Björn",
    title = "The Up System for The 2016 {DCASE} Challenge Using Deep Recurrent Neural Network and Multiscale Kernel Subspace Learning",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "We propose a system for acoustic scene classification using pairwise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the one-against-all paradigm to perform a pairwise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.5\% is achieved. In this technical report, we provide a description of the actual system submitted to the challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Marques2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Marques2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        TUT Acoustic Scene Classification Submission
       </h4>
<p style="text-align:left">
        Gonçalo Marques<sup>1</sup> and Thibault Langlois<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Electronic Telecom. and Comp. Dept., Instituto Superior de Engenharia de Lisboa, Lisboa, Portugal, <sup>2</sup>Informatics Dept., Faculdade de Ciências da Universidade de Lisboa, Lisboa, Portugal
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Marques_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Marques2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Marques2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Marques2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Marques_1012.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Marques2016" class="panel-collapse collapse" id="collapse-Marques2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       TUT Acoustic Scene Classification Submission
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This technical report presents the details of our submission to the D-CASE classification challenge, Task 1: Acoustic Scene Classification. The method used consists in a feature extraction phase followed by two dimensionality reduction steps (PCA and LDA) the classification being done using the k nearest-neighbours algorithm.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         kNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Marques2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Marques_1012.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Marques2016label" class="modal fade" id="bibtex-Marques2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMarques2016label">
        TUT Acoustic Scene Classification Submission
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Marques2016,
    Author = "Marques, Gonçalo and Langlois, Thibault",
    title = "{TUT} Acoustic Scene Classification Submission",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This technical report presents the details of our submission to the D-CASE classification challenge, Task 1: Acoustic Scene Classification. The method used consists in a feature extraction phase followed by two dimensionality reduction steps (PCA and LDA) the classification being done using the k nearest-neighbours algorithm."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Moritz2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Moritz2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features
       </h4>
<p style="text-align:left">
        Niko Moritz<sup>1</sup>, Jens Schröder<sup>1</sup>, Stefan Goetze<sup>1</sup>, Jörn Anemüller<sup>2</sup> and Birger Kollmeier<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Project Group for Hearing, Speech, and Audio Processing, Fraunhofer IDMT, Oldenburg, Germany, <sup>2</sup>Medizinische Physik &amp; Hearing4all, University of Oldenburg, Oldenburg, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Moritz_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Moritz2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Moritz2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Moritz2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Moritz_1027.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Moritz2016" class="panel-collapse collapse" id="collapse-Moritz2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE'16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in comparison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual information similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recognition accuracy of 76.5 %, which is 4.0 % higher compared to the DCASE'16 baseline system.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left+right+mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         amplitude modulation filter bank
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         TDNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Moritz2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Moritz_1027.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Moritz2016label" class="modal fade" id="bibtex-Moritz2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMoritz2016label">
        Acoustic Scene Classification Using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Moritz2016,
    Author = "Moritz, Niko and Schröder, Jens and Goetze, Stefan and Anemüller, Jörn and Kollmeier, Birger",
    title = "Acoustic Scene Classification Using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE'16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in comparison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual information similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recognition accuracy of 76.5 \%, which is 4.0 \% higher compared to the DCASE'16 baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mulimani2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Mulimani2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using MFCC and MP Features
       </h4>
<p style="text-align:left">
        Manjunath Mulimani and Shashidhar G. Koolagudi
       </p>
<p style="text-align:left">
<em>
         Dept. of Computer Science &amp; Engineering, National Institute of Technology, Karnataka, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mulimani_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mulimani2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mulimani2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mulimani2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Mulimani_1023.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mulimani2016" class="panel-collapse collapse" id="collapse-Mulimani2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using MFCC and MP Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper, clearly describes our experiments for efficient acoustic scene classification task as a part of Detection and Classification of Acoustic Scenes and Events-2016 (DCASE-2016) IEEE Audio and Acoustic Signal Processing (AASP) challenge. Identification of features from given audio clips to appropriate acoustic scene classification is a challenging task because of heterogeneity by thier nature. In order to identify such features, in this paper we have implemented few methods using Matching Pursuit (MP) algorithm in order to extract Time-Frequency (TF) based features. MP algorithm is used to select atoms iteratively among the set of parameterized waveforms in the dictionary that best correlates the original signal structure. Using these selected set of atoms mean and standard deviation of amplitude and frequency parameters of first few (n) atoms are calculated separately, resulting into four MP feature sets. Combination of twenty MFCCs along with four MP features enhanced the recognition accuracy of acoustic scenes using GMM classifier.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC+matching pursuit
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mulimani2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Mulimani_1023.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mulimani2016label" class="modal fade" id="bibtex-Mulimani2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMulimani2016label">
        Acoustic Scene Classification Using MFCC and MP Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mulimani2016,
    Author = "Mulimani, Manjunath and Koolagudi, Shashidhar G.",
    title = "Acoustic Scene Classification Using {MFCC} and {MP} Features",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This paper, clearly describes our experiments for efficient acoustic scene classification task as a part of Detection and Classification of Acoustic Scenes and Events-2016 (DCASE-2016) IEEE Audio and Acoustic Signal Processing (AASP) challenge. Identification of features from given audio clips to appropriate acoustic scene classification is a challenging task because of heterogeneity by thier nature. In order to identify such features, in this paper we have implemented few methods using Matching Pursuit (MP) algorithm in order to extract Time-Frequency (TF) based features. MP algorithm is used to select atoms iteratively among the set of parameterized waveforms in the dictionary that best correlates the original signal structure. Using these selected set of atoms mean and standard deviation of amplitude and frequency parameters of first few (n) atoms are calculated separately, resulting into four MP feature sets. Combination of twenty MFCCs along with four MP features enhanced the recognition accuracy of acoustic scenes using GMM classifier."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mun2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Mun2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Deep Neural Network Bottleneck Feature for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Seongkyu Mun<sup>1</sup>, Sangwook Park<sup>2</sup>, Younglo Lee<sup>2</sup> and Hanseok Ko<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Visual Information Processing, Korea University, Seoul, South Korea, <sup>2</sup>School of Electrical Engineering, Korea University, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ko_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mun2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mun2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mun2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Ko_1014.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mun2016" class="panel-collapse collapse" id="collapse-Mun2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Deep Neural Network Bottleneck Feature for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Bottleneck features have been shown to be effective in improv-ing the accuracy of speaker recognition, language identification and automatic speech recognition. However, few works have focused on bottleneck features for acoustic scene classification. This report proposes a novel acoustic scene feature extraction using bottleneck features derived from a Deep Neural Network (DNN). On the official development set with our settings, a fea-ture set that includes bottleneck features and Perceptual Linear Prediction (PLP) feature shows a best accuracy rate.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left+right+mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mun2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Ko_1014.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mun2016label" class="modal fade" id="bibtex-Mun2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMun2016label">
        Deep Neural Network Bottleneck Feature for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mun2016,
    Author = "Mun, Seongkyu and Park, Sangwook and Lee, Younglo and Ko, Hanseok",
    title = "Deep Neural Network Bottleneck Feature for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "Bottleneck features have been shown to be effective in improv-ing the accuracy of speaker recognition, language identification and automatic speech recognition. However, few works have focused on bottleneck features for acoustic scene classification. This report proposes a novel acoustic scene feature extraction using bottleneck features derived from a Deep Neural Network (DNN). On the official development set with our settings, a fea-ture set that includes bottleneck features and Perceptual Linear Prediction (PLP) feature shows a best accuracy rate."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nogueira2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Nogueira2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Scene Identification Based on Monaural and Binaural Features
       </h4>
<p style="text-align:left">
        Waldo Nogueira<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Medical University Hannover, Hannover, Germany, <sup>2</sup>Cluster of Excellence Hearing4all, Hannover, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nogueira_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nogueira2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nogueira2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nogueira2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Nogueira_1009.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nogueira2016" class="panel-collapse collapse" id="collapse-Nogueira2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Scene Identification Based on Monaural and Binaural Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This submission to the sub-task acoustic scene classification of the IEEE DCASE 2016 Challenge: Acoustic scene classification is based on a feature extraction module based on the concatenation of monaural and binaural features. Monaural features are based on Mel Frequency Cepstrums summarized using recurrence quantification analysis. Binaural features are based on the extraction of inter-aural differences (level and time) and the coherence between the two channel stereo recordings. These features are used in conjunction with a support vector-machine for the classification of the acoustic sound scenes. In this short paper the impact of different features is analyzed.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nogueira2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Nogueira_1009.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nogueira2016label" class="modal fade" id="bibtex-Nogueira2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNogueira2016label">
        Sound Scene Identification Based on Monaural and Binaural Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nogueira2016,
    Author = "Nogueira, Waldo",
    title = "Sound Scene Identification Based on Monaural and Binaural Features",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This submission to the sub-task acoustic scene classification of the IEEE DCASE 2016 Challenge: Acoustic scene classification is based on a feature extraction module based on the concatenation of monaural and binaural features. Monaural features are based on Mel Frequency Cepstrums summarized using recurrence quantification analysis. Binaural features are based on the extraction of inter-aural differences (level and time) and the coherence between the two channel stereo recordings. These features are used in conjunction with a support vector-machine for the classification of the acoustic sound scenes. In this short paper the impact of different features is analyzed."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Park2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Score Fusion of Classification Systems for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Sangwook Park<sup>1</sup>, Seongkyu Mun<sup>2</sup>, Younglo Lee<sup>1</sup> and Hanseok Ko<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Electrical Engineering, Korea University, Seoul, South Korea, <sup>2</sup>Department of Visual Information Processing, Korea University, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ko_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Park2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Ko_1024.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2016" class="panel-collapse collapse" id="collapse-Park2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Score Fusion of Classification Systems for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This is a technical report about our study for an acoustic scene classification which is a task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In order to achieve this purpose, we investigated several methods in three aspects, feature extraction, generative/discriminative machine learning, and score fusion for a final decision. For finding an appropriate frame based feature, a new feature was devised after investigating several features. And then, models based on both generative and discriminative learning were applied for classifying the feature. From these studies, several systems composed of feature and classifier were considered. The final result was determined by fusing individual results. In Section 3, experiment results are summarized, and concluding remarks of this report are presented in Section 4.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Ko_1024.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2016label" class="modal fade" id="bibtex-Park2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2016label">
        Score Fusion of Classification Systems for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Park2016,
    Author = "Park, Sangwook and Mun, Seongkyu and Lee, Younglo and Ko, Hanseok",
    title = "Score Fusion of Classification Systems for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This is a technical report about our study for an acoustic scene classification which is a task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In order to achieve this purpose, we investigated several methods in three aspects, feature extraction, generative/discriminative machine learning, and score fusion for a final decision. For finding an appropriate frame based feature, a new feature was devised after investigating several features. And then, models based on both generative and discriminative learning were applied for classifying the feature. From these studies, several systems composed of feature and classifier were considered. The final result was determined by fusing individual results. In Section 3, experiment results are summarized, and concluding remarks of this report are presented in Section 4."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Patiyal2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Patiyal2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep Learning
       </h4>
<p style="text-align:left">
        Rohit Patiyal and Padmanabhan Rajan
       </p>
<p style="text-align:left">
<em>
         School of Computing and Electrical Engineering, Indian Institute of Technology Mandi, Himachal Pradesh, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Patiyal_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Patiyal2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Patiyal2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Patiyal2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Patiyal_1026.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Patiyal2016" class="panel-collapse collapse" id="collapse-Patiyal2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep Learning
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic Scene Classification (ASC) is the task of classifying audio samples on the basis of their soundscapes. This is one of the tasks taken up by Detection and Classification of Acoustic Scenes and Events 2016 (DCASE-2016) challenge. A labeled dataset of audio samples from various scenes is provided and solutions are invited. In this paper, use of Deep Neural Networks (DNN) is proposed for the task of ASC. Here, different methods for extracting features with different classification algorithms are explored. It is observed that DNN works significantly better as compared to other methods trained over the same set of features. It performs at par with the state of-the-art techniques presented in DCASE-2013. It is concluded that the use of MFCC features with DNN works the best, giving 97.6 % cross-validation score on the development dataset-2016 data for a particular set of parameters for the DNN. Also training a DNN does not take larger run times compared to others methods.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Patiyal2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Patiyal_1026.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Patiyal2016label" class="modal fade" id="bibtex-Patiyal2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPatiyal2016label">
        Acoustic Scene Classification Using Deep Learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Patiyal2016,
    Author = "Patiyal, Rohit and Rajan, Padmanabhan",
    title = "Acoustic Scene Classification Using Deep Learning",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "Acoustic Scene Classification (ASC) is the task of classifying audio samples on the basis of their soundscapes. This is one of the tasks taken up by Detection and Classification of Acoustic Scenes and Events 2016 (DCASE-2016) challenge. A labeled dataset of audio samples from various scenes is provided and solutions are invited. In this paper, use of Deep Neural Networks (DNN) is proposed for the task of ASC. Here, different methods for extracting features with different classification algorithms are explored. It is observed that DNN works significantly better as compared to other methods trained over the same set of features. It performs at par with the state of-the-art techniques presented in DCASE-2013. It is concluded that the use of MFCC features with DNN works the best, giving 97.6 \% cross-validation score on the development dataset-2016 data for a particular set of parameters for the DNN. Also training a DNN does not take larger run times compared to others methods."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Phan2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Phan2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label Tree Embeddings for Audio Scene Recognition
       </h4>
<p style="text-align:left">
        Huy Phan<sup>1,2</sup>, Lars Hertel<sup>1</sup>, Marco Maass<sup>1</sup>, Philipp Koch<sup>1</sup> and Alfred Mertins<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute for Signal Processing, University of Luebeck, Luebeck, Germany, <sup>2</sup>Graduate School for Computing in Medicine and Life Sciences, University of Luebeck, Luebeck, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Phan_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Phan2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Phan2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Phan2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Phan_1007.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Phan2016" class="panel-collapse collapse" id="collapse-Phan2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label Tree Embeddings for Audio Scene Recognition
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We describe in this report our audio scene recognition system submitted to the DCASE 2016 challenge [1]. Firstly, given the label set of the scenes, a label tree is automatically constructed. This category taxonomy is then used in the feature extraction step in which an audio scene instance is represented by a label tree embedding image. Different convolutional neural networks, which are tailored for the task at hand, are finally learned on top of the image features for scene recognition. Our system reaches an overall recognition accuracy of 81.2% and outperforms the DCASE 2016 baseline with an absolute improvement of 8.7% on the development data.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         label tree embedding
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Phan2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Phan_1007.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Phan2016label" class="modal fade" id="bibtex-Phan2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPhan2016label">
        CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label Tree Embeddings for Audio Scene Recognition
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Phan2016,
    Author = "Phan, Huy and Hertel, Lars and Maass, Marco and Koch, Philipp and Mertins, Alfred",
    title = "{CNN}-{LTE}: a Class of 1-X Pooling Convolutional Neural Networks on Label Tree Embeddings for Audio Scene Recognition",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "We describe in this report our audio scene recognition system submitted to the DCASE 2016 challenge [1]. Firstly, given the label set of the scenes, a label tree is automatically constructed. This category taxonomy is then used in the feature extraction step in which an audio scene instance is represented by a label tree embedding image. Different convolutional neural networks, which are tailored for the task at hand, are finally learned on top of the image features for scene recognition. Our system reaches an overall recognition accuracy of 81.2\% and outperforms the DCASE 2016 baseline with an absolute improvement of 8.7\% on the development data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pugachev2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Pugachev2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Deep Neural Network for Acoustic Scene Detection
       </h4>
<p style="text-align:left">
        Alexei Pugachev and Dmitrii Ubskii
       </p>
<p style="text-align:left">
<em>
         Chair of Speech Information Systems, ITMO University, St. Petersburg, Russia
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Pugachev_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Pugachev2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pugachev2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pugachev2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Pugachev_1015.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pugachev2016" class="panel-collapse collapse" id="collapse-Pugachev2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Deep Neural Network for Acoustic Scene Detection
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The DCASE 2016 challenge comprised the task of Acoustic Scene Classification. The goal of this task was to classify test recordings into one of predefined classes that characterizes the environment during the recording.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pugachev2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Pugachev_1015.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pugachev2016label" class="modal fade" id="bibtex-Pugachev2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPugachev2016label">
        Deep Neural Network for Acoustic Scene Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Pugachev2016,
    Author = "Pugachev, Alexei and Ubskii, Dmitrii",
    title = "Deep Neural Network for Acoustic Scene Detection",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "The DCASE 2016 challenge comprised the task of Acoustic Scene Classification. The goal of this task was to classify test recordings into one of predefined classes that characterizes the environment during the recording."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Rakotomamonjy2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Rakotomamonjy2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Enriched Supervised Feature Learning for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Alain Rakotomamonjy
       </p>
<p style="text-align:left">
<em>
         Music and Audio Research Group, Normandie Université, Saint Etienne du Rouvray, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Rakotomamonjy_task1_1</span> <span class="label label-primary">Rakotomamonjy_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Rakotomamonjy2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Rakotomamonjy2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Rakotomamonjy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Rakotomamonjy_1002.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Rakotomamonjy2016" class="panel-collapse collapse" id="collapse-Rakotomamonjy2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Enriched Supervised Feature Learning for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents the methodology we have followed for our submission at the DCASE 2016 competition on acoustic scene classification (Task 1). The approach is based on a supervised feature learning technique which is built upon matrix factorization of timefrequency representation of an audio scene. As an original contribution, we have introduced a non-negative supervised matrix factorization that helps in learning discriminative codes. Our experiments have shown that these supervised features perform slightly better than convolutional neural networks for this challenge. In addition, when they are coupled with some hand-crafted features such as histogram of gradient, their performances are further boosted.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Rakotomamonjy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Rakotomamonjy_1002.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Rakotomamonjy2016label" class="modal fade" id="bibtex-Rakotomamonjy2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRakotomamonjy2016label">
        Enriched Supervised Feature Learning for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Rakotomamonjy2016,
    Author = "Rakotomamonjy, Alain",
    title = "Enriched Supervised Feature Learning for Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This paper presents the methodology we have followed for our submission at the DCASE 2016 competition on acoustic scene classification (Task 1). The approach is based on a supervised feature learning technique which is built upon matrix factorization of timefrequency representation of an audio scene. As an original contribution, we have introduced a non-negative supervised matrix factorization that helps in learning discriminative codes. Our experiments have shown that these supervised features perform slightly better than convolutional neural networks for this challenge. In addition, when they are coupled with some hand-crafted features such as histogram of gradient, their performances are further boosted."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Santoso2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Santoso2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Network-In-Network Based Convolutional Neural Network
       </h4>
<p style="text-align:left">
        Andri Santoso, Chien-Yao Wang and Jia-Ching Wang
       </p>
<p style="text-align:left">
<em>
         National Central University, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Santoso_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Santoso2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Santoso2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Santoso2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Santoso_1031.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Santoso2016" class="panel-collapse collapse" id="collapse-Santoso2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Network-In-Network Based Convolutional Neural Network
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present our entry to the challenge of detection and classification of acoustic scenes and events (DCASE). The submission for this challenge is for the task of automatic audio scene classification. Our approach is based on the deep learning method that is adopted from computer vision research field. The convolutional neural network is adopted to solve the problem of audio based scene classification, specifically the architecture of network-in-network is utilized to build the classifier. For the feature extraction part, mel frequency spectral coefficients (MFCC) is used as the input vector for the classifier. Differ from the original architecture of network-in-network, in this work we perform 1-D convolution operation instead of performing 2-D convolution. The classifier is trained using every frames from MFCC feature set, and the results for every frames are then thresholded and voted to choose the final scene label of audio data. The proposed work in this paper shows a better performance of the provided baseline system of DCASE challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Santoso2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Santoso_1031.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Santoso2016label" class="modal fade" id="bibtex-Santoso2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSantoso2016label">
        Acoustic Scene Classification Using Network-In-Network Based Convolutional Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Santoso2016,
    Author = "Santoso, Andri and Wang, Chien-Yao and Wang, Jia-Ching",
    title = "Acoustic Scene Classification Using Network-In-Network Based Convolutional Neural Network",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "In this paper, we present our entry to the challenge of detection and classification of acoustic scenes and events (DCASE). The submission for this challenge is for the task of automatic audio scene classification. Our approach is based on the deep learning method that is adopted from computer vision research field. The convolutional neural network is adopted to solve the problem of audio based scene classification, specifically the architecture of network-in-network is utilized to build the classifier. For the feature extraction part, mel frequency spectral coefficients (MFCC) is used as the input vector for the classifier. Differ from the original architecture of network-in-network, in this work we perform 1-D convolution operation instead of performing 2-D convolution. The classifier is trained using every frames from MFCC feature set, and the results for every frames are then thresholded and voted to choose the final scene label of audio data. The proposed work in this paper shows a better performance of the provided baseline system of DCASE challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sena_Mafra2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Sena_Mafra2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification: an Evaluation of an Extremely Compact Feature Representation
       </h4>
<p style="text-align:left">
        Gustavo Sena Mafra<sup>1</sup>, Quang-Khanh-Ngoc Duong<sup>2</sup>, Alexey Ozerov<sup>2</sup> and Patrick Perez<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Universidade Federal de Santa Catarina, Santa Catarina, Brazil, <sup>2</sup>Technicolor, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Duong_task1_1</span> <span class="label label-primary">Duong_task1_2</span> <span class="label label-primary">Duong_task1_3</span> <span class="label label-primary">Duong_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sena_Mafra2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sena_Mafra2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sena_Mafra2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Duong_1005.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sena_Mafra2016" class="panel-collapse collapse" id="collapse-Sena_Mafra2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification: an Evaluation of an Extremely Compact Feature Representation
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper investigates several approaches to address the acoustic scene classification (ASC) task. We start from low-level feature representation for segmented audio frames and investigate different time granularity for feature aggregation. We study the use of support vector machine (SVM), as a well-known classifier, together with two popular neural network (NN) architectures, namely multilayer perceptron (MLP) and convolutional neural network (CNN), for higher level feature learning and classification. We evaluate the performance of these approaches on benchmark datasets provided from the 2013 and 2016 Detection and Classification of Acoustic Scenes and Events (DCASE) challenges. We observe that a simple approach exploiting averaged Mel-log-spectrogram, as an extremely compact feature, and SVM can obtain even better result than NN-based approaches and comparable performance with the best systems in the DCASE 2013 challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         monophonic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM; DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sena_Mafra2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Duong_1005.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sena_Mafra2016label" class="modal fade" id="bibtex-Sena_Mafra2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSena_Mafra2016label">
        Acoustic Scene Classification: an Evaluation of an Extremely Compact Feature Representation
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sena_Mafra2016,
    Author = "Sena Mafra, Gustavo and Duong, Quang-Khanh-Ngoc and Ozerov, Alexey and Perez, Patrick",
    title = "Acoustic Scene Classification: an Evaluation of an Extremely Compact Feature Representation",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This paper investigates several approaches to address the acoustic scene classification (ASC) task. We start from low-level feature representation for segmented audio frames and investigate different time granularity for feature aggregation. We study the use of support vector machine (SVM), as a well-known classifier, together with two popular neural network (NN) architectures, namely multilayer perceptron (MLP) and convolutional neural network (CNN), for higher level feature learning and classification. We evaluate the performance of these approaches on benchmark datasets provided from the 2013 and 2016 Detection and Classification of Acoustic Scenes and Events (DCASE) challenges. We observe that a simple approach exploiting averaged Mel-log-spectrogram, as an extremely compact feature, and SVM can obtain even better result than NN-based approaches and comparable performance with the best systems in the DCASE 2013 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Takahashi2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Takahashi2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep Neural Network and Frame-Concatenated Acoustic Feature
       </h4>
<p style="text-align:left">
        Gen Takahashi<sup>1</sup>, Takeshi Yamada<sup>1</sup>, Shoji Makino<sup>1</sup> and Nobutaka Ono<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>University of Tsukuba, Tsukuba, Japan, <sup>2</sup>National Institute of Informatics / SOKENDAI, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Takahashi_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Takahashi2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Takahashi2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Takahashi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Takahashi_1022.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Takahashi2016" class="panel-collapse collapse" id="collapse-Takahashi2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep Neural Network and Frame-Concatenated Acoustic Feature
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes our contribution to the task of acoustic scene classification in the DCASE2016 (Detection and Classification of Acoustic Scenes and Events 2016) Challenge set by IEEE AASP. In this work, we applied the DNN-GMM (Deep Neural Network-Gaussian Mixture Model) to acoustic scene classification. We introduced high-dimensional features that are concatenated with acoustic features in temporally adjacent frames. As a result, it was confirmed that the classification accuracy of the DNN-GMM was improved by 5.0% in comparison with that of the GMM, which was used as the baseline classifier.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN-GMM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Takahashi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Takahashi_1022.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Takahashi2016label" class="modal fade" id="bibtex-Takahashi2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTakahashi2016label">
        Acoustic Scene Classification Using Deep Neural Network and Frame-Concatenated Acoustic Feature
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Takahashi2016,
    Author = "Takahashi, Gen and Yamada, Takeshi and Makino, Shoji and Ono, Nobutaka",
    title = "Acoustic Scene Classification Using Deep Neural Network and Frame-Concatenated Acoustic Feature",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This paper describes our contribution to the task of acoustic scene classification in the DCASE2016 (Detection and Classification of Acoustic Scenes and Events 2016) Challenge set by IEEE AASP. In this work, we applied the DNN-GMM (Deep Neural Network-Gaussian Mixture Model) to acoustic scene classification. We introduced high-dimensional features that are concatenated with acoustic features in temporally adjacent frames. As a result, it was confirmed that the classification accuracy of the DNN-GMM was improved by 5.0\% in comparison with that of the GMM, which was used as the baseline classifier."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Valenti2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Valenti2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Michele Valenti<sup>1</sup>, Aleksandr Diment<sup>2</sup>, Giambattista Parascandolo<sup>2</sup>, Stefano Squartini<sup>1</sup> and Tuomas Virtanen<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy, <sup>2</sup>Department of Signal Processing, Tampere University of Technology, Tampere, Finland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Valenti_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Valenti2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Valenti2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Valenti2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Valenti_1011.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Valenti2016" class="panel-collapse collapse" id="collapse-Valenti2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This workshop paper presents our contribution for the task of acoustic scene classification proposed for the Detection and classification of acoustic scenes and events (D-CASE) 2016 challenge. We propose the use of a convolutional neural network trained to classify short sequences of audio, represented by their log-mel spectrogram. In addition we use a training method that can be used when the validation performance of the system saturates as the training proceeds. The performance is evaluated on the public acoustic scene classification development dataset provided for the D-CASE challenge. The best accuracy score obtained by our configuration on a four-folded crossvalidation setup is 79.0%. It constitutes a 8.8% relative improvement with respect to the baseline system, based on a Gaussian mixture model classifier.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Valenti2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Valenti_1011.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Valenti2016label" class="modal fade" id="bibtex-Valenti2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexValenti2016label">
        DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Valenti2016,
    Author = "Valenti, Michele and Diment, Aleksandr and Parascandolo, Giambattista and Squartini, Stefano and Virtanen, Tuomas",
    title = "{DCASE} 2016 Acoustic Scene Classification Using Convolutional Neural Networks",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This workshop paper presents our contribution for the task of acoustic scene classification proposed for the Detection and classification of acoustic scenes and events (D-CASE) 2016 challenge. We propose the use of a convolutional neural network trained to classify short sequences of audio, represented by their log-mel spectrogram. In addition we use a training method that can be used when the validation performance of the system saturates as the training proceeds. The performance is evaluated on the public acoustic scene classification development dataset provided for the D-CASE challenge. The best accuracy score obtained by our configuration on a four-folded crossvalidation setup is 79.0\%. It constitutes a 8.8\% relative improvement with respect to the baseline system, based on a Gaussian mixture model classifier."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vij2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Vij2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Spectral Analysis and Feature-Level Channel Combination
       </h4>
<p style="text-align:left">
        Dinesh Vij<sup>1</sup>, Naveen Aggarwal<sup>1</sup>, Bhaskaran Raman<sup>2</sup>, K.K. Ramakrishnan<sup>3</sup> and Divya Bansal<sup>4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>UIET, Panjab University, Chandigarh, India, <sup>2</sup>IIT, Bombay, India, <sup>3</sup>University of California, USA, <sup>4</sup>PEC University of Technology, Chandigarh, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Aggarwal_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Vij2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vij2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vij2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Aggarwal_1008.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vij2016" class="panel-collapse collapse" id="collapse-Vij2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Spectral Analysis and Feature-Level Channel Combination
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper is a submission to the sub-task Acoustic Scene Classification of the IEEE Audio and Acoustic Signal Processing challenge: Detection and Classification of Acoustic Scenes and Events 2016. The aim of the sub-task is to correctly detect 15 different acoustic scenes, which consist of indoor, outdoor, and vehicle categories. This work is based on spectral analysis, feature-level channel combination, and support vector machine classifier. In this short paper, the impact of different parameters while extracting features is analyzed. The accuracy gain obtained by feature-level channel combination is then reported.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vij2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Aggarwal_1008.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vij2016label" class="modal fade" id="bibtex-Vij2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVij2016label">
        Acoustic Scene Classification Based on Spectral Analysis and Feature-Level Channel Combination
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Vij2016,
    Author = "Vij, Dinesh and Aggarwal, Naveen and Raman, Bhaskaran and Ramakrishnan, K.K. and Bansal, Divya",
    title = "Acoustic Scene Classification Based on Spectral Analysis and Feature-Level Channel Combination",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This paper is a submission to the sub-task Acoustic Scene Classification of the IEEE Audio and Acoustic Signal Processing challenge: Detection and Classification of Acoustic Scenes and Events 2016. The aim of the sub-task is to correctly detect 15 different acoustic scenes, which consist of indoor, outdoor, and vehicle categories. This work is based on spectral analysis, feature-level channel combination, and support vector machine classifier. In this short paper, the impact of different parameters while extracting features is analyzed. The accuracy gain obtained by feature-level channel combination is then reported."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vikaskumar2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Vikaskumar2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Block Based MFCC Features
       </h4>
<p style="text-align:left">
        Ghodasara Vikaskumar, Shefali Waldekar, Dipjyoti Paul and Goutam Saha
       </p>
<p style="text-align:left">
<em>
         Electronics &amp; Electrical Communication Engineering Department, Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Vikaskumar_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Vikaskumar2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vikaskumar2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vikaskumar2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Vikaskumar_1036.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vikaskumar2016" class="panel-collapse collapse" id="collapse-Vikaskumar2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Block Based MFCC Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic Scene Classification (ASC) is receiving wide spread attention due to its wide variety of applications in smart wearable devices, surveillance, life log diarization etc. This work describes our contribution to the Acoustic scene classification task of the DCASE2016 Challenge for Detection and Classification of Acoustic Scenes and Events. In this work, we apply block based MFCC along with few traditional short term audio features with mean and standard deviation as statistics and Support Vector Machine (SVM) as a classifier to ASC. It is observed that block based MFCC feature performs better than classical MFCC. For evaluation purpose, we used three different datasets.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vikaskumar2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Vikaskumar_1036.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vikaskumar2016label" class="modal fade" id="bibtex-Vikaskumar2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVikaskumar2016label">
        Acoustic Scene Classification Using Block Based MFCC Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Vikaskumar2016,
    Author = "Vikaskumar, Ghodasara and Waldekar, Shefali and Paul, Dipjyoti and Saha, Goutam",
    title = "Acoustic Scene Classification Using Block Based {MFCC} Features",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "Acoustic Scene Classification (ASC) is receiving wide spread attention due to its wide variety of applications in smart wearable devices, surveillance, life log diarization etc. This work describes our contribution to the Acoustic scene classification task of the DCASE2016 Challenge for Detection and Classification of Acoustic Scenes and Events. In this work, we apply block based MFCC along with few traditional short term audio features with mean and standard deviation as statistics and Support Vector Machine (SVM) as a classifier to ASC. It is observed that block based MFCC feature performs better than classical MFCC. For evaluation purpose, we used three different datasets."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vu2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Vu2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene and Event Recognition Using Recurrent Neural Networks
       </h4>
<p style="text-align:left">
        Toan H. Vu and Jia-Ching Wang
       </p>
<p style="text-align:left">
<em>
         Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Vu_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Vu2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vu2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Vu_1018.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vu2016" class="panel-collapse collapse" id="collapse-Vu2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene and Event Recognition Using Recurrent Neural Networks
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The DCASE2016 challenge is designed particularly for research in environmental sound analysis. It consists of four tasks that spread on various problems such as acoustic scene classification and sound event detection. This paper reports our results on all the tasks by using Recurrent Neural Networks (RNNs). Experiments show that our models achieved superior performances compared with the baselines.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         RNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Vu_1018.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vu2016label" class="modal fade" id="bibtex-Vu2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVu2016label">
        Acoustic Scene and Event Recognition Using Recurrent Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Vu2016,
    Author = "Vu, Toan H. and Wang, Jia-Ching",
    title = "Acoustic Scene and Event Recognition Using Recurrent Neural Networks",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "The DCASE2016 challenge is designed particularly for research in environmental sound analysis. It consists of four tasks that spread on various problems such as acoustic scene classification and sound event detection. This paper reports our results on all the tasks by using Recurrent Neural Networks (RNNs). Experiments show that our models achieved superior performances compared with the baselines."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Xu2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Hierarchical Learning for DNN-Based Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Yong Xu, Qiang Huang, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vision, Speech and Signal Processing, University of Surrey, Surrey, United Kingdom
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xu_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xu2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Xu_1030.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2016" class="panel-collapse collapse" id="collapse-Xu2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Hierarchical Learning for DNN-Based Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present a deep neural network (DNN)-based acoustic scene classification framework. Two hierarchical learning methods are proposed to improve the DNN baseline performance by incorporating the hierarchical taxonomy information of environmental sounds. Firstly, the parameters of the DNN are initialized by the proposed hierarchical pre-training. Multi-level objective function is then adopted to add more constraint on the cross-entropy based loss function. A series of experiments were conducted on the Task1 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The final DNN-based system achieved a 22.9% relative improvement on average scene classification error as compared with the Gaussian Mixture Model (GMM)-based benchmark system across four standard folds.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel energy
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Xu_1030.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2016label" class="modal fade" id="bibtex-Xu2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2016label">
        Hierarchical Learning for DNN-Based Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xu2016,
    Author = "Xu, Yong and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.",
    title = "Hierarchical Learning for {DNN}-Based Acoustic Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "In this paper, we present a deep neural network (DNN)-based acoustic scene classification framework. Two hierarchical learning methods are proposed to improve the DNN baseline performance by incorporating the hierarchical taxonomy information of environmental sounds. Firstly, the parameters of the DNN are initialized by the proposed hierarchical pre-training. Multi-level objective function is then adopted to add more constraint on the cross-entropy based loss function. A series of experiments were conducted on the Task1 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The final DNN-based system achieved a 22.9\% relative improvement on average scene classification error as compared with the Gaussian Mixture Model (GMM)-based benchmark system across four standard folds."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yun2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Yun2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Discriminative Training of GMM Parameters for Audio Scene Classification
       </h4>
<p style="text-align:left">
        Sungrack Yun, Sungwoong Kim, Sunkuk Moon, Juncheol Cho and Taesu Kim
       </p>
<p style="text-align:left">
<em>
         Qualcomm Research, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yun2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yun2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yun2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Kim_1006.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yun2016" class="panel-collapse collapse" id="collapse-Yun2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Discriminative Training of GMM Parameters for Audio Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes the algorithm for audio scene classification and audio tagging and the result for DCASE 2016 challenge data. We propose a discriminative training algorithm to improve the baseline GMM performance. The algorithm updates the baseline GMM parameters by maximizing the margin between classes to improve discriminative performance. For Task1, we use a hierarchical classifier to maximize discriminative performance, and achieve 84% accuracy for given cross validation data. For Task4, we apply binary classifier for each label, and achieve 16.71% EER for given cross validation data.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yun2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Kim_1006.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yun2016label" class="modal fade" id="bibtex-Yun2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYun2016label">
        Discriminative Training of GMM Parameters for Audio Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yun2016,
    Author = "Yun, Sungrack and Kim, Sungwoong and Moon, Sunkuk and Cho, Juncheol and Kim, Taesu",
    title = "Discriminative Training of {GMM} Parameters for Audio Scene Classification",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "This report describes the algorithm for audio scene classification and audio tagging and the result for DCASE 2016 challenge data. We propose a discriminative training algorithm to improve the baseline GMM performance. The algorithm updates the baseline GMM parameters by maximizing the margin between classes to improve discriminative performance. For Task1, we use a hierarchical classifier to maximize discriminative performance, and achieve 84\% accuracy for given cross validation data. For Task4, we apply binary classifier for each label, and achieve 16.71\% EER for given cross validation data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zoehrer2016" style="box-shadow: none">
<div class="panel-heading" id="heading-Zoehrer2016" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Gated Recurrent Networks Applied To Acoustic Scene Classification and Acoustic Event Detection
       </h4>
<p style="text-align:left">
        Matthias Zöhrer and Franz Pernkopf
       </p>
<p style="text-align:left">
<em>
         Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zoehrer_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zoehrer2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zoehrer2016" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zoehrer2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Zoehrer_1020.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zoehrer2016" class="panel-collapse collapse" id="collapse-Zoehrer2016" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Gated Recurrent Networks Applied To Acoustic Scene Classification and Acoustic Event Detection
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present two resource efficient frameworks for acoustic scene classification and acoustic event detection. In particular, we combine gated recurrent neural networks (GRNNs) and linear discriminant analysis (LDA) for efficiently classifying environmental sound scenes of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). Our system reaches an overall accuracy of 79.1% on DCASE 2016 task 1 development data, resulting in a relative improvement of 8.34% compared to the baseline GMM system. By applying GRNNs on DCASE2016 real event detection data using a MSE objective, we obtain a segment-based error rate (ER) score of 0.73 - which is a relative improvement of 19.8% compared to the baseline GMM system. We further investigate semi-supervised learning applied to acoustic scene analysis. In particular, we evaluate the effects of a hybrid, i.e. generative discriminative, objective function.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zoehrer2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2016/technical_reports/DCASE2016_Zoehrer_1020.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zoehrer2016label" class="modal fade" id="bibtex-Zoehrer2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZoehrer2016label">
        Gated Recurrent Networks Applied To Acoustic Scene Classification and Acoustic Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zoehrer2016,
    Author = "Zöhrer, Matthias and Pernkopf, Franz",
    title = "Gated Recurrent Networks Applied To Acoustic Scene Classification and Acoustic Event Detection",
    institution = "DCASE2016 Challenge",
    year = "2016",
    month = "September",
    abstract = "We present two resource efficient frameworks for acoustic scene classification and acoustic event detection. In particular, we combine gated recurrent neural networks (GRNNs) and linear discriminant analysis (LDA) for efficiently classifying environmental sound scenes of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). Our system reaches an overall accuracy of 79.1\% on DCASE 2016 task 1 development data, resulting in a relative improvement of 8.34\% compared to the baseline GMM system. By applying GRNNs on DCASE2016 real event detection data using a MSE objective, we obtain a segment-based error rate (ER) score of 0.73 - which is a relative improvement of 19.8\% compared to the baseline GMM system. We further investigate semi-supervised learning applied to acoustic scene analysis. In particular, we evaluate the effects of a hybrid, i.e. generative discriminative, objective function."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>