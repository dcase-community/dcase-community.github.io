<!DOCTYPE html><html lang="en">
<head>
    <title>Acoustic Scene Classification with mismatched recording devices - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2019/task-acoustic-scene-classification-results-b">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description This subtask is concerned with the situation in which an application will be tested with different devices, possibly not the same as the ones used to record the development data. In this case, evaluation data contains more devices than the development data. The development data consists of the â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2019</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2019/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2019/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class=" active">
        <a href="/challenge2019/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-urban-sound-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2019/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2019/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2019/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/grid-08.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><strong class="fa-stack-1x icon-text">B</strong><strong class="fa-stack-1x dcase-icon-top-text">Mismatch</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2019_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Acoustic Scene Classification with<br> mismatched recording devices</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#device-wise-performance">Device-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#public-leaderboard">Public leaderboard</a>
<ul>
<li><a href="#scores">Scores</a></li>
<li><a href="#entries">Entries</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>This subtask is concerned with the situation in which an application will be tested with different devices, possibly not the same as the ones used to record the development data. In this case, evaluation data contains more devices than the development data. </p>
<p>The development data consists of the same recordings as in subtask A, and a small amount of parallel data recorded with devices B and C. The amount of data is as follows: </p>
<ul>
<li><strong>Device A</strong>: 40 hours (14400 segments, same as subtask A, but resampled and single-channel)</li>
<li><strong>Device B</strong>: 3 hours (108 segments per acoustic scene)</li>
<li><strong>Device C</strong>: 3 hours (108 segments per acoustic scene)</li>
</ul>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2019/task-acoustic-scene-classification#subtask-b" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy (B/C)<br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy (B/C)<br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_1</td>
<td>mmd_shake_res_snapi</td>
<td>Eghbal-zadeh2019</td>
<td>74.5 (73.5 - 75.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_2</td>
<td>mmd_shake_res</td>
<td>Eghbal-zadeh2019</td>
<td>74.5 (73.5 - 75.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_3</td>
<td>mmd_shake_snapi</td>
<td>Eghbal-zadeh2019</td>
<td>73.4 (72.4 - 74.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_4</td>
<td>mmd_shake</td>
<td>Eghbal-zadeh2019</td>
<td>73.4 (72.3 - 74.4)</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019 baseline</td>
<td>Baseline</td>
<td></td>
<td>47.7 (46.5 - 48.8)</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_1</td>
<td>Randomforest_16</td>
<td>Jiang2019</td>
<td>70.3 (69.2 - 71.3)</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_2</td>
<td>Randomforest_8</td>
<td>Jiang2019</td>
<td>69.9 (68.9 - 71.0)</td>
<td>64.2</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_3</td>
<td>Averaging_16</td>
<td>Jiang2019</td>
<td>69.0 (68.0 - 70.1)</td>
<td>63.2</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_4</td>
<td>Averaging_8</td>
<td>Jiang2019</td>
<td>69.6 (68.6 - 70.7)</td>
<td>64.0</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task1b_1</td>
<td>cvssp_cnn9</td>
<td>Kong2019</td>
<td>61.6 (60.4 - 62.7)</td>
<td>52.7</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_1</td>
<td>SC+IC+RCV</td>
<td>Komider2019</td>
<td>75.1 (74.1 - 76.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_2</td>
<td>SC+ALL+SV</td>
<td>Komider2019</td>
<td>75.3 (74.3 - 76.3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_3</td>
<td>SC+IC+RCV</td>
<td>Komider2019</td>
<td>74.9 (73.9 - 75.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_4</td>
<td>SC+FULL+SV</td>
<td>Komider2019</td>
<td>75.2 (74.3 - 76.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LamPham_KentGroup_task1b_1</td>
<td>Kent</td>
<td>Pham2019</td>
<td>72.8 (71.8 - 73.8)</td>
<td>72.9</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_1</td>
<td>UniSA_1b1</td>
<td>Gao2019</td>
<td>74.2 (73.2 - 75.2)</td>
<td>66.3</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_2</td>
<td>UniSA_1b2</td>
<td>Gao2019</td>
<td>74.1 (73.1 - 75.2)</td>
<td>62.5</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>UniSA_1b3</td>
<td>Gao2019</td>
<td>74.9 (73.9 - 75.9)</td>
<td>64.2</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_4</td>
<td>UniSA_1b4</td>
<td>Gao2019</td>
<td>74.4 (73.4 - 75.4)</td>
<td>66.3</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_1</td>
<td>CPR-NoDA</td>
<td>Primus2019</td>
<td>71.3 (70.2 - 72.3)</td>
<td>61.2</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_2</td>
<td>CPR-MSE</td>
<td>Primus2019</td>
<td>73.4 (72.4 - 74.4)</td>
<td>64.3</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_3</td>
<td>CPR-MI</td>
<td>Primus2019</td>
<td>71.6 (70.6 - 72.7)</td>
<td>62.5</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_4</td>
<td>CPR-Ensemble</td>
<td>Primus2019</td>
<td>74.2 (73.2 - 75.2)</td>
<td>65.1</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_1</td>
<td>hitsplab_1</td>
<td>Song2019</td>
<td>67.3 (66.2 - 68.3)</td>
<td>65.6</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_2</td>
<td>hitsplab_2</td>
<td>Song2019</td>
<td>72.2 (71.2 - 73.3)</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_3</td>
<td>hitsplab_3</td>
<td>Song2019</td>
<td>72.1 (71.1 - 73.1)</td>
<td>70.3</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_MFDWC19</td>
<td>Waldekar2019</td>
<td>62.1 (60.9 - 63.2)</td>
<td>52.3</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_1</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>65.7 (64.6 - 66.8)</td>
<td>54.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_2</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>68.5 (67.4 - 69.6)</td>
<td>55.2</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_3</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>70.3 (69.3 - 71.4)</td>
<td>54.8</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_2</td>
<td>mmd_shake_res</td>
<td>Eghbal-zadeh2019</td>
<td>74.5 (73.5 - 75.5)</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019 baseline</td>
<td>Baseline</td>
<td></td>
<td>47.7 (46.5 - 48.8)</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_1</td>
<td>Randomforest_16</td>
<td>Jiang2019</td>
<td>70.3 (69.2 - 71.3)</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task1b_1</td>
<td>cvssp_cnn9</td>
<td>Kong2019</td>
<td>61.6 (60.4 - 62.7)</td>
<td>52.7</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_2</td>
<td>SC+ALL+SV</td>
<td>Komider2019</td>
<td>75.3 (74.3 - 76.3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LamPham_KentGroup_task1b_1</td>
<td>Kent</td>
<td>Pham2019</td>
<td>72.8 (71.8 - 73.8)</td>
<td>72.9</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>UniSA_1b3</td>
<td>Gao2019</td>
<td>74.9 (73.9 - 75.9)</td>
<td>64.2</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_4</td>
<td>CPR-Ensemble</td>
<td>Primus2019</td>
<td>74.2 (73.2 - 75.2)</td>
<td>65.1</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_2</td>
<td>hitsplab_2</td>
<td>Song2019</td>
<td>72.2 (71.2 - 73.3)</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_MFDWC19</td>
<td>Waldekar2019</td>
<td>62.1 (60.9 - 63.2)</td>
<td>52.3</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_3</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>70.3 (69.3 - 71.4)</td>
<td>54.8</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2019 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Kosmider_SRPOL_task1b_2" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_bus", "class_accuracy_eval_metro", "class_accuracy_eval_metro_station", "class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_shopping_mall", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic", "class_accuracy_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_metro_station", "class_accuracy_eval_shopping_mall"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_bus","class_accuracy_eval_metro","class_accuracy_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission<br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="class_accuracy_eval_airport" data-sortable="true" data-value-type="float1-percentage">
                Airport
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_bus" data-sortable="true" data-value-type="float1-percentage">
                Bus
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro" data-sortable="true" data-value-type="float1-percentage">
                Metro
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro_station" data-sortable="true" data-value-type="float1-percentage">
                Metro <br/>station
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_park" data-sortable="true" data-value-type="float1-percentage">
                Park
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_public_square" data-sortable="true" data-value-type="float1-percentage">
                Public <br/>square
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_shopping_mall" data-sortable="true" data-value-type="float1-percentage">
                Shopping <br/>mall
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_pedestrian" data-sortable="true" data-value-type="float1-percentage">
                Street <br/>pedestrian
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_traffic" data-sortable="true" data-value-type="float1-percentage">
                Street <br/>traffic
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_tram" data-sortable="true" data-value-type="float1-percentage">
                Tram
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_1</td>
<td>mmd_shake_res_snapi</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>71.2</td>
<td>90.3</td>
<td>74.9</td>
<td>66.0</td>
<td>84.6</td>
<td>59.9</td>
<td>81.8</td>
<td>45.0</td>
<td>89.7</td>
<td>81.5</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_2</td>
<td>mmd_shake_res</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>70.1</td>
<td>90.6</td>
<td>74.4</td>
<td>66.2</td>
<td>82.1</td>
<td>58.3</td>
<td>82.6</td>
<td>48.1</td>
<td>90.3</td>
<td>82.2</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_3</td>
<td>mmd_shake_snapi</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>73.6</td>
<td>89.6</td>
<td>71.0</td>
<td>66.1</td>
<td>86.1</td>
<td>56.5</td>
<td>78.8</td>
<td>44.0</td>
<td>87.2</td>
<td>81.4</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_4</td>
<td>mmd_shake</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>71.7</td>
<td>90.1</td>
<td>72.5</td>
<td>65.1</td>
<td>85.8</td>
<td>58.9</td>
<td>79.3</td>
<td>43.1</td>
<td>88.1</td>
<td>79.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019 baseline</td>
<td>Baseline</td>
<td></td>
<td>47.7</td>
<td>36.5</td>
<td>57.8</td>
<td>57.8</td>
<td>35.8</td>
<td>54.9</td>
<td>15.8</td>
<td>76.8</td>
<td>28.3</td>
<td>67.8</td>
<td>45.1</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_1</td>
<td>Randomforest_16</td>
<td>Jiang2019</td>
<td>70.3</td>
<td>61.3</td>
<td>68.2</td>
<td>76.7</td>
<td>71.9</td>
<td>79.6</td>
<td>59.4</td>
<td>80.7</td>
<td>47.5</td>
<td>86.8</td>
<td>70.7</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_2</td>
<td>Randomforest_8</td>
<td>Jiang2019</td>
<td>69.9</td>
<td>62.6</td>
<td>69.2</td>
<td>74.0</td>
<td>71.7</td>
<td>79.0</td>
<td>58.9</td>
<td>79.3</td>
<td>46.5</td>
<td>87.4</td>
<td>70.8</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_3</td>
<td>Averaging_16</td>
<td>Jiang2019</td>
<td>69.0</td>
<td>54.7</td>
<td>67.1</td>
<td>74.9</td>
<td>71.1</td>
<td>80.7</td>
<td>61.9</td>
<td>85.3</td>
<td>39.7</td>
<td>89.2</td>
<td>65.8</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_4</td>
<td>Averaging_8</td>
<td>Jiang2019</td>
<td>69.6</td>
<td>54.2</td>
<td>71.2</td>
<td>71.9</td>
<td>70.8</td>
<td>80.3</td>
<td>62.4</td>
<td>84.9</td>
<td>40.0</td>
<td>89.6</td>
<td>71.0</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task1b_1</td>
<td>cvssp_cnn9</td>
<td>Kong2019</td>
<td>61.6</td>
<td>50.4</td>
<td>63.7</td>
<td>69.7</td>
<td>52.2</td>
<td>77.4</td>
<td>41.1</td>
<td>56.8</td>
<td>60.7</td>
<td>84.3</td>
<td>59.2</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_1</td>
<td>SC+IC+RCV</td>
<td>Komider2019</td>
<td>75.1</td>
<td>64.0</td>
<td>82.4</td>
<td>78.6</td>
<td>65.0</td>
<td>92.1</td>
<td>62.4</td>
<td>85.0</td>
<td>49.3</td>
<td>87.4</td>
<td>84.6</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_2</td>
<td>SC+ALL+SV</td>
<td>Komider2019</td>
<td>75.3</td>
<td>68.3</td>
<td>85.8</td>
<td>81.2</td>
<td>65.6</td>
<td>94.3</td>
<td>53.6</td>
<td>86.4</td>
<td>45.1</td>
<td>90.1</td>
<td>82.4</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_3</td>
<td>SC+IC+RCV</td>
<td>Komider2019</td>
<td>74.9</td>
<td>64.0</td>
<td>82.2</td>
<td>79.0</td>
<td>65.4</td>
<td>92.2</td>
<td>61.1</td>
<td>84.4</td>
<td>49.0</td>
<td>86.8</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_4</td>
<td>SC+FULL+SV</td>
<td>Komider2019</td>
<td>75.2</td>
<td>67.9</td>
<td>85.8</td>
<td>80.8</td>
<td>65.0</td>
<td>94.4</td>
<td>54.7</td>
<td>86.4</td>
<td>43.2</td>
<td>89.9</td>
<td>84.3</td>
</tr>
<tr>
<td></td>
<td>LamPham_KentGroup_task1b_1</td>
<td>Kent</td>
<td>Pham2019</td>
<td>72.8</td>
<td>69.9</td>
<td>91.9</td>
<td>66.1</td>
<td>56.7</td>
<td>87.6</td>
<td>44.9</td>
<td>73.3</td>
<td>64.3</td>
<td>89.6</td>
<td>83.9</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_1</td>
<td>UniSA_1b1</td>
<td>Gao2019</td>
<td>74.2</td>
<td>68.9</td>
<td>88.1</td>
<td>77.1</td>
<td>67.4</td>
<td>84.3</td>
<td>55.6</td>
<td>85.3</td>
<td>49.3</td>
<td>91.9</td>
<td>73.9</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_2</td>
<td>UniSA_1b2</td>
<td>Gao2019</td>
<td>74.1</td>
<td>73.9</td>
<td>87.5</td>
<td>73.6</td>
<td>70.0</td>
<td>81.5</td>
<td>55.6</td>
<td>82.2</td>
<td>52.8</td>
<td>91.1</td>
<td>73.2</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>UniSA_1b3</td>
<td>Gao2019</td>
<td>74.9</td>
<td>72.1</td>
<td>88.6</td>
<td>75.4</td>
<td>70.1</td>
<td>83.8</td>
<td>56.2</td>
<td>84.0</td>
<td>52.5</td>
<td>92.1</td>
<td>74.2</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_4</td>
<td>UniSA_1b4</td>
<td>Gao2019</td>
<td>74.4</td>
<td>70.4</td>
<td>86.8</td>
<td>77.8</td>
<td>69.2</td>
<td>83.1</td>
<td>55.0</td>
<td>86.8</td>
<td>49.6</td>
<td>92.1</td>
<td>73.5</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_1</td>
<td>CPR-NoDA</td>
<td>Primus2019</td>
<td>71.3</td>
<td>78.8</td>
<td>86.0</td>
<td>66.7</td>
<td>64.0</td>
<td>79.3</td>
<td>51.2</td>
<td>74.4</td>
<td>40.7</td>
<td>90.0</td>
<td>81.8</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_2</td>
<td>CPR-MSE</td>
<td>Primus2019</td>
<td>73.4</td>
<td>75.4</td>
<td>86.1</td>
<td>71.9</td>
<td>71.7</td>
<td>87.8</td>
<td>57.1</td>
<td>74.6</td>
<td>36.0</td>
<td>91.0</td>
<td>82.2</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_3</td>
<td>CPR-MI</td>
<td>Primus2019</td>
<td>71.6</td>
<td>76.1</td>
<td>83.1</td>
<td>76.0</td>
<td>61.8</td>
<td>78.8</td>
<td>59.3</td>
<td>70.3</td>
<td>36.4</td>
<td>91.1</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_4</td>
<td>CPR-Ensemble</td>
<td>Primus2019</td>
<td>74.2</td>
<td>77.5</td>
<td>86.2</td>
<td>74.4</td>
<td>72.4</td>
<td>86.4</td>
<td>59.9</td>
<td>78.1</td>
<td>36.0</td>
<td>89.9</td>
<td>81.5</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_1</td>
<td>hitsplab_1</td>
<td>Song2019</td>
<td>67.3</td>
<td>41.4</td>
<td>74.9</td>
<td>59.2</td>
<td>70.0</td>
<td>86.8</td>
<td>45.0</td>
<td>86.2</td>
<td>46.1</td>
<td>88.2</td>
<td>74.9</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_2</td>
<td>hitsplab_2</td>
<td>Song2019</td>
<td>72.2</td>
<td>63.1</td>
<td>80.6</td>
<td>76.5</td>
<td>73.5</td>
<td>86.9</td>
<td>37.8</td>
<td>87.5</td>
<td>51.5</td>
<td>92.5</td>
<td>72.6</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_3</td>
<td>hitsplab_3</td>
<td>Song2019</td>
<td>72.1</td>
<td>56.2</td>
<td>82.1</td>
<td>70.7</td>
<td>74.3</td>
<td>87.2</td>
<td>40.3</td>
<td>86.9</td>
<td>51.1</td>
<td>93.2</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_MFDWC19</td>
<td>Waldekar2019</td>
<td>62.1</td>
<td>55.6</td>
<td>69.7</td>
<td>55.0</td>
<td>51.8</td>
<td>83.8</td>
<td>43.2</td>
<td>66.2</td>
<td>42.4</td>
<td>86.2</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_1</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>65.7</td>
<td>55.7</td>
<td>68.2</td>
<td>69.7</td>
<td>60.0</td>
<td>81.8</td>
<td>45.3</td>
<td>62.6</td>
<td>55.6</td>
<td>90.3</td>
<td>67.6</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_2</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>68.5</td>
<td>58.8</td>
<td>70.1</td>
<td>71.5</td>
<td>64.3</td>
<td>82.1</td>
<td>51.9</td>
<td>68.6</td>
<td>53.1</td>
<td>89.7</td>
<td>74.9</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_3</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>70.3</td>
<td>60.3</td>
<td>72.6</td>
<td>72.8</td>
<td>65.8</td>
<td>81.1</td>
<td>54.4</td>
<td>73.2</td>
<td>53.3</td>
<td>90.6</td>
<td>78.9</td>
</tr>
</tbody>
</table>
<h1 id="device-wise-performance">Device-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_b" data-scatter-y="accuracy_eval_c" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th data-field="code" data-sortable="true" rowspan="2">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true" rowspan="2">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor" rowspan="2">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" colspan="5">Accuracy / Evaluation dataset</th>
</tr>
<tr>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Avg BC) " data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Average<br/> Dev B / Dev C
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev B)" data-chartable="true" data-field="accuracy_eval_b" data-sortable="true" data-value-type="float1-percentage">
                Dev B
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev C)" data-chartable="true" data-field="accuracy_eval_c" data-sortable="true" data-value-type="float1-percentage">
                Dev C
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev A)" data-chartable="true" data-field="accuracy_eval_a" data-sortable="true" data-value-type="float1-percentage">
                Dev A
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev D)" data-chartable="true" data-field="accuracy_eval_d" data-sortable="true" data-value-type="float1-percentage">
                Dev D
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_1</td>
<td>mmd_shake_res_snapi</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>73.8</td>
<td>75.2</td>
<td>81.3</td>
<td>54.4</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_2</td>
<td>mmd_shake_res</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>74.0</td>
<td>75.0</td>
<td>81.2</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_3</td>
<td>mmd_shake_snapi</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>72.8</td>
<td>74.1</td>
<td>80.3</td>
<td>55.3</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_4</td>
<td>mmd_shake</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>72.6</td>
<td>74.2</td>
<td>79.9</td>
<td>55.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019 baseline</td>
<td>Baseline</td>
<td></td>
<td>47.7</td>
<td>48.9</td>
<td>46.4</td>
<td>63.2</td>
<td>26.7</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_1</td>
<td>Randomforest_16</td>
<td>Jiang2019</td>
<td>70.3</td>
<td>69.1</td>
<td>71.4</td>
<td>75.1</td>
<td>53.0</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_2</td>
<td>Randomforest_8</td>
<td>Jiang2019</td>
<td>69.9</td>
<td>68.5</td>
<td>71.4</td>
<td>75.1</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_3</td>
<td>Averaging_16</td>
<td>Jiang2019</td>
<td>69.0</td>
<td>68.4</td>
<td>69.6</td>
<td>74.3</td>
<td>53.2</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_4</td>
<td>Averaging_8</td>
<td>Jiang2019</td>
<td>69.6</td>
<td>68.8</td>
<td>70.5</td>
<td>73.9</td>
<td>54.2</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task1b_1</td>
<td>cvssp_cnn9</td>
<td>Kong2019</td>
<td>61.6</td>
<td>60.3</td>
<td>62.8</td>
<td>70.2</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_1</td>
<td>SC+IC+RCV</td>
<td>Komider2019</td>
<td>75.1</td>
<td>74.5</td>
<td>75.7</td>
<td>79.8</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_2</td>
<td>SC+ALL+SV</td>
<td>Komider2019</td>
<td>75.3</td>
<td>74.3</td>
<td>76.2</td>
<td>80.8</td>
<td>38.6</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_3</td>
<td>SC+IC+RCV</td>
<td>Komider2019</td>
<td>74.9</td>
<td>74.4</td>
<td>75.3</td>
<td>78.9</td>
<td>35.5</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_4</td>
<td>SC+FULL+SV</td>
<td>Komider2019</td>
<td>75.2</td>
<td>74.3</td>
<td>76.2</td>
<td>80.1</td>
<td>40.0</td>
</tr>
<tr>
<td></td>
<td>LamPham_KentGroup_task1b_1</td>
<td>Kent</td>
<td>Pham2019</td>
<td>72.8</td>
<td>71.8</td>
<td>73.8</td>
<td>78.2</td>
<td>24.6</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_1</td>
<td>UniSA_1b1</td>
<td>Gao2019</td>
<td>74.2</td>
<td>73.2</td>
<td>75.1</td>
<td>79.3</td>
<td>63.4</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_2</td>
<td>UniSA_1b2</td>
<td>Gao2019</td>
<td>74.1</td>
<td>73.6</td>
<td>74.7</td>
<td>79.9</td>
<td>63.8</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>UniSA_1b3</td>
<td>Gao2019</td>
<td>74.9</td>
<td>74.2</td>
<td>75.6</td>
<td>79.8</td>
<td>65.2</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_4</td>
<td>UniSA_1b4</td>
<td>Gao2019</td>
<td>74.4</td>
<td>73.8</td>
<td>75.1</td>
<td>80.1</td>
<td>63.6</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_1</td>
<td>CPR-NoDA</td>
<td>Primus2019</td>
<td>71.3</td>
<td>70.9</td>
<td>71.7</td>
<td>78.1</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_2</td>
<td>CPR-MSE</td>
<td>Primus2019</td>
<td>73.4</td>
<td>73.6</td>
<td>73.1</td>
<td>72.1</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_3</td>
<td>CPR-MI</td>
<td>Primus2019</td>
<td>71.6</td>
<td>71.4</td>
<td>71.8</td>
<td>72.8</td>
<td>49.3</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_4</td>
<td>CPR-Ensemble</td>
<td>Primus2019</td>
<td>74.2</td>
<td>74.1</td>
<td>74.3</td>
<td>73.7</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_1</td>
<td>hitsplab_1</td>
<td>Song2019</td>
<td>67.3</td>
<td>65.3</td>
<td>69.2</td>
<td>73.1</td>
<td>47.7</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_2</td>
<td>hitsplab_2</td>
<td>Song2019</td>
<td>72.2</td>
<td>71.7</td>
<td>72.8</td>
<td>79.9</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_3</td>
<td>hitsplab_3</td>
<td>Song2019</td>
<td>72.1</td>
<td>71.1</td>
<td>73.1</td>
<td>78.4</td>
<td>59.1</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_MFDWC19</td>
<td>Waldekar2019</td>
<td>62.1</td>
<td>59.7</td>
<td>64.4</td>
<td>71.4</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_1</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>65.7</td>
<td>64.9</td>
<td>66.4</td>
<td>75.4</td>
<td>39.9</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_2</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>68.5</td>
<td>67.8</td>
<td>69.2</td>
<td>76.9</td>
<td>46.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_3</td>
<td>Rui_task1b</td>
<td>Wang2019</td>
<td>70.3</td>
<td>68.8</td>
<td>71.9</td>
<td>79.6</td>
<td>47.2</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_1</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>perceptual weighted power spectrogram</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_2</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>perceptual weighted power spectrogram</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_3</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>perceptual weighted power spectrogram</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_4</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>perceptual weighted power spectrogram</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019 baseline</td>
<td></td>
<td>47.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_1</td>
<td>Jiang2019</td>
<td>70.3</td>
<td>44.1kHz</td>
<td>HPSS, NNF, vocal separation, HRTF</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_2</td>
<td>Jiang2019</td>
<td>69.9</td>
<td>44.1kHz</td>
<td>HPSS, NNF, vocal separation, HRTF</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_3</td>
<td>Jiang2019</td>
<td>69.0</td>
<td>44.1kHz</td>
<td>HPSS, NNF, vocal separation, HRTF</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_4</td>
<td>Jiang2019</td>
<td>69.6</td>
<td>44.1kHz</td>
<td>HPSS, NNF, vocal separation, HRTF</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task1b_1</td>
<td>Kong2019</td>
<td>61.6</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_1</td>
<td>Komider2019</td>
<td>75.1</td>
<td>44.1kHz</td>
<td>Spectrum Correction, SpecAugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_2</td>
<td>Komider2019</td>
<td>75.3</td>
<td>44.1kHz</td>
<td>Spectrum Correction, SpecAugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_3</td>
<td>Komider2019</td>
<td>74.9</td>
<td>44.1kHz</td>
<td>Spectrum Correction, SpecAugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_4</td>
<td>Komider2019</td>
<td>75.2</td>
<td>44.1kHz</td>
<td>Spectrum Correction, SpecAugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LamPham_KentGroup_task1b_1</td>
<td>Pham2019</td>
<td>72.8</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>Gammatone, log-mel energies, CQT</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_1</td>
<td>Gao2019</td>
<td>74.2</td>
<td>44.1kHz</td>
<td>mixup, temporal cropping</td>
<td>log-mel energies, deltas and delta-deltas</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_2</td>
<td>Gao2019</td>
<td>74.1</td>
<td>44.1kHz</td>
<td>mixup, temporal cropping</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>Gao2019</td>
<td>74.9</td>
<td>44.1kHz</td>
<td>mixup, temporal cropping</td>
<td>log-mel energies, deltas and delta-deltas</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_4</td>
<td>Gao2019</td>
<td>74.4</td>
<td>44.1kHz</td>
<td>mixup, temporal cropping</td>
<td>log-mel energies, deltas and delta-deltas</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_1</td>
<td>Primus2019</td>
<td>71.3</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_2</td>
<td>Primus2019</td>
<td>73.4</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_3</td>
<td>Primus2019</td>
<td>71.6</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_4</td>
<td>Primus2019</td>
<td>74.2</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_1</td>
<td>Song2019</td>
<td>67.3</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_2</td>
<td>Song2019</td>
<td>72.2</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_3</td>
<td>Song2019</td>
<td>72.1</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>Waldekar2019</td>
<td>62.1</td>
<td>44.1kHz</td>
<td></td>
<td>MFDWC</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_1</td>
<td>Wang2019</td>
<td>65.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_2</td>
<td>Wang2019</td>
<td>68.5</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_3</td>
<td>Wang2019</td>
<td>70.3</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-filter-control="select" data-sortable="true" data-tag="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
<th class="text-center narrow-col" data-field="system_device_mismatch_handling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Device mismatch <br/>handling
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_1</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>747596080</td>
<td>CNN, Receptive Field Regularization</td>
<td>220</td>
<td>average</td>
<td>maximum mean discrepancy, domain adaptation, transfer learning</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_2</td>
<td>Eghbal-zadeh2019</td>
<td>74.5</td>
<td>37379804</td>
<td>CNN, Receptive Field Regularization</td>
<td>11</td>
<td>average</td>
<td>maximum mean discrepancy, domain adaptation, transfer learning</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_3</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>286137920</td>
<td>CNN, Receptive Field Regularization</td>
<td>80</td>
<td>average</td>
<td>maximum mean discrepancy, domain adaptation, transfer learning</td>
</tr>
<tr>
<td></td>
<td>Eghbal-zadeh_CPJKU_task1b_4</td>
<td>Eghbal-zadeh2019</td>
<td>73.4</td>
<td>12878416</td>
<td>CNN, Receptive Field Regularization</td>
<td>4</td>
<td>average</td>
<td>maximum mean discrepancy, domain adaptation, transfer learning</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019 baseline</td>
<td></td>
<td>47.7</td>
<td>116118</td>
<td>CNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_1</td>
<td>Jiang2019</td>
<td>70.3</td>
<td>1448794</td>
<td>CNN</td>
<td>16</td>
<td>stacking</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_2</td>
<td>Jiang2019</td>
<td>69.9</td>
<td>1448794</td>
<td>CNN</td>
<td>8</td>
<td>stacking</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_3</td>
<td>Jiang2019</td>
<td>69.0</td>
<td>1448794</td>
<td>CNN</td>
<td>16</td>
<td>averaging</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Jiang_UESTC_task1b_4</td>
<td>Jiang2019</td>
<td>69.6</td>
<td>1448794</td>
<td>CNN</td>
<td>8</td>
<td>averaging</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task1b_1</td>
<td>Kong2019</td>
<td>61.6</td>
<td>4686144</td>
<td>CNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_1</td>
<td>Komider2019</td>
<td>75.1</td>
<td>6100840</td>
<td>CNN</td>
<td>36</td>
<td>isotonic-calibrated soft-voting</td>
<td>spectrum correction</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_2</td>
<td>Komider2019</td>
<td>75.3</td>
<td>18095576</td>
<td>CNN</td>
<td>124</td>
<td>soft-voting</td>
<td>spectrum correction</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_3</td>
<td>Komider2019</td>
<td>74.9</td>
<td>3077046</td>
<td>CNN</td>
<td>31</td>
<td>soft-voting</td>
<td>spectrum correction</td>
</tr>
<tr>
<td></td>
<td>Kosmider_SRPOL_task1b_4</td>
<td>Komider2019</td>
<td>75.2</td>
<td>10768964</td>
<td>CNN</td>
<td>58</td>
<td>soft-voting</td>
<td>spectrum correction</td>
</tr>
<tr>
<td></td>
<td>LamPham_KentGroup_task1b_1</td>
<td>Pham2019</td>
<td>72.8</td>
<td>12346325</td>
<td>CNN, DNN</td>
<td>2</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_1</td>
<td>Gao2019</td>
<td>74.2</td>
<td>3253148</td>
<td>CNN</td>
<td></td>
<td></td>
<td>aggressive regularization and augmentation</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_2</td>
<td>Gao2019</td>
<td>74.1</td>
<td>3252268</td>
<td>CNN</td>
<td></td>
<td></td>
<td>aggressive regularization and augmentation</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>Gao2019</td>
<td>74.9</td>
<td>6505416</td>
<td>CNN</td>
<td>2</td>
<td>average</td>
<td>aggressive regularization and augmentation</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_4</td>
<td>Gao2019</td>
<td>74.4</td>
<td>6506296</td>
<td>CNN</td>
<td>2</td>
<td>average</td>
<td>aggressive regularization and augmentation</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_1</td>
<td>Primus2019</td>
<td>71.3</td>
<td>13047888</td>
<td>CNN, ensemble</td>
<td>4</td>
<td>average</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_2</td>
<td>Primus2019</td>
<td>73.4</td>
<td>13047888</td>
<td>CNN, ensemble</td>
<td>4</td>
<td>average</td>
<td>domain adaptation</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_3</td>
<td>Primus2019</td>
<td>71.6</td>
<td>13047888</td>
<td>CNN, ensemble</td>
<td>8</td>
<td>average</td>
<td>domain adaptation</td>
</tr>
<tr>
<td></td>
<td>Primus_CPJKU_task1b_4</td>
<td>Primus2019</td>
<td>74.2</td>
<td>26095776</td>
<td>CNN, ensemble</td>
<td>8</td>
<td>average</td>
<td>domain adaptation</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_1</td>
<td>Song2019</td>
<td>67.3</td>
<td>22758197</td>
<td>CNN</td>
<td></td>
<td></td>
<td>feature transform</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_2</td>
<td>Song2019</td>
<td>72.2</td>
<td>68274591</td>
<td>CNN</td>
<td>3</td>
<td>probability aggregation</td>
<td>feature transform</td>
</tr>
<tr>
<td></td>
<td>Song_HIT_task1b_3</td>
<td>Song2019</td>
<td>72.1</td>
<td>68274591</td>
<td>CNN</td>
<td>3</td>
<td>majority vote</td>
<td>feature transform</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>Waldekar2019</td>
<td>62.1</td>
<td>9000</td>
<td>SVM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_1</td>
<td>Wang2019</td>
<td>65.7</td>
<td>116118</td>
<td>CNN, DNN</td>
<td>7</td>
<td></td>
<td>domain adaptation</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_2</td>
<td>Wang2019</td>
<td>68.5</td>
<td>116118</td>
<td>CNN, DNN</td>
<td>7</td>
<td>average</td>
<td>domain adaptation</td>
</tr>
<tr>
<td></td>
<td>Wang_NWPU_task1b_3</td>
<td>Wang2019</td>
<td>70.3</td>
<td>116118</td>
<td>CNN, DNN</td>
<td>7</td>
<td>average</td>
<td>domain adaptation</td>
</tr>
</tbody>
</table>
<h1 id="public-leaderboard">Public leaderboard</h1>
<h2 id="scores">Scores</h2>
<table class="datatable table" data-chart-modes="line" data-id-field="date" data-line-fields2="max_team_public_lb,top5team_interval_public_lb" data-line-height="500" data-line-hline="false" data-line-horizontal-highlight-label-opacity="0.1" data-line-horizontal-highlight-label-position="bottom-left" data-line-horizontal-highlight-opacity="0.1" data-line-horizontal-highlights="48.0;Baseline score;#333" data-line-legend-position="top" data-line-show-legend="true" data-line-show-point="false" data-line-show-xaxis="true" data-line-tooltip-position="bottom" data-line-xaxis-field="date" data-line-xaxis-gridlines="false" data-line-xaxis-sorted="true" data-line-xaxis-timestepsize="14" data-line-xaxis-timeunit="day" data-line-yaxis-gridlines="true" data-line-yaxis-label="Accuracy" data-show-chart="true" data-show-toolbar="false" data-sort-name="date" data-sort-order="asc" data-table-default-mode="hide">
<thead>
<tr>
<th class="" data-chartable="false" data-field="date" data-sortable="true" data-visible="true">
                Date
            </th>
<th class="sep-left-cell" data-beginatzero="false" data-chartable="true" data-field="max_team_public_lb" data-sortable="true" data-value-type="float1-percentage" data-visible="true">
                Top Team
            </th>
<th class="sep-left-cell" data-chartable="true" data-field="top10team_median_interval_public_lb" data-sortable="true" data-value-type="float1-percentage-interval" data-visible="true">
                Top 10 Team median
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-05-14</td>
<td>64.8</td>
<td>64.8 (64.8 - 64.8)</td>
</tr>
<tr>
<td>2019-05-15</td>
<td>64.8</td>
<td>62.4 (60.0 - 64.8)</td>
</tr>
<tr>
<td>2019-05-16</td>
<td>66.3</td>
<td>65.6 (64.8 - 66.3)</td>
</tr>
<tr>
<td>2019-05-17</td>
<td>66.7</td>
<td>65.8 (64.8 - 66.7)</td>
</tr>
<tr>
<td>2019-05-18</td>
<td>66.7</td>
<td>64.8 (60.5 - 66.7)</td>
</tr>
<tr>
<td>2019-05-19</td>
<td>68.5</td>
<td>66.7 (64.8 - 68.5)</td>
</tr>
<tr>
<td>2019-05-20</td>
<td>73.3</td>
<td>67.8 (64.8 - 73.3)</td>
</tr>
<tr>
<td>2019-05-21</td>
<td>73.3</td>
<td>64.8 (56.7 - 73.3)</td>
</tr>
<tr>
<td>2019-05-22</td>
<td>73.3</td>
<td>67.8 (59.3 - 73.3)</td>
</tr>
<tr>
<td>2019-05-23</td>
<td>73.3</td>
<td>66.3 (53.2 - 73.3)</td>
</tr>
<tr>
<td>2019-05-24</td>
<td>73.3</td>
<td>66.3 (58.3 - 73.3)</td>
</tr>
<tr>
<td>2019-05-25</td>
<td>73.3</td>
<td>66.3 (60.3 - 73.3)</td>
</tr>
<tr>
<td>2019-05-26</td>
<td>73.3</td>
<td>66.3 (60.3 - 73.3)</td>
</tr>
<tr>
<td>2019-05-27</td>
<td>73.3</td>
<td>66.3 (60.3 - 73.3)</td>
</tr>
<tr>
<td>2019-05-28</td>
<td>73.3</td>
<td>66.3 (60.7 - 73.3)</td>
</tr>
<tr>
<td>2019-05-29</td>
<td>73.3</td>
<td>68.2 (60.7 - 73.3)</td>
</tr>
<tr>
<td>2019-05-30</td>
<td>73.3</td>
<td>66.3 (44.0 - 73.3)</td>
</tr>
<tr>
<td>2019-05-31</td>
<td>73.3</td>
<td>66.9 (58.3 - 73.3)</td>
</tr>
<tr>
<td>2019-06-01</td>
<td>73.3</td>
<td>68.2 (62.5 - 73.3)</td>
</tr>
<tr>
<td>2019-06-02</td>
<td>73.7</td>
<td>68.2 (62.5 - 73.7)</td>
</tr>
<tr>
<td>2019-06-03</td>
<td>73.7</td>
<td>69.0 (62.5 - 73.7)</td>
</tr>
<tr>
<td>2019-06-04</td>
<td>73.7</td>
<td>69.0 (62.5 - 73.7)</td>
</tr>
<tr>
<td>2019-06-05</td>
<td>76.5</td>
<td>69.7 (64.8 - 76.5)</td>
</tr>
<tr>
<td>2019-06-06</td>
<td>76.5</td>
<td>69.7 (66.7 - 76.5)</td>
</tr>
<tr>
<td>2019-06-07</td>
<td>76.5</td>
<td>69.7 (66.7 - 76.5)</td>
</tr>
<tr>
<td>2019-06-08</td>
<td>76.5</td>
<td>69.7 (67.7 - 76.5)</td>
</tr>
<tr>
<td>2019-06-09</td>
<td>76.5</td>
<td>69.7 (68.3 - 76.5)</td>
</tr>
<tr>
<td>2019-06-10</td>
<td>76.5</td>
<td>70.4 (69.0 - 76.5)</td>
</tr>
</tbody>
</table>
<h2 id="entries">Entries</h2>
<h3>Total entries</h3>
<table class="datatable table" data-chart-modes="line" data-id-field="date" data-line-height="200" data-line-show-point="false" data-line-show-xaxis="false" data-line-tooltip-position="average" data-line-xaxis-field="date" data-line-xaxis-gridlines="false" data-line-xaxis-sorted="true" data-line-xaxis-timestepsize="14" data-line-xaxis-timeunit="day" data-line-yaxis-gridlines="true" data-line-yaxis-label="Entries" data-show-chart="true" data-show-toolbar="false" data-sort-name="date" data-sort-order="asc" data-table-default-mode="hide">
<thead>
<tr>
<th class="" data-chartable="false" data-field="date" data-sortable="true" data-visible="true">
        Date
    </th>
<th class="sep-left-cell" data-beginatzero="false" data-chartable="true" data-field="total_entries" data-sortable="true" data-value-type="int" data-visible="true">
        Entries
    </th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-05-14</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-15</td>
<td>2</td>
</tr>
<tr>
<td>2019-05-16</td>
<td>3</td>
</tr>
<tr>
<td>2019-05-17</td>
<td>4</td>
</tr>
<tr>
<td>2019-05-18</td>
<td>6</td>
</tr>
<tr>
<td>2019-05-19</td>
<td>7</td>
</tr>
<tr>
<td>2019-05-20</td>
<td>9</td>
</tr>
<tr>
<td>2019-05-21</td>
<td>11</td>
</tr>
<tr>
<td>2019-05-22</td>
<td>13</td>
</tr>
<tr>
<td>2019-05-23</td>
<td>16</td>
</tr>
<tr>
<td>2019-05-24</td>
<td>19</td>
</tr>
<tr>
<td>2019-05-25</td>
<td>21</td>
</tr>
<tr>
<td>2019-05-26</td>
<td>21</td>
</tr>
<tr>
<td>2019-05-27</td>
<td>22</td>
</tr>
<tr>
<td>2019-05-28</td>
<td>23</td>
</tr>
<tr>
<td>2019-05-29</td>
<td>27</td>
</tr>
<tr>
<td>2019-05-30</td>
<td>32</td>
</tr>
<tr>
<td>2019-05-31</td>
<td>39</td>
</tr>
<tr>
<td>2019-06-01</td>
<td>44</td>
</tr>
<tr>
<td>2019-06-02</td>
<td>49</td>
</tr>
<tr>
<td>2019-06-03</td>
<td>53</td>
</tr>
<tr>
<td>2019-06-04</td>
<td>56</td>
</tr>
<tr>
<td>2019-06-05</td>
<td>63</td>
</tr>
<tr>
<td>2019-06-06</td>
<td>67</td>
</tr>
<tr>
<td>2019-06-07</td>
<td>74</td>
</tr>
<tr>
<td>2019-06-08</td>
<td>80</td>
</tr>
<tr>
<td>2019-06-09</td>
<td>88</td>
</tr>
<tr>
<td>2019-06-10</td>
<td>97</td>
</tr>
</tbody>
</table>
<h3>Entries per day</h3>
<table class="datatable table" data-chart-modes="line" data-id-field="date" data-line-height="200" data-line-show-point="false" data-line-show-xaxis="false" data-line-tooltip-position="average" data-line-xaxis-field="date" data-line-xaxis-gridlines="false" data-line-xaxis-sorted="true" data-line-xaxis-timestepsize="14" data-line-xaxis-timeunit="day" data-line-yaxis-gridlines="true" data-line-yaxis-label="Entries" data-show-chart="true" data-show-toolbar="false" data-sort-name="date" data-sort-order="asc" data-table-default-mode="hide">
<thead>
<tr>
<th class="" data-chartable="false" data-field="date" data-sortable="true" data-visible="true">
            Date
        </th>
<th class="sep-left-cell" data-beginatzero="false" data-chartable="true" data-field="current_entries" data-sortable="true" data-value-type="int" data-visible="true">
            Entries per day
        </th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-05-14</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-15</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-16</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-17</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-18</td>
<td>2</td>
</tr>
<tr>
<td>2019-05-19</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-20</td>
<td>2</td>
</tr>
<tr>
<td>2019-05-21</td>
<td>2</td>
</tr>
<tr>
<td>2019-05-22</td>
<td>2</td>
</tr>
<tr>
<td>2019-05-23</td>
<td>3</td>
</tr>
<tr>
<td>2019-05-24</td>
<td>3</td>
</tr>
<tr>
<td>2019-05-25</td>
<td>2</td>
</tr>
<tr>
<td>2019-05-26</td>
<td>0</td>
</tr>
<tr>
<td>2019-05-27</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-28</td>
<td>1</td>
</tr>
<tr>
<td>2019-05-29</td>
<td>4</td>
</tr>
<tr>
<td>2019-05-30</td>
<td>5</td>
</tr>
<tr>
<td>2019-05-31</td>
<td>7</td>
</tr>
<tr>
<td>2019-06-01</td>
<td>5</td>
</tr>
<tr>
<td>2019-06-02</td>
<td>5</td>
</tr>
<tr>
<td>2019-06-03</td>
<td>4</td>
</tr>
<tr>
<td>2019-06-04</td>
<td>3</td>
</tr>
<tr>
<td>2019-06-05</td>
<td>7</td>
</tr>
<tr>
<td>2019-06-06</td>
<td>4</td>
</tr>
<tr>
<td>2019-06-07</td>
<td>7</td>
</tr>
<tr>
<td>2019-06-08</td>
<td>6</td>
</tr>
<tr>
<td>2019-06-09</td>
<td>8</td>
</tr>
<tr>
<td>2019-06-10</td>
<td>9</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2019/technical_reports_task1.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Arabnezhad2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Arabnezhad2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Urban Acoustic Scene Classification Using Binaural Wavelet Scattering and Random Subspace Discrimination Method
       </h4>
<p style="text-align:left">
        Fateme Arabnezhad and Babak Nasersharif
       </p>
<p style="text-align:left">
<em>
         Computer Engineering Department, Khaje Nasir Toosi, Tehran, Iran
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Fmta91_KNToosi_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Arabnezhad2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Arabnezhad2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Arabnezhad2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Fmta91_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Arabnezhad2019" class="panel-collapse collapse" id="collapse-Arabnezhad2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Urban Acoustic Scene Classification Using Binaural Wavelet Scattering and Random Subspace Discrimination Method
      </h4>
<p style="text-align:left">
<small>
        Fateme Arabnezhad and Babak Nasersharif
       </small>
<br/>
<small>
<em>
         Computer Engineering Department, Khaje Nasir Toosi, Tehran, Iran
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describe our contribution to Detection and Classification of Urban Acoustic Scenes on DCASE 2019 challenge (Task1 â€“Subtask A). We propose to use wavelet scatterings spectrum as a good representation and feature where we extracted from both average of 2 audio recorded(mono) and also difference of 2 audio recorded channels (side). The concatenation of these two set of wavelet scattering spectrum are used as a feature vector which is fed into a classifier based on random subspace method. In this work, Regularized Linear Discriminant Analysis (RLDA) is used as a base learner and a classification approach for Random Subspace Method. The experimental results shows that the proposed structure learn acoustic characteristics from audio segments. This structure achieved 87.98% accuracy on whole development set (without cross-validation) and 78.83% on leaderboard dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         wavelet scattering spectra
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         random subspace
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         highest average score
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Arabnezhad2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Fmta91_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Arabnezhad2019label" class="modal fade" id="bibtex-Arabnezhad2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexArabnezhad2019label">
        Urban Acoustic Scene Classification Using Binaural Wavelet Scattering and Random Subspace Discrimination Method
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Arabnezhad2019,
    Author = "Arabnezhad, Fateme and Nasersharif, Babak",
    title = "Urban Acoustic Scene Classification Using Binaural Wavelet Scattering and Random Subspace Discrimination Method",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describe our contribution to Detection and Classification of Urban Acoustic Scenes on DCASE 2019 challenge (Task1 â€“Subtask A). We propose to use wavelet scatterings spectrum as a good representation and feature where we extracted from both average of 2 audio recorded(mono) and also difference of 2 audio recorded channels (side). The concatenation of these two set of wavelet scattering spectrum are used as a feature vector which is fed into a classifier based on random subspace method. In this work, Regularized Linear Discriminant Analysis (RLDA) is used as a base learner and a classification approach for Random Subspace Method. The experimental results shows that the proposed structure learn acoustic characteristics from audio segments. This structure achieved 87.98\% accuracy on whole development set (without cross-validation) and 78.83\% on leaderboard dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bilot2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Bilot2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification with Multiple Instance Learning and Fusion
       </h4>
<p style="text-align:left">
        Valentin Bilot and Quang Khanh Ngoc Duong
       </p>
<p style="text-align:left">
<em>
         Audio R&amp;D, InterDigital R&amp;D, Rennes, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bilot_IDG_task1a_1</span> <span class="label label-primary">Bilot_IDG_task1a_2</span> <span class="label label-primary">Bilot_IDG_task1a_3</span> <span class="label label-primary">Bilot_IDG_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bilot2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bilot2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bilot2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Bilot_47.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bilot2019" class="panel-collapse collapse" id="collapse-Bilot2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification with Multiple Instance Learning and Fusion
      </h4>
<p style="text-align:left">
<small>
        Valentin Bilot and Quang Khanh Ngoc Duong
       </small>
<br/>
<small>
<em>
         Audio R&amp;D, InterDigital R&amp;D, Rennes, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Audio classification has been an emerging topic in the last few years, especially with the benchmark dataset and evaluation from DCASE. This paper present our deep learning models to address the acoustic scene classification (ASC) task of the DCASE 2019. The models exploit multiple instance learning (MIL) method as a way of guiding the network attention to different temporal segments of a recording. We then propose a simple late fusion of results obtained by the three investigated MIL-based models. Such fusion system uses multi-layer perceptron (MLP) to predict the final classes from the initial class probability predictions and obtains a better result on the development and the leaderboard dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural, difference
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         MLP
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bilot2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Bilot_47.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bilot2019label" class="modal fade" id="bibtex-Bilot2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBilot2019label">
        Acoustic Scene Classification with Multiple Instance Learning and Fusion
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bilot2019,
    Author = "Bilot, Valentin and Duong, Quang Khanh Ngoc",
    title = "Acoustic Scene Classification with Multiple Instance Learning and Fusion",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Audio classification has been an emerging topic in the last few years, especially with the benchmark dataset and evaluation from DCASE. This paper present our deep learning models to address the acoustic scene classification (ASC) task of the DCASE 2019. The models exploit multiple instance learning (MIL) method as a way of guiding the network attention to different temporal segments of a recording. We then propose a simple late fusion of results obtained by the three investigated MIL-based models. Such fusion system uses multi-layer perceptron (MLP) to predict the final classes from the initial class probability predictions and obtains a better result on the development and the leaderboard dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chen2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Chen2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling
       </h4>
<p style="text-align:left">
        Hangting Chen, Zuozhen Liu, Zongming Liu, Pengyuan Zhang and Yonghong Yan
       </p>
<p style="text-align:left">
<em>
         Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_IOA_task1a_1</span> <span class="label label-primary">Zhang_IOA_task1a_2</span> <span class="label label-primary">Zhang_IOA_task1a_3</span> <span class="label label-primary">Zhang_IOA_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chen2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chen2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chen2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chen2019" class="panel-collapse collapse" id="collapse-Chen2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling
      </h4>
<p style="text-align:left">
<small>
        Hangting Chen, Zuozhen Liu, Zongming Liu, Pengyuan Zhang and Yonghong Yan
       </small>
<br/>
<small>
<em>
         Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the IOA teamâ€™s submission for TASK1A of DCASE2019 challenge. Our acoustic scene classification (ASC) system adopts a data augmentation scheme employing generative adversary networks. Two major classifiers, 1D deep convolutional neural network integrated with scalogram features and 2D fully convolutional neural network integrated with Mel filter bank features, are deployed in the scheme. Other approaches, such as adversary city adaptation, temporal module based on discrete cosine transform and hybrid architectures, have been developed for further fusion. The results of our experiments indicates that the final fusion systems A-D could achieve the accuracy above 85% on the officially provided fold 1 evaluation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         generative neural network; generative neural network, variational autoencoder
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chen2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chen2019label" class="modal fade" id="bibtex-Chen2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChen2019label">
        Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chen2019,
    Author = "Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong",
    title = "Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes the IOA teamâ€™s submission for TASK1A of DCASE2019 challenge. Our acoustic scene classification (ASC) system adopts a data augmentation scheme employing generative adversary networks. Two major classifiers, 1D deep convolutional neural network integrated with scalogram features and 2D fully convolutional neural network integrated with Mel filter bank features, are deployed in the scheme. Other approaches, such as adversary city adaptation, temporal module based on discrete cosine transform and hybrid architectures, have been developed for further fusion. The results of our experiments indicates that the final fusion systems A-D could achieve the accuracy above 85\% on the officially provided fold 1 evaluation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ding2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Ding2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Ensemble System
       </h4>
<p style="text-align:left">
        Biyun Ding, Ganjun Liu and Jinhua Liang
       </p>
<p style="text-align:left">
<em>
         School of Electrical and Information Engineering, TianJin University, Tianjin, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DSPLAB_TJU_task1a_1</span> <span class="label label-primary">DSPLAB_TJU_task1a_2</span> <span class="label label-primary">DSPLAB_TJU_task1a_3</span> <span class="label label-primary">DSPLAB_TJU_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ding2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ding2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ding2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_DSPLAB_44.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ding2019" class="panel-collapse collapse" id="collapse-Ding2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Ensemble System
      </h4>
<p style="text-align:left">
<small>
        Biyun Ding, Ganjun Liu and Jinhua Liang
       </small>
<br/>
<small>
<em>
         School of Electrical and Information Engineering, TianJin University, Tianjin, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report is for the Task 1A Acoustic scene classification of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this task, the features of audio will affect the performance. To improve the performance, we implement Acoustic scene classification task using multiple features and applying ensemble system which composed of CNN and GMM. According to the experiments which were performed with the DCASE 2019 challenge development dataset, the class average accuracy of GMM with 103 features is 64.3%, which is an improvement of 4.2% compared to Baseline CNN. Besides, the class average accuracy of the ensemble system is 66.3% , which is an improvement of 7.4% compared to Baseline CNN
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono; mono, left, right, mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies; MFCC, log-mel energies, ZRC, RMSE, spectrogram centroid
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM; GMM, CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ding2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_DSPLAB_44.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ding2019label" class="modal fade" id="bibtex-Ding2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDing2019label">
        Acoustic Scene Classification Based on Ensemble System
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ding2019,
    Author = "Ding, Biyun and Liu, Ganjun and Liang, Jinhua",
    title = "Acoustic Scene Classification Based on Ensemble System",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report is for the Task 1A Acoustic scene classification of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this task, the features of audio will affect the performance. To improve the performance, we implement Acoustic scene classification task using multiple features and applying ensemble system which composed of CNN and GMM. According to the experiments which were performed with the DCASE 2019 challenge development dataset, the class average accuracy of GMM with 103 features is 64.3\%, which is an improvement of 4.2\% compared to Baseline CNN. Besides, the class average accuracy of the ensemble system is 66.3\% , which is an improvement of 7.4\% compared to Baseline CNN"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Eghbal-zadeh2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Eghbal-zadeh2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized CNNs
       </h4>
<p style="text-align:left">
        Hamid Eghbal-zadeh, Khaled Koutini and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Eghbal-zadeh_CPJKU_task1b_1</span> <span class="label label-info">Eghbal-zadeh_CPJKU_task1b_2</span> <span class="label label-info">Eghbal-zadeh_CPJKU_task1b_3</span> <span class="label label-info">Eghbal-zadeh_CPJKU_task1b_4</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Eghbal-zadeh2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Eghbal-zadeh2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Eghbal-zadeh2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Eghbal-zadeh_99.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Eghbal-zadeh2019').collapse('show');window.location.hash='#Eghbal-zadeh2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Eghbal-zadeh2019" class="panel-collapse collapse" id="collapse-Eghbal-zadeh2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized CNNs
      </h4>
<p style="text-align:left">
<small>
        Hamid Eghbal-zadeh, Khaled Koutini and Gerhard Widmer
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we detail the CP-JKU submissions to the DCASE-2019 challenge Task 1 (acoustic scene classification) and Task 2 (audio tagging with noisy labels and minimal supervision). In all of our submissions, we use fully convolutional deep neural networks architectures that are regularized with Receptive Field (RF) adjustments. We adjust the RF of variants of Resnet and Densenet architectures to best fit the various audio processing tasks that use the spectrogram features as input. Additionally, we propose novel CNN layers such as Frequency-Aware CNNs, and new noise compensation techniques such as Adaptive Weighting for Learning from Noisy Labels to cope with the complexities of each task. We prepared all of our submissions without the use of any external data. Our focus in this yearâ€™s submissions is to provide the best-performing single-model submission, using our proposed approaches.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         perceptual weighted power spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, Receptive Field Regularization
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Eghbal-zadeh2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Eghbal-zadeh_99.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/kkoutini/cpjku_dcase19" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Eghbal-zadeh2019label" class="modal fade" id="bibtex-Eghbal-zadeh2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEghbal-zadeh2019label">
        Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized CNNs
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Eghbal-zadeh2019,
    Author = "Eghbal-zadeh, Hamid and Koutini, Khaled and Widmer, Gerhard",
    title = "Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized {CNNs}",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we detail the CP-JKU submissions to the DCASE-2019 challenge Task 1 (acoustic scene classification) and Task 2 (audio tagging with noisy labels and minimal supervision). In all of our submissions, we use fully convolutional deep neural networks architectures that are regularized with Receptive Field (RF) adjustments. We adjust the RF of variants of Resnet and Densenet architectures to best fit the various audio processing tasks that use the spectrogram features as input. Additionally, we propose novel CNN layers such as Frequency-Aware CNNs, and new noise compensation techniques such as Adaptive Weighting for Learning from Noisy Labels to cope with the complexities of each task. We prepared all of our submissions without the use of any external data. Our focus in this yearâ€™s submissions is to provide the best-performing single-model submission, using our proposed approaches."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="FangLi2019" style="box-shadow: none">
<div class="panel-heading" id="heading-FangLi2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on the Dataset with Deep Convolutional Generated Against Network
       </h4>
<p style="text-align:left">
        Ning FangLi and Duan Shuang
       </p>
<p style="text-align:left">
<em>
         Mechanical Engineering, Northwestern Polytechnical University School, 127 West Youyi Road, Xi'an, 710072, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_NPU_task1a_1</span> <span class="label label-primary">Li_NPU_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-FangLi2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-FangLi2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-FangLi2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Li_12.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-FangLi2019" class="panel-collapse collapse" id="collapse-FangLi2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on the Dataset with Deep Convolutional Generated Against Network
      </h4>
<p style="text-align:left">
<small>
        Ning FangLi and Duan Shuang
       </small>
<br/>
<small>
<em>
         Mechanical Engineering, Northwestern Polytechnical University School, 127 West Youyi Road, Xi'an, 710072, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       As is known to us all, Convolutional Neural Networks have been the most excellent solution for image classification challenges. From the results of DCASE 2018 [1], the Convolutional Neural Network has also achieved excellent results in the classification of acoustic scenes. Therefore, our team also adopted Convolutional Neural Network for DCASE 2019 Task 1a. In order to make the audio features are exposed more, our team used multiple Mel-spectrograms to characterize the audio, trained multiple classifiers, and finally weighted the prediction results of each classifier to make results ensemble. The performance of classifier is largely limited by the quality and quantity of the data. From the results of the technical report [2], the use of GAN to augment the data set can play a vital role in the final performance, and our team also introduced Deep Convolution GANs (DCGAN) [3] to our solution to Task 1a Challenge, Our model ultimately achieved an accuracy of 0.846 on the development set and an accuracy of 0.671 on the leaderboard dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         DCGAN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-FangLi2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Li_12.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-FangLi2019label" class="modal fade" id="bibtex-FangLi2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFangLi2019label">
        Acoustic Scene Classification Based on the Dataset with Deep Convolutional Generated Against Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{FangLi2019,
    Author = "FangLi, Ning and Shuang, Duan",
    title = "Acoustic Scene Classification Based on the Dataset with Deep Convolutional Generated Against Network",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "As is known to us all, Convolutional Neural Networks have been the most excellent solution for image classification challenges. From the results of DCASE 2018 [1], the Convolutional Neural Network has also achieved excellent results in the classification of acoustic scenes. Therefore, our team also adopted Convolutional Neural Network for DCASE 2019 Task 1a. In order to make the audio features are exposed more, our team used multiple Mel-spectrograms to characterize the audio, trained multiple classifiers, and finally weighted the prediction results of each classifier to make results ensemble. The performance of classifier is largely limited by the quality and quantity of the data. From the results of the technical report [2], the use of GAN to augment the data set can play a vital role in the final performance, and our team also introduced Deep Convolution GANs (DCGAN) [3] to our solution to Task 1a Challenge, Our model ultimately achieved an accuracy of 0.846 on the development set and an accuracy of 0.671 on the leaderboard dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fraile2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Fraile2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps
       </h4>
<p style="text-align:left">
        Ruben Fraile, Juan Carlos Reina, Juana Gutierrez-Arriola and Elena Blanco
       </p>
<p style="text-align:left">
<em>
         CITSEM, Universidad Politecnica de Madrid, Madrid, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Fraile_UPM_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fraile2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fraile2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fraile2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Fraile_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fraile2019" class="panel-collapse collapse" id="collapse-Fraile2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps
      </h4>
<p style="text-align:left">
<small>
        Ruben Fraile, Juan Carlos Reina, Juana Gutierrez-Arriola and Elena Blanco
       </small>
<br/>
<small>
<em>
         CITSEM, Universidad Politecnica de Madrid, Madrid, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       A system for the automatic classification of acoustic scenes is proposed that uses the stereophonic signal captured by a binaural microphone. This system uses one channel for calculating the spectral distribution of energy across auditory-relevant frequency bands. It further obtains some descriptors of the envelope modulation spectrum (EMS) by applying the discrete cosine transform to the logarithm of the EMS. The availability of the two-channel binaural recordings is used for representing the spatial distribution of acoustic sources by means of position-pitch maps. These maps are further parametrized using the two-dimensional Fourier transform. These three types of features (energy spectrum, EMS and positionpitch maps) are used as inputs for a standard Gaussian Mixture Model with 64 components.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram, modulation spectrum, position-pitch maps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average log-likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fraile2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Fraile_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fraile2019label" class="modal fade" id="bibtex-Fraile2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFraile2019label">
        Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fraile2019,
    Author = "Fraile, Ruben and Reina, Juan Carlos and Gutierrez-Arriola, Juana and Blanco, Elena",
    title = "Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "A system for the automatic classification of acoustic scenes is proposed that uses the stereophonic signal captured by a binaural microphone. This system uses one channel for calculating the spectral distribution of energy across auditory-relevant frequency bands. It further obtains some descriptors of the envelope modulation spectrum (EMS) by applying the discrete cosine transform to the logarithm of the EMS. The availability of the two-channel binaural recordings is used for representing the spatial distribution of acoustic sources by means of position-pitch maps. These maps are further parametrized using the two-dimensional Fourier transform. These three types of features (energy spectrum, EMS and positionpitch maps) are used as inputs for a standard Gaussian Mixture Model with 64 components."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gao2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Gao2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep Residual Networks with Late Fusion of Separated High and Low Frequency Paths
       </h4>
<p style="text-align:left">
        Wei Gao and Mark McDonnell
       </p>
<p style="text-align:left">
<em>
         School of Information Technology and Mathematical Sciences, University of South Australia, Mawson Lakes, Australia
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">McDonnell_USA_task1a_1</span> <span class="label label-primary">McDonnell_USA_task1a_2</span> <span class="label label-primary">McDonnell_USA_task1a_3</span> <span class="label label-primary">McDonnell_USA_task1a_4</span> <span class="clearfix"></span><span class="label label-info">McDonnell_USA_task1b_1</span> <span class="label label-info">McDonnell_USA_task1b_2</span> <span class="label label-info">McDonnell_USA_task1b_3</span> <span class="label label-info">McDonnell_USA_task1b_4</span> <span class="clearfix"></span><span class="label label-warning">McDonnell_USA_task1c_1</span> <span class="label label-warning">McDonnell_USA_task1c_2</span> <span class="label label-warning">McDonnell_USA_task1c_3</span> <span class="label label-warning">McDonnell_USA_task1c_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Gao2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gao2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gao2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_McDonnell_53.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Gao2019').collapse('show');window.location.hash='#Gao2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gao2019" class="panel-collapse collapse" id="collapse-Gao2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep Residual Networks with Late Fusion of Separated High and Low Frequency Paths
      </h4>
<p style="text-align:left">
<small>
        Wei Gao and Mark McDonnell
       </small>
<br/>
<small>
<em>
         School of Information Technology and Mathematical Sciences, University of South Australia, Mawson Lakes, Australia
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our approach to Tasks 1a, 1b and 1c in the 2019 DCASE acoustic scene classification challenge. Our focus was on developing strong single models, without use of any supplementary data. We investigated the use of a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We designed the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we used two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right; mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, temporal cropping
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, deltas and delta-deltas; log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gao2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_McDonnell_53.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/McDonnell-Lab/DCASE2019-Task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gao2019label" class="modal fade" id="bibtex-Gao2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGao2019label">
        Acoustic Scene Classification Using Deep Residual Networks with Late Fusion of Separated High and Low Frequency Paths
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Gao2019,
    Author = "Gao, Wei and McDonnell, Mark",
    title = "Acoustic Scene Classification Using Deep Residual Networks with Late Fusion of Separated High and Low Frequency Paths",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes our approach to Tasks 1a, 1b and 1c in the 2019 DCASE acoustic scene classification challenge. Our focus was on developing strong single models, without use of any supplementary data. We investigated the use of a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We designed the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we used two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Haocong2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Haocong2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using CNN Ensembles and Primary Ambient Extraction
       </h4>
<p style="text-align:left">
        Yang Haocong, Shi Chuang and Li Huiyong
       </p>
<p style="text-align:left">
<em>
         Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yang_UESTC_task1a_1</span> <span class="label label-primary">Yang_UESTC_task1a_2</span> <span class="label label-primary">Yang_UESTC_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Haocong2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Haocong2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Haocong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Yang_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Haocong2019" class="panel-collapse collapse" id="collapse-Haocong2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using CNN Ensembles and Primary Ambient Extraction
      </h4>
<p style="text-align:left">
<small>
        Yang Haocong, Shi Chuang and Li Huiyong
       </small>
<br/>
<small>
<em>
         Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes our submission for Task 1a (acoustic scene classification) of the DCASE 2019 challenge. The results of the DCASE 2018 challenge demonstrate that the convolution neural networks (CNNs) and their ensembles can achieve excellent clas-sification accuracies. Inspired by the previous works, our method continues to work on the ensembles of CNNs, whereas the prima-ry ambient extraction is newly introduced to decompose a binaural audio sample into four channels by using the spatial information. The feature extraction is still carried out with mel spectrograms. 6 CNN models are trained by using the 4-fold cross validation. Ensemble is applied to further improve the performance. Finally, our method has achieved classification accuracies of 0.84 on the public leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average; random forest
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Haocong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Yang_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Haocong2019label" class="modal fade" id="bibtex-Haocong2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHaocong2019label">
        Acoustic Scene Classification Using CNN Ensembles and Primary Ambient Extraction
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Haocong2019,
    Author = "Haocong, Yang and Chuang, Shi and Huiyong, Li",
    title = "Acoustic Scene Classification Using {CNN} Ensembles and Primary Ambient Extraction",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describes our submission for Task 1a (acoustic scene classification) of the DCASE 2019 challenge. The results of the DCASE 2018 challenge demonstrate that the convolution neural networks (CNNs) and their ensembles can achieve excellent clas-sification accuracies. Inspired by the previous works, our method continues to work on the ensembles of CNNs, whereas the prima-ry ambient extraction is newly introduced to decompose a binaural audio sample into four channels by using the spatial information. The feature extraction is still carried out with mel spectrograms. 6 CNN models are trained by using the 4-fold cross validation. Ensemble is applied to further improve the performance. Finally, our method has achieved classification accuracies of 0.84 on the public leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep Learning-Based Ensemble Averaging
       </h4>
<p style="text-align:left">
        Jonathan Huang<sup>1</sup>, Paulo Lopez Meyer<sup>2</sup>, Hong Lu<sup>1</sup>, Hector Cordourier Maruri<sup>2</sup> and Juan Del Hoyo<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Intel Labs, Intel Corporation, Santa Clara, CA, USA, <sup>2</sup>Intel Labs, Intel Corporation, Zapopan, Jalisco, Mexico
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Huang_IL_task1a_1</span> <span class="label label-primary">Huang_IL_task1a_2</span> <span class="label label-primary">Huang_IL_task1a_3</span> <span class="label label-primary">Huang_IL_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Huang_83.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2019" class="panel-collapse collapse" id="collapse-Huang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep Learning-Based Ensemble Averaging
      </h4>
<p style="text-align:left">
<small>
        Jonathan Huang<sup>1</sup>, Paulo Lopez Meyer<sup>2</sup>, Hong Lu<sup>1</sup>, Hector Cordourier Maruri<sup>2</sup> and Juan Del Hoyo<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Intel Labs, Intel Corporation, Santa Clara, CA, USA, <sup>2</sup>Intel Labs, Intel Corporation, Zapopan, Jalisco, Mexico
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In our submission to the DCASE 2019 Task 1a, we have explored the use of four different deep learning based neural networks architectures: Vgg12, ResNet50, AclNet, and AclSincNet. In order to improve performance, these four network architectures were pretrained with Audioset data, and then fine-tuned over the development set for the task. The outputs produced by these networks, due to the diversity of feature front-end and of architecture differences, proved to be complementary when fused together. The ensemble of these modelsâ€™ outputs improved from best single model accuracy of 77.9% to 83.0% on the validation set, trained with the challenge defaultâ€™s development split.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono; mono , binaural; binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz; 48kHz, 16kHz; 48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         raw waveform, log-mel energies; log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         Max value of soft ensemble
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Huang_83.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2019label" class="modal fade" id="bibtex-Huang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2019label">
        Acoustic Scene Classification Using Deep Learning-Based Ensemble Averaging
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang2019,
    Author = "Huang, Jonathan and Lopez Meyer, Paulo and Lu, Hong and Cordourier Maruri, Hector and Del Hoyo, Juan",
    title = "Acoustic Scene Classification Using Deep Learning-Based Ensemble Averaging",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In our submission to the DCASE 2019 Task 1a, we have explored the use of four different deep learning based neural networks architectures: Vgg12, ResNet50, AclNet, and AclSincNet. In order to improve performance, these four network architectures were pretrained with Audioset data, and then fine-tuned over the development set for the task. The outputs produced by these networks, due to the diversity of feature front-end and of architecture differences, proved to be complementary when fused together. The ensemble of these modelsâ€™ outputs improved from best single model accuracy of 77.9\% to 83.0\% on the validation set, trained with the challenge defaultâ€™s development split."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2019a" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang2019a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Deep Convolutional Neuralnetwork with Spatial-Temporal Attention Pooling
       </h4>
<p style="text-align:left">
        Zhenyi Huang and Dacan Jiang
       </p>
<p style="text-align:left">
<em>
         School of Computer, South China Normal University, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Huang_SCNU_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang2019a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2019a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Huang_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2019a" class="panel-collapse collapse" id="collapse-Huang2019a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Deep Convolutional Neuralnetwork with Spatial-Temporal Attention Pooling
      </h4>
<p style="text-align:left">
<small>
        Zhenyi Huang and Dacan Jiang
       </small>
<br/>
<small>
<em>
         School of Computer, South China Normal University, Guangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic scene classification is a challenging task in machine learn-ing with limited data sets. In this report, several different spectro-grams are applied to classify the acoustic scenes using deep convo-lutional neural network with spatial-temporal attention pooling. Inaddition, mixup augmentation is performed to further improve theclassification performance. Finally, majority voting is performed onsix different models and an accuracy of 73.86% is achieved which is11.36 percentage points higher than the one of the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Huang_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2019alabel" class="modal fade" id="bibtex-Huang2019a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2019alabel">
        Acoustic Scene Classification Based on Deep Convolutional Neuralnetwork with Spatial-Temporal Attention Pooling
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang2019a,
    Author = "Huang, Zhenyi and Jiang, Dacan",
    title = "Acoustic Scene Classification Based on Deep Convolutional Neuralnetwork with Spatial-Temporal Attention Pooling",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Acoustic scene classification is a challenging task in machine learn-ing with limited data sets. In this report, several different spectro-grams are applied to classify the acoustic scenes using deep convo-lutional neural network with spatial-temporal attention pooling. Inaddition, mixup augmentation is performed to further improve theclassification performance. Finally, majority voting is performed onsix different models and an accuracy of 73.86\% is achieved which is11.36 percentage points higher than the one of the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hyeji2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Hyeji2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Various Pre-Processed Features and Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Seo Hyeji and Park Jihwan
       </p>
<p style="text-align:left">
<em>
         Advanced Robotics Lab, LG Electronics, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Seo_LGE_task1a_1</span> <span class="label label-primary">Seo_LGE_task1a_2</span> <span class="label label-primary">Seo_LGE_task1a_3</span> <span class="label label-primary">Seo_LGE_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hyeji2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hyeji2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hyeji2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Seo_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hyeji2019" class="panel-collapse collapse" id="collapse-Hyeji2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Various Pre-Processed Features and Convolutional Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Seo Hyeji and Park Jihwan
       </small>
<br/>
<small>
<em>
         Advanced Robotics Lab, LG Electronics, Seoul, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our acoustic scene classification algorithm submitted in DCASE 2019 Task 1a. We focus on various pre-processed features to categorize the class of acoustic scenes using only stereo microphone input signal. In the frontend system, the pre-processed and spatial information are extracted from the stereo microphone input. Residual network, subspectral network, and conventional convolutional neural network (CNN) are used for back-end systems. Finally, we ensemble all of the models to take advantage of each algorithm. By using proposed systems, we achieved a classification accuracy of 80.4%, which is 17.9% over than the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, spectrogram, chromagram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hyeji2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Seo_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hyeji2019label" class="modal fade" id="bibtex-Hyeji2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHyeji2019label">
        Acoustic Scene Classification Using Various Pre-Processed Features and Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hyeji2019,
    Author = "Hyeji, Seo and Jihwan, Park",
    title = "Acoustic Scene Classification Using Various Pre-Processed Features and Convolutional Neural Networks",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we describe our acoustic scene classification algorithm submitted in DCASE 2019 Task 1a. We focus on various pre-processed features to categorize the class of acoustic scenes using only stereo microphone input signal. In the frontend system, the pre-processed and spatial information are extracted from the stereo microphone input. Residual network, subspectral network, and conventional convolutional neural network (CNN) are used for back-end systems. Finally, we ensemble all of the models to take advantage of each algorithm. By using proposed systems, we achieved a classification accuracy of 80.4\%, which is 17.9\% over than the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jiang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Jiang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Ensembles of Convolutional Neural Networks and Spectrogram Decompositions
       </h4>
<p style="text-align:left">
        Shengwang Jiang and Chuang Shi
       </p>
<p style="text-align:left">
<em>
         School of Communication and Information Engineering, University of Electronic Science and Technology of China, Chengdu, China
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Jiang_UESTC_task1b_1</span> <span class="label label-info">Jiang_UESTC_task1b_2</span> <span class="label label-info">Jiang_UESTC_task1b_3</span> <span class="label label-info">Jiang_UESTC_task1b_4</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jiang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jiang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jiang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Jiang_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jiang2019" class="panel-collapse collapse" id="collapse-Jiang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Ensembles of Convolutional Neural Networks and Spectrogram Decompositions
      </h4>
<p style="text-align:left">
<small>
        Shengwang Jiang and Chuang Shi
       </small>
<br/>
<small>
<em>
         School of Communication and Information Engineering, University of Electronic Science and Technology of China, Chengdu, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report proposes ensembles of convolutional neural networks (CNNs) for the task 1 / subtask B of the DCASE 2019 challenge, with emphasis on using different spectrogram decompositions. The harmonic percussive source separation (HPSS), nearest neighbor filter (NNF), and vocal separation are applied to the monaural samples. Head-related transfer function (HRTF) is also proposed to transform monaural samples to binaural ones with augmented spatial information. Finally, 16 neural networks are trained and put together. The classification accuracy of the proposed system achieves 0.70166 on the public leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         HPSS, NNF, vocal separation, HRTF
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         stacking; averaging
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jiang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Jiang_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jiang2019label" class="modal fade" id="bibtex-Jiang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJiang2019label">
        Acoustic Scene Classification Using Ensembles of Convolutional Neural Networks and Spectrogram Decompositions
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jiang2019,
    Author = "Jiang, Shengwang and Shi, Chuang",
    title = "Acoustic Scene Classification Using Ensembles of Convolutional Neural Networks and Spectrogram Decompositions",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report proposes ensembles of convolutional neural networks (CNNs) for the task 1 / subtask B of the DCASE 2019 challenge, with emphasis on using different spectrogram decompositions. The harmonic percussive source separation (HPSS), nearest neighbor filter (NNF), and vocal separation are applied to the monaural samples. Head-related transfer function (HRTF) is also proposed to transform monaural samples to binaural ones with augmented spatial information. Finally, 16 neural networks are trained and put together. The classification accuracy of the proposed system achieves 0.70166 on the public leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jung2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Jung2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Knowledge Distillation with Specialist Models in Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Jee-weon Jung, Hee-Soo Heo, Hye-jin Shim and Ha-Jin Yu
       </p>
<p style="text-align:left">
<em>
         Computing Sciences, Univerisity of Seoul, Seoul, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jung_UOS_task1a_1</span> <span class="label label-primary">Jung_UOS_task1a_2</span> <span class="label label-primary">Jung_UOS_task1a_3</span> <span class="label label-primary">Jung_UOS_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jung2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jung2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jung2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Jung_98.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jung2019" class="panel-collapse collapse" id="collapse-Jung2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Knowledge Distillation with Specialist Models in Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Jee-weon Jung, Hee-Soo Heo, Hye-jin Shim and Ha-Jin Yu
       </small>
<br/>
<small>
<em>
         Computing Sciences, Univerisity of Seoul, Seoul, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission for the Detection and Classification of Acoustic Scenes and Events 2019 task1-a competition which exploits knowledge distillation with specialist models. Different acoustic scenes that share common properties are one of the main obstacles that hinder successful acoustic scene classification. We found that confusion between scenes, sharing the common properties, causes most of the errors in the acoustic scene classification. For example, the confusing scene pairs such as airport-shopping mall and metro-tram have caused the most errors in various systems. We applied knowledge distillation based on the specialist models to address the errors from the most confusing scene pairs. Specialist models where each model concentrates on discriminating a pair two similar scenes are exploited to provide soft-labels. We expected that knowledge distillation from multiple specialist models and a pre-trained generalist model to a single model could train an ensemble of models that gives more emphasis on discriminating specific acoustic scene pairs. Through knowledge distillation from well trained model and specialist models to single model, we report improved accuracy on the validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         raw waveform, log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote; score-sum
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jung2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Jung_98.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jung2019label" class="modal fade" id="bibtex-Jung2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJung2019label">
        Knowledge Distillation with Specialist Models in Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jung2019,
    Author = "Jung, Jee-weon and Heo, Hee-Soo and Shim, Hye-jin and Yu, Ha-Jin",
    title = "Knowledge Distillation with Specialist Models in Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we describe our submission for the Detection and Classification of Acoustic Scenes and Events 2019 task1-a competition which exploits knowledge distillation with specialist models. Different acoustic scenes that share common properties are one of the main obstacles that hinder successful acoustic scene classification. We found that confusion between scenes, sharing the common properties, causes most of the errors in the acoustic scene classification. For example, the confusing scene pairs such as airport-shopping mall and metro-tram have caused the most errors in various systems. We applied knowledge distillation based on the specialist models to address the errors from the most confusing scene pairs. Specialist models where each model concentrates on discriminating a pair two similar scenes are exploited to provide soft-labels. We expected that knowledge distillation from multiple specialist models and a pre-trained generalist model to a single model could train an ensemble of models that gives more emphasis on discriminating specific acoustic scene pairs. Through knowledge distillation from well trained model and specialist models to single model, we report improved accuracy on the validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="KK2019" style="box-shadow: none">
<div class="panel-heading" id="heading-KK2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The I2r Submission to DCASE 2019 Challenge
       </h4>
<p style="text-align:left">
        Teh KK<sup>1</sup>, Sun HW<sup>2</sup> and Tran Huy Dat<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>I2R, A-star, Singapore, <sup>2</sup>I2R, A-Star, Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">KK_I2R_task1a_1</span> <span class="label label-primary">KK_I2R_task1a_2</span> <span class="label label-primary">KK_I2R_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-KK2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-KK2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-KK2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_KK_39.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-KK2019" class="panel-collapse collapse" id="collapse-KK2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The I2r Submission to DCASE 2019 Challenge
      </h4>
<p style="text-align:left">
<small>
        Teh KK<sup>1</sup>, Sun HW<sup>2</sup> and Tran Huy Dat<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>I2R, A-star, Singapore, <sup>2</sup>I2R, A-Star, Singapore
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper proposes Convolutional Neural Network (CNN) ensembles for acoustic scene classification of tasks1A of the DCASE 2019 challenge. In this approach various preprocessing features method: mel-filterbank and delta feature vectors, harmonic-percussive and subband power distribution are used to train CNN model. We also used score-fusion of the features to find an optimum feature configuration. On the official leaderboard data set of the task1A challenge, an accuracy of 79.67% is achieved.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, HPSS; log-mel energies, HPSS, subband power distribution
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted averaging vote; multi-class linear logistic regression
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-KK2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_KK_39.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-KK2019label" class="modal fade" id="bibtex-KK2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKK2019label">
        The I2r Submission to DCASE 2019 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{KK2019,
    Author = "KK, Teh and HW, Sun and Huy Dat, Tran",
    title = "The I2r Submission to {DCASE} 2019 Challenge",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper proposes Convolutional Neural Network (CNN) ensembles for acoustic scene classification of tasks1A of the DCASE 2019 challenge. In this approach various preprocessing features method: mel-filterbank and delta feature vectors, harmonic-percussive and subband power distribution are used to train CNN model. We also used score-fusion of the features to find an optimum feature configuration. On the official leaderboard data set of the task1A challenge, an accuracy of 79.67\% is achieved."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Komider2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Komider2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Calibrating Neural Networks for Secondary Recording Devices
       </h4>
<p style="text-align:left">
        MichaÅ‚ KoÅ›mider
       </p>
<p style="text-align:left">
<em>
         Artificial Intelligence, Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Kosmider_SRPOL_task1b_1</span> <span class="label label-info">Kosmider_SRPOL_task1b_2</span> <span class="label label-info">Kosmider_SRPOL_task1b_3</span> <span class="label label-info">Kosmider_SRPOL_task1b_4</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Komider2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Komider2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Komider2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kosmider_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Komider2019" class="panel-collapse collapse" id="collapse-Komider2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Calibrating Neural Networks for Secondary Recording Devices
      </h4>
<p style="text-align:left">
<small>
        MichaÅ‚ KoÅ›mider
       </small>
<br/>
<small>
<em>
         Artificial Intelligence, Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes the solution to Task 1B of the DCASE 2019 challenge proposed by Samsung R&amp;D Institute Poland. Primary focus of the system for task 1B was a novel technique designed to address issues with learning from microphones with different frequency responses in settings with limited examples for the targeted secondary devices. This technique is independent from the architecture of the predictive model and requires just a few examples to become effective.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Spectrum Correction, SpecAugment, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         isotonic-calibrated soft-voting; soft-voting
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Komider2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kosmider_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Komider2019label" class="modal fade" id="bibtex-Komider2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKomider2019label">
        Calibrating Neural Networks for Secondary Recording Devices
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Komider2019,
    Author = "KoÅ›mider, MichaÅ‚",
    title = "Calibrating Neural Networks for Secondary Recording Devices",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describes the solution to Task 1B of the DCASE 2019 challenge proposed by Samsung R\&amp;D Institute Poland. Primary focus of the system for task 1B was a novel technique designed to address issues with learning from microphones with different frequency responses in settings with limited examples for the targeted secondary devices. This technique is independent from the architecture of the predictive model and requires just a few examples to become effective."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Cross-Task Learning for Audio Tagging, Sound Event Detection and Spatial Localization: DCASE 2019 Baseline Systems
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Yin Cao, Turab Iqbal, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, England
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kong_SURREY_task1a_1</span> <span class="clearfix"></span><span class="label label-info">Kong_SURREY_task1b_1</span> <span class="clearfix"></span><span class="label label-warning">Kong_SURREY_task1c_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2019').collapse('show');window.location.hash='#Kong2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2019" class="panel-collapse collapse" id="collapse-Kong2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Cross-Task Learning for Audio Tagging, Sound Event Detection and Spatial Localization: DCASE 2019 Baseline Systems
      </h4>
<p style="text-align:left">
<small>
        Qiuqiang Kong, Yin Cao, Turab Iqbal, Wenwu Wang and Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, England
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2019_task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2019label" class="modal fade" id="bibtex-Kong2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2019label">
        Cross-Task Learning for Audio Tagging, Sound Event Detection and Spatial Localization: DCASE 2019 Baseline Systems
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2019,
    Author = "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.",
    title = "Cross-Task Learning for Audio Tagging, Sound Event Detection and Spatial Localization: {DCASE} 2019 Baseline Systems",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koutini2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Koutini2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized CNNs
       </h4>
<p style="text-align:left">
        Khaled Koutini, Hamid Eghbal-zadeh and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Koutini_CPJKU_task1a_1</span> <span class="label label-primary">Koutini_CPJKU_task1a_2</span> <span class="label label-primary">Koutini_CPJKU_task1a_3</span> <span class="label label-primary">Koutini_CPJKU_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Koutini2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koutini2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koutini2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Koutini_99.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Koutini2019').collapse('show');window.location.hash='#Koutini2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koutini2019" class="panel-collapse collapse" id="collapse-Koutini2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized CNNs
      </h4>
<p style="text-align:left">
<small>
        Khaled Koutini, Hamid Eghbal-zadeh and Gerhard Widmer
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we detail the CP-JKU submissions to the DCASE-2019 challenge Task 1 (acoustic scene classification) and Task 2 (audio tagging with noisy labels and minimal supervision). In all of our submissions, we use fully convolutional deep neural networks architectures that are regularized with Receptive Field (RF) adjustments. We adjust the RF of variants of Resnet and Densenet architectures to best fit the various audio processing tasks that use the spectrogram features as input. Additionally, we propose novel CNN layers such as Frequency-Aware CNNs, and new noise compensation techniques such as Adaptive Weighting for Learning from Noisy Labels to cope with the complexities of each task. We prepared all of our submissions without the use of any external data. Our focus in this yearâ€™s submissions is to provide the best-performing single-model submission, using our proposed approaches.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         perceptual weighted power spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, Receptive Field Regularization
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koutini2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Koutini_99.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/kkoutini/cpjku_dcase19" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koutini2019label" class="modal fade" id="bibtex-Koutini2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoutini2019label">
        Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized CNNs
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Koutini2019,
    Author = "Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard",
    title = "Acoustic Scene Classification and Audio Tagging with Receptive-Field-Regularized {CNNs}",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we detail the CP-JKU submissions to the DCASE-2019 challenge Task 1 (acoustic scene classification) and Task 2 (audio tagging with noisy labels and minimal supervision). In all of our submissions, we use fully convolutional deep neural networks architectures that are regularized with Receptive Field (RF) adjustments. We adjust the RF of variants of Resnet and Densenet architectures to best fit the various audio processing tasks that use the spectrogram features as input. Additionally, we propose novel CNN layers such as Frequency-Aware CNNs, and new noise compensation techniques such as Adaptive Weighting for Learning from Noisy Labels to cope with the complexities of each task. We prepared all of our submissions without the use of any external data. Our focus in this yearâ€™s submissions is to provide the best-performing single-model submission, using our proposed approaches."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lehner2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Lehner2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification with Reject Option Based on Resnets
       </h4>
<p style="text-align:left">
        Bernhard Lehner<sup>1</sup> and Khaled Koutini<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Silicon Austria Labs, JKU, Linz, Austria, <sup>2</sup>Institute of Computational Perception, JKU, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="clearfix"></span><span class="label label-warning">Lehner_SAL_task1c_1</span> <span class="label label-warning">Lehner_SAL_task1c_2</span> <span class="label label-warning">Lehner_SAL_task1c_3</span> <span class="label label-warning">Lehner_SAL_task1c_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lehner2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lehner2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lehner2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lehner_103.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lehner2019" class="panel-collapse collapse" id="collapse-Lehner2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification with Reject Option Based on Resnets
      </h4>
<p style="text-align:left">
<small>
        Bernhard Lehner<sup>1</sup> and Khaled Koutini<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Silicon Austria Labs, JKU, Linz, Austria, <sup>2</sup>Institute of Computational Perception, JKU, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the submissions from the SAL/CP JKU team for Task 1 - Subtask C (classification on data that includes classes not encountered in the training data) of the DCASE-2019 challenge. Our method uses a ResNet variant specifically adapted to be used along with spectrograms in the context of Acoustic Scene Classification (ASC). The reject option is based on the logit values of the same networks. We do not use any of the provided external data sets, and perform data augmentation only with the mixup technique [1]. The result of our experiments is a system that achieves classification accuracies of up to around 60% on the public Kaggle-Leaderboard. This is an improvement of around 14 percentage points compared to the official DCASE 2019 baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         logit averaging
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lehner2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lehner_103.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lehner2019label" class="modal fade" id="bibtex-Lehner2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLehner2019label">
        Acoustic Scene Classification with Reject Option Based on Resnets
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lehner2019,
    Author = "Lehner, Bernhard and Koutini, Khaled",
    title = "Acoustic Scene Classification with Reject Option Based on Resnets",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes the submissions from the SAL/CP JKU team for Task 1 - Subtask C (classification on data that includes classes not encountered in the training data) of the DCASE-2019 challenge. Our method uses a ResNet variant specifically adapted to be used along with spectrograms in the context of Acoustic Scene Classification (ASC). The reject option is based on the logit values of the same networks. We do not use any of the provided external data sets, and perform data augmentation only with the mixup technique [1]. The result of our experiments is a system that achieves classification accuracies of up to around 60\% on the public Kaggle-Leaderboard. This is an improvement of around 14 percentage points compared to the official DCASE 2019 baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lei2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Lei2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Scale Recalibrated Features Fusion for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Chongqin Lei and Zixu Wang
       </p>
<p style="text-align:left">
<em>
         Intelligent Information Technology and System Lab, CHONGQING UNIVERSITY, Chongqing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lei_CQU_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lei2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lei2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lei2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lei_45.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lei2019" class="panel-collapse collapse" id="collapse-Lei2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Scale Recalibrated Features Fusion for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Chongqin Lei and Zixu Wang
       </small>
<br/>
<small>
<em>
         Intelligent Information Technology and System Lab, CHONGQING UNIVERSITY, Chongqing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We investigate the effectiveness of multi-scale recalibrated features fusion for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2019). A general problem in acoustic scene classification task is audio signal segment contains less effective information. In order to further utilize features with less effective information to improve classification accuracy, we introduce the Squeeze-and-Excitation unit to embed the backbone structure of Xception to recalibrate the channel weights of feature maps in each block. In addition, the recalibrated features of multiscale are fused and finally fed into the full connection layer to get more useful information. Furthermore, we introduce Mixup method to augment the data in training stage to reduce the degree of over-fitting of network. The proposed method attains a recognition accuracy of 77.5%, which is 13% higher compared to the baseline system of the DCASE 2019 Acoustic Scenes Classification task.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lei2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lei_45.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lei2019label" class="modal fade" id="bibtex-Lei2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLei2019label">
        Multi-Scale Recalibrated Features Fusion for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lei2019,
    Author = "Lei, Chongqin and Wang, Zixu",
    title = "Multi-Scale Recalibrated Features Fusion for Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "We investigate the effectiveness of multi-scale recalibrated features fusion for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2019). A general problem in acoustic scene classification task is audio signal segment contains less effective information. In order to further utilize features with less effective information to improve classification accuracy, we introduce the Squeeze-and-Excitation unit to embed the backbone structure of Xception to recalibrate the channel weights of feature maps in each block. In addition, the recalibrated features of multiscale are fused and finally fed into the full connection layer to get more useful information. Furthermore, we introduce Mixup method to augment the data in training stage to reduce the degree of over-fitting of network. The proposed method attains a recognition accuracy of 77.5\%, which is 13\% higher compared to the baseline system of the DCASE 2019 Acoustic Scenes Classification task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Liang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Attention-Based Convolutional Neural Network
       </h4>
<p style="text-align:left">
        Han Liang and Yaxiong Ma
       </p>
<p style="text-align:left">
<em>
         Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liang_HUST_task1a_1</span> <span class="label label-primary">Liang_HUST_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Liang_3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liang2019" class="panel-collapse collapse" id="collapse-Liang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Attention-Based Convolutional Neural Network
      </h4>
<p style="text-align:left">
<small>
        Han Liang and Yaxiong Ma
       </small>
<br/>
<small>
<em>
         Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE 2019 challenge whose goal is to classify a test audio recording into one of the predefined classes that characterizes the environment. We detemine to use mel-spectrogram as audio feature and deep convolutional neural networks (CNNs) as classifier to classify acoustic scenes. In our method, spectrogram of every audio clip is divided in two ways. In addition, we introduce attention mechanism to further improve the performance. Experimental results illustrate that our best model can achieve classification accuracy of around 70.7% for Development dataset, which is superior to the baseline system with the accuracy of 62.5%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Liang_3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liang2019label" class="modal fade" id="bibtex-Liang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiang2019label">
        Acoustic Scene Classification Using Attention-Based Convolutional Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liang2019,
    Author = "Liang, Han and Ma, Yaxiong",
    title = "Acoustic Scene Classification Using Attention-Based Convolutional Neural Network",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes the Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE 2019 challenge whose goal is to classify a test audio recording into one of the predefined classes that characterizes the environment. We detemine to use mel-spectrogram as audio feature and deep convolutional neural networks (CNNs) as classifier to classify acoustic scenes. In our method, spectrogram of every audio clip is divided in two ways. In addition, we introduce attention mechanism to further improve the performance. Experimental results illustrate that our best model can achieve classification accuracy of around 70.7\% for Development dataset, which is superior to the baseline system with the accuracy of 62.5\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ma2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Ma2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Jsnu_wdxy Submission for DCASE-2019: Acoustic Scene Classification with Convolution Neural Networks
       </h4>
<p style="text-align:left">
        Xinixn Ma and Mingliang Gu
       </p>
<p style="text-align:left">
<em>
         School of Physics and Electronic, Jiangsu Normal University, Xuzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">JSNU_WDXY_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ma2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ma2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ma2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_JSNU_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ma2019" class="panel-collapse collapse" id="collapse-Ma2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Jsnu_wdxy Submission for DCASE-2019: Acoustic Scene Classification with Convolution Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Xinixn Ma and Mingliang Gu
       </small>
<br/>
<small>
<em>
         School of Physics and Electronic, Jiangsu Normal University, Xuzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic Scene Classification (ASC) is the task of identifying the scene from which the audio signal is recorded. It is one of the core research problems in the field of Computational Sound Scene Analysis. Most of current best performing Acoustic Scene Classification systems utilize Mel scale spectrograms with Convolutional Neural Networks (CNNs). In this paper, we demonstrate how we applied convolutional neural network for DCASE 2019 task1, acoustic scene classification. First, we applied Mel scale spectrogram to extract acoustic features. Mel scale is a common way to suit frequency warping of human ears, with strict decreasing frequency resolution on low to high frequency range. Second, we generate Mel spectrogram from binaural audio, adaptively learn 5 Convolutional Neural Networks. The best classification result of the proposed system was 71.1% for Development dataset and 73.16% for Leaderboard dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ma2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_JSNU_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ma2019label" class="modal fade" id="bibtex-Ma2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMa2019label">
        Jsnu_wdxy Submission for DCASE-2019: Acoustic Scene Classification with Convolution Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ma2019,
    Author = "Ma, Xinixn and Gu, Mingliang",
    title = "Jsnu\_wdxy Submission for {DCASE}-2019: Acoustic Scene Classification with Convolution Neural Networks",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Acoustic Scene Classification (ASC) is the task of identifying the scene from which the audio signal is recorded. It is one of the core research problems in the field of Computational Sound Scene Analysis. Most of current best performing Acoustic Scene Classification systems utilize Mel scale spectrograms with Convolutional Neural Networks (CNNs). In this paper, we demonstrate how we applied convolutional neural network for DCASE 2019 task1, acoustic scene classification. First, we applied Mel scale spectrogram to extract acoustic features. Mel scale is a common way to suit frequency warping of human ears, with strict decreasing frequency resolution on low to high frequency range. Second, we generate Mel spectrogram from binaural audio, adaptively learn 5 Convolutional Neural Networks. The best classification result of the proposed system was 71.1\% for Development dataset and 73.16\% for Leaderboard dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ma2019a" style="box-shadow: none">
<div class="panel-heading" id="heading-Ma2019a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with Neural Network
       </h4>
<p style="text-align:left">
        Sifan Ma and Wei Liu
       </p>
<p style="text-align:left">
<em>
         Laboratory of Modern Communication, Beijing Institute of Technology, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">MaLiu_BIT_task1a_1</span> <span class="label label-primary">MaLiu_BIT_task1a_2</span> <span class="label label-primary">MaLiu_BIT_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ma2019a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ma2019a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ma2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_MaLiu_112.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Ma2019a').collapse('show');window.location.hash='#Ma2019a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ma2019a" class="panel-collapse collapse" id="collapse-Ma2019a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with Neural Network
      </h4>
<p style="text-align:left">
<small>
        Sifan Ma and Wei Liu
       </small>
<br/>
<small>
<em>
         Laboratory of Modern Communication, Beijing Institute of Technology, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents our approach for the acoustic scene classification of DCASE2019 task1a. Compared to traditional audio features such as Mel-frequency Cepstral Coefficients (MFCC) and Constant-Q Transform (CQT), we choose Deep Scattering Spectra (DSS) features which are more suitable for characterizing acoustic scenes. DSS is a good way to preserve high frequency information. Based on DSS features, we choose a network model of Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) to classify acoustic scenes. The experimental results show that our approach increase the classification accuracy from 62.5% (DCASE2019 baseline) to 85%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left,right
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         DSS
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN,DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ma2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_MaLiu_112.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Pandasea/DCASE2019/blob/master/task1a_1.py" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ma2019alabel" class="modal fade" id="bibtex-Ma2019a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMa2019alabel">
        Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ma2019a,
    Author = "Ma, Sifan and Liu, Wei",
    title = "Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with Neural Network",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report presents our approach for the acoustic scene classification of DCASE2019 task1a. Compared to traditional audio features such as Mel-frequency Cepstral Coefficients (MFCC) and Constant-Q Transform (CQT), we choose Deep Scattering Spectra (DSS) features which are more suitable for characterizing acoustic scenes. DSS is a good way to preserve high frequency information. Based on DSS features, we choose a network model of Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) to classify acoustic scenes. The experimental results show that our approach increase the classification accuracy from 62.5\% (DCASE2019 baseline) to 85\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mars2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Mars2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification From Binaural Signals Using Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Rohith Mars, Pranay Pratik, Srikanth Nagisetty and Chong Soon Lim
       </p>
<p style="text-align:left">
<em>
         Core Technology Group, Panasonic R&amp;D Center, Singapore, Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mars_PRDCSG_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mars2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mars2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mars2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Mars_84.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mars2019" class="panel-collapse collapse" id="collapse-Mars2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification From Binaural Signals Using Convolutional Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Rohith Mars, Pranay Pratik, Srikanth Nagisetty and Chong Soon Lim
       </small>
<br/>
<small>
<em>
         Core Technology Group, Panasonic R&amp;D Center, Singapore, Singapore
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present the technical details of our proposed framework and solution for the DCASE 2019 Task 1A - Acoustic Scene Classification challenge. We describe the audio pre-processing, feature extraction steps and the time-frequency (TF) representations used for acoustic scene classification using binaural audio recordings. We employ two distinct architectures of convolutional neural networks (CNNs) for processing the extracted audio features for classification and compare their relative performance in terms of both accuracy and model complexity. Using an ensemble of the predictions from multiple models based on the above CNNs, we achieved an average classification accuracy of 79.35% on the test split of the development dataset for this task and a system score of 82.33% in the Kaggle public leaderboard, which is an improvement of â‰ˆ 18% over the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, left, right, mid, side
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         max probability
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mars2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Mars_84.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mars2019label" class="modal fade" id="bibtex-Mars2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMars2019label">
        Acoustic Scene Classification From Binaural Signals Using Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mars2019,
    Author = "Mars, Rohith and Pratik, Pranay and Nagisetty, Srikanth and Lim, Chong Soon",
    title = "Acoustic Scene Classification From Binaural Signals Using Convolutional Neural Networks",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we present the technical details of our proposed framework and solution for the DCASE 2019 Task 1A - Acoustic Scene Classification challenge. We describe the audio pre-processing, feature extraction steps and the time-frequency (TF) representations used for acoustic scene classification using binaural audio recordings. We employ two distinct architectures of convolutional neural networks (CNNs) for processing the extracted audio features for classification and compare their relative performance in terms of both accuracy and model complexity. Using an ensemble of the predictions from multiple models based on the above CNNs, we achieved an average classification accuracy of 79.35\% on the test split of the development dataset for this task and a system score of 82.33\% in the Kaggle public leaderboard, which is an improvement of â‰ˆ 18\% over the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mingle2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Mingle2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The System for Acoustic Scene Classification Using Resnet
       </h4>
<p style="text-align:left">
        Liu Mingle and Li Yanxiong
       </p>
<p style="text-align:left">
<em>
         School of Electronic and Information Enginnering, South China University of Technology, GuangZhou, GuangDong Province
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_SCUT_task1a_1</span> <span class="label label-primary">Liu_SCUT_task1a_2</span> <span class="label label-primary">Liu_SCUT_task1a_3</span> <span class="label label-primary">Liu_SCUT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mingle2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mingle2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mingle2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_SCUT_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mingle2019" class="panel-collapse collapse" id="collapse-Mingle2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The System for Acoustic Scene Classification Using Resnet
      </h4>
<p style="text-align:left">
<small>
        Liu Mingle and Li Yanxiong
       </small>
<br/>
<small>
<em>
         School of Electronic and Information Enginnering, South China University of Technology, GuangZhou, GuangDong Province
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our works concerning task 1a of DCASE 2019, i.e. acoustic scene classification (ASC) with mismatched recording devices. We propose a strategy of classifiers voting for ASC. Specifically, an audio feature, such as logarithmic filter-bank (LFB), is first extracted from audio recordings. Then a series of convolutional neural network (CNN) is built for obtaining classifiers ensemble. Finally, classification result for each test sample is based on the voting of all classifiers.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono,binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ResNet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mingle2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_SCUT_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mingle2019label" class="modal fade" id="bibtex-Mingle2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMingle2019label">
        The System for Acoustic Scene Classification Using Resnet
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mingle2019,
    Author = "Mingle, Liu and Yanxiong, Li",
    title = "The System for Acoustic Scene Classification Using Resnet",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we present our works concerning task 1a of DCASE 2019, i.e. acoustic scene classification (ASC) with mismatched recording devices. We propose a strategy of classifiers voting for ASC. Specifically, an audio feature, such as logarithmic filter-bank (LFB), is first extracted from audio recordings. Then a series of convolutional neural network (CNN) is built for obtaining classifiers ensemble. Finally, classification result for each test sample is based on the voting of all classifiers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Naranjo-Alcazar2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Naranjo-Alcazar2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019: CNN Depth Analysis with Different Channel Inputs for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Javier Naranjo-Alcazar<sup>1</sup>, Sergi Perez-Castanos<sup>1</sup>, Pedro Zuccarello<sup>1</sup> and Maximo Cobos<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Visualfy AI, Visualfy, Benisano, Spain, <sup>2</sup>Computer Science, Universitat de Valencia, Burjassot, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Naranjo-Alcazar_VfyAI_task1a_1</span> <span class="label label-primary">Naranjo-Alcazar_VfyAI_task1a_2</span> <span class="label label-primary">Naranjo-Alcazar_VfyAI_task1a_3</span> <span class="label label-primary">Naranjo-Alcazar_VfyAI_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Naranjo-Alcazar2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Naranjo-Alcazar2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Naranjo-Alcazar2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Naranjo-Alcazar_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Naranjo-Alcazar2019" class="panel-collapse collapse" id="collapse-Naranjo-Alcazar2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019: CNN Depth Analysis with Different Channel Inputs for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Javier Naranjo-Alcazar<sup>1</sup>, Sergi Perez-Castanos<sup>1</sup>, Pedro Zuccarello<sup>1</sup> and Maximo Cobos<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Visualfy AI, Visualfy, Benisano, Spain, <sup>2</sup>Computer Science, Universitat de Valencia, Burjassot, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The objective of this technical report is to describe the framework used in Task 1, Acoustic scene classification (ASC), of the DCASE 2019 challenge. The presented approach is based on Log-Mel spectrogram representations and VGG-based Convolutional Neural Networks (CNNs). Three different CNNs, with very similar architectures, have been implemented. The main difference is the number of filters in their convolutional blocks. Experiments show that the depth of the network is not the most relevant factor for improving the accuracy of the results. The performance seems to be more sensitive to the input audio representation. This conclusion is important for the implementation of real-time audio recognition and classification system on edge devices. In the presented experiments the best audio representation is the Log-Mel spectrogram of the harmonic and percussive sources plus the Log-Mel spectrogram of the difference between left and right stereo-channels (L âˆ’ R). Also, in order to improve accuracy, ensemble methods combining different model predictions with different inputs are explored. Besides geometric and arithmetic means, ensembles aggregated with the Orness Weighted Averaged (OWA) operator have shown interesting and novel results. The proposed framework outperforms the baseline system by 14.34 percentage points. For Task 1a, the obtained development accuracy is 76.84%, being 62.5% the baseline, whereas the accuracy obtained in public leaderboard is 77.33%, being 64.33% the baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, left, right, difference, harmonic, percussive
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble, CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean; geometric mean; orness weighted average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Naranjo-Alcazar2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Naranjo-Alcazar_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Naranjo-Alcazar2019label" class="modal fade" id="bibtex-Naranjo-Alcazar2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNaranjo-Alcazar2019label">
        DCASE 2019: CNN Depth Analysis with Different Channel Inputs for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Naranjo-Alcazar2019,
    Author = "Naranjo-Alcazar, Javier and Perez-Castanos, Sergi and Zuccarello, Pedro and Cobos, Maximo",
    title = "{DCASE} 2019: {CNN} Depth Analysis with Different Channel Inputs for Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The objective of this technical report is to describe the framework used in Task 1, Acoustic scene classification (ASC), of the DCASE 2019 challenge. The presented approach is based on Log-Mel spectrogram representations and VGG-based Convolutional Neural Networks (CNNs). Three different CNNs, with very similar architectures, have been implemented. The main difference is the number of filters in their convolutional blocks. Experiments show that the depth of the network is not the most relevant factor for improving the accuracy of the results. The performance seems to be more sensitive to the input audio representation. This conclusion is important for the implementation of real-time audio recognition and classification system on edge devices. In the presented experiments the best audio representation is the Log-Mel spectrogram of the harmonic and percussive sources plus the Log-Mel spectrogram of the difference between left and right stereo-channels (L âˆ’ R). Also, in order to improve accuracy, ensemble methods combining different model predictions with different inputs are explored. Besides geometric and arithmetic means, ensembles aggregated with the Orness Weighted Averaged (OWA) operator have shown interesting and novel results. The proposed framework outperforms the baseline system by 14.34 percentage points. For Task 1a, the obtained development accuracy is 76.84\%, being 62.5\% the baseline, whereas the accuracy obtained in public leaderboard is 77.33\%, being 64.33\% the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Paseddula2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Paseddula2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019 Task 1a: Acoustic Scene Classification by Sffcc and DNN
       </h4>
<p style="text-align:left">
        Chandrasekhar Paseddula<sup>1</sup> and Suryakanth V.Gangashetty<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>International Institute of Information Technology, Hyderabad department:Electronics and Communication Engineering, Hyderabad, India, <sup>2</sup>Computor Science Engineering, International Institute of Information Technology, Hyderabad, Hyderabad, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chandrasekhar_IIITH_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Paseddula2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Paseddula2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Paseddula2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chandrasekhar_4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Paseddula2019" class="panel-collapse collapse" id="collapse-Paseddula2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019 Task 1a: Acoustic Scene Classification by Sffcc and DNN
      </h4>
<p style="text-align:left">
<small>
        Chandrasekhar Paseddula<sup>1</sup> and Suryakanth V.Gangashetty<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>International Institute of Information Technology, Hyderabad department:Electronics and Communication Engineering, Hyderabad, India, <sup>2</sup>Computor Science Engineering, International Institute of Information Technology, Hyderabad, Hyderabad, India
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this study, we dealt with the acoustic scene classification (ASC) task in the Detection and Classification of Acoustic Scenes and Events (DCASE)-2019 challenge Task 1A. Single frequency filtering cepstral coefficients (SFFCC) features and Deep Neural networks (DNN) model is proposed for ASC. We have adopted a late fusion mechanism to further improve the performance and finally, to validate the performance of the model and compare it to the baseline system. We used the TAU Urban Acoustic Scenes 2019 development dataset for training and cross-validation, resulting in a 7.9% improvement when compared to the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         single frequency cepstral coefficients (SFCC), log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maxrule
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Paseddula2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chandrasekhar_4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Paseddula2019label" class="modal fade" id="bibtex-Paseddula2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPaseddula2019label">
        DCASE 2019 Task 1a: Acoustic Scene Classification by Sffcc and DNN
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Paseddula2019,
    Author = "Paseddula, Chandrasekhar and V.Gangashetty, Suryakanth",
    title = "{DCASE} 2019 Task 1a: Acoustic Scene Classification by Sffcc and {DNN}",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this study, we dealt with the acoustic scene classification (ASC) task in the Detection and Classification of Acoustic Scenes and Events (DCASE)-2019 challenge Task 1A. Single frequency filtering cepstral coefficients (SFFCC) features and Deep Neural networks (DNN) model is proposed for ASC. We have adopted a late fusion mechanism to further improve the performance and finally, to validate the performance of the model and compare it to the baseline system. We used the TAU Urban Acoustic Scenes 2019 development dataset for training and cross-validation, resulting in a 7.9\% improvement when compared to the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pham2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Pham2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Cdnn-CRNN Joined Model for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Lam Pham<sup>1</sup>, Tan Doan<sup>2</sup>, Dat Thanh Ngo<sup>2</sup>, Hung Nguyen<sup>2</sup> and Ha Hoang Kha<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Computing, University of Kent, Chatham, United Kingdom, <sup>2</sup>Electrical and Electronics Engineering, HoChiMinh City University of Technology, HoChiMinh City, Vietnam
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">LamPham_HCMGroup_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Pham2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pham2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pham2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_HCM_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pham2019" class="panel-collapse collapse" id="collapse-Pham2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Cdnn-CRNN Joined Model for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Lam Pham<sup>1</sup>, Tan Doan<sup>2</sup>, Dat Thanh Ngo<sup>2</sup>, Hung Nguyen<sup>2</sup> and Ha Hoang Kha<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>School of Computing, University of Kent, Chatham, United Kingdom, <sup>2</sup>Electrical and Electronics Engineering, HoChiMinh City University of Technology, HoChiMinh City, Vietnam
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This work proposes a deep learning framework applied for Acoustic Scene Classification (ASC), targeting DCASE2019 task 1A. In general, the front-end process shows a combination of three types of spectrograms: Gammatone (GAM), log-Mel and Constant Q Transform (CQT). The back-end classification presents a joined learning model between CDNN and CRNN. Our experiments over the development dataset of DCASE2019 challenge task 1A show a significant improvement, increasing 11.2% compared to DCASE2019 baseline of 62.5%. The Kaggle reports the classification accuracy of 74.6% when we train all development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         Gammatone, log-mel energies, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, RNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pham2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_HCM_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pham2019label" class="modal fade" id="bibtex-Pham2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPham2019label">
        Cdnn-CRNN Joined Model for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Pham2019,
    Author = "Pham, Lam and Doan, Tan and Thanh Ngo, Dat and Nguyen, Hung and Hoang Kha, Ha",
    title = "Cdnn-{CRNN} Joined Model for Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This work proposes a deep learning framework applied for Acoustic Scene Classification (ASC), targeting DCASE2019 task 1A. In general, the front-end process shows a combination of three types of spectrograms: Gammatone (GAM), log-Mel and Constant Q Transform (CQT). The back-end classification presents a joined learning model between CDNN and CRNN. Our experiments over the development dataset of DCASE2019 challenge task 1A show a significant improvement, increasing 11.2\% compared to DCASE2019 baseline of 62.5\%. The Kaggle reports the classification accuracy of 74.6\% when we train all development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pham2019a" style="box-shadow: none">
<div class="panel-heading" id="heading-Pham2019a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Multi-Spectrogram Deep Neural Network for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Lam Pham, Ian McLoughlin, Huy Phan and Ramaswamy Palaniappan
       </p>
<p style="text-align:left">
<em>
         School of Computing, University of Kent, Chatham, United Kingdom
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">LamPham_KentGroup_task1a_1</span> <span class="clearfix"></span><span class="label label-info">LamPham_KentGroup_task1b_1</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Pham2019a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pham2019a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pham2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_LamPham_9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pham2019a" class="panel-collapse collapse" id="collapse-Pham2019a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Multi-Spectrogram Deep Neural Network for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Lam Pham, Ian McLoughlin, Huy Phan and Ramaswamy Palaniappan
       </small>
<br/>
<small>
<em>
         School of Computing, University of Kent, Chatham, United Kingdom
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This work targets the task 1A and 1B of DCASE2019 challenge that are Acoustic Scene Classification (ASC) over ten different classes recorded by a same device (task 1A) and mismatched devices (task 1B). For the front-end feature extraction, this work proposes a combination of three types of spectrograms: Gammatone (GAM), log- Mel and Constant Q Transform (CQT). The back-end classification shows two training processes, namely pre-trained CNN and post- trained DNN, and the result of post-trained DNN is reported. Our experiments over the development dataset of DCASE2019 1A and 1B show significant improvement, increasing 14% and 17.4 % compared to DCASE2019 baseline of 62.5% and 41.4%, respectively. The Kaggle report also confirms the classification accuracy of 79% and 69.2% for task 1A and 1B.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         Gammatone, log-mel energies, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pham2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_LamPham_9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pham2019alabel" class="modal fade" id="bibtex-Pham2019a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPham2019alabel">
        A Multi-Spectrogram Deep Neural Network for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Pham2019a,
    Author = "Pham, Lam and McLoughlin, Ian and Phan, Huy and Palaniappan, Ramaswamy",
    title = "A Multi-Spectrogram Deep Neural Network for Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This work targets the task 1A and 1B of DCASE2019 challenge that are Acoustic Scene Classification (ASC) over ten different classes recorded by a same device (task 1A) and mismatched devices (task 1B). For the front-end feature extraction, this work proposes a combination of three types of spectrograms: Gammatone (GAM), log- Mel and Constant Q Transform (CQT). The back-end classification shows two training processes, namely pre-trained CNN and post- trained DNN, and the result of post-trained DNN is reported. Our experiments over the development dataset of DCASE2019 1A and 1B show significant improvement, increasing 14\% and 17.4 \% compared to DCASE2019 baseline of 62.5\% and 41.4\%, respectively. The Kaggle report also confirms the classification accuracy of 79\% and 69.2\% for task 1A and 1B."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Plata2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Plata2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Deep Neural Networks with Supported Clusters Preclassification Procedure for Acoustic Scene Recognition
       </h4>
<p style="text-align:left">
        Marcin Plata
       </p>
<p style="text-align:left">
<em>
         Data Intelligence Group, Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Plata_SRPOL_task1a_1</span> <span class="label label-primary">Plata_SRPOL_task1a_2</span> <span class="label label-primary">Plata_SRPOL_task1a_3</span> <span class="label label-primary">Plata_SRPOL_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Plata2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Plata2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Plata2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Plata_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Plata2019" class="panel-collapse collapse" id="collapse-Plata2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Deep Neural Networks with Supported Clusters Preclassification Procedure for Acoustic Scene Recognition
      </h4>
<p style="text-align:left">
<small>
        Marcin Plata
       </small>
<br/>
<small>
<em>
         Data Intelligence Group, Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we presented a system for acoustic scene classification focuses on deeper analysis of data. We made an impact analysis of various combinations of arguments for short time Fourier transform (STFT) and Mel filter bank. We also used the harmonic and percussive source separation (HPSS) algorithm as an additional features extractor. Finally, next to common spectrograms divided and non-overlap classification neural networks, we decided to present an out-of-the-box solution with one main neural network trained on clustered labels and a few supporting neural network to distinguish between most difficult scenes, e.g. street pedestrian and public square.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, left, right
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, harmonic, percussive
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, random forest; CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         random forest; majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Plata2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Plata_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Plata2019label" class="modal fade" id="bibtex-Plata2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPlata2019label">
        Deep Neural Networks with Supported Clusters Preclassification Procedure for Acoustic Scene Recognition
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Plata2019,
    Author = "Plata, Marcin",
    title = "Deep Neural Networks with Supported Clusters Preclassification Procedure for Acoustic Scene Recognition",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we presented a system for acoustic scene classification focuses on deeper analysis of data. We made an impact analysis of various combinations of arguments for short time Fourier transform (STFT) and Mel filter bank. We also used the harmonic and percussive source separation (HPSS) algorithm as an additional features extractor. Finally, next to common spectrograms divided and non-overlap classification neural networks, we decided to present an out-of-the-box solution with one main neural network trained on clustered labels and a few supporting neural network to distinguish between most difficult scenes, e.g. street pedestrian and public square."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Primus2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Primus2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification with Mismatched Recording Devices
       </h4>
<p style="text-align:left">
        Paul Primus and David Eitelsebner
       </p>
<p style="text-align:left">
<em>
         Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Primus_CPJKU_task1b_1</span> <span class="label label-info">Primus_CPJKU_task1b_2</span> <span class="label label-info">Primus_CPJKU_task1b_3</span> <span class="label label-info">Primus_CPJKU_task1b_4</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Primus2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Primus2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Primus2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Primus_75.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Primus2019').collapse('show');window.location.hash='#Primus2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Primus2019" class="panel-collapse collapse" id="collapse-Primus2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification with Mismatched Recording Devices
      </h4>
<p style="text-align:left">
<small>
        Paul Primus and David Eitelsebner
       </small>
<br/>
<small>
<em>
         Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes CP-JKU Student teamâ€™s approach for Task 1 - Subtask B of the DCASE 2019 challenge. In this context, we propose two loss functions for domain adaptation to learn invariant representations given time-aligned recordings. We show that these methods improve the classification performance on our cross-validation, as well as performance on the Kaggle leader board, up to three percentage points compared to our baseline model. Our best scoring submission is an ensemble of eight classifiers.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Primus2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Primus_75.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/OptimusPrimus/dcase2019_task1b" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Primus2019label" class="modal fade" id="bibtex-Primus2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPrimus2019label">
        Acoustic Scene Classification with Mismatched Recording Devices
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Primus2019,
    Author = "Primus, Paul and Eitelsebner, David",
    title = "Acoustic Scene Classification with Mismatched Recording Devices",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes CP-JKU Student teamâ€™s approach for Task 1 - Subtask B of the DCASE 2019 challenge. In this context, we propose two loss functions for domain adaptation to learn invariant representations given time-aligned recordings. We show that these methods improve the classification performance on our cross-validation, as well as performance on the Kaggle leader board, up to three percentage points compared to our baseline model. Our best scoring submission is an ensemble of eight classifiers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Rakowski2019_t1" style="box-shadow: none">
<div class="panel-heading" id="heading-Rakowski2019_t1" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Frequency-Aware CNN for Open Set Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Alexander Rakowski<sup>1</sup> and MichaÅ‚ KoÅ›mider<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Audio Intelligence, Samsung R&amp;D Institute Poland, Warsaw, Poland, <sup>2</sup>Artificial Intelligence, Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="clearfix"></span><span class="label label-warning">Rakowski_SRPOL_task1c_1</span> <span class="label label-warning">Rakowski_SRPOL_task1c_2</span> <span class="label label-warning">Rakowski_SRPOL_task1c_3</span> <span class="label label-warning">Rakowski_SRPOL_task1c_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Rakowski2019_t1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Rakowski2019_t1" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Rakowski2019_t1" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Rakowski_58_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Rakowski2019_t1" class="panel-collapse collapse" id="collapse-Rakowski2019_t1" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Frequency-Aware CNN for Open Set Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Alexander Rakowski<sup>1</sup> and MichaÅ‚ KoÅ›mider<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Audio Intelligence, Samsung R&amp;D Institute Poland, Warsaw, Poland, <sup>2</sup>Artificial Intelligence, Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes systems used for Task 1c of the DCASE 2019 Challenge - Open Set Acoustic Scene Classification. The main system consists of a 5-layer convolutional neural network which preserves the location of features on the frequency axis. This is in contrast to the standard approach where global pooling is applied along the frequency-related dimension. Additionally the main system is combined with an ensemble of calibrated neural networks in order to improve generalization.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         soft-voting
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Rakowski2019_t1" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Rakowski_58_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Rakowski2019_t1label" class="modal fade" id="bibtex-Rakowski2019_t1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRakowski2019_t1label">
        Frequency-Aware CNN for Open Set Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Rakowski2019_t1,
    Author = "Rakowski, Alexander and KoÅ›mider, MichaÅ‚",
    title = "Frequency-Aware {CNN} for Open Set Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describes systems used for Task 1c of the DCASE 2019 Challenge - Open Set Acoustic Scene Classification. The main system consists of a 5-layer convolutional neural network which preserves the location of features on the frequency axis. This is in contrast to the standard approach where global pooling is applied along the frequency-related dimension. Additionally the main system is combined with an ensemble of calibrated neural networks in order to improve generalization."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Salvati2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Salvati2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Urban Acoustic Scene Classification Using Raw Waveform Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Daniele Salvati, Carlo Drioli and Gian Luca Foresti
       </p>
<p style="text-align:left">
<em>
         Mathematics, Computer Science and Physics, University of Udine, Udine, Italy
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Salvati_DMIF_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Salvati2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Salvati2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Salvati2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Salvati_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Salvati2019" class="panel-collapse collapse" id="collapse-Salvati2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Urban Acoustic Scene Classification Using Raw Waveform Convolutional Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Daniele Salvati, Carlo Drioli and Gian Luca Foresti
       </small>
<br/>
<small>
<em>
         Mathematics, Computer Science and Physics, University of Udine, Udine, Italy
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We present the signal processing framework and the results obtained with the development dataset (task 1, subtask A) for the detection and classification of acoustic scenes and events (DCASE 2019) challenge. The framework for the classification of urban acoustic scenes consists of a raw waveform (RW) end-to-end computational scheme based on convolutional neural networks (CNNs). The RW-CNN operates on a time-domain signal segment of 0.5 s and consists of 5 one-dimensional convolutional layers and 3 fully connected layers. The overall classification accuracy with the development dataset of the proposed RW-CNN is 69.7 %.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         raw waveform
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Salvati2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Salvati_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Salvati2019label" class="modal fade" id="bibtex-Salvati2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSalvati2019label">
        Urban Acoustic Scene Classification Using Raw Waveform Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Salvati2019,
    Author = "Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca",
    title = "Urban Acoustic Scene Classification Using Raw Waveform Convolutional Neural Networks",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "We present the signal processing framework and the results obtained with the development dataset (task 1, subtask A) for the detection and classification of acoustic scenes and events (DCASE 2019) challenge. The framework for the classification of urban acoustic scenes consists of a raw waveform (RW) end-to-end computational scheme based on convolutional neural networks (CNNs). The RW-CNN operates on a time-domain signal segment of 0.5 s and consists of 5 one-dimensional convolutional layers and 3 fully connected layers. The overall classification accuracy with the development dataset of the proposed RW-CNN is 69.7 \%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sangwon2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Sangwon2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Specaugment and Convolutional Neural Network with Inception Modules
       </h4>
<p style="text-align:left">
        Suh Sangwon, Jeong Youngho, Lim Wootaek and Park Sooyoung
       </p>
<p style="text-align:left">
<em>
         Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">SSW_ETRI_task1a_1</span> <span class="label label-primary">SSW_ETRI_task1a_2</span> <span class="label label-primary">SSW_ETRI_task1a_3</span> <span class="label label-primary">SSW_ETRI_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sangwon2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sangwon2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sangwon2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_SSW_86.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sangwon2019" class="panel-collapse collapse" id="collapse-Sangwon2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Specaugment and Convolutional Neural Network with Inception Modules
      </h4>
<p style="text-align:left">
<small>
        Suh Sangwon, Jeong Youngho, Lim Wootaek and Park Sooyoung
       </small>
<br/>
<small>
<em>
         Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes the system submitted to the Task 1a (Acoustic Scene Classification, ASC). By analyzing the major systems submitted in 2017 and 2018, we have selected a two-dimensional convolutional neural network (CNN) as the most suitable model for this task. The proposed model is composed of four convolution blocks; two of them are conventional CNN structures but the following two blocks consist of Inception modules. We have constructed a meta-learning problem with this model in order to train the super learner. For each base model training, we have applied different validation split methods to take advantage in generalized result with the ensemble method. In addition, we have applied data augmentation in real time with SpecAugment, which was performed for each base model. With our final system with all of the above techniques have applied, we have achieved an accuracy of 76.1% with the development dataset and 81.3% with the leader board set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; ensemble
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sangwon2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_SSW_86.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sangwon2019label" class="modal fade" id="bibtex-Sangwon2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSangwon2019label">
        Acoustic Scene Classification Using Specaugment and Convolutional Neural Network with Inception Modules
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sangwon2019,
    Author = "Sangwon, Suh and Youngho, Jeong and Wootaek, Lim and Sooyoung, Park",
    title = "Acoustic Scene Classification Using Specaugment and Convolutional Neural Network with Inception Modules",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper describes the system submitted to the Task 1a (Acoustic Scene Classification, ASC). By analyzing the major systems submitted in 2017 and 2018, we have selected a two-dimensional convolutional neural network (CNN) as the most suitable model for this task. The proposed model is composed of four convolution blocks; two of them are conventional CNN structures but the following two blocks consist of Inception modules. We have constructed a meta-learning problem with this model in order to train the super learner. For each base model training, we have applied different validation split methods to take advantage in generalized result with the ensemble method. In addition, we have applied data augmentation in real time with SpecAugment, which was performed for each base model. With our final system with all of the above techniques have applied, we have achieved an accuracy of 76.1\% with the development dataset and 81.3\% with the leader board set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Song2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Song2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Feature Enhancement for Robust Acoustic Scene Classification with Device Mismatch
       </h4>
<p style="text-align:left">
        Hongwei Song and Hao Yang
       </p>
<p style="text-align:left">
<em>
         Computer Sciences and Technology, Harbin Institute of Technology, Harbin, China
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Song_HIT_task1b_1</span> <span class="label label-info">Song_HIT_task1b_2</span> <span class="label label-info">Song_HIT_task1b_3</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Song2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Song2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Song2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Song_114.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Song2019').collapse('show');window.location.hash='#Song2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Song2019" class="panel-collapse collapse" id="collapse-Song2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Feature Enhancement for Robust Acoustic Scene Classification with Device Mismatch
      </h4>
<p style="text-align:left">
<small>
        Hongwei Song and Hao Yang
       </small>
<br/>
<small>
<em>
         Computer Sciences and Technology, Harbin Institute of Technology, Harbin, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our system for DCASE2019 Task1 SubtaskB. We focus on analyzing how device distortions affect the classic log Mel feature, which is the most adopted feature for convolutional neural networks (CNN) based models. We demonstrate mathematically that for log Mel feature, the influence of device distortion shows as an additive constant vector over the log Mel spectrogram. Based on this analysis, we propose to use feature enhancement methods such as spectrogram-wise mean subtraction and median filtering, to remove the additive term of channel distortions. Information loss introduced by the enhancement methods is discussed. We also motivate to use mixup technique to generate virtual samples with various device distortions. Combining the proposed techniques, we rank the second on the public kaggle leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         probability aggregation; majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Song2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Song_114.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/hackerekcah/dcase19_task1_hitsplab" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Song2019label" class="modal fade" id="bibtex-Song2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSong2019label">
        Feature Enhancement for Robust Acoustic Scene Classification with Device Mismatch
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Song2019,
    Author = "Song, Hongwei and Yang, Hao",
    title = "Feature Enhancement for Robust Acoustic Scene Classification with Device Mismatch",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes our system for DCASE2019 Task1 SubtaskB. We focus on analyzing how device distortions affect the classic log Mel feature, which is the most adopted feature for convolutional neural networks (CNN) based models. We demonstrate mathematically that for log Mel feature, the influence of device distortion shows as an additive constant vector over the log Mel spectrogram. Based on this analysis, we propose to use feature enhancement methods such as spectrogram-wise mean subtraction and median filtering, to remove the additive term of channel distortions. Information loss introduced by the enhancement methods is discussed. We also motivate to use mixup technique to generate virtual samples with various device distortions. Combining the proposed techniques, we rank the second on the public kaggle leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Waldekar2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Waldekar2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Wavelet Based Mel-Scaled Features for DCASE 2019 Task 1a and Task 1b
       </h4>
<p style="text-align:left">
        Shefali Waldekar and Goutam Saha
       </p>
<p style="text-align:left">
<em>
         Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Waldekar_IITKGP_task1a_1</span> <span class="clearfix"></span><span class="label label-info">Waldekar_IITKGP_task1b_1</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Waldekar2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Waldekar2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Waldekar2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Waldekar_111.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Waldekar2019" class="panel-collapse collapse" id="collapse-Waldekar2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Wavelet Based Mel-Scaled Features for DCASE 2019 Task 1a and Task 1b
      </h4>
<p style="text-align:left">
<small>
        Shefali Waldekar and Goutam Saha
       </small>
<br/>
<small>
<em>
         Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes a submission for IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 for Task 1 (acoustic scene classification (ASC)), sub-task A (basic ASC) and sub-task B (ASC with mismatched recording devices). The system exploits time-frequency representation of audio to obtain the scene labels. It follows a simple pattern classification framework employing wavelet transform based mel-scaled features along with support vector machine as classifier. The proposed system relatively outperforms the deep-learning based baseline system by almost 8% for sub-task A and 26% for sub-task B on the development dataset provided for the respective sub-tasks.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFDWC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Waldekar2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Waldekar_111.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Waldekar2019label" class="modal fade" id="bibtex-Waldekar2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWaldekar2019label">
        Wavelet Based Mel-Scaled Features for DCASE 2019 Task 1a and Task 1b
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Waldekar2019,
    Author = "Waldekar, Shefali and Saha, Goutam",
    title = "Wavelet Based Mel-Scaled Features for {DCASE} 2019 Task 1a and Task 1b",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describes a submission for IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 for Task 1 (acoustic scene classification (ASC)), sub-task A (basic ASC) and sub-task B (ASC with mismatched recording devices). The system exploits time-frequency representation of audio to obtain the scene labels. It follows a simple pattern classification framework employing wavelet transform based mel-scaled features along with support vector machine as classifier. The proposed system relatively outperforms the deep-learning based baseline system by almost 8\% for sub-task A and 26\% for sub-task B on the development dataset provided for the respective sub-tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on CNN System
       </h4>
<p style="text-align:left">
        Zhuhe Wang, Jingkai Ma and Chunyang Li
       </p>
<p style="text-align:left">
<em>
         Noise and Vibration Laboratory, Beijing Technology and Business University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_BTBU_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wang_2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2019" class="panel-collapse collapse" id="collapse-Wang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on CNN System
      </h4>
<p style="text-align:left">
<small>
        Zhuhe Wang, Jingkai Ma and Chunyang Li
       </small>
<br/>
<small>
<em>
         Noise and Vibration Laboratory, Beijing Technology and Business University, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this study, we present a solution for the acoustic scene classification task1A in the DCASE 2019 Challenge. Our model uses a convolutional neural network and makes some improvements on the basis of CNN. Then we extract the MFCC (Mel frequency cepstral coefficient) feature from the official audio file and recreate the data set. Use this as an input to the neural network. Finally, comparing our model to the performance of the baseline system, the results were 12% more accurate than the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         one
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wang_2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2019label" class="modal fade" id="bibtex-Wang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2019label">
        Acoustic Scene Classification Based on CNN System
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2019,
    Author = "Wang, Zhuhe and Ma, Jingkai and Li, Chunyang",
    title = "Acoustic Scene Classification Based on {CNN} System",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this study, we present a solution for the acoustic scene classification task1A in the DCASE 2019 Challenge. Our model uses a convolutional neural network and makes some improvements on the basis of CNN. Then we extract the MFCC (Mel frequency cepstral coefficient) feature from the official audio file and recreate the data set. Use this as an input to the neural network. Finally, comparing our model to the performance of the baseline system, the results were 12\% more accurate than the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2019a_t1" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2019a_t1" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Ciaic-ASC System for DCASE 2019 Challenge Task1
       </h4>
<p style="text-align:left">
        Mou Wang and Rui Wang
       </p>
<p style="text-align:left">
<em>
         School of Marine Sciences and Technology, Northwestern Polytechnical University, Xi'an, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_NWPU_task1a_1</span> <span class="label label-primary">Wang_NWPU_task1a_2</span> <span class="label label-primary">Wang_NWPU_task1a_3</span> <span class="label label-primary">Wang_NWPU_task1a_4</span> <span class="clearfix"></span><span class="label label-info">Wang_NWPU_task1b_1</span> <span class="label label-info">Wang_NWPU_task1b_2</span> <span class="label label-info">Wang_NWPU_task1b_3</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2019a_t1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2019a_t1" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2019a_t1" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Mou_41_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2019a_t1" class="panel-collapse collapse" id="collapse-Wang2019a_t1" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Ciaic-ASC System for DCASE 2019 Challenge Task1
      </h4>
<p style="text-align:left">
<small>
        Mou Wang and Rui Wang
       </small>
<br/>
<small>
<em>
         School of Marine Sciences and Technology, Northwestern Polytechnical University, Xi'an, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our systems for subtask A and subtask B of the DCASE 2019 Task1, i.e. acoustic scene classification. The subtask A is a problem of basic closed set classification with data from a single device. In our system, we firstly extracted several acoustic features such as mel-spectrogram, hybrid constant-Q transform, harmonic-percussive source separation and etc.. Convolution neural networks (CNN) with average pooling are used to classify acoustic scenes. We averaged the outputs of CNN fed by different features to ensemble those methods. The subtask B is a classification problem with mismatched devices. So, we introduce a Domain Adaptation Neural Network (DANN) to extract the feature, which is uncorrelated with domain. We further ensemble DANN with CNN methods to obtain a better performance. The accuracy of the our system for subtask A is 0.783 on validation dataset and 0.816 on leaderboard dataset. The accuracy of subtask B achieves 0:717 on leaderborad dataset, which shows that our method can solve such a cross-domain problem and outperforms baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; CNN, DNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2019a_t1" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Mou_41_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2019a_t1label" class="modal fade" id="bibtex-Wang2019a_t1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2019a_t1label">
        Ciaic-ASC System for DCASE 2019 Challenge Task1
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2019a_t1,
    Author = "Wang, Mou and Wang, Rui",
    title = "Ciaic-{ASC} System for {DCASE} 2019 Challenge Task1",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we present our systems for subtask A and subtask B of the DCASE 2019 Task1, i.e. acoustic scene classification. The subtask A is a problem of basic closed set classification with data from a single device. In our system, we firstly extracted several acoustic features such as mel-spectrogram, hybrid constant-Q transform, harmonic-percussive source separation and etc.. Convolution neural networks (CNN) with average pooling are used to classify acoustic scenes. We averaged the outputs of CNN fed by different features to ensemble those methods. The subtask B is a classification problem with mismatched devices. So, we introduce a Domain Adaptation Neural Network (DANN) to extract the feature, which is uncorrelated with domain. We further ensemble DANN with CNN methods to obtain a better performance. The accuracy of the our system for subtask A is 0.783 on validation dataset and 0.816 on leaderboard dataset. The accuracy of subtask B achieves 0:717 on leaderborad dataset, which shows that our method can solve such a cross-domain problem and outperforms baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2019b" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2019b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The SEIE-SCUT Systems for Acoustic Scene Classification Using CNN Ensemble
       </h4>
<p style="text-align:left">
        Wucheng Wang and Mingle Liu
       </p>
<p style="text-align:left">
<em>
         School of Electronic and Information Enginnering, South China University of Technology, GuangZhou, GuangDong Province
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_SCUT_task1a_1</span> <span class="label label-primary">Wang_SCUT_task1a_2</span> <span class="label label-primary">Wang_SCUT_task1a_3</span> <span class="label label-primary">Wang_SCUT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2019b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2019b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2019b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wang_7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2019b" class="panel-collapse collapse" id="collapse-Wang2019b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The SEIE-SCUT Systems for Acoustic Scene Classification Using CNN Ensemble
      </h4>
<p style="text-align:left">
<small>
        Wucheng Wang and Mingle Liu
       </small>
<br/>
<small>
<em>
         School of Electronic and Information Enginnering, South China University of Technology, GuangZhou, GuangDong Province
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our works concerning task 1b of DCASE 2019, i.e. acoustic scene classification (ASC) with mismatched recording devices. We propose a strategy of CNN ensemble for ASC. Specifically, an audio feature, such as Mel-frequency cepstral coefficients (MFCCs) and logarithmic filter-bank (LFB), is first extracted from audio recordings. Then a series of convolutional neural network (CNN) is built for obtaining CNN ensemble. Finally, classification result for each test sample is based on the voting of all CNNS contained in the CNN ensemble.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono,binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies,MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         VGG,Inception,ResNet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2019b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wang_7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2019blabel" class="modal fade" id="bibtex-Wang2019b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2019blabel">
        The SEIE-SCUT Systems for Acoustic Scene Classification Using CNN Ensemble
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2019b,
    Author = "Wang, Wucheng and Liu, Mingle",
    title = "The {SEIE-SCUT} Systems for Acoustic Scene Classification Using {CNN} Ensemble",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we present our works concerning task 1b of DCASE 2019, i.e. acoustic scene classification (ASC) with mismatched recording devices. We propose a strategy of CNN ensemble for ASC. Specifically, an audio feature, such as Mel-frequency cepstral coefficients (MFCCs) and logarithmic filter-bank (LFB), is first extracted from audio recordings. Then a series of convolutional neural network (CNN) is built for obtaining CNN ensemble. Finally, classification result for each test sample is based on the voting of all CNNS contained in the CNN ensemble."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wilkinghoff2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Wilkinghoff2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders
       </h4>
<p style="text-align:left">
        Kevin Wilkinghoff and Frank Kurth
       </p>
<p style="text-align:left">
<em>
         Communication Systems, Fraunhofer Institute for Communication, Information Processing and Ergonomics, Wachtberg, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wilkinghoff_FKIE_task1a_1</span> <span class="label label-primary">Wilkinghoff_FKIE_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="label label-warning">Wilkinghoff_FKIE_task1c_1</span> <span class="label label-warning">Wilkinghoff_FKIE_task1c_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wilkinghoff2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wilkinghoff2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wilkinghoff2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wilkinghoff_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Wilkinghoff2019').collapse('show');window.location.hash='#Wilkinghoff2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wilkinghoff2019" class="panel-collapse collapse" id="collapse-Wilkinghoff2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders
      </h4>
<p style="text-align:left">
<small>
        Kevin Wilkinghoff and Frank Kurth
       </small>
<br/>
<small>
<em>
         Communication Systems, Fraunhofer Institute for Communication, Information Processing and Ergonomics, Wachtberg, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic scene classification is the task of determining the environment in which a given audio file has been recorded. If it is a priori not known whether all possible environments that may be encountered during test time are also known when training the system, the task is referred to as open-set classification. This paper contains a description of an open-set acoustic scene classification system submitted to Task 1C of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. Our system consists of a combination of convolutional neural networks for closed-set identification and deep convolutional autoencoders for outlier detection. In evaluations conducted on the leaderboard dataset of the challenge, the proposed system significantly outperforms the baseline systems and improves the score by 35.4% from 0.46666 to 0.63166.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, cutout, width shift, height shift
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies; log-mel energies, harmonic part, percussive part
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; CNN, ensemble; CNN, DCAE, logistic regression; CNN, DCAE, logistic regression, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood; geometric mean, maximum likelihood; threshold, maximum likelihood; geometric mean, threshold, maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wilkinghoff2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wilkinghoff_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/wilkinghoff/dcase2019" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wilkinghoff2019label" class="modal fade" id="bibtex-Wilkinghoff2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWilkinghoff2019label">
        Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wilkinghoff2019,
    Author = "Wilkinghoff, Kevin and Kurth, Frank",
    title = "Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Acoustic scene classification is the task of determining the environment in which a given audio file has been recorded. If it is a priori not known whether all possible environments that may be encountered during test time are also known when training the system, the task is referred to as open-set classification. This paper contains a description of an open-set acoustic scene classification system submitted to Task 1C of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. Our system consists of a combination of convolutional neural networks for closed-set identification and deep convolutional autoencoders for outlier detection. In evaluations conducted on the leaderboard dataset of the challenge, the proposed system significantly outperforms the baseline systems and improves the score by 35.4\% from 0.46666 to 0.63166."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wu2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Wu2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Stratified Time-Frequency Features for CNN-Based Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Yuzhong Wu and Tan Lee
       </p>
<p style="text-align:left">
<em>
         Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wu_CUHK_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wu2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wu2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wu_24.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Wu2019').collapse('show');window.location.hash='#Wu2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wu2019" class="panel-collapse collapse" id="collapse-Wu2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Stratified Time-Frequency Features for CNN-Based Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Yuzhong Wu and Tan Lee
       </small>
<br/>
<small>
<em>
         Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic scene signal is a mixture of diverse sound events, which are frequently overlapped with each other. The CNN models for acoustic scene classification usually suffer from model overfitting because they might memorize the overlapped sounds as the representative patterns for acoustic scenes, and might fail to recognize the scene when only one of the sound is present. Based on a standard CNN setup with log-Mel feature as input, we propose to stratify the log-Mel image to several component images based on sound duration, and each component image should contain a specific type of time-frequency patterns. Then we emphasize the independent modeling of time-frequency patterns to better utilize the stratified features. The experiment results on TAU Urban Acoustic Scenes 2019 development dataset [1] show that the use of stratified feature can significantly improve the classification performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wu_24.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/yzwu2017/DCASE2019_task1a" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wu2019label" class="modal fade" id="bibtex-Wu2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWu2019label">
        Stratified Time-Frequency Features for CNN-Based Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wu2019,
    Author = "Wu, Yuzhong and Lee, Tan",
    title = "Stratified Time-Frequency Features for {CNN}-Based Acoustic Scene Classification",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Acoustic scene signal is a mixture of diverse sound events, which are frequently overlapped with each other. The CNN models for acoustic scene classification usually suffer from model overfitting because they might memorize the overlapped sounds as the representative patterns for acoustic scenes, and might fail to recognize the scene when only one of the sound is present. Based on a standard CNN setup with log-Mel feature as input, we propose to stratify the log-Mel image to several component images based on sound duration, and each component image should contain a specific type of time-frequency patterns. Then we emphasize the independent modeling of time-frequency patterns to better utilize the stratified features. The experiment results on TAU Urban Acoustic Scenes 2019 development dataset [1] show that the use of stratified feature can significantly improve the classification performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zeinali2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Zeinali2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Fusion of Attentive Convolutional Neural Networks for Dcase2019 Challenge
       </h4>
<p style="text-align:left">
        Hossein Zeinali, Lukas Burget and Honza Cernocky
       </p>
<p style="text-align:left">
<em>
         Information Technology, Brno University of Technology, Brno, Czech Republic
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zeinali_BUT_task1a_1</span> <span class="label label-primary">Zeinali_BUT_task1a_2</span> <span class="label label-primary">Zeinali_BUT_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zeinali2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zeinali2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zeinali2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zeinali_102.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zeinali2019" class="panel-collapse collapse" id="collapse-Zeinali2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Fusion of Attentive Convolutional Neural Networks for Dcase2019 Challenge
      </h4>
<p style="text-align:left">
<small>
        Hossein Zeinali, Lukas Burget and Honza Cernocky
       </small>
<br/>
<small>
<em>
         Information Technology, Brno University of Technology, Brno, Czech Republic
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, the Brno University of Technology (BUT) team submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge are described. Also, the analysis of different methods is provided. The proposed approach is a fusion of three different Convolutional Neural Network (CNN) topologies. The first one is a VGG like two-dimensional CNNs. The second one is again a two-dimensional CNN network which uses MaxFeature-Map activation and called Light-CNN (LCNN). The third network is a one-dimensional CNN which mainly used for speaker verification and called x-vector topology. All proposed networks use self-attention mechanism for statistic pooling. As a feature, we use a 256-dimensional log Mel-spectrogram. Our submissions are a fusion of several networks trained on 4-folds generated evaluation setup using different fusion strategies.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         score fusion; majority vote; majority vote, score fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zeinali2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zeinali_102.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zeinali2019label" class="modal fade" id="bibtex-Zeinali2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZeinali2019label">
        Acoustic Scene Classification Using Fusion of Attentive Convolutional Neural Networks for Dcase2019 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zeinali2019,
    Author = "Zeinali, Hossein and Burget, Lukas and Cernocky, Honza",
    title = "Acoustic Scene Classification Using Fusion of Attentive Convolutional Neural Networks for Dcase2019 Challenge",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, the Brno University of Technology (BUT) team submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge are described. Also, the analysis of different methods is provided. The proposed approach is a fusion of three different Convolutional Neural Network (CNN) topologies. The first one is a VGG like two-dimensional CNNs. The second one is again a two-dimensional CNN network which uses MaxFeature-Map activation and called Light-CNN (LCNN). The third network is a one-dimensional CNN which mainly used for speaker verification and called x-vector topology. All proposed networks use self-attention mechanism for statistic pooling. As a feature, we use a 256-dimensional log Mel-spectrogram. Our submissions are a fusion of several networks trained on 4-folds generated evaluation setup using different fusion strategies."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zheng2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Zheng2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Combining Log-Mel CNN Model and End-To-End Model
       </h4>
<p style="text-align:left">
        Xu Zheng and Jie Yan
       </p>
<p style="text-align:left">
<em>
         Computing Sciences, University of Science of Techonology of China, Hefei,Anhui,China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zheng_USTC_task1a_1</span> <span class="label label-primary">Zheng_USTC_task1a_2</span> <span class="label label-primary">Zheng_USTC_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zheng2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zheng2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zheng2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zheng_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zheng2019" class="panel-collapse collapse" id="collapse-Zheng2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Combining Log-Mel CNN Model and End-To-End Model
      </h4>
<p style="text-align:left">
<small>
        Xu Zheng and Jie Yan
       </small>
<br/>
<small>
<em>
         Computing Sciences, University of Science of Techonology of China, Hefei,Anhui,China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the Zheng-USTC teamâ€™s submissions for Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge. In this paper, two different models for Acoustic Scene Classification are provided.The first one is a common two-dimensional CNN model in which the log-mel energies spectrogram is treated as an image. The second one is an end-to-end model, in which the features of a speech are extracted by a 3-layer CNN model with 64 filters. The experimental results on the fold1 validation set of 4185 samples and the leaderboard showed that the class-wise accuracy of the two models are complementary in some way. Finally we fused the softmax ouput scores of the two different systems by using a simple non-weighted average.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural; mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz; 16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment, RandomCrop; Between-Class learning; SpecAugment, RandomCrop, Between-Class learning
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies; raw waveform
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zheng2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zheng_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zheng2019label" class="modal fade" id="bibtex-Zheng2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZheng2019label">
        Acoustic Scene Classification Combining Log-Mel CNN Model and End-To-End Model
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zheng2019,
    Author = "Zheng, Xu and Yan, Jie",
    title = "Acoustic Scene Classification Combining Log-Mel {CNN} Model and End-To-End Model",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes the Zheng-USTC teamâ€™s submissions for Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge. In this paper, two different models for Acoustic Scene Classification are provided.The first one is a common two-dimensional CNN model in which the log-mel energies spectrogram is treated as an image. The second one is an end-to-end model, in which the features of a speech are extracted by a 3-layer CNN model with 64 filters. The experimental results on the fold1 validation set of 4185 samples and the leaderboard showed that the class-wise accuracy of the two models are complementary in some way. Finally we fused the softmax ouput scores of the two different systems by using a simple non-weighted average."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhou2019_t1" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhou2019_t1" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Audio Scene Calssification Based on Deeper CNN and Mixed Mono Channel Feature
       </h4>
<p style="text-align:left">
        Nai Zhou, Yanfang Liu and Qingkai Wei
       </p>
<p style="text-align:left">
<em>
         Beijing Kuaiyu Electronics Co., Ltd., Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhou_Kuaiyu_task1a_1</span> <span class="label label-primary">Zhou_Kuaiyu_task1a_2</span> <span class="label label-primary">Zhou_Kuaiyu_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhou2019_t1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhou2019_t1" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhou2019_t1" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhou_38_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhou2019_t1" class="panel-collapse collapse" id="collapse-Zhou2019_t1" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Audio Scene Calssification Based on Deeper CNN and Mixed Mono Channel Feature
      </h4>
<p style="text-align:left">
<small>
        Nai Zhou, Yanfang Liu and Qingkai Wei
       </small>
<br/>
<small>
<em>
         Beijing Kuaiyu Electronics Co., Ltd., Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes Kuaiyu teamâ€™s submissions for Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge. Refering the results of DCASE 2018, a convolution neural network and log-mel spectrogram generated from mono audio are used, log-mel specture is converted into multiple channels spectrogram and as a input to 8 convolutional layer neural networks. The result of our experiments is a classification system that achieves classification accuracies of around 75.5% on the public Kaggle-Leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhou2019_t1" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhou_38_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhou2019_t1label" class="modal fade" id="bibtex-Zhou2019_t1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhou2019_t1label">
        Audio Scene Calssification Based on Deeper CNN and Mixed Mono Channel Feature
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhou2019_t1,
    Author = "Zhou, Nai and Liu, Yanfang and Wei, Qingkai",
    title = "Audio Scene Calssification Based on Deeper {CNN} and Mixed Mono Channel Feature",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes Kuaiyu teamâ€™s submissions for Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge. Refering the results of DCASE 2018, a convolution neural network and log-mel spectrogram generated from mono audio are used, log-mel specture is converted into multiple channels spectrogram and as a input to 8 convolutional layer neural networks. The result of our experiments is a classification system that achieves classification accuracies of around 75.5\% on the public Kaggle-Leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhu2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhu2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019 Challenge Task1 Technical Report
       </h4>
<p style="text-align:left">
        Houwei Zhu<sup>1</sup>, Chunxia Ren<sup>2</sup>, Jun Wang<sup>2</sup>, Shengchen Li<sup>2</sup>, Lizhong Wang<sup>1</sup> and Lei Yang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Speech Lab, Samsung Research China-Beijing, Beijing, China, <sup>2</sup>Institute of Information Photonics and Optical Communication, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="clearfix"></span><span class="label label-warning">Zhu_SRCBBUPT_task1c_1</span> <span class="label label-warning">Zhu_SRCBBUPT_task1c_2</span> <span class="label label-warning">Zhu_SRCBBUPT_task1c_3</span> <span class="label label-warning">Zhu_SRCBBUPT_task1c_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhu2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhu2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhu_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhu2019" class="panel-collapse collapse" id="collapse-Zhu2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019 Challenge Task1 Technical Report
      </h4>
<p style="text-align:left">
<small>
        Houwei Zhu<sup>1</sup>, Chunxia Ren<sup>2</sup>, Jun Wang<sup>2</sup>, Shengchen Li<sup>2</sup>, Lizhong Wang<sup>1</sup> and Lei Yang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Speech Lab, Samsung Research China-Beijing, Beijing, China, <sup>2</sup>Institute of Information Photonics and Optical Communication, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describe our methods for the DCASE 2019 task1a and task1c of Acoustic Scene Classification(ASC).Especially task1c for unknown scene, which not included in training data set, We use less training data and a threshold to classify the known and unknown scenes. In our method, we use Log MelSpectrogram with different divisions, as the input of multiple neural network , the ensemble learning output shows good accuracy. For task 1a we use VGG and xception as network and 3 different divisions ensemble, the accuracy is 0.807 for Leadboard dataset. For task 1c we use Convolutional Recurrent Neural Network (CRNN) and self-attention mechanism with 2 different features division ensemble, and 0.4 as threshold for unknown judgment, the Leadboard accuracy is 0.648.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, BGRU, self-attention
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhu_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhu2019label" class="modal fade" id="bibtex-Zhu2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhu2019label">
        DCASE 2019 Challenge Task1 Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhu2019,
    Author = "Zhu, Houwei and Ren, Chunxia and Wang, Jun and Li, Shengchen and Wang, Lizhong and Yang, Lei",
    title = "{DCASE} 2019 Challenge Task1 Technical Report",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describe our methods for the DCASE 2019 task1a and task1c of Acoustic Scene Classification(ASC).Especially task1c for unknown scene, which not included in training data set, We use less training data and a threshold to classify the known and unknown scenes. In our method, we use Log MelSpectrogram with different divisions, as the input of multiple neural network , the ensemble learning output shows good accuracy. For task 1a we use VGG and xception as network and 3 different divisions ensemble, the accuracy is 0.807 for Leadboard dataset. For task 1c we use Convolutional Recurrent Neural Network (CRNN) and self-attention mechanism with 2 different features division ensemble, and 0.4 as threshold for unknown judgment, the Leadboard accuracy is 0.648."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhu2019a" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhu2019a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019 Challenge Task1 Technical Report
       </h4>
<p style="text-align:left">
        Houwei Zhu<sup>1</sup>, Chunxia Ren<sup>2</sup>, Jun Wang<sup>2</sup>, Shengchen Li<sup>2</sup>, Lizhong Wang<sup>1</sup> and Lei Yang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Speech Lab, Samsung Research China-Beijing, Beijing, China, <sup>2</sup>Institute of Information Photonics and Optical Communication, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhu_SSLabBUPT_task1a_1</span> <span class="label label-primary">Zhu_SSLabBUPT_task1a_2</span> <span class="label label-primary">Zhu_SSLabBUPT_task1a_3</span> <span class="label label-primary">Zhu_SSLabBUPT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhu2019a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhu2019a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhu2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhu_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhu2019a" class="panel-collapse collapse" id="collapse-Zhu2019a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019 Challenge Task1 Technical Report
      </h4>
<p style="text-align:left">
<small>
        Houwei Zhu<sup>1</sup>, Chunxia Ren<sup>2</sup>, Jun Wang<sup>2</sup>, Shengchen Li<sup>2</sup>, Lizhong Wang<sup>1</sup> and Lei Yang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Speech Lab, Samsung Research China-Beijing, Beijing, China, <sup>2</sup>Institute of Information Photonics and Optical Communication, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describe our methods for the DCASE 2019 task1a and task1c of Acoustic Scene Classification(ASC).Especially task1c for unknown scene, which not included in training data set, We use less training data and a threshold to classify the known and unknown scenes. In our method, we use Log MelSpectrogram with different divisions, as the input of multiple neural network , the ensemble learning output shows good accuracy. For task 1a we use VGG and xception as network and 3 different divisions ensemble, the accuracy is 0.807 for Leadboard dataset. For task 1c we use Convolutional Recurrent Neural Network (CRNN) and self-attention mechanism with 2 different features division ensemble, and 0.4 as threshold for unknown judgment, the Leadboard accuracy is 0.648.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         multiple
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, BGRU, self-attention, ensemble
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhu2019a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhu_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhu2019alabel" class="modal fade" id="bibtex-Zhu2019a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhu2019alabel">
        DCASE 2019 Challenge Task1 Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhu2019a,
    Author = "Zhu, Houwei and Ren, Chunxia and Wang, Jun and Li, Shengchen and Wang, Lizhong and Yang, Lei",
    title = "{DCASE} 2019 Challenge Task1 Technical Report",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report describe our methods for the DCASE 2019 task1a and task1c of Acoustic Scene Classification(ASC).Especially task1c for unknown scene, which not included in training data set, We use less training data and a threshold to classify the known and unknown scenes. In our method, we use Log MelSpectrogram with different divisions, as the input of multiple neural network , the ensemble learning output shows good accuracy. For task 1a we use VGG and xception as network and 3 different divisions ensemble, the accuracy is 0.807 for Leadboard dataset. For task 1c we use Convolutional Recurrent Neural Network (CRNN) and self-attention mechanism with 2 different features division ensemble, and 0.4 as threshold for unknown judgment, the Leadboard accuracy is 0.648."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>