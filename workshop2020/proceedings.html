<!DOCTYPE html><html lang="en">
<head>
    <title>Proceedings - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2020/proceedings">
        <meta name="author" content="Yuki Nikaido" />
        <meta name="description" content="The proceedings of the DCASE2020 Workshop have been published as an electronic publication: Nobutaka Ono, Noboru Harada, Yohei Kawaguchi, Annamaria Mesaros, Keisuke Imoto, Yuma Koizumi, and Tatsuya Komatsu (eds.), Proceedings of the 5th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2020), Nov. 2020. ISBN (Electronic): 978-4-600-00566-5 …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2020</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2020/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-list fa-fw"></i>&nbsp;Program&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2020/technical-program"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule & Talks (JST)</a>
    </li>
            <li class="">
        <a href="/workshop2020/technical-program-gmt"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule & Talks (GMT)</a>
    </li>
            <li class="">
        <a href="/workshop2020/technical-program-est"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule & Talks (EST)</a>
    </li>
            <li class="">
        <a href="/workshop2020/technical-program-pst"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule & Talks (PST)</a>
    </li>
            <li class="">
        <a href="/workshop2020/keynotes"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Keynotes</a>
    </li>
            <li class="">
        <a href="/workshop2020/poster_session"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Poster session</a>
    </li>
        </ul>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2020/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Registration">
        <a href="/workshop2020/registration"><i class="fa fa-key fa-fw"></i>&nbsp;Registration</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2020/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2020/presentations"><i class="fa fa-comments-o fa-fw"></i>&nbsp;Presentations</a>
    </li>
            <li class="">
        <a href="/workshop2020/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
            <li class="">
        <a href="/workshop2020/submission"><i class="fa fa-upload fa-fw"></i>&nbsp;Submission</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Code of conduct">
        <a href="/workshop2020/code-of-conduct"><i class="fa fa-thumbs-up fa-fw"></i>&nbsp;Code of conduct</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2020/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metropol-sevilla-03.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><h1 class="bold">Proceedings</h1></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p>The proceedings of the DCASE2020 Workshop have been published as an electronic publication:</p>
<div class="row" style="display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;">
<div class="col-xs-2">
<a data-placement="bottom" href="https://zenodo.org/record/4061782/files/DCASE2020.pdf" rel="tooltip" target="_blank" title="PDF"><img class="img-responsive img-thumbnail" src="../images/covers/DCASE2020Workshop_proceedings_cover.png"/></a>
</div>
<div class="col-xs-10 bg-light-gray">
<p>Nobutaka Ono, Noboru Harada, Yohei Kawaguchi, Annamaria Mesaros, Keisuke Imoto, Yuma Koizumi, and Tatsuya Komatsu (eds.), Proceedings of the 5th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2020), Nov. 2020.</p>
<p>ISBN (Electronic): 978-4-600-00566-5<br/>
        DOI: <a href=" https://doi.org/10.5281/zenodo.4061782"> https://doi.org/10.5281/zenodo.4061782</a></p>
<br/>
<div class="btn-group">
<a class="btn btn-xs btn-primary" data-placement="bottom" href="https://doi.org/10.5281/zenodo.4061782" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" target="_blank" title="Permanent link"><i class="fa fa-link fa-1x"></i> Link</a>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://zenodo.org/record/4061782/files/DCASE2020.pdf" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" title="PDF"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button class="btn btn-xs btn-danger" data-target="#bibtex_proceedings" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
</div>
</div>
<div aria-hidden="true" aria-labelledby="bibtex_proceedingslabel" class="modal fade" id="bibtex_proceedings" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtex_proceedingslabel">Proceedings of the Fifth Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2020)</h4>
</div>
<div class="modal-body">
<pre>
@book{DCASE2020Workshop,
    title = "Proceedings of the Fifth Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2020)",
    author = "Nobutaka Ono, Noboru Harada, Yohei Kawaguchi, Annamaria Mesaros, Keisuke Imoto, Yuma Koizumi, and Tatsuya Komatsu",
    year = "2020",
    month = "November",
    address = "Tokyo, Japan"
}
                </pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
<div class="clearfix"></div>
<div class="btex" data-scholar-cite-counts="true" data-source="content/data/workshop2020/proceedings.bib" data-stats="true">
<em>
  Total cites: 1228 (updated 30.11.2023)
 </em>
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Barak2020" style="box-shadow: none">
<div class="panel-heading" id="headingBarak2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Microphone Array Optimization for Autonomous-Vehicle Audio Localization Based on the Radon Transform
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ohad Barak<sup>1</sup>, Nizar Sallem<sup>1</sup>, and Marc Fischer<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Siemens Digital Industries Software Corporation
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Barak2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Barak_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=uXMtRa8uJjI" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Barak2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Barak2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Barak2020" class="panel-collapse collapse" id="collapse-Barak2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Beamforming is a standard method of determining the Direction-of-Arrival (DoA) of wave energy to an array of receivers. In the case of acoustic waves in an air medium, the array would comprise microphones. The angular resolution of an array depends on the frequency of the data, the number of microphones, the size of the array relative to the wavelengths in the medium, and the geometry of the array, i.e., the positions of the microphones in relation to each other. The task of finding the right balance between the aforementioned parameters is microphone-array optimization. This task is rendered even more complicated in the particular context of sound classification and localization for self driving cars as a result of the design limitations imposed by the automotive industry. We present a microphone array optimization method suitable for designing arrays to be placed on vehicles, which applies beamforming using the Radon transform. We show how our method produces an array geometry with reasonable angular resolution for audio frequencies that are in the range of interest for a road scenario.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Barak2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Barak_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=uXMtRa8uJjI" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Barak2020label" class="modal fade" id="bibtex-Barak2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBarak2020label">
        Microphone Array Optimization for Autonomous-Vehicle Audio Localization Based on the Radon Transform
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Barak2020,
    author = "Barak, Ohad and Sallem, Nizar and Fischer, Marc",
    title = "Microphone Array Optimization for Autonomous-Vehicle Audio Localization Based on the Radon Transform",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "1--5",
    abstract = "Beamforming is a standard method of determining the Direction-of-Arrival (DoA) of wave energy to an array of receivers. In the case of acoustic waves in an air medium, the array would comprise microphones. The angular resolution of an array depends on the frequency of the data, the number of microphones, the size of the array relative to the wavelengths in the medium, and the geometry of the array, i.e., the positions of the microphones in relation to each other. The task of finding the right balance between the aforementioned parameters is microphone-array optimization. This task is rendered even more complicated in the particular context of sound classification and localization for self driving cars as a result of the design limitations imposed by the automotive industry. We present a microphone array optimization method suitable for designing arrays to be placed on vehicles, which applies beamforming using the Radon transform. We show how our method produces an array geometry with reasonable angular resolution for audio frequencies that are in the range of interest for a road scenario."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cakir2020" style="box-shadow: none">
<div class="panel-heading" id="headingCakir2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Multi-Task Regularization Based on Infrequent Classes for Audio Captioning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Emre Çakır<sup>1</sup>, Konstantinos Drossos<sup>1</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Audio Research Group, Tampere University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          21 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cakir2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cakir_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=_coIubMxqRs" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Cakir2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cakir2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cakir2020" class="panel-collapse collapse" id="collapse-Cakir2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio captioning is a multi-modal task, focusing on using natural language for describing the contents of general audio. Most audio captioning methods are based on deep neural networks, employing an encoder-decoder scheme and a dataset with audio clips and corresponding natural language descriptions (i.e. captions). A significant challenge for audio captioning is the distribution of words in the captions: some words are very frequent but acoustically non-informative, i.e. the function words (e.g. “a”, “the”), and other words are infrequent but informative, i.e. the content words (e.g. adjectives, nouns). In this paper we propose two methods to mitigate this class imbalance problem. First, in an autoencoder setting for audio captioning, we weigh each word's contribution to the training loss inversely proportional to its number of occurrences in the whole dataset. Secondly, in addition to multi-class, word-level audio captioning task, we define a multi-label side task based on clip-level content word detection by training a separate decoder. We use the loss from the second task to regularize the jointly trained encoder for the audio captioning task. We evaluate our method using Clotho, a recently published, wide-scale audio captioning dataset, and our results show an increase of 37% relative improvement with SPIDEr metric over the baseline method.
      </p>
<p>
<strong>
        Cites:
       </strong>
       21 (
       <a href="https://scholar.google.com/scholar?cites=16808739781515471358" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cakir2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cakir_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=_coIubMxqRs" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cakir2020label" class="modal fade" id="bibtex-Cakir2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCakir2020label">
        Multi-Task Regularization Based on Infrequent Classes for Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cakir2020,
    author = "Çakır, Emre and Drossos, Konstantinos and Virtanen, Tuomas",
    title = "Multi-Task Regularization Based on Infrequent Classes for Audio Captioning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "6--10",
    abstract = "Audio captioning is a multi-modal task, focusing on using natural language for describing the contents of general audio. Most audio captioning methods are based on deep neural networks, employing an encoder-decoder scheme and a dataset with audio clips and corresponding natural language descriptions (i.e. captions). A significant challenge for audio captioning is the distribution of words in the captions: some words are very frequent but acoustically non-informative, i.e. the function words (e.g. “a”, “the”), and other words are infrequent but informative, i.e. the content words (e.g. adjectives, nouns). In this paper we propose two methods to mitigate this class imbalance problem. First, in an autoencoder setting for audio captioning, we weigh each word's contribution to the training loss inversely proportional to its number of occurrences in the whole dataset. Secondly, in addition to multi-class, word-level audio captioning task, we define a multi-label side task based on clip-level content word detection by training a separate decoder. We use the loss from the second task to regularize the jointly trained encoder for the audio captioning task. We evaluate our method using Clotho, a recently published, wide-scale audio captioning dataset, and our results show an increase of 37\% relative improvement with SPIDEr metric over the baseline method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cao2020" style="box-shadow: none">
<div class="panel-heading" id="headingCao2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Event-Independent Network for Polyphonic Sound Event Localization and Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yin Cao<sup>1</sup>, Turab Iqbal<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Yue Zhong<sup>1</sup>, Wenwu Wang<sup>1</sup>, and Mark D. Plumbley<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, <sup>2</sup>ByteDance Shanghai
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          35 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cao2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cao_82.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=cxQQsXGk9oc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Cao2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cao2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cao2020" class="panel-collapse collapse" id="collapse-Cao2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Polyphonic sound event localization and detection is not only detecting what sound events are happening but localizing corresponding sound sources. This series of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound event localization and detection task introduces additional challenges in moving sound sources and overlapping cases, which include two events of the same type with two different direction-of-arrival (DoA) angles. In this paper, a novel event-independent network for polyphonic sound event localization and detection is proposed. Unlike the two-stage method we proposed in DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the network are first-order Ambisonics (FOA) time-domain signals, which are then fed into a 1-D convolutional layer to extract acoustic features. The network is then split into two parallel branches. The first branch is for sound event detection (SED), and the second branch is for DoA estimation. There are three types of predictions from the network, SED predictions, DoA predictions, and event activity detection (EAD) predictions that are used to combine the SED and DoA features for on-set and off-set estimation. All of these predictions have the format of two tracks indicating that there are at most two overlapping events. Within each track, there could be at most one event happening. This architecture introduces a problem of track permutation. To address this problem, a frame-level permutation invariant training method is used. Experimental results show that the proposed method can detect polyphonic sound events and their corresponding DoAs. Its performance on the Task 3 dataset is greatly increased as compared with that of the baseline method.
      </p>
<p>
<strong>
        Cites:
       </strong>
       35 (
       <a href="https://scholar.google.com/scholar?cites=17091237164743527347" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cao2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cao_82.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=cxQQsXGk9oc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cao2020label" class="modal fade" id="bibtex-Cao2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCao2020label">
        Event-Independent Network for Polyphonic Sound Event Localization and Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cao2020,
    author = "Cao, Yin and Iqbal, Turab and Kong, Qiuqiang and Zhong, Yue and Wang, Wenwu and Plumbley, Mark D.",
    title = "Event-Independent Network for Polyphonic Sound Event Localization and Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "11--15",
    abstract = "Polyphonic sound event localization and detection is not only detecting what sound events are happening but localizing corresponding sound sources. This series of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound event localization and detection task introduces additional challenges in moving sound sources and overlapping cases, which include two events of the same type with two different direction-of-arrival (DoA) angles. In this paper, a novel event-independent network for polyphonic sound event localization and detection is proposed. Unlike the two-stage method we proposed in DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the network are first-order Ambisonics (FOA) time-domain signals, which are then fed into a 1-D convolutional layer to extract acoustic features. The network is then split into two parallel branches. The first branch is for sound event detection (SED), and the second branch is for DoA estimation. There are three types of predictions from the network, SED predictions, DoA predictions, and event activity detection (EAD) predictions that are used to combine the SED and DoA features for on-set and off-set estimation. All of these predictions have the format of two tracks indicating that there are at most two overlapping events. Within each track, there could be at most one event happening. This architecture introduces a problem of track permutation. To address this problem, a frame-level permutation invariant training method is used. Experimental results show that the proposed method can detect polyphonic sound events and their corresponding DoAs. Its performance on the Task 3 dataset is greatly increased as compared with that of the baseline method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cartwright2020" style="box-shadow: none">
<div class="panel-heading" id="headingCartwright2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Mark Cartwright<sup>1</sup>, Aurora Cramer<sup>1</sup>, Ana Elisa Mendez Mendez<sup>1</sup>, Yu Wang<sup>1</sup>, Ho-Hsiang Wu<sup>1</sup>, Vincent Lostanlen<sup>1,2</sup>, Magdalena Fuentes<sup>1</sup>, Graham Dove<sup>1</sup>, Charlie Mydlarz<sup>1</sup>, Justin Salamon<sup>3</sup>, Oded Nov<sup>1</sup>, and Juan Pablo Bello<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>New York University, <sup>2</sup>Cornell Lab of Ornithology, <sup>3</sup>Adobe Research, San Francisco
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          34 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cartwright2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cartwright_68.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Cartwright2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cartwright2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cartwright2020" class="panel-collapse collapse" id="collapse-Cartwright2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present SONYC-UST-V2, a dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the “Sounds of New York City” (SONYC) acoustic sensor network, including the timestamp of audio acquisition (at the hour scale) and location of the sensor (at the urban block level). The dataset contains annotations by volunteers from the Zooniverse citizen science platform, as well as a two-stage verification with our team. In this article, we describe our data collection procedure and propose evaluation metrics for multilabel classification of urban sound tags. We report the results of a simple baseline model that exploits temporal information.
      </p>
<p>
<strong>
        Cites:
       </strong>
       34 (
       <a href="https://scholar.google.com/scholar?cites=8160969653082798458" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cartwright2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cartwright_68.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cartwright2020label" class="modal fade" id="bibtex-Cartwright2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCartwright2020label">
        SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cartwright2020,
    author = "Cartwright, Mark and Cramer, Aurora and Mendez Mendez, Ana Elisa and Wang, Yu and Wu, Ho-Hsiang and Lostanlen, Vincent and Fuentes, Magdalena and Dove, Graham and Mydlarz, Charlie and Salamon, Justin and Nov, Oded and Bello, Juan Pablo",
    title = "SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "16--20",
    abstract = "We present SONYC-UST-V2, a dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the “Sounds of New York City” (SONYC) acoustic sensor network, including the timestamp of audio acquisition (at the hour scale) and location of the sensor (at the urban block level). The dataset contains annotations by volunteers from the Zooniverse citizen science platform, as well as a two-stage verification with our team. In this article, we describe our data collection procedure and propose evaluation metrics for multilabel classification of urban sound tags. We report the results of a simple baseline model that exploits temporal information."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chen2020" style="box-shadow: none">
<div class="panel-heading" id="headingChen2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Audio Captioning Based on Transformer and Pre-Trained CNN
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kun Chen<sup>1</sup>, Yusong Wu<sup>1</sup>, Ziyue Wang<sup>2</sup>, Xuan Zhang<sup>2</sup>, Fudong Nian<sup>3</sup>, Shengchen Li<sup>1</sup>, and Xi Shao<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Beijing University of Posts and Telecommunications, <sup>2</sup>Nanjing University of Posts and Telecommunications, <sup>3</sup>Anhui University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          45 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Chen_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=YH2ipYBeymE" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Chen2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chen2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chen2020" class="panel-collapse collapse" id="collapse-Chen2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automated audio captioning is the task that generates text description of a piece of audio. This paper proposes a solution of automated audio captioning based on a combination of pre-trained CNN layers and a sequence-to-sequence architecture based on Transformer. The pre-trained CNN layers are adopted from a CNN based neural network for acoustic event tagging, which makes the latent variable resulted more efficient on generating captions. Transformer decoder is used in the sequence-to-sequence architecture as a consequence of comparing the performance of the more classical LSTM layers. The proposed system achieves a SPIDEr score of 0.227 for the DCASE challenge 2020 Task 6 with data augmentation and label smoothing applied.
      </p>
<p>
<strong>
        Cites:
       </strong>
       45 (
       <a href="https://scholar.google.com/scholar?cites=954192600848471704" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Chen_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=YH2ipYBeymE" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chen2020label" class="modal fade" id="bibtex-Chen2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChen2020label">
        Audio Captioning Based on Transformer and Pre-Trained CNN
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Chen2020,
    author = "Chen, Kun and Wu, Yusong and Wang, Ziyue and Zhang, Xuan and Nian, Fudong and Li, Shengchen and Shao, Xi",
    title = "Audio Captioning Based on Transformer and Pre-Trained CNN",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "21--25",
    abstract = "Automated audio captioning is the task that generates text description of a piece of audio. This paper proposes a solution of automated audio captioning based on a combination of pre-trained CNN layers and a sequence-to-sequence architecture based on Transformer. The pre-trained CNN layers are adopted from a CNN based neural network for acoustic event tagging, which makes the latent variable resulted more efficient on generating captions. Transformer decoder is used in the sequence-to-sequence architecture as a consequence of comparing the performance of the more classical LSTM layers. The proposed system achieves a SPIDEr score of 0.227 for the DCASE challenge 2020 Task 6 with data augmentation and label smoothing applied."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cornell2020a" style="box-shadow: none">
<div class="panel-heading" id="headingCornell2020a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Domain-Adversarial Training and Trainable Parallel Front-End for the DCASE 2020 Task 4 Sound Event Detection Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Samuele Cornell<sup>1</sup>, Michel Olvera<sup>2</sup>, Manuel Pariente<sup>2</sup>, Giovanni Pepe<sup>1</sup>, Emanuele Principi<sup>1</sup>, Leonardo Gabrielli<sup>1</sup>, and Stefano Squartini<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Università Politecnica delle Marche, Dept. Information Engineering, <sup>2</sup>INRIA Nancy Grand-Est, Dept. Information and Communication Sciences and Technologies
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          6 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cornell2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cornell_38.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=2kr9DE4NclQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Cornell2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cornell2020a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cornell2020a" class="panel-collapse collapse" id="collapse-Cornell2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose several methods for improving Sound Event Detection systems performance in the context of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Task 4 challenge. Our main contributions are in the training techniques, feature pre-processing and prediction post-processing. Given the mismatch between synthetic labelled data and target domain data, we exploit domain adversarial training to improve the network generalization. We show that such technique is especially effective when coupled with dynamic mixing and data augmentation. Together with Hidden Markov Models prediction smoothing, by coupling the challenge baseline with aforementioned techniques we are able to improve event-based macro <i>F</i><sub>1</sub> score by more than 10% on the development set, without computational overhead at inference time. Moreover, we propose a novel, effective Parallel Per-Channel Energy Normalization front-end layer and show that it brings an additional improvement of more than one percent with minimal computational overhead.
      </p>
<p>
<strong>
        Cites:
       </strong>
       6 (
       <a href="https://scholar.google.com/scholar?cites=9247583082544555577" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cornell2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cornell_38.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=2kr9DE4NclQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cornell2020alabel" class="modal fade" id="bibtex-Cornell2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCornell2020alabel">
        Domain-Adversarial Training and Trainable Parallel Front-End for the DCASE 2020 Task 4 Sound Event Detection Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cornell2020a,
    author = "Cornell, Samuele and Olvera, Michel and Pariente, Manuel and Pepe, Giovanni and Principi, Emanuele and Gabrielli, Leonardo and Squartini, Stefano",
    title = "Domain-Adversarial Training and Trainable Parallel Front-End for the DCASE 2020 Task 4 Sound Event Detection Challenge",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "26--30",
    abstract = "In this paper, we propose several methods for improving Sound Event Detection systems performance in the context of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Task 4 challenge. Our main contributions are in the training techniques, feature pre-processing and prediction post-processing. Given the mismatch between synthetic labelled data and target domain data, we exploit domain adversarial training to improve the network generalization. We show that such technique is especially effective when coupled with dynamic mixing and data augmentation. Together with Hidden Markov Models prediction smoothing, by coupling the challenge baseline with aforementioned techniques we are able to improve event-based macro <i>F</i><sub>1</sub> score by more than 10\% on the development set, without computational overhead at inference time. Moreover, we propose a novel, effective Parallel Per-Channel Energy Normalization front-end layer and show that it brings an additional improvement of more than one percent with minimal computational overhead."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cornell2020b" style="box-shadow: none">
<div class="panel-heading" id="headingCornell2020b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Task-Aware Separation for the DCASE 2020 Task 4 Sound Event Detection and Separation Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Samuele Cornell<sup>1</sup>, Michel Olvera<sup>2</sup>, Manuel Pariente<sup>2</sup>, Giovanni Pepe<sup>1</sup>, Emanuele Principi<sup>1</sup>, Leonardo Gabrielli<sup>1</sup>, and Stefano Squartini<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Università Politecnica delle Marche, Dept. Information Engineering, <sup>2</sup>INRIA Nancy Grand-Est, Dept. Information and Communication Sciences and Technologies
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cornell2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cornell_90.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=5gYz5--06sU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Cornell2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cornell2020b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cornell2020b" class="panel-collapse collapse" id="collapse-Cornell2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Source Separation is often used as a pre-processing step in many signal-processing tasks. In this work we propose a novel approach for combined Source Separation and Sound Event Detection in which a Source Separation algorithm is used to enhance the Sound Even -Detection back-end performance. In particular, we present a permutation-invariant training scheme for optimizing the Source Separation system directly with the back-end Sound Event Detection objective without requiring joint training or fine-tuning of the two systems. We show that such an approach has significant advantages over the more standard approach of training the Source Separation system separately using only a Source Separation based objective such as Scale-Invariant Signal-To-Distortion Ratio. On the 2020 Detection and Classification of Acoustic Scenes and Events Task 4 Challenge our proposed approach is able to outperform the baseline source separation system by more than one percent in event-based macro <i>F</i><sub>1</sub> score on the development set with significantly less computational requirements.
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="https://scholar.google.com/scholar?cites=13426661949726567920" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cornell2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Cornell_90.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=5gYz5--06sU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cornell2020blabel" class="modal fade" id="bibtex-Cornell2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCornell2020blabel">
        Task-Aware Separation for the DCASE 2020 Task 4 Sound Event Detection and Separation Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cornell2020b,
    author = "Cornell, Samuele and Olvera, Michel and Pariente, Manuel and Pepe, Giovanni and Principi, Emanuele and Gabrielli, Leonardo and Squartini, Stefano",
    title = "Task-Aware Separation for the DCASE 2020 Task 4 Sound Event Detection and Separation Challenge",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "31--35",
    abstract = "Source Separation is often used as a pre-processing step in many signal-processing tasks. In this work we propose a novel approach for combined Source Separation and Sound Event Detection in which a Source Separation algorithm is used to enhance the Sound Even -Detection back-end performance. In particular, we present a permutation-invariant training scheme for optimizing the Source Separation system directly with the back-end Sound Event Detection objective without requiring joint training or fine-tuning of the two systems. We show that such an approach has significant advantages over the more standard approach of training the Source Separation system separately using only a Source Separation based objective such as Scale-Invariant Signal-To-Distortion Ratio. On the 2020 Detection and Classification of Acoustic Scenes and Events Task 4 Challenge our proposed approach is able to outperform the baseline source separation system by more than one percent in event-based macro <i>F</i><sub>1</sub> score on the development set with significantly less computational requirements."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="de-Benito-Gorron2020" style="box-shadow: none">
<div class="panel-heading" id="headingde-Benito-Gorron2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Multi-Resolution Approach to Sound Event Detection in DCASE 2020 Task4
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Diego de Benito-Gorron<sup>1</sup>, Daniel Ramos<sup>1</sup>, and Doroteo T. Toledano<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>AUDIAS Research Group, Universidad Autónoma de Madrid
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          7 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-de-Benito-Gorron2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_de-Benito-Gorron_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=4oJCPTb85jg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-de-Benito-Gorron2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-de-Benito-Gorron2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-de-Benito-Gorron2020" class="panel-collapse collapse" id="collapse-de-Benito-Gorron2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose a multi-resolution analysis for feature extraction in Sound Event Detection. Because of the specific temporal and spectral characteristics of the different acoustic events, we hypothesize that different time-frequency resolutions can be more appropriate to locate each sound category. We carry out our experiments using the DESED dataset in the context of the DCASE 2020 Task 4 challenge, where the combination of up to five different time-frequency resolutions via model fusion is able to outperform the baseline results. In addition, we propose class-specific thresholds for the <i>F</i><sub>1</sub>-score metric, further improving the results over the Validation and Public Evaluation sets.
      </p>
<p>
<strong>
        Cites:
       </strong>
       7 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-de-Benito-Gorron2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_de-Benito-Gorron_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=4oJCPTb85jg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-de-Benito-Gorron2020label" class="modal fade" id="bibtex-de-Benito-Gorron2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexde-Benito-Gorron2020label">
        A Multi-Resolution Approach to Sound Event Detection in DCASE 2020 Task4
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{de-Benito-Gorron2020,
    author = "de Benito-Gorron, Diego and Ramos, Daniel and Toledano, Doroteo T.",
    title = "A Multi-Resolution Approach to Sound Event Detection in DCASE 2020 Task4",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "36--40",
    abstract = "In this paper, we propose a multi-resolution analysis for feature extraction in Sound Event Detection. Because of the specific temporal and spectral characteristics of the different acoustic events, we hypothesize that different time-frequency resolutions can be more appropriate to locate each sound category. We carry out our experiments using the DESED dataset in the context of the DCASE 2020 Task 4 challenge, where the combination of up to five different time-frequency resolutions via model fusion is able to outperform the baseline results. In addition, we propose class-specific thresholds for the <i>F</i><sub>1</sub>-score metric, further improving the results over the Validation and Public Evaluation sets."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ebbers2020" style="box-shadow: none">
<div class="panel-heading" id="headingEbbers2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Forward-Backward Convolutional Recurrent Neural Networks and Tag-Conditioned Convolutional Neural Networks for Weakly Labeled Semi-Supervised Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Janek Ebbers<sup>1</sup>, and Reinhold Haeb-Umbach<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Paderborn University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          10 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ebbers2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Ebbers_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Ix68flh6X6g" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Ebbers2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ebbers2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ebbers2020" class="panel-collapse collapse" id="collapse-Ebbers2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper we present our system for the <i>detection and classification of acoustic scenes and events (DCASE) 2020 Challenge Task 4: Sound event detection and separation in domestic environments</i>. We introduce two new models: the forwatd-backward convolutional recurrent neural network (FBCRNN) and the tag-conditioned convolutional neural network (CNN). The FBCRNN employs tworecurrent neural network (RNN) classifiers sharing the same CNN for preprocessing. With one RNN processing a recording in forward direction and the other in backward direction, the two networks are trained to jointly predict audio tags, i.e., weak labels, at each time step within a recording, given that at each time step they have jointly processed the whole recording. The proposed training encourages the classifiers to tag events as soon as possible. Therefore, after training, the networks can be applied to shorter audio segments of, e.g., 200 ms, allowing sound event detection (SED). Further, we propose a tag-conditioned CNN to complement SED. It is trained to predict strong labels while using (predicted) tags, i.e., weak labels, as additional input. For training pseudo strong labels from a FBCRNN ensemble are used. The presented system scored the fourth and third place in the systems and teams rankings, respectively. Subsequent improvements allow our system to even outperform the challenge baseline and winner systems in average by, respectively, 18.0% and 2.2% event-based F1-score on the validation set. Source code is publicly available at https://github.com/fgnt/pb_sed.
      </p>
<p>
<strong>
        Cites:
       </strong>
       10 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ebbers2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Ebbers_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Ix68flh6X6g" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ebbers2020label" class="modal fade" id="bibtex-Ebbers2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEbbers2020label">
        Forward-Backward Convolutional Recurrent Neural Networks and Tag-Conditioned Convolutional Neural Networks for Weakly Labeled Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ebbers2020,
    author = "Ebbers, Janek and Haeb-Umbach, Reinhold",
    title = "Forward-Backward Convolutional Recurrent Neural Networks and Tag-Conditioned Convolutional Neural Networks for Weakly Labeled Semi-Supervised Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "41--45",
    abstract = "In this paper we present our system for the <i>detection and classification of acoustic scenes and events (DCASE) 2020 Challenge Task 4: Sound event detection and separation in domestic environments</i>. We introduce two new models: the forwatd-backward convolutional recurrent neural network (FBCRNN) and the tag-conditioned convolutional neural network (CNN). The FBCRNN employs tworecurrent neural network (RNN) classifiers sharing the same CNN for preprocessing. With one RNN processing a recording in forward direction and the other in backward direction, the two networks are trained to jointly predict audio tags, i.e., weak labels, at each time step within a recording, given that at each time step they have jointly processed the whole recording. The proposed training encourages the classifiers to tag events as soon as possible. Therefore, after training, the networks can be applied to shorter audio segments of, e.g., 200 ms, allowing sound event detection (SED). Further, we propose a tag-conditioned CNN to complement SED. It is trained to predict strong labels while using (predicted) tags, i.e., weak labels, as additional input. For training pseudo strong labels from a FBCRNN ensemble are used. The presented system scored the fourth and third place in the systems and teams rankings, respectively. Subsequent improvements allow our system to even outperform the challenge baseline and winner systems in average by, respectively, 18.0\% and 2.2\% event-based F1-score on the validation set. Source code is publicly available at https://github.com/fgnt/pb\_sed."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Giri2020a" style="box-shadow: none">
<div class="panel-heading" id="headingGiri2020a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Self-Supervised Classification for Detecting Anomalous Sounds
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ritwik Giri<sup>1</sup>, Srikanth V. Tenneti<sup>1</sup>, Fangzhou Cheng<sup>1</sup>, Karim Helwani<sup>1</sup>, Umut Isik<sup>1</sup>, and Arvindh Krishnaswamy<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Amazon Web Services
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          43 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Giri2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Giri_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=SM4TQnZFLYo" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Giri2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Giri2020a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Giri2020a" class="panel-collapse collapse" id="collapse-Giri2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Representation learning, using self-supervised classification has recently been shown to give state-of-the-art accuracies for anomaly detection on computer vision datasets. Geometric transformations on images such as rotations, translations and flipping have been used in these recent works to create auxiliary classification tasks for feature learning. This paper introduces a new self-supervised classification framework for anomaly detection in audio signals. Classification tasks are set up based on differences in the metadata associated with the audio files. Synthetic augmentations such as linearly combining and warping audio-spectrograms are also used to increase the complexity of the classification task, to learn finer features. The proposed approach is validated using the publicly available DCASE 2020 challenge task 2: <i>Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring dataset</i>. We demonstrate the effectiveness of our approach by comparing against the baseline autoencoder model, showing an improvement of over 12.5% in the average AUC metrics.
      </p>
<p>
<strong>
        Cites:
       </strong>
       43 (
       <a href="https://scholar.google.com/scholar?cites=12421557475400288564" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Giri2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Giri_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=SM4TQnZFLYo" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Giri2020alabel" class="modal fade" id="bibtex-Giri2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGiri2020alabel">
        Self-Supervised Classification for Detecting Anomalous Sounds
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Giri2020a,
    author = "Giri, Ritwik and Tenneti, Srikanth V. and Cheng, Fangzhou and Helwani, Karim and Isik, Umut and Krishnaswamy, Arvindh",
    title = "Self-Supervised Classification for Detecting Anomalous Sounds",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "46--50",
    abstract = "Representation learning, using self-supervised classification has recently been shown to give state-of-the-art accuracies for anomaly detection on computer vision datasets. Geometric transformations on images such as rotations, translations and flipping have been used in these recent works to create auxiliary classification tasks for feature learning. This paper introduces a new self-supervised classification framework for anomaly detection in audio signals. Classification tasks are set up based on differences in the metadata associated with the audio files. Synthetic augmentations such as linearly combining and warping audio-spectrograms are also used to increase the complexity of the classification task, to learn finer features. The proposed approach is validated using the publicly available DCASE 2020 challenge task 2: <i>Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring dataset</i>. We demonstrate the effectiveness of our approach by comparing against the baseline autoencoder model, showing an improvement of over 12.5\% in the average AUC metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Giri2020b" style="box-shadow: none">
<div class="panel-heading" id="headingGiri2020b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Group Masked Autoencoder Based Density Estimator for Audio Anomaly Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ritwik Giri<sup>1</sup>, Fangzhou Cheng<sup>1</sup>, Karim Helwani<sup>1</sup>, Srikanth V. Tenneti<sup>1</sup>, Umut Isik<sup>1</sup>, and Arvindh Krishnaswamy<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Amazon Web Services
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          24 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Giri2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Giri_66.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Uttope7MXUQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Giri2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Giri2020b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Giri2020b" class="panel-collapse collapse" id="collapse-Giri2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we address the problem of detecting previously unseen anomalous audio events, when the training dataset itself does not contain any examples of anomalies. While the traditional density estimation techniques, such as Gaussian Mixture Model (GMM) showed promise in past for the problem at hand, recent advances in neural density estimation techniques, have made them suitable for anomaly detection task. In this work, we develop a novel neural density estimation technique based on the Group-Masked Autoencoder, that estimates the density of an audio time series by taking into account the intra-frame statistics of the signal. Our proposed approach has been validated using the DCASE 2020 challenge dataset (Task 2 - <i>Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring</i>). We demonstrate the effectiveness of our approach by comparing against the baseline autoencoder model, and also against recently proposed Interpolating Deep Neural Network (IDNN) model.
      </p>
<p>
<strong>
        Cites:
       </strong>
       24 (
       <a href="https://scholar.google.com/scholar?cites=15950967633610185650" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Giri2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Giri_66.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Uttope7MXUQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Giri2020blabel" class="modal fade" id="bibtex-Giri2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGiri2020blabel">
        Group Masked Autoencoder Based Density Estimator for Audio Anomaly Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Giri2020b,
    author = "Giri, Ritwik and Cheng, Fangzhou and Helwani, Karim and Tenneti, Srikanth V. and Isik, Umut and Krishnaswamy, Arvindh",
    title = "Group Masked Autoencoder Based Density Estimator for Audio Anomaly Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "51--55",
    abstract = "In this paper, we address the problem of detecting previously unseen anomalous audio events, when the training dataset itself does not contain any examples of anomalies. While the traditional density estimation techniques, such as Gaussian Mixture Model (GMM) showed promise in past for the problem at hand, recent advances in neural density estimation techniques, have made them suitable for anomaly detection task. In this work, we develop a novel neural density estimation technique based on the Group-Masked Autoencoder, that estimates the density of an audio time series by taking into account the intra-frame statistics of the signal. Our proposed approach has been validated using the DCASE 2020 challenge dataset (Task 2 - <i>Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring</i>). We demonstrate the effectiveness of our approach by comparing against the baseline autoencoder model, and also against recently proposed Interpolating Deep Neural Network (IDNN) model."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Heittola2020" style="box-shadow: none">
<div class="panel-heading" id="headingHeittola2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification in DCASE 2020 Challenge: Generalization Across Devices and Low Complexity Solutions
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Toni Heittola<sup>1</sup>, Annamaria Mesaros<sup>1</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Computing Sciences, Tampere University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          174 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Heittola2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Heittola_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Heittola2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Heittola2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Heittola2020" class="panel-collapse collapse" id="collapse-Heittola2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents the details of Task 1. Acoustic Scene Classification in the DCASE 2020 Challenge. The task consisted of two subtasks: classification of data from multiple devices, requiring good generalization properties, and classification using low-complexity solutions. Each subtask received around 90 submissions, and most of them outperformed the baseline system. The most used techniques among the submissions were data augmentation in Subtask A, to compensate for the device mismatch, and post-training quantization of neural network weights in Subtask B, to bring the model size under the required limit. The maximum classification accuracy on the evaluation set in Subtask A was 76.5%, compared to the baseline performance of 51.4%. In Subtask B, many systems are just below the size limit, and the maximum classification accuracy was 96.5%, compared to the baseline performance of 89.5%.
      </p>
<p>
<strong>
        Cites:
       </strong>
       174 (
       <a href="https://scholar.google.com/scholar?cites=9107801601223979821" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Heittola2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Heittola_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Heittola2020label" class="modal fade" id="bibtex-Heittola2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHeittola2020label">
        Acoustic Scene Classification in DCASE 2020 Challenge: Generalization Across Devices and Low Complexity Solutions
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Heittola2020,
    author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas",
    title = "Acoustic Scene Classification in DCASE 2020 Challenge: Generalization Across Devices and Low Complexity Solutions",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "56--60",
    abstract = "This paper presents the details of Task 1. Acoustic Scene Classification in the DCASE 2020 Challenge. The task consisted of two subtasks: classification of data from multiple devices, requiring good generalization properties, and classification using low-complexity solutions. Each subtask received around 90 submissions, and most of them outperformed the baseline system. The most used techniques among the submissions were data augmentation in Subtask A, to compensate for the device mismatch, and post-training quantization of neural network weights in Subtask B, to bring the model size under the required limit. The maximum classification accuracy on the evaluation set in Subtask A was 76.5\%, compared to the baseline performance of 51.4\%. In Subtask B, many systems are just below the size limit, and the maximum classification accuracy was 96.5\%, compared to the baseline performance of 89.5\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2020" style="box-shadow: none">
<div class="panel-heading" id="headingHuang2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Guided Multi-Branch Learning Systems for Sound Event Detection with Sound Separation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yuxin Huang<sup>1,2</sup>, Liwei Lin<sup>1,2</sup>, Shuo Ma<sup>1,2</sup>, Xiangdong Wang<sup>1</sup>, Hong Liu<sup>1</sup>, Yueliang Qian<sup>1</sup>, Min Liu<sup>3</sup>, and Kazushige Ouchi<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, <sup>2</sup>University of Chinese Academy of Sciences, <sup>3</sup>Toshiba China R&amp;D Center
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          6 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Huang_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=2QJ-3FD3bMo" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Huang2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2020" class="panel-collapse collapse" id="collapse-Huang2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe in detail our systems for DCASE 2020 Task 4. The systems are based on the 1st-place system of DCASE 2019 Task 4, which adopts weakly-supervised framework with an attention-based embedding-level pooling module and a semi-supervised learning approach named guided learning. This year, we incorporate multi-branch learning (MBL) into the original system to further improve its performance. MBL uses different branches with different pooling strategies (including instance-level and embedding-level strategies) and different pooling modules (including attention pooling, global max pooling or global average pooling modules), which share the same feature encoder of the model. Therefore, multiple branches pursuing different purposes and focusing on different characteristics of the data can help the feature encoder model the feature space better and avoid over-fitting. To better exploit the strongly-labeled synthetic data, inspired by multi-task learning, we also employ a sound event detection branch. To combine sound separation (SS) with sound event detection (SED), we fuse the results of SED systems with SS-SED systems which are trained using separated sound output by an SS system. The experimental results prove that MBL can improve the model performance and using SS has great potential to improve the performance of SED ensemble system.
      </p>
<p>
<strong>
        Cites:
       </strong>
       6 (
       <a href="https://scholar.google.com/scholar?cites=17220534914937152256" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Huang_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=2QJ-3FD3bMo" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2020label" class="modal fade" id="bibtex-Huang2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2020label">
        Guided Multi-Branch Learning Systems for Sound Event Detection with Sound Separation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Huang2020,
    author = "Huang, Yuxin and Lin, Liwei and Ma, Shuo and Wang, Xiangdong and Liu, Hong and Qian, Yueliang and Liu, Min and Ouchi, Kazushige",
    title = "Guided Multi-Branch Learning Systems for Sound Event Detection with Sound Separation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "61--65",
    abstract = "In this paper, we describe in detail our systems for DCASE 2020 Task 4. The systems are based on the 1st-place system of DCASE 2019 Task 4, which adopts weakly-supervised framework with an attention-based embedding-level pooling module and a semi-supervised learning approach named guided learning. This year, we incorporate multi-branch learning (MBL) into the original system to further improve its performance. MBL uses different branches with different pooling strategies (including instance-level and embedding-level strategies) and different pooling modules (including attention pooling, global max pooling or global average pooling modules), which share the same feature encoder of the model. Therefore, multiple branches pursuing different purposes and focusing on different characteristics of the data can help the feature encoder model the feature space better and avoid over-fitting. To better exploit the strongly-labeled synthetic data, inspired by multi-task learning, we also employ a sound event detection branch. To combine sound separation (SS) with sound event detection (SED), we fuse the results of SED systems with SS-SED systems which are trained using separated sound output by an SS system. The experimental results prove that MBL can improve the model performance and using SS has great potential to improve the performance of SED ensemble system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Inoue2020" style="box-shadow: none">
<div class="panel-heading" id="headingInoue2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Detection of Anomalous Sounds for Machine Condition Monitoring using Classification Confidence
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Tadanobu Inoue<sup>1</sup>, Phongtharin Vinayavekhin<sup>1</sup>, Shu Morikuni<sup>1</sup>, Shiqiang Wang<sup>1</sup>, Tuan Hoang Trong<sup>1</sup>, David Wood<sup>1</sup>, Michiaki Tatsubori<sup>1</sup>, and Ryuki Tachibana<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IBM Research
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          26 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Inoue2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Inoue_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=LlM-Jli3-mo" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Inoue2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Inoue2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Inoue2020" class="panel-collapse collapse" id="collapse-Inoue2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Anomaly-detection methods based on classification confidence are applied to the DCASE 2020 Task 2 Challenge on Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring. The final systems for submitting to the challenge are ensembles of two classification-based detectors. Both classifiers are trained with either known or generated properties of normal sounds as labels: one is a model to classify sounds into machine type and ID; the other is a model to classify transformed sounds into data-augmentation type. As for the latter model, the normal sound is augmented by using sound-transformation techniques such as pitch shifting, and data-augmentation type is used as a label. For both classifiers, classification confidence is used as the normality score for an input sample at runtime. An ensemble of these approaches is created by using probability aggregation of their anomaly scores. The experimental results on AUC show superior performance by each detector in relation to the baseline provided by the DCASE organizer. Moreover, the proposed ensemble of two detectors generally shows further improvement on the anomaly detection performance. The proposed anomaly-detection system was ranked fourth in the team ranking according to the metrics of the DCASE Challenge, and it achieves 90.93% in terms of average of AUC and pAUC scores for all the machine types, and that score is the highest of those scores achieved by all of the submitted systems.
      </p>
<p>
<strong>
        Cites:
       </strong>
       26 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Inoue2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Inoue_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=LlM-Jli3-mo" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Inoue2020label" class="modal fade" id="bibtex-Inoue2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexInoue2020label">
        Detection of Anomalous Sounds for Machine Condition Monitoring using Classification Confidence
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Inoue2020,
    author = "Inoue, Tadanobu and Vinayavekhin, Phongtharin and Morikuni, Shu and Wang, Shiqiang and Hoang Trong, Tuan and Wood, David and Tatsubori, Michiaki and Tachibana, Ryuki",
    title = "Detection of Anomalous Sounds for Machine Condition Monitoring using Classification Confidence",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "66--70",
    abstract = "Anomaly-detection methods based on classification confidence are applied to the DCASE 2020 Task 2 Challenge on Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring. The final systems for submitting to the challenge are ensembles of two classification-based detectors. Both classifiers are trained with either known or generated properties of normal sounds as labels: one is a model to classify sounds into machine type and ID; the other is a model to classify transformed sounds into data-augmentation type. As for the latter model, the normal sound is augmented by using sound-transformation techniques such as pitch shifting, and data-augmentation type is used as a label. For both classifiers, classification confidence is used as the normality score for an input sample at runtime. An ensemble of these approaches is created by using probability aggregation of their anomaly scores. The experimental results on AUC show superior performance by each detector in relation to the baseline provided by the DCASE organizer. Moreover, the proposed ensemble of two detectors generally shows further improvement on the anomaly detection performance. The proposed anomaly-detection system was ranked fourth in the team ranking according to the metrics of the DCASE Challenge, and it achieves 90.93\% in terms of average of AUC and pAUC scores for all the machine types, and that score is the highest of those scores achieved by all of the submitted systems."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kapka2020" style="box-shadow: none">
<div class="panel-heading" id="headingKapka2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sławomir Kapka<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Samsung R&amp;D Institute Poland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          27 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kapka2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Kapka_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=qqtRF5X35a0" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Kapka2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kapka2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kapka2020" class="panel-collapse collapse" id="collapse-Kapka2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we introduce ID-Conditioned Auto-Encoder for unsupervised anomaly detection. Our method is an adaptation of the Class-Conditioned Auto-Encoder (C2AE) designed for the open-set recognition. Assuming that non-anomalous samples constitute of distinct IDs, we apply Conditioned Auto-Encoder with labels provided by these IDs. Opposed to C2AE, our approach omits the classification subtask and reduces the learning process to the single run. We simplify the learning process further by fixing a constant vector as the target for non-matching labels. We apply our method in the context of sounds for machine condition monitoring. We evaluate our method on the ToyADMOS and MIMII datasets from the DCASE 2020 Challenge Task 2. We conduct an ablation study to indicate which steps of our method influences results the most.
      </p>
<p>
<strong>
        Cites:
       </strong>
       27 (
       <a href="https://scholar.google.com/scholar?cites=752017100645158375" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kapka2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Kapka_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=qqtRF5X35a0" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kapka2020label" class="modal fade" id="bibtex-Kapka2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKapka2020label">
        ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kapka2020,
    author = "Kapka, Sławomir",
    title = "ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "71--75",
    abstract = "In this paper, we introduce ID-Conditioned Auto-Encoder for unsupervised anomaly detection. Our method is an adaptation of the Class-Conditioned Auto-Encoder (C2AE) designed for the open-set recognition. Assuming that non-anomalous samples constitute of distinct IDs, we apply Conditioned Auto-Encoder with labels provided by these IDs. Opposed to C2AE, our approach omits the classification subtask and reduces the learning process to the single run. We simplify the learning process further by fixing a constant vector as the target for non-matching labels. We apply our method in the context of sounds for machine condition monitoring. We evaluate our method on the ToyADMOS and MIMII datasets from the DCASE 2020 Challenge Task 2. We conduct an ablation study to indicate which steps of our method influences results the most."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2020" style="box-shadow: none">
<div class="panel-heading" id="headingKim2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Audio Tag Representation Guided Dual Attention Network for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ju-Ho Kim<sup>1</sup>, Jee-Weon Jung<sup>1</sup>, Hye-Jin Shim<sup>1</sup>, and Ha-Jin Yu<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>University of Seoul, School of Computer Science
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          7 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Kim_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=2OfwZsw11eg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Kim2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2020" class="panel-collapse collapse" id="collapse-Kim2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound events are crucial to discern a specific acoustic scene, which establishes a close relationship between audio tagging and acoustic scene classification (ASC). In this study, we explore the role and application of sound events based on the ASC task and propose the use of the last hidden layer's output of an audio tagging system (<i>tag representation</i>), rather than the output itself (<i>tag vector</i>), in ASC. We hypothesize that the tag representation contains sound event information that can improve the classification accuracy of acoustic scenes. The dual attention mechanism is investigated to adequately emphasize the frequency-time and channel dimensions of the feature map of an ASC system using tag representation. Experiments are conducted using the Detection and Classification of Acoustic Scenes and Events 2020 task1-a dataset. The proposed system demonstrates an overall classification accuracy of 69.3%, compared to 65.3% of the baseline.
      </p>
<p>
<strong>
        Cites:
       </strong>
       7 (
       <a href="https://scholar.google.com/scholar?cites=325715162406439618" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Kim_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=2OfwZsw11eg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2020label" class="modal fade" id="bibtex-Kim2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2020label">
        Audio Tag Representation Guided Dual Attention Network for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kim2020,
    author = "Kim, Ju-Ho and Jung, Jee-Weon and Shim, Hye-Jin and Yu, Ha-Jin",
    title = "Audio Tag Representation Guided Dual Attention Network for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "76--80",
    abstract = "Sound events are crucial to discern a specific acoustic scene, which establishes a close relationship between audio tagging and acoustic scene classification (ASC). In this study, we explore the role and application of sound events based on the ASC task and propose the use of the last hidden layer's output of an audio tagging system (<i>tag representation</i>), rather than the output itself (<i>tag vector</i>), in ASC. We hypothesize that the tag representation contains sound event information that can improve the classification accuracy of acoustic scenes. The dual attention mechanism is investigated to adequately emphasize the frequency-time and channel dimensions of the feature map of an ASC system using tag representation. Experiments are conducted using the Detection and Classification of Acoustic Scenes and Events 2020 task1-a dataset. The proposed system demonstrates an overall classification accuracy of 69.3\%, compared to 65.3\% of the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koizumi2020" style="box-shadow: none">
<div class="panel-heading" id="headingKoizumi2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Description and Discussion on DCASE2020 Challenge Task2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yuma Koizumi<sup>1</sup>, Yohei Kawaguchi<sup>2</sup>, Keisuke Imoto<sup>3</sup>, Toshiki Nakamura<sup>2</sup>, Yuki Nikaido<sup>2</sup>, Ryo Tanabe<sup>2</sup>, Harsh Purohit<sup>2</sup>, Kaori Suefusa<sup>2</sup>, Takashi Endo<sup>2</sup>, Masahiro Yasuda<sup>1</sup>, and Noboru Harada<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>NTT Corporation, <sup>2</sup>Hitachi, Ltd., <sup>3</sup>Doshisha University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          180 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koizumi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Koizumi_3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Koizumi2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koizumi2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koizumi2020" class="panel-collapse collapse" id="collapse-Koizumi2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present the task description and discuss the results of the DCASE 2020 Challenge Task 2: Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring. The goal of anomalous sound detection (ASD) is to identify whether the sound emitted from a target machine is normal or anomalous. The main challenge of this task is to detect unknown anomalous sounds under the condition that only normal sound samples have been provided as training data. We have designed this challenge as the first benchmark of ASD research, which includes a large-scale dataset, evaluation metrics, and a simple baseline system. We received 117 submissions from 40 teams, and several novel approaches have been developed as a result of this challenge. On the basis of the analysis of the evaluation results, we discuss two new approaches and their problems.
      </p>
<p>
<strong>
        Cites:
       </strong>
       180 (
       <a href="https://scholar.google.com/scholar?cites=4998326539557590839" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koizumi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Koizumi_3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koizumi2020label" class="modal fade" id="bibtex-Koizumi2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoizumi2020label">
        Description and Discussion on DCASE2020 Challenge Task2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Koizumi2020,
    author = "Koizumi, Yuma and Kawaguchi, Yohei and Imoto, Keisuke and Nakamura, Toshiki and Nikaido, Yuki and Tanabe, Ryo and Purohit, Harsh and Suefusa, Kaori and Endo, Takashi and Yasuda, Masahiro and Harada, Noboru",
    title = "Description and Discussion on DCASE2020 Challenge Task2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "81--85",
    abstract = "In this paper, we present the task description and discuss the results of the DCASE 2020 Challenge Task 2: Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring. The goal of anomalous sound detection (ASD) is to identify whether the sound emitted from a target machine is normal or anomalous. The main challenge of this task is to detect unknown anomalous sounds under the condition that only normal sound samples have been provided as training data. We have designed this challenge as the first benchmark of ASD research, which includes a large-scale dataset, evaluation metrics, and a simple baseline system. We received 117 submissions from 40 teams, and several novel approaches have been developed as a result of this challenge. On the basis of the analysis of the evaluation results, we discuss two new approaches and their problems."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koutini2020" style="box-shadow: none">
<div class="panel-heading" id="headingKoutini2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Low-Complexity Models for Acoustic Scene Classification Based on Receptive Field Regularization and Frequency Damping
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Khaled Koutini<sup>1</sup>, Florian Henkel<sup>1</sup>, Hamid Eghbal-Zadeh<sup>1,2</sup>, and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Computational Perception (CP-JKU), Johannes Kepler University Linz, <sup>2</sup>LIT Artificial Intelligence Lab, Johannes Kepler University Linz
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          14 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koutini2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Koutini_91.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=dlgbN6Leiwg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Koutini2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koutini2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koutini2020" class="panel-collapse collapse" id="collapse-Koutini2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deep Neural Networks are known to be very demanding in terms of computing and memory requirements. Due to the ever increasing use of embedded systems and mobile devices with a limited resource budget, designing low-complexity models without sacrificing too much of their predictive performance gained great importance. In this work, we investigate and compare several well-known methods to reduce the number of parameters in neural networks. We further put these into the context of a recent study on the effect of the Receptive Field (RF) on a model's performance, and empirically show that we can achieve high-performing low-complexity models by applying specific restrictions on the RFs, in combination with parameter reduction methods. Additionally, we propose a filter-damping technique for regularizing the RF of models, without altering their architecture and changing their parameter counts. We will show that incorporating this technique improves the performance in various low-complexity settings such as pruning and decomposed convolution. Using our proposed filter damping, we achieved the 1st rank at the DCASE-2020 Challenge in the task of Low-Complexity Acoustic Scene Classification.
      </p>
<p>
<strong>
        Cites:
       </strong>
       14 (
       <a href="https://scholar.google.com/scholar?cites=13195579960675675128" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koutini2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Koutini_91.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=dlgbN6Leiwg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koutini2020label" class="modal fade" id="bibtex-Koutini2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoutini2020label">
        Low-Complexity Models for Acoustic Scene Classification Based on Receptive Field Regularization and Frequency Damping
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Koutini2020,
    author = "Koutini, Khaled and Henkel, Florian and Eghbal-Zadeh, Hamid and Widmer, Gerhard",
    title = "Low-Complexity Models for Acoustic Scene Classification Based on Receptive Field Regularization and Frequency Damping",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "86--90",
    abstract = "Deep Neural Networks are known to be very demanding in terms of computing and memory requirements. Due to the ever increasing use of embedded systems and mobile devices with a limited resource budget, designing low-complexity models without sacrificing too much of their predictive performance gained great importance. In this work, we investigate and compare several well-known methods to reduce the number of parameters in neural networks. We further put these into the context of a recent study on the effect of the Receptive Field (RF) on a model's performance, and empirically show that we can achieve high-performing low-complexity models by applying specific restrictions on the RFs, in combination with parameter reduction methods. Additionally, we propose a filter-damping technique for regularizing the RF of models, without altering their architecture and changing their parameter counts. We will show that incorporating this technique improves the performance in various low-complexity settings such as pruning and decomposed convolution. Using our proposed filter damping, we achieved the 1st rank at the DCASE-2020 Challenge in the task of Low-Complexity Acoustic Scene Classification."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2020" style="box-shadow: none">
<div class="panel-heading" id="headingLiu2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Model Selection for Deep Audio Source Separation via Clustering Analysis
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Alisa Liu<sup>1</sup>, Prem Seetharaman<sup>1</sup>, and Bryan Pardo<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Northwestern University, Computer Science Department
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Liu_89.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=mG7wb3HCSo4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Liu2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2020" class="panel-collapse collapse" id="collapse-Liu2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio source separation is the process of separating a mixture into isolated sounds from individual sources. Deep learning models are the state-of-the-art in source separation, given that the mixture to be separated is similar to the mixtures the deep model was trained on. This requires the end user to know enough about each model's training to select the correct model for a given audio mixture. In this work, we propose a confidence measure that can be broadly applied to any clustering-based separation model. The proposed confidence measure does not require ground truth to estimate the quality of a separated source. We use our confidence measure to automate selection of the appropriate deep clustering model for an audio mixture. Results show that our confidence measure can reliably select the highest-performing model for an audio mixture without knowledge of the domain the audio mixture came from, enabling automatic selection of deep models.
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="https://scholar.google.com/scholar?cites=15399368305879289449" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Liu_89.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=mG7wb3HCSo4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2020label" class="modal fade" id="bibtex-Liu2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2020label">
        Model Selection for Deep Audio Source Separation via Clustering Analysis
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Liu2020,
    author = "Liu, Alisa and Seetharaman, Prem and Pardo, Bryan",
    title = "Model Selection for Deep Audio Source Separation via Clustering Analysis",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "91--95",
    abstract = "Audio source separation is the process of separating a mixture into isolated sounds from individual sources. Deep learning models are the state-of-the-art in source separation, given that the mixture to be separated is similar to the mixtures the deep model was trained on. This requires the end user to know enough about each model's training to select the correct model for a given audio mixture. In this work, we propose a confidence measure that can be broadly applied to any clustering-based separation model. The proposed confidence measure does not require ground truth to estimate the quality of a separated source. We use our confidence measure to automate selection of the appropriate deep clustering model for an audio mixture. Results show that our confidence measure can reliably select the highest-performing model for an audio mixture without knowledge of the domain the audio mixture came from, enabling automatic selection of deep models."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lopez2020" style="box-shadow: none">
<div class="panel-heading" id="headingLopez2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Speaker Recognition Approach to Anomaly Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jose A. Lopez<sup>1</sup>, Hong Lu<sup>1</sup>, Paulo Lopez-Meyer<sup>1</sup>, Lama Nachman<sup>1</sup>, Georg Stemmer<sup>1</sup>, and Jonathan Huang<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Intel Corp, Intel Labs, <sup>2</sup>Work done at Intel
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lopez2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Lopez_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=zMiMpfLKgJQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Lopez2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lopez2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lopez2020" class="panel-collapse collapse" id="collapse-Lopez2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present our submission to the DCASE 2020 Challenge Task 2, which aims to promote research in anomalous sound detection. We found that a speaker recognition approach enables the use of all the training data, even from different machine types, to detect anomalies in specific machines. Using this approach, we obtained good results for 5 out of 6 machines on the development data. We also discuss the modifications needed to surpass the baseline score for the remaining (ToyConveyor) machine which we found to be particularly difficult. On the challenge evaluation test data, our results were skewed by the system's uninspiring performance on the Toy machines. However, we placed 18th in the challenge due to our results on the industrial machine data where we reached the top 5 in team pAUC scores.
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=16375503096180806615" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lopez2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Lopez_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=zMiMpfLKgJQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lopez2020label" class="modal fade" id="bibtex-Lopez2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLopez2020label">
        A Speaker Recognition Approach to Anomaly Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lopez2020,
    author = "Lopez, Jose A. and Lu, Hong and Lopez-Meyer, Paulo and Nachman, Lama and Stemmer, Georg and Huang, Jonathan",
    title = "A Speaker Recognition Approach to Anomaly Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "96--99",
    abstract = "We present our submission to the DCASE 2020 Challenge Task 2, which aims to promote research in anomalous sound detection. We found that a speaker recognition approach enables the use of all the training data, even from different machine types, to detect anomalies in specific machines. Using this approach, we obtained good results for 5 out of 6 machines on the development data. We also discuss the modifications needed to surpass the baseline score for the remaining (ToyConveyor) machine which we found to be particularly difficult. On the challenge evaluation test data, our results were skewed by the system's uninspiring performance on the Toy machines. However, we placed 18th in the challenge due to our results on the industrial machine data where we reached the top 5 in team pAUC scores."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Miyazaki2020" style="box-shadow: none">
<div class="panel-heading" id="headingMiyazaki2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Conformer-Based Sound Event Detection with Semi-Supervised Learning and Data Augmentation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Koichi Miyazaki<sup>1</sup>, Tatsuya Komatsu<sup>2</sup>, Tomoki Hayashi<sup>1,3</sup>, Shinji Watanabe<sup>4</sup>, Tomoki Toda<sup>1</sup>, and Kazuya Takeda<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nagoya University, <sup>2</sup>LINE Corporation, <sup>3</sup>Human Dataware Lab. Co., Ltd., <sup>4</sup>Johns Hopkins University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          50 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Miyazaki2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Miyazaki_92.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=ExQOJaAWcSY" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Miyazaki2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Miyazaki2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Miyazaki2020" class="panel-collapse collapse" id="collapse-Miyazaki2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents a Conformer-based sound event detection (SED) method, which uses semi-supervised learning and data augmentation. The proposed method employs Conformer, a convolution-augmented Transformer that is able to exploit local features of audio data more effectively using CNNs, while global features are captured with Transformer. For SED, both global information on background sound and local information on foreground sound events are essential for modeling and identifying various types of sounds. Since Conformer can capture both global and local features using a single architecture, our proposed method is able to model various characteristics of sound events effectively. In addition to this novel architecture, we further improve performance by utilizing a semi-supervised learning technique, data augmentation, and post-processing optimized for each sound event class. We demonstrate the performance of our proposed method through experimental evaluation using the DCASE2020 Task4 dataset. Our experimental results show that the proposed method can achieve an event-based macro F1 score of 50.6% when using the validation set, significantly outperforming the baseline method score (34.8%). Our system achieved a score of 51.1% when using the DCASE2020 challenge’s evaluation set, the best results among the 72 submissions.
      </p>
<p>
<strong>
        Cites:
       </strong>
       50 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Miyazaki2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Miyazaki_92.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=ExQOJaAWcSY" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Miyazaki2020label" class="modal fade" id="bibtex-Miyazaki2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMiyazaki2020label">
        Conformer-Based Sound Event Detection with Semi-Supervised Learning and Data Augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Miyazaki2020,
    author = "Miyazaki, Koichi and Komatsu, Tatsuya and Hayashi, Tomoki and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya",
    title = "Conformer-Based Sound Event Detection with Semi-Supervised Learning and Data Augmentation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "100--104",
    abstract = "This paper presents a Conformer-based sound event detection (SED) method, which uses semi-supervised learning and data augmentation. The proposed method employs Conformer, a convolution-augmented Transformer that is able to exploit local features of audio data more effectively using CNNs, while global features are captured with Transformer. For SED, both global information on background sound and local information on foreground sound events are essential for modeling and identifying various types of sounds. Since Conformer can capture both global and local features using a single architecture, our proposed method is able to model various characteristics of sound events effectively. In addition to this novel architecture, we further improve performance by utilizing a semi-supervised learning technique, data augmentation, and post-processing optimized for each sound event class. We demonstrate the performance of our proposed method through experimental evaluation using the DCASE2020 Task4 dataset. Our experimental results show that the proposed method can achieve an event-based macro F1 score of 50.6\% when using the validation set, significantly outperforming the baseline method score (34.8\%). Our system achieved a score of 51.1\% when using the DCASE2020 challenge’s evaluation set, the best results among the 72 submissions."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Naccari2020" style="box-shadow: none">
<div class="panel-heading" id="headingNaccari2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Embedded Acoustic Scene Classification for Low Power Microcontroller Devices
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Filippo Naccari<sup>1</sup>, Ivana Guarneri<sup>1</sup>, Salvatore Curti<sup>1</sup>, and Alberto Amilcare Savi<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>STMicroelectronics
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Naccari2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Naccari_44.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=7wY0Qmy3BF8" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Naccari2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Naccari2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Naccari2020" class="panel-collapse collapse" id="collapse-Naccari2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automatic sound understanding tasks have been very popular within research community during the last years. The success of deep learning data driven applications in many signal understanding fields is now moving from centralized cloud services to the edge of the network, close to the nodes where raw data are generated from different type of sensors. In this paper we show a complete workflow for a context awareness acoustic scene classification (ASC) application and its effective embedding process into an ultra-low power microcontroller (MCU). It can widen the capabilities of edge AI applications, from environmental and inertial sensors up to acoustic signals, which require more bandwidth and generate more data. In the paper the entire workflow of such development is described in terms of dataset collection, selection and annotations, acoustic features representation, neural net modeling and optimization as well as the efficient embedding step of the whole application into the target low power 32-bit microcontroller device. Moreover, the overall accuracy of the proposed model and the capability to be real time executed together with an audio feature extraction process shows that such kind of audio understanding application can be efficiently deployed on power constrained battery-operated devices.
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="https://scholar.google.com/scholar?cites=7662616244728218564" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Naccari2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Naccari_44.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=7wY0Qmy3BF8" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Naccari2020label" class="modal fade" id="bibtex-Naccari2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNaccari2020label">
        Embedded Acoustic Scene Classification for Low Power Microcontroller Devices
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Naccari2020,
    author = "Naccari, Filippo and Guarneri, Ivana and Curti, Salvatore and Savi, Alberto Amilcare",
    title = "Embedded Acoustic Scene Classification for Low Power Microcontroller Devices",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "105--109",
    abstract = "Automatic sound understanding tasks have been very popular within research community during the last years. The success of deep learning data driven applications in many signal understanding fields is now moving from centralized cloud services to the edge of the network, close to the nodes where raw data are generated from different type of sensors. In this paper we show a complete workflow for a context awareness acoustic scene classification (ASC) application and its effective embedding process into an ultra-low power microcontroller (MCU). It can widen the capabilities of edge AI applications, from environmental and inertial sensors up to acoustic signals, which require more bandwidth and generate more data. In the paper the entire workflow of such development is described in terms of dataset collection, selection and annotations, acoustic features representation, neural net modeling and optimization as well as the efficient embedding step of the whole application into the target low power 32-bit microcontroller device. Moreover, the overall accuracy of the proposed model and the capability to be real time executed together with an audio feature extraction process shows that such kind of audio understanding application can be efficiently deployed on power constrained battery-operated devices."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2020a" style="box-shadow: none">
<div class="panel-heading" id="headingNguyen2020a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Temporal Sub-Sampling of Audio Feature Sequences for Automated Audio Captioning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Khoa Nguyen<sup>1</sup>, Konstantinos Drossos<sup>2</sup>, and Tuomas Virtanen<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>3D Media Group, Tampere University, <sup>2</sup>Audio Research Group, Tampere University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Nguyen_45.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=oeySQrvo4-4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Nguyen2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2020a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2020a" class="panel-collapse collapse" id="collapse-Nguyen2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio captioning is the task of automatically creating a textual description for the contents of a general audio signal. Typical audio captioning methods rely on deep neural networks (DNNs), where the target of the DNN is to map the input audio sequence to an output sequence of words, i.e. the caption. Though, the length of the textual description is considerably less than the length of the audio signal, for example 10 words versus some thousands of audio feature vectors. This clearly indicates that an output word corresponds to multiple input feature vectors. In this work we present an approach that focuses on explicitly taking advantage of this difference of lengths between sequences, by applying a temporal sub-sampling to the audio input sequence. We employ the baseline method of the DCASE 2020 audio captioning task, and we apply temporal sub-sampling between the RNNs of the encoder. We evaluate the benefit of our approach by employing the freely available dataset Clotho and comparing the performance of our method with the performance of the DCASE 2020 baseline method. Our results show an improvement to all considered metrics.
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=18338491956785470602" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Nguyen_45.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=oeySQrvo4-4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2020alabel" class="modal fade" id="bibtex-Nguyen2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2020alabel">
        Temporal Sub-Sampling of Audio Feature Sequences for Automated Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nguyen2020a,
    author = "Nguyen, Khoa and Drossos, Konstantinos and Virtanen, Tuomas",
    title = "Temporal Sub-Sampling of Audio Feature Sequences for Automated Audio Captioning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "110--114",
    abstract = "Audio captioning is the task of automatically creating a textual description for the contents of a general audio signal. Typical audio captioning methods rely on deep neural networks (DNNs), where the target of the DNN is to map the input audio sequence to an output sequence of words, i.e. the caption. Though, the length of the textual description is considerably less than the length of the audio signal, for example 10 words versus some thousands of audio feature vectors. This clearly indicates that an output word corresponds to multiple input feature vectors. In this work we present an approach that focuses on explicitly taking advantage of this difference of lengths between sequences, by applying a temporal sub-sampling to the audio input sequence. We employ the baseline method of the DCASE 2020 audio captioning task, and we apply temporal sub-sampling between the RNNs of the encoder. We evaluate the benefit of our approach by employing the freely available dataset Clotho and comparing the performance of our method with the performance of the DCASE 2020 baseline method. Our results show an improvement to all considered metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2020b" style="box-shadow: none">
<div class="panel-heading" id="headingNguyen2020b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         On the Effectiveness of Spatial and Multi-Channel Features for Multi-Channel Polyphonic Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Thi Ngoc Tho Nguyen<sup>1</sup>, Douglas L. Jones<sup>2</sup>, and Woon Seng Gan<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nanyang Technological University, School of Electrical and Electronic Engineering, <sup>2</sup>University of Illinois at Urbana-Champaign, Dept. of Electrical and Computer Engineering
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Nguyen_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=oCjfxE1cIlU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Nguyen2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2020b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2020b" class="panel-collapse collapse" id="collapse-Nguyen2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Multi-channel log-mel spectrograms and spatial features such as generalized cross-correlation with phase transform have been demonstrated to be useful for multi-channel polyphonic sound event detection for static-source cases. The multi-channel log-mel spectrograms and spatial features are often stacked along the channel dimension similar to RGB images before being passed to a convolutional model to detect sound events better in multi-source cases. In this paper, we investigate the usage of multi-channel log-mel spectrograms and spatial features for polyphonic sound event detection in both static and dynamic-source cases using DCASE2019 and DCASE2020 sound event localization and detection datasets. Our experimental results show that multi-channel log-mel spectrogram and spatial features are more useful for static-source cases than for dynamic-source cases. The best use of multi-channel audio inputs for polyphonic sound event detection in both static and dynamic scenarios is to train a model that use all the single-channel log-mel spectrograms separately as input features and the final prediction during the inference stage is obtained by taking the arithmetic mean of the model's output predictions of all the input channels.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="https://scholar.google.com/scholar?cites=126503454132864164" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Nguyen_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=oCjfxE1cIlU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2020blabel" class="modal fade" id="bibtex-Nguyen2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2020blabel">
        On the Effectiveness of Spatial and Multi-Channel Features for Multi-Channel Polyphonic Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nguyen2020b,
    author = "Nguyen, Thi Ngoc Tho and Jones, Douglas L. and Gan, Woon Seng",
    title = "On the Effectiveness of Spatial and Multi-Channel Features for Multi-Channel Polyphonic Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "115--119",
    abstract = "Multi-channel log-mel spectrograms and spatial features such as generalized cross-correlation with phase transform have been demonstrated to be useful for multi-channel polyphonic sound event detection for static-source cases. The multi-channel log-mel spectrograms and spatial features are often stacked along the channel dimension similar to RGB images before being passed to a convolutional model to detect sound events better in multi-source cases. In this paper, we investigate the usage of multi-channel log-mel spectrograms and spatial features for polyphonic sound event detection in both static and dynamic-source cases using DCASE2019 and DCASE2020 sound event localization and detection datasets. Our experimental results show that multi-channel log-mel spectrogram and spatial features are more useful for static-source cases than for dynamic-source cases. The best use of multi-channel audio inputs for polyphonic sound event detection in both static and dynamic scenarios is to train a model that use all the single-channel log-mel spectrograms separately as input features and the final prediction during the inference stage is obtained by taking the arithmetic mean of the model's output predictions of all the input channels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2020c" style="box-shadow: none">
<div class="panel-heading" id="headingNguyen2020c" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Ensemble of Sequence Matching Networks for Dynamic Sound Event Localization, Detection, and Tracking
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Thi Ngoc Tho Nguyen<sup>1</sup>, Douglas L. Jones<sup>2</sup>, and Woon Seng Gan<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nanyang Technological University, School of Electrical and Electronic Engineering, <sup>2</sup>University of Illinois at Urbana-Champaign, Dept. of Electrical and Computer Engineering
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          9 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2020c" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Nguyen_55.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=WWzieXXh12U" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Nguyen2020c" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2020c" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2020c" class="panel-collapse collapse" id="collapse-Nguyen2020c" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event localization and detection consists of two subtasks which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different sound classes, direction-of-arrival estimation uses magnitude or phase differences between microphones to estimate source directions. Therefore, it is often difficult to jointly train two subtasks simultaneously. Our previous sequence matching approach solved sound event detection and direction-of-arrival separately and trained a convolutional recurrent neural network to associate the sound classes with the directions-of-arrival using onsets and offsets of the sound events. This approach achieved better performance than other state-of-the-art networks such as the SELDnet, and the two-stage networks for static sources. In order to estimate directions-of-arrival of moving sound sources with higher required spatial resolutions than those of static sources, we propose to separate the directional estimates into azimuth and elevation estimates before passing them to the sequence matching network. Experimental results on the new DCASE dataset for sound event localization, detection, and tracking of multiple moving sound sources show that the sequence matching network with separated azimuth and elevation inputs outperforms the sequence matching network with joint azimuth and elevation input. We combined several sequence matching networks with the new proposed directional inputs into an ensemble to boost the system performance. Our proposed ensemble achieves localization error of 9.3 degrees, localization recall of 90%, and ranked 2<sup><i>nd</i></sup> in the team category of the DCASE2020 sound event localization and detection challenge.
      </p>
<p>
<strong>
        Cites:
       </strong>
       9 (
       <a href="https://scholar.google.com/scholar?cites=4229276071961311922" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2020c" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Nguyen_55.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=WWzieXXh12U" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2020clabel" class="modal fade" id="bibtex-Nguyen2020c" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2020clabel">
        Ensemble of Sequence Matching Networks for Dynamic Sound Event Localization, Detection, and Tracking
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nguyen2020c,
    author = "Nguyen, Thi Ngoc Tho and Jones, Douglas L. and Gan, Woon Seng",
    title = "Ensemble of Sequence Matching Networks for Dynamic Sound Event Localization, Detection, and Tracking",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "120--124",
    abstract = "Sound event localization and detection consists of two subtasks which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different sound classes, direction-of-arrival estimation uses magnitude or phase differences between microphones to estimate source directions. Therefore, it is often difficult to jointly train two subtasks simultaneously. Our previous sequence matching approach solved sound event detection and direction-of-arrival separately and trained a convolutional recurrent neural network to associate the sound classes with the directions-of-arrival using onsets and offsets of the sound events. This approach achieved better performance than other state-of-the-art networks such as the SELDnet, and the two-stage networks for static sources. In order to estimate directions-of-arrival of moving sound sources with higher required spatial resolutions than those of static sources, we propose to separate the directional estimates into azimuth and elevation estimates before passing them to the sequence matching network. Experimental results on the new DCASE dataset for sound event localization, detection, and tracking of multiple moving sound sources show that the sequence matching network with separated azimuth and elevation inputs outperforms the sequence matching network with joint azimuth and elevation input. We combined several sequence matching networks with the new proposed directional inputs into an ensemble to boost the system performance. Our proposed ensemble achieves localization error of 9.3 degrees, localization recall of 90\%, and ranked 2<sup><i>nd</i></sup> in the team category of the DCASE2020 sound event localization and detection challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Okamoto2020" style="box-shadow: none">
<div class="panel-heading" id="headingOkamoto2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         RWCP-SSD-Onomatopoeia: Onomatopoeic Word Dataset for Environmental Sound Synthesis
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yuki Okamoto<sup>1</sup>, Keisuke Imoto<sup>2,1</sup>, Shinnosuke Takamichi<sup>3</sup>, Ryosuke Yamanishi<sup>1,4</sup>, Takahiro Fukumori<sup>1</sup>, and Yoichi Yamashita<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Ritsumeikan University, <sup>2</sup>Doshisha University, <sup>3</sup>The University of Tokyo, <sup>4</sup>Kansai University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Okamoto2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Okamoto_21.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=z0AYQIHl7-Q" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Okamoto2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Okamoto2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Okamoto2020" class="panel-collapse collapse" id="collapse-Okamoto2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Environmental sound synthesis is a technique for generating a natural environmental sound. Conventional work on environmental sound synthesis using sound event labels can not control synthesized sounds finely, for example, the pitch and timbre. We have considered that onomatopoeic words can be used for environmental sound synthesis. Onomatopoeic words are effective for explaining the feature of sounds. We believe that using onomatopoeic words enable us to control the fine time-frequency structure of synthesized sounds. However, there is no dataset available for environmental sound synthesis using onomatopoeic words. In this paper, we thus present RWCP-SSD-Onomatopoeia, a dataset consisting of 155,568 onomatopoeic words pairing with audio samples for environmental sound synthesis. We also have collected self-reported confidence scores and others-reported acceptance scores of onomatopoeic words, which help to select the words.
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="https://scholar.google.com/scholar?cites=15775129094973411128" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Okamoto2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Okamoto_21.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=z0AYQIHl7-Q" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Okamoto2020label" class="modal fade" id="bibtex-Okamoto2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexOkamoto2020label">
        RWCP-SSD-Onomatopoeia: Onomatopoeic Word Dataset for Environmental Sound Synthesis
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Okamoto2020,
    author = "Okamoto, Yuki and Imoto, Keisuke and Takamichi, Shinnosuke and Yamanishi, Ryosuke and Fukumori, Takahiro and Yamashita, Yoichi",
    title = "RWCP-SSD-Onomatopoeia: Onomatopoeic Word Dataset for Environmental Sound Synthesis",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "125--129",
    abstract = "Environmental sound synthesis is a technique for generating a natural environmental sound. Conventional work on environmental sound synthesis using sound event labels can not control synthesized sounds finely, for example, the pitch and timbre. We have considered that onomatopoeic words can be used for environmental sound synthesis. Onomatopoeic words are effective for explaining the feature of sounds. We believe that using onomatopoeic words enable us to control the fine time-frequency structure of synthesized sounds. However, there is no dataset available for environmental sound synthesis using onomatopoeic words. In this paper, we thus present RWCP-SSD-Onomatopoeia, a dataset consisting of 155,568 onomatopoeic words pairing with audio samples for environmental sound synthesis. We also have collected self-reported confidence scores and others-reported acceptance scores of onomatopoeic words, which help to select the words."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ooi2020" style="box-shadow: none">
<div class="panel-heading" id="headingOoi2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Ensemble of Pruned Low-Complexity Models for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kenneth Ooi<sup>1</sup>, Santi Peksi<sup>1</sup>, and Woon-Seng Gan<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nanyang Technological University, School of Electrical and Electronic Engineering
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          5 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ooi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Ooi_24.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=CiaBpajZLDU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Ooi2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ooi2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ooi2020" class="panel-collapse collapse" id="collapse-Ooi2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       For the DCASE 2020 Challenge, the focus of Task 1B is to develop low-complexity models for classification of 3 different types of acoustic scenes, which have potential applications in resource-scarce edge devices deployed in a large-scale acoustic network. In this paper, we present the training methodology for our submissions for the challenge, with the best-performing system consisting of an ensemble of VGGNet- and InceptionNet-based lightweight classification models. The subsystems in the ensemble classifier were pruned by setting low-magnitude weights periodically to zero with a polynomial decay schedule to achieve an 80% reduction in individual subsystem size. The resultant ensemble classifier outperformed the baseline model on the validation set over 10 runs and had 119758 non-zero parameters taking up 468KB of memory. This shows the efficacy of the pruning technique used. We also performed experiments to compare the performance of various data augmentation schemes, input feature representations, and model architectures in our training methodology. No external data was used, and source code for the submission can be found at https://github.com/kenowr/DCASE-2020-Task-1B.
      </p>
<p>
<strong>
        Cites:
       </strong>
       5 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ooi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Ooi_24.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=CiaBpajZLDU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ooi2020label" class="modal fade" id="bibtex-Ooi2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexOoi2020label">
        Ensemble of Pruned Low-Complexity Models for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ooi2020,
    author = "Ooi, Kenneth and Peksi, Santi and Gan, Woon-Seng",
    title = "Ensemble of Pruned Low-Complexity Models for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "130--134",
    abstract = "For the DCASE 2020 Challenge, the focus of Task 1B is to develop low-complexity models for classification of 3 different types of acoustic scenes, which have potential applications in resource-scarce edge devices deployed in a large-scale acoustic network. In this paper, we present the training methodology for our submissions for the challenge, with the best-performing system consisting of an ensemble of VGGNet- and InceptionNet-based lightweight classification models. The subsystems in the ensemble classifier were pruned by setting low-magnitude weights periodically to zero with a polynomial decay schedule to achieve an 80\% reduction in individual subsystem size. The resultant ensemble classifier outperformed the baseline model on the validation set over 10 runs and had 119758 non-zero parameters taking up 468KB of memory. This shows the efficacy of the pruning technique used. We also performed experiments to compare the performance of various data augmentation schemes, input feature representations, and model architectures in our training methodology. No external data was used, and source code for the submission can be found at https://github.com/kenowr/DCASE-2020-Task-1B."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pajusco2020" style="box-shadow: none">
<div class="panel-heading" id="headingPajusco2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Lightweight Convolutional Neural Networks on Binaural Waveforms for Low Complexity Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Nicolas Pajusco<sup>1</sup>, Richard Huang<sup>1</sup>, and Nicolas Farrugia<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IMT Atlantique, Lab-STICC, Department of Electronics
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          6 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pajusco2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Pajusco_63.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=rBSkp6mIEmE" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Pajusco2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pajusco2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pajusco2020" class="panel-collapse collapse" id="collapse-Pajusco2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we investigate the feasibility of training low complexity convolutional neural networks directly from waveforms. While the vast majority of proposed approaches perform fixed feature extraction based on time-frequency representations such as spectrograms, we propose to fully exploit the information in waveforms directly and to minimize the model size. To do so, we train one dimensional Convolutional Neural Networks (1D-CNN) on raw, subsampled binaural audio waveforms, thus exploiting phase information within and across the two input channels. In addition, our approach relies heavily on data augmentation in the temporal domain. Finally, we apply iterative structured parameter pruning to remove the least important convolutional kernels, and perform weight quantization in floating point half precision. We apply this approach on the TAU Urban Acoustic Scenes 2020 3class dataset, with two network architectures : a 1D-CNN based on VGG-like blocks, as well as a ResNet architecture with 1D convolutions, and compare our results with the baseline model from the DCASE 2020 challenge, task 1 subtask B. We report four models that constitute our submission to the DCASE 2020 challenge, task 1 subtask B. Our results show that we can train, prune and quantify a small VGG model to make it 20 times smaller than the 500 KB challenge limit with an accuracy at baseline level (87.6 %), as well as a larger model achieving 91 % of accuracy while being 8 times smaller than the challenge limit. ResNets could be successfully trained, pruned and quantify in order to be below the 500 KB limit, achieving up to 91.2% accuracy. We also report the stability of these results according to data augmentation and monoraul versus binaural inputs.
      </p>
<p>
<strong>
        Cites:
       </strong>
       6 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pajusco2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Pajusco_63.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=rBSkp6mIEmE" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pajusco2020label" class="modal fade" id="bibtex-Pajusco2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPajusco2020label">
        Lightweight Convolutional Neural Networks on Binaural Waveforms for Low Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Pajusco2020,
    author = "Pajusco, Nicolas and Huang, Richard and Farrugia, Nicolas",
    title = "Lightweight Convolutional Neural Networks on Binaural Waveforms for Low Complexity Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "135--139",
    abstract = "In this paper, we investigate the feasibility of training low complexity convolutional neural networks directly from waveforms. While the vast majority of proposed approaches perform fixed feature extraction based on time-frequency representations such as spectrograms, we propose to fully exploit the information in waveforms directly and to minimize the model size. To do so, we train one dimensional Convolutional Neural Networks (1D-CNN) on raw, subsampled binaural audio waveforms, thus exploiting phase information within and across the two input channels. In addition, our approach relies heavily on data augmentation in the temporal domain. Finally, we apply iterative structured parameter pruning to remove the least important convolutional kernels, and perform weight quantization in floating point half precision. We apply this approach on the TAU Urban Acoustic Scenes 2020 3class dataset, with two network architectures : a 1D-CNN based on VGG-like blocks, as well as a ResNet architecture with 1D convolutions, and compare our results with the baseline model from the DCASE 2020 challenge, task 1 subtask B. We report four models that constitute our submission to the DCASE 2020 challenge, task 1 subtask B. Our results show that we can train, prune and quantify a small VGG model to make it 20 times smaller than the 500 KB challenge limit with an accuracy at baseline level (87.6 \%), as well as a larger model achieving 91 \% of accuracy while being 8 times smaller than the challenge limit. ResNets could be successfully trained, pruned and quantify in order to be below the 500 KB limit, achieving up to 91.2\% accuracy. We also report the stability of these results according to data augmentation and monoraul versus binaural inputs."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2020" style="box-shadow: none">
<div class="panel-heading" id="headingPark2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE 2020 Task2: Anomalous Sound Detection using Relevant Spectral Feature and Focusing Techniques in the Unsupervised Learning Scenario
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jihwan Park<sup>1</sup>, and Sooyeon Yoo<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Advanced Robot Research Laboratory, LG Electronics
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Park_37.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=SsLww2REjho" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Park2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2020" class="panel-collapse collapse" id="collapse-Park2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose an improved version of the anomalous sound detection (ASD) system for noisy and reverberant conditions, which was submitted to DCASE 2020 Challenge Task2. The improved system consists of three phases: feature extraction, autoencoder (AE) model, and focusing techniques. In the feature extraction phase, we used spectrograms instead of log-mel energies for more effective distinction of normal and abnormal machine sounds, and validated this feature for the baseline autoencoder model and interpolation DNN (IDNN). We also applied the focusing techniques in both train and evaluation phases, which focuses on machine-adaptive ranges of reconstructed errors for performance improvements. Through experiments, we found that our proposed ASD system outperforms baseline methods under the unsupervised learning scenario. The performance improvement was especially remarkable for non-stationary sounds; above 95% of AUC score was achieved for slider and valve sounds with the proposed system.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Park_37.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=SsLww2REjho" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2020label" class="modal fade" id="bibtex-Park2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2020label">
        DCASE 2020 Task2: Anomalous Sound Detection using Relevant Spectral Feature and Focusing Techniques in the Unsupervised Learning Scenario
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Park2020,
    author = "Park, Jihwan and Yoo, Sooyeon",
    title = "DCASE 2020 Task2: Anomalous Sound Detection using Relevant Spectral Feature and Focusing Techniques in the Unsupervised Learning Scenario",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "140--144",
    abstract = "In this paper, we propose an improved version of the anomalous sound detection (ASD) system for noisy and reverberant conditions, which was submitted to DCASE 2020 Challenge Task2. The improved system consists of three phases: feature extraction, autoencoder (AE) model, and focusing techniques. In the feature extraction phase, we used spectrograms instead of log-mel energies for more effective distinction of normal and abnormal machine sounds, and validated this feature for the baseline autoencoder model and interpolation DNN (IDNN). We also applied the focusing techniques in both train and evaluation phases, which focuses on machine-adaptive ranges of reconstructed errors for performance improvements. Through experiments, we found that our proposed ASD system outperforms baseline methods under the unsupervised learning scenario. The performance improvement was especially remarkable for non-stationary sounds; above 95\% of AUC score was achieved for slider and valve sounds with the proposed system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Perez-Castanos2020a" style="box-shadow: none">
<div class="panel-heading" id="headingPerez-Castanos2020a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Anomalous Sound Detection using Unsupervised and Semi-Supervised Autoencoders and Gammatone Audio Representation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sergi Perez-Castanos<sup>1</sup>, Javier Naranjo-Alcazar<sup>1</sup>, Pedro Zuccarello<sup>1</sup>, and Maximo Cobos<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Visualfy, <sup>2</sup>Universitat de València
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          12 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Perez-Castanos2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Perez-Castanos_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=KvEO8EQH1NU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Perez-Castanos2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Perez-Castanos2020a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Perez-Castanos2020a" class="panel-collapse collapse" id="collapse-Perez-Castanos2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Anomalous sound detection (ASD) is, nowadays, one of the topical subjects in machine listening discipline. Unsupervised detection is attracting a lot of interest due to its immediate applicability in many fields. For example, related to industrial processes, the early detection of malfunctions or damage in machines can mean great savings and an improvement in the efficiency of industrial processes. This problem can be solved with an unsupervised ASD solution since industrial machines will not be damaged simply by having this audio data in the training stage. This paper proposes a novel framework based on convolutional autoencoders (both unsupervised and semi-supervised) and a Gammatone-based representation of the audio. The results obtained by these architectures substantially exceed the results presented as a baseline.
      </p>
<p>
<strong>
        Cites:
       </strong>
       12 (
       <a href="https://scholar.google.com/scholar?cites=1912252600912492721" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Perez-Castanos2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Perez-Castanos_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=KvEO8EQH1NU" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Perez-Castanos2020alabel" class="modal fade" id="bibtex-Perez-Castanos2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPerez-Castanos2020alabel">
        Anomalous Sound Detection using Unsupervised and Semi-Supervised Autoencoders and Gammatone Audio Representation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Perez-Castanos2020a,
    author = "Perez-Castanos, Sergi and Naranjo-Alcazar, Javier and Zuccarello, Pedro and Cobos, Maximo",
    title = "Anomalous Sound Detection using Unsupervised and Semi-Supervised Autoencoders and Gammatone Audio Representation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "145--149",
    abstract = "Anomalous sound detection (ASD) is, nowadays, one of the topical subjects in machine listening discipline. Unsupervised detection is attracting a lot of interest due to its immediate applicability in many fields. For example, related to industrial processes, the early detection of malfunctions or damage in machines can mean great savings and an improvement in the efficiency of industrial processes. This problem can be solved with an unsupervised ASD solution since industrial machines will not be damaged simply by having this audio data in the training stage. This paper proposes a novel framework based on convolutional autoencoders (both unsupervised and semi-supervised) and a Gammatone-based representation of the audio. The results obtained by these architectures substantially exceed the results presented as a baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Perez-Castanos2020b" style="box-shadow: none">
<div class="panel-heading" id="headingPerez-Castanos2020b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Listen Carefully and Tell: An Audio Captioning System Based on Residual Learning and Gammatone Audio Representation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sergi Perez-Castanos, Javier Naranjo-Alcazar<sup>1</sup>, Pedro Zuccarello, and Maximo Cobos<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universitat de València
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          12 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Perez-Castanos2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Perez-Castanos_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=xKO_9UfHFRk" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Perez-Castanos2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Perez-Castanos2020b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Perez-Castanos2020b" class="panel-collapse collapse" id="collapse-Perez-Castanos2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automated audio captioning is machine listening task whose goal is to describe an audio using free text. An automated audio captioning system has to be implemented as it accepts an audio as input and outputs as textual description, that is, the caption of the signal. This task can be useful in many applications such as automatic content description or machine-to-machine interaction. In this work, an automatic audio captioning based on residual learning on the encoder phase is proposed. The encoder phase is implemented via different Residual Networks configurations. The decoder phase (create the caption) is run using recurrent layers plus attention mechanism. The audio representation chosen has been Gammatone. Results show that the framework proposed in this work surpass the baseline system in challenge results.
      </p>
<p>
<strong>
        Cites:
       </strong>
       12 (
       <a href="https://scholar.google.com/scholar?cites=13634079940925553619" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Perez-Castanos2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Perez-Castanos_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=xKO_9UfHFRk" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Perez-Castanos2020blabel" class="modal fade" id="bibtex-Perez-Castanos2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPerez-Castanos2020blabel">
        Listen Carefully and Tell: An Audio Captioning System Based on Residual Learning and Gammatone Audio Representation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Perez-Castanos2020b,
    author = "Perez-Castanos, Sergi and Naranjo-Alcazar, Javier and Zuccarello, Pedro and Cobos, Maximo",
    title = "Listen Carefully and Tell: An Audio Captioning System Based on Residual Learning and Gammatone Audio Representation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "150--154",
    abstract = "Automated audio captioning is machine listening task whose goal is to describe an audio using free text. An automated audio captioning system has to be implemented as it accepts an audio as input and outputs as textual description, that is, the caption of the signal. This task can be useful in many applications such as automatic content description or machine-to-machine interaction. In this work, an automatic audio captioning based on residual learning on the encoder phase is proposed. The encoder phase is implemented via different Residual Networks configurations. The decoder phase (create the caption) is run using recurrent layers plus attention mechanism. The audio representation chosen has been Gammatone. Results show that the framework proposed in this work surpass the baseline system in challenge results."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Perez-Lopez2020" style="box-shadow: none">
<div class="panel-heading" id="headingPerez-Lopez2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Papafil: A Low Complexity Sound Event Localization and Detection Method with Parametric Particle Filtering and Gradient Boosting
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Andrés Pérez-López<sup>1,2</sup>, and Rafael Ibáñez-Usach<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Music Technology Group, Universitat Pompeu Fabra, <sup>2</sup>Eurecat, Centre Tecnòlogic de Catalunya, <sup>3</sup>STRATIO
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Perez-Lopez2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Perez-Lopez_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=FMvTrxfeZHk" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Perez-Lopez2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Perez-Lopez2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Perez-Lopez2020" class="panel-collapse collapse" id="collapse-Perez-Lopez2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The present article describes the architecture of a system submitted to the DCASE 2020 Challenge - Task 3: Sound Event Localization and Detection. The proposed method conforms a low complexity solution for the task. It is based on four building blocks: a spatial parametric analysis to find single-source spectrogram bins, a particle tracker to estimate trajectories and temporal activities, a spatial filter, and a single-class classifier implemented with a gradient boosting machine . Results from the development dataset show that the proposed method outperforms a deep learning baseline in three out of the four evaluation metrics considered in the challenge, and obtains an overall score almost ten points above the baseline.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="https://scholar.google.com/scholar?cites=9498880574518738807" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Perez-Lopez2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Perez-Lopez_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=FMvTrxfeZHk" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Perez-Lopez2020label" class="modal fade" id="bibtex-Perez-Lopez2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPerez-Lopez2020label">
        Papafil: A Low Complexity Sound Event Localization and Detection Method with Parametric Particle Filtering and Gradient Boosting
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Perez-Lopez2020,
    author = "Pérez-López, Andrés and Ibáñez-Usach, Rafael",
    title = "Papafil: A Low Complexity Sound Event Localization and Detection Method with Parametric Particle Filtering and Gradient Boosting",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "155--159",
    abstract = "The present article describes the architecture of a system submitted to the DCASE 2020 Challenge - Task 3: Sound Event Localization and Detection. The proposed method conforms a low complexity solution for the task. It is based on four building blocks: a spatial parametric analysis to find single-source spectrogram bins, a particle tracker to estimate trajectories and temporal activities, a spatial filter, and a single-class classifier implemented with a gradient boosting machine . Results from the development dataset show that the proposed method outperforms a deep learning baseline in three out of the four evaluation metrics considered in the challenge, and obtains an overall score almost ten points above the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Phan2020" style="box-shadow: none">
<div class="panel-heading" id="headingPhan2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         On Multitask Loss Function for Audio Event Detection and Localization
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Huy Phan<sup>1</sup>, Lam Pham<sup>2</sup>, Philipp Koch<sup>3</sup>, Ngoc Q. K. Duong<sup>4</sup>, Ian McLoughlin<sup>5</sup>, and Alfred Mertins<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Electric Engineering and Computer Science, Queen Mary University of London, <sup>2</sup>School of Computing, University of Kent, <sup>3</sup>Institute for Signal Processing, University of Lübeck, <sup>4</sup>InterDigital R&amp;D France, <sup>5</sup>Singapore Institute of Technology
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          22 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Phan2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Phan_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=bWqKRjwqWwE" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Phan2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Phan2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Phan2020" class="panel-collapse collapse" id="collapse-Phan2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio event localization and detection (SELD) have been commonly tackled using multitask models. Such a model usually consists of a multi-label event classification branch with sigmoid cross-entropy loss for event activity detection and a regression branch with mean squared error loss for direction-of-arrival estimation. In this work, we propose a multitask regression model, in which both (multi-label) event detection and localization are formulated as regression problems and use the mean squared error loss homogeneously for model training. We show that the common combination of heterogeneous loss functions causes the network to underfit the data whereas the homogeneous mean squared error loss leads to better convergence and performance. Experiments on the development and validation sets of the DCASE 2020 SELD task demonstrate that the proposed system also outperforms the DCASE 2020 SELD baseline across all the detection and localization metrics, reducing the overall SELD error (the combined metric) by approximately 10% absolute.
      </p>
<p>
<strong>
        Cites:
       </strong>
       22 (
       <a href="https://scholar.google.com/scholar?cites=9387635891544462978" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Phan2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Phan_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=bWqKRjwqWwE" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Phan2020label" class="modal fade" id="bibtex-Phan2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPhan2020label">
        On Multitask Loss Function for Audio Event Detection and Localization
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Phan2020,
    author = "Phan, Huy and Pham, Lam and Koch, Philipp and Duong, Ngoc Q. K. and McLoughlin, Ian and Mertins, Alfred",
    title = "On Multitask Loss Function for Audio Event Detection and Localization",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "160--164",
    abstract = "Audio event localization and detection (SELD) have been commonly tackled using multitask models. Such a model usually consists of a multi-label event classification branch with sigmoid cross-entropy loss for event activity detection and a regression branch with mean squared error loss for direction-of-arrival estimation. In this work, we propose a multitask regression model, in which both (multi-label) event detection and localization are formulated as regression problems and use the mean squared error loss homogeneously for model training. We show that the common combination of heterogeneous loss functions causes the network to underfit the data whereas the homogeneous mean squared error loss leads to better convergence and performance. Experiments on the development and validation sets of the DCASE 2020 SELD task demonstrate that the proposed system also outperforms the DCASE 2020 SELD baseline across all the detection and localization metrics, reducing the overall SELD error (the combined metric) by approximately 10\% absolute."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Politis2020" style="box-shadow: none">
<div class="panel-heading" id="headingPolitis2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Dataset of Reverberant Spatial Sound Scenes with Moving Sources for Sound Event Localization and Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Archontis Politis<sup>1</sup>, Sharath Adavanne<sup>1</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Audio and Speech Processing Research Group, Tampere University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          100 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Politis2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Politis_88.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Politis2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Politis2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Politis2020" class="panel-collapse collapse" id="collapse-Politis2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report details the dataset and the evaluation setup of the Sound Event Localization &amp; Detection (SELD) task for the DCASE 2020 Challenge. Training and testing SELD systems requires datasets of diverse sound events occurring under realistic acoustic conditions. A significantly more complex dataset is created for DCASE 2020 compared to the previous challenge. The two key differences are a more diverse range of acoustical conditions, and dynamic conditions, i.e. moving sources. The spatial sound scene recordings for all conditions are generated using real room impulse responses, while ambient noise recorded on location is added to the spatialized sound events. Additionally, an improved version of the SELD baseline used in the previous challenge is included, providing benchmark scores for the task.
      </p>
<p>
<strong>
        Cites:
       </strong>
       100 (
       <a href="https://scholar.google.com/scholar?cites=4119094318324903641" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Politis2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Politis_88.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Politis2020label" class="modal fade" id="bibtex-Politis2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPolitis2020label">
        A Dataset of Reverberant Spatial Sound Scenes with Moving Sources for Sound Event Localization and Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Politis2020,
    author = "Politis, Archontis and Adavanne, Sharath and Virtanen, Tuomas",
    title = "A Dataset of Reverberant Spatial Sound Scenes with Moving Sources for Sound Event Localization and Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "165--169",
    abstract = "This report details the dataset and the evaluation setup of the Sound Event Localization \&amp; Detection (SELD) task for the DCASE 2020 Challenge. Training and testing SELD systems requires datasets of diverse sound events occurring under realistic acoustic conditions. A significantly more complex dataset is created for DCASE 2020 compared to the previous challenge. The two key differences are a more diverse range of acoustical conditions, and dynamic conditions, i.e. moving sources. The spatial sound scene recordings for all conditions are generated using real room impulse responses, while ambient noise recorded on location is added to the spatialized sound events. Additionally, an improved version of the SELD baseline used in the previous challenge is included, providing benchmark scores for the task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Primus2020" style="box-shadow: none">
<div class="panel-heading" id="headingPrimus2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Anomalous Sound Detection as a Simple Binary Classification Problem with Careful Selection of Proxy Outlier Examples
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Paul Primus<sup>1</sup>, Verena Haunschmid<sup>1</sup>, Patrick Praher<sup>3</sup>, and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Computational Perception, Johannes Kepler University, <sup>2</sup>LIT Artificial Intelligence Lab, Johannes Kepler University, <sup>3</sup>Software Competence Center Hagenberg GmbH
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          32 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Primus2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Primus_41.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=3YuW7zIyvNc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Primus2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Primus2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Primus2020" class="panel-collapse collapse" id="collapse-Primus2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Unsupervised anomalous sound detection is concerned with identifying sounds that deviate from what is defined as “normal”, without explicitly specifying the types of anomalies. A significant obstacle is the diversity and rareness of outliers, which typically prevent us from collecting a representative set of anomalous sounds. As a consequence, most anomaly detection methods use unsupervised rather than supervised machine learning methods. Nevertheless, we will show that anomalous sound detection can be effectively framed as a supervised classification problem if the set of anomalous samples is <i>carefully substituted</i> with what we call <i>proxy outliers</i>. Candidates for proxy outliers are available in abundance as they potentially include all recordings that are neither normal nor abnormal sounds. We experiment with the machine condition monitoring data set of the 2020's DCASE Challenge and find proxy outliers with matching recording conditions and high similarity to the target sounds particularly informative. If no data with similar sounds and matching recording conditions is available, data sets with a larger diversity in these two dimensions are preferable. Our models based on supervised training with proxy outliers achieved rank three in Task 2 of the DCASE2020 Challenge.
      </p>
<p>
<strong>
        Cites:
       </strong>
       32 (
       <a href="https://scholar.google.com/scholar?cites=14367120700290600183" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Primus2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Primus_41.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=3YuW7zIyvNc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Primus2020label" class="modal fade" id="bibtex-Primus2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPrimus2020label">
        Anomalous Sound Detection as a Simple Binary Classification Problem with Careful Selection of Proxy Outlier Examples
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Primus2020,
    author = "Primus, Paul and Haunschmid, Verena and Praher, Patrick and Widmer, Gerhard",
    title = "Anomalous Sound Detection as a Simple Binary Classification Problem with Careful Selection of Proxy Outlier Examples",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "170--174",
    abstract = "Unsupervised anomalous sound detection is concerned with identifying sounds that deviate from what is defined as “normal”, without explicitly specifying the types of anomalies. A significant obstacle is the diversity and rareness of outliers, which typically prevent us from collecting a representative set of anomalous sounds. As a consequence, most anomaly detection methods use unsupervised rather than supervised machine learning methods. Nevertheless, we will show that anomalous sound detection can be effectively framed as a supervised classification problem if the set of anomalous samples is <i>carefully substituted</i> with what we call <i>proxy outliers</i>. Candidates for proxy outliers are available in abundance as they potentially include all recordings that are neither normal nor abnormal sounds. We experiment with the machine condition monitoring data set of the 2020's DCASE Challenge and find proxy outliers with matching recording conditions and high similarity to the target sounds particularly informative. If no data with similar sounds and matching recording conditions is available, data sets with a larger diversity in these two dimensions are preferable. Our models based on supervised training with proxy outliers achieved rank three in Task 2 of the DCASE2020 Challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Purohit2020" style="box-shadow: none">
<div class="panel-heading" id="headingPurohit2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Deep Autoencoding GMM-Based Unsupervised Anomaly Detection in Acoustic Signals and its Hyper-Parameter Optimization
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Harsh Purohit<sup>1</sup>, Ryo Tanabe<sup>1</sup>, Takashi Endo<sup>1</sup>, Kaori Suefusa<sup>1</sup>, Yuki Nikaido<sup>1</sup>, and Yohei Kawaguchi<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Research and Development Group, Hitachi, Ltd.
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Purohit2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Purohit_42.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Dt6G3n8UBjg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Purohit2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Purohit2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Purohit2020" class="panel-collapse collapse" id="collapse-Purohit2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Failures or breakdowns in factory machinery can cause a significant cost to companies. Therefore, there is an increasing demand for automatic machine inspection. In this work, our aim is to develop an acoustic signal based unsupervised anomaly detection method. Existing approaches such as deep autoencoder (DA) and Gaussian mixture model (GMM) have poor anomaly-detection performance. We propose a new method based on deep autoencoding Gaussian mixture model with hyper-parameter optimization (DAGMM-HO). The DAGMM-HO applies the conventional DAGMM to the audio domain for the first time, expecting that its total optimization on reduction of dimensions and statistical modelling improves anomaly-detection performance. In addition, the DAGMM-HO solves the hyper-parameter sensitivity problem of the conventional DAGMM by hyper-parameter optimization based on the gap statistic and the cumulative eigenvalues. We evaluated the proposed method with experimental data of the industrial fans. We found that it significantly outperforms previous approaches, and achieves up to a 20% improvement based on the standard AUC score.
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=2218082542867814384" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Purohit2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Purohit_42.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Dt6G3n8UBjg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Purohit2020label" class="modal fade" id="bibtex-Purohit2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPurohit2020label">
        Deep Autoencoding GMM-Based Unsupervised Anomaly Detection in Acoustic Signals and its Hyper-Parameter Optimization
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Purohit2020,
    author = "Purohit, Harsh and Tanabe, Ryo and Endo, Takashi and Suefusa, Kaori and Nikaido, Yuki and Kawaguchi, Yohei",
    title = "Deep Autoencoding GMM-Based Unsupervised Anomaly Detection in Acoustic Signals and its Hyper-Parameter Optimization",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "175--179",
    abstract = "Failures or breakdowns in factory machinery can cause a significant cost to companies. Therefore, there is an increasing demand for automatic machine inspection. In this work, our aim is to develop an acoustic signal based unsupervised anomaly detection method. Existing approaches such as deep autoencoder (DA) and Gaussian mixture model (GMM) have poor anomaly-detection performance. We propose a new method based on deep autoencoding Gaussian mixture model with hyper-parameter optimization (DAGMM-HO). The DAGMM-HO applies the conventional DAGMM to the audio domain for the first time, expecting that its total optimization on reduction of dimensions and statistical modelling improves anomaly-detection performance. In addition, the DAGMM-HO solves the hyper-parameter sensitivity problem of the conventional DAGMM by hyper-parameter optimization based on the gap statistic and the cumulative eigenvalues. We evaluated the proposed method with experimental data of the industrial fans. We found that it significantly outperforms previous approaches, and achieves up to a 20\% improvement based on the standard AUC score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ronchini2020" style="box-shadow: none">
<div class="panel-heading" id="headingRonchini2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound Event Localization and Detection Based on CRNN using Rectangular Filters and Channel Rotation Data Augmentation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Francesca Ronchini<sup>1</sup>, Daniel Arteaga<sup>1,2</sup>, and Andrés Pérez-López<sup>1,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universitat Pompeu Fabra, <sup>2</sup>Dolby Iberia, SL, <sup>3</sup>Eurecat, Centre Tecnologic de Catalunya
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          13 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ronchini2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Ronchini_64.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=iXzZkvh5bJ8" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Ronchini2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ronchini2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ronchini2020" class="panel-collapse collapse" id="collapse-Ronchini2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound Event Localization and Detection refers to the problem of identifying the presence of independent or temporally-overlapped sound sources, correctly identifying to which sound class it belongs, and estimating their spatial directions while they are active. In the last years, neural networks have become the prevailing method for Sound Event Localization and Detection task, with convolutional recurrent neural networks being among the most used systems. This paper presents a system submitted to the Detection and Classification of Acoustic Scenes and Events 2020 Challenge Task 3. The algorithm consists of a convolutional recurrent neural network using rectangular filters, specialized in recognizing significant spectral features related to the task. In order to further improve the score and to generalize the system performance to unseen data, the training dataset size has been increased using data augmentation. The technique used for that is based on channel rotations and reflection on the xy plane in the First Order Ambisonic domain, which allows improving Direction of Arrival labels keeping the physical relationships between channels. Evaluation results on the development dataset show that the proposed system outperforms the baseline results, considerably improving Error Rate and F-score for location-aware detection.
      </p>
<p>
<strong>
        Cites:
       </strong>
       13 (
       <a href="https://scholar.google.com/scholar?cites=8124851937834506187" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ronchini2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Ronchini_64.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=iXzZkvh5bJ8" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ronchini2020label" class="modal fade" id="bibtex-Ronchini2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRonchini2020label">
        Sound Event Localization and Detection Based on CRNN using Rectangular Filters and Channel Rotation Data Augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ronchini2020,
    author = "Ronchini, Francesca and Arteaga, Daniel and Pérez-López, Andrés",
    title = "Sound Event Localization and Detection Based on CRNN using Rectangular Filters and Channel Rotation Data Augmentation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "180--184",
    abstract = "Sound Event Localization and Detection refers to the problem of identifying the presence of independent or temporally-overlapped sound sources, correctly identifying to which sound class it belongs, and estimating their spatial directions while they are active. In the last years, neural networks have become the prevailing method for Sound Event Localization and Detection task, with convolutional recurrent neural networks being among the most used systems. This paper presents a system submitted to the Detection and Classification of Acoustic Scenes and Events 2020 Challenge Task 3. The algorithm consists of a convolutional recurrent neural network using rectangular filters, specialized in recognizing significant spectral features related to the task. In order to further improve the score and to generalize the system performance to unseen data, the training dataset size has been increased using data augmentation. The technique used for that is based on channel rotations and reflection on the xy plane in the First Order Ambisonic domain, which allows improving Direction of Arrival labels keeping the physical relationships between channels. Evaluation results on the development dataset show that the proposed system outperforms the baseline results, considerably improving Error Rate and F-score for location-aware detection."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Safavi2020" style="box-shadow: none">
<div class="panel-heading" id="headingSafavi2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Open-Window: A Sound Event Dataset for Window State Detection and Recognition
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Saeid Safavi<sup>1</sup>, Turab Iqbal<sup>1</sup>, Wenwu Wang<sup>1</sup>, Philip Coleman<sup>2</sup>, and Mark D. Plumbley<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Vision, Speech and Signal Processing, University of Surrey, <sup>2</sup>The Institute of Sound Recording, University of Surrey
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Safavi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Safavi_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=X_w2gCfTzu4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Safavi2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Safavi2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Safavi2020" class="panel-collapse collapse" id="collapse-Safavi2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Situated in the domain of urban sound scene classification by humans and machines, this research is the first step towards mapping urban noise pollution experienced indoors and finding ways to reduce its negative impact in peoples’ homes. We have recorded a sound dataset, called Open-Window, which contains recordings from three different locations and four different window states; two stationary states (open and close) and two transitional states (open to close and close to open). We have then built our machine recognition baselines for different scenarios (open set versus closed set) using a deep learning framework. The human listening test is also performed to be able to compare the human and machine performance for detecting the window state just using the acoustic cues. Our experimental results reveal that when using a simple machine baseline system, humans and machines are achieving similar average performance for closed set experiments.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="https://scholar.google.com/scholar?cites=74430609519174754" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Safavi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Safavi_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=X_w2gCfTzu4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Safavi2020label" class="modal fade" id="bibtex-Safavi2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSafavi2020label">
        Open-Window: A Sound Event Dataset for Window State Detection and Recognition
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Safavi2020,
    author = "Safavi, Saeid and Iqbal, Turab and Wang, Wenwu and Coleman, Philip and Plumbley, Mark D.",
    title = "Open-Window: A Sound Event Dataset for Window State Detection and Recognition",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "185--189",
    abstract = "Situated in the domain of urban sound scene classification by humans and machines, this research is the first step towards mapping urban noise pollution experienced indoors and finding ways to reduce its negative impact in peoples’ homes. We have recorded a sound dataset, called Open-Window, which contains recordings from three different locations and four different window states; two stationary states (open and close) and two transitional states (open to close and close to open). We have then built our machine recognition baselines for different scenarios (open set versus closed set) using a deep learning framework. The human listening test is also performed to be able to compare the human and machine performance for detecting the window state just using the acoustic cues. Our experimental results reveal that when using a simple machine baseline system, humans and machines are achieving similar average performance for closed set experiments."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Takeuchi2020" style="box-shadow: none">
<div class="panel-heading" id="headingTakeuchi2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Effects of Word-Frequency Based Pre- and Post- Processings for Audio Captioning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Daiki Takeuchi<sup>1</sup>, Yuma Koizumi<sup>1</sup>, Yasunori Ohishi<sup>1</sup>, Noboru Harada<sup>1</sup>, and Kunio Kashino<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>NTT Corporation
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          24 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Takeuchi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Takeuchi_79.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=vp2dhx5GOOQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Takeuchi2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Takeuchi2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Takeuchi2020" class="panel-collapse collapse" id="collapse-Takeuchi2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes the use of three elements, namely, data augmentation, multi-task learning, and post-processing, in combination for audio captioning and clarifies their individual effectiveness. The system was used for our submission to Task 6 (Automated Audio Captioning) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge and obtained the highest evaluation scores, but it was yet to clarify which of those elements contributed to its performance. Therefore, we first conduct an element-wise ablation study on our system in order to estimate to what extent each element is effective. We then conduct a detailed module-wise ablation study to further clarify the key processing modules for improving accuracy. The results show that data augmentation and post-processing significantly improve the score in our system. In particular, mix-up data augmentation and beam search in post-processing improve SPIDEr by 0.8 and 1.6 points, respectively.
      </p>
<p>
<strong>
        Cites:
       </strong>
       24 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Takeuchi2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Takeuchi_79.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=vp2dhx5GOOQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Takeuchi2020label" class="modal fade" id="bibtex-Takeuchi2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTakeuchi2020label">
        Effects of Word-Frequency Based Pre- and Post- Processings for Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Takeuchi2020,
    author = "Takeuchi, Daiki and Koizumi, Yuma and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio",
    title = "Effects of Word-Frequency Based Pre- and Post- Processings for Audio Captioning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "190--194",
    abstract = "This paper proposes the use of three elements, namely, data augmentation, multi-task learning, and post-processing, in combination for audio captioning and clarifies their individual effectiveness. The system was used for our submission to Task 6 (Automated Audio Captioning) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge and obtained the highest evaluation scores, but it was yet to clarify which of those elements contributed to its performance. Therefore, we first conduct an element-wise ablation study on our system in order to estimate to what extent each element is effective. We then conduct a detailed module-wise ablation study to further clarify the key processing modules for improving accuracy. The results show that data augmentation and post-processing significantly improve the score in our system. In particular, mix-up data augmentation and beam search in post-processing improve SPIDEr by 0.8 and 1.6 points, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tonami2020" style="box-shadow: none">
<div class="panel-heading" id="headingTonami2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Evaluation Metric of Sound Event Detection Considering Severe Misdetection by Scenes
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Noriyuki Tonami<sup>1</sup>, Keisuke Imoto<sup>2</sup>, Takahiro Fukumori<sup>1</sup>, and Yoichi Yamashita<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Ritsumeikan University, <sup>2</sup>Doshisha University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tonami2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Tonami_17.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Shpq38B4Bi4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Tonami2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tonami2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tonami2020" class="panel-collapse collapse" id="collapse-Tonami2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose a new evaluation metric for sound event detection (SED) and discuss a problem frequently encountered in conventional metrics. In conventional evaluation metrics, misdetected sound events are treated equally, e.g., the misdetected sound event “car” in the acoustic scenes “office” and “street” are regarded as the same type of misdetection. However, the misdetected event“car” in “office”is as evere mistake compared with its misdetection in“street.” The event “car” rarely occurs in the “office.” SED systems that are evaluated using conventional metrics may cause severe/catastrophic problems and lead to confusion in practice owing to lack of consideration of the relationship between sound events and scenes. Our evaluation metric for SED considers severe misdetections on the basis of the relationship between sound events and scenes. We demonstrate the utility of our proposed method by com-paring it with the conventional evaluation metrics on two datasets with events and scenes. Experimental results show that the pro-posed metric can accurately evaluate whether SED systems appropriately consider the relationship between sound events and scenes,i.e., realistic situations.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tonami2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Tonami_17.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=Shpq38B4Bi4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tonami2020label" class="modal fade" id="bibtex-Tonami2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTonami2020label">
        Evaluation Metric of Sound Event Detection Considering Severe Misdetection by Scenes
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tonami2020,
    author = "Tonami, Noriyuki and Imoto, Keisuke and Fukumori, Takahiro and Yamashita, Yoichi",
    title = "Evaluation Metric of Sound Event Detection Considering Severe Misdetection by Scenes",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "195--199",
    abstract = "In this paper, we propose a new evaluation metric for sound event detection (SED) and discuss a problem frequently encountered in conventional metrics. In conventional evaluation metrics, misdetected sound events are treated equally, e.g., the misdetected sound event “car” in the acoustic scenes “office” and “street” are regarded as the same type of misdetection. However, the misdetected event“car” in “office”is as evere mistake compared with its misdetection in“street.” The event “car” rarely occurs in the “office.” SED systems that are evaluated using conventional metrics may cause severe/catastrophic problems and lead to confusion in practice owing to lack of consideration of the relationship between sound events and scenes. Our evaluation metric for SED considers severe misdetections on the basis of the relationship between sound events and scenes. We demonstrate the utility of our proposed method by com-paring it with the conventional evaluation metrics on two datasets with events and scenes. Experimental results show that the pro-posed metric can accurately evaluate whether SED systems appropriately consider the relationship between sound events and scenes,i.e., realistic situations."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Turpault2020a" style="box-shadow: none">
<div class="panel-heading" id="headingTurpault2020a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Training Sound Event Detection on a Heterogeneous Dataset
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Nicolas Turpault<sup>1</sup>, and Romain Serizel<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Université de Lorraine, CNRS, Inria, Loria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          68 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Turpault2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Turpault_85.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=TREuzqyjKz0" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Turpault2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Turpault2020a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Turpault2020a" class="panel-collapse collapse" id="collapse-Turpault2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default are shown to be sub-optimal.
      </p>
<p>
<strong>
        Cites:
       </strong>
       68 (
       <a href="https://scholar.google.com/scholar?cites=14348855512003149967" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Turpault2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Turpault_85.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=TREuzqyjKz0" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Turpault2020alabel" class="modal fade" id="bibtex-Turpault2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTurpault2020alabel">
        Training Sound Event Detection on a Heterogeneous Dataset
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Turpault2020a,
    author = "Turpault, Nicolas and Serizel, Romain",
    title = "Training Sound Event Detection on a Heterogeneous Dataset",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "200--204",
    abstract = "Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default are shown to be sub-optimal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Turpault2020b" style="box-shadow: none">
<div class="panel-heading" id="headingTurpault2020b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Improving Sound Event Detection in Domestic Environments using Sound Separation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Nicolas Turpault<sup>1</sup>, Scott Wisdom<sup>2</sup>, Hakan Erdogan<sup>2</sup>, John R. Hershey<sup>2</sup>, Romain Serizel<sup>1</sup>, Eduardo Fonseca<sup>3</sup>, Prem Seetharaman<sup>4</sup>, and Justin Salamon<sup>5</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Université de Lorraine, CNRS, Inria, Loria, <sup>2</sup>Google Research, AI Perception, <sup>3</sup>Music Technology Group, Universitat Pompeu Fabra, <sup>4</sup>Interactive Audio Lab, Northwestern University, <sup>5</sup>Adobe Research
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          53 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Turpault2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Turpault_86.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=y3FUSzi4ChA" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Turpault2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Turpault2020b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Turpault2020b" class="panel-collapse collapse" id="collapse-Turpault2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods to combine separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the sound separation model to the sound event detection data on both the sound separation and the sound event detection.
      </p>
<p>
<strong>
        Cites:
       </strong>
       53 (
       <a href="https://scholar.google.com/scholar?cites=15973377504844186542" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Turpault2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Turpault_86.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=y3FUSzi4ChA" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Turpault2020blabel" class="modal fade" id="bibtex-Turpault2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTurpault2020blabel">
        Improving Sound Event Detection in Domestic Environments using Sound Separation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Turpault2020b,
    author = "Turpault, Nicolas and Wisdom, Scott and Erdogan, Hakan and Hershey, John R. and Serizel, Romain and Fonseca, Eduardo and Seetharaman, Prem and Salamon, Justin",
    title = "Improving Sound Event Detection in Domestic Environments using Sound Separation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "205--209",
    abstract = "Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods to combine separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the sound separation model to the sound event detection data on both the sound separation and the sound event detection."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2020" style="box-shadow: none">
<div class="panel-heading" id="headingWang2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification with Spectrogram Processing Strategies
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Helin Wang<sup>1</sup>, Yuexian Zou<sup>1,2</sup>, and DaDing Chong<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>ADSPLAB, School of ECE, Peking University, <sup>2</sup>Peng Cheng Laboratory
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          14 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Wang_58.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=p-p8uvJceBc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Wang2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2020" class="panel-collapse collapse" id="collapse-Wang2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Recently, convolutional neural networks (CNN) have achieved the state-of-the-art performance in acoustic scene classification (ASC) task. The audio data is often transformed into two-dimensional spectrogram representations, which are then fed to the neural networks. In this paper, we study the problem of efficiently taking advantage of different spectrogram representations through discriminative processing strategies. There are two main contributions. The first contribution is exploring the impact of the combination of multiple spectrogram representations at different stages, which provides a meaningful reference for the effective spectrogram fusion. The second contribution is that the processing strategies in multiple frequency bands and multiple temporal frames are proposed to make fully use of a single spectrogram representation. The proposed spectrogram processing strategies can be easily transferred to any network structures. The experiments are carried out on the DCASE 2020 Task1 datasets, and the results show that our method could achieve the accuracy of 81.8% (official baseline: 54.1%) and 92.1% (official baseline: 87.3%) on the officially provided fold 1 evaluation dataset of Task1A and Task1B, respectively.
      </p>
<p>
<strong>
        Cites:
       </strong>
       14 (
       <a href="https://scholar.google.com/scholar?cites=12874138599546960701" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Wang_58.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=p-p8uvJceBc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2020label" class="modal fade" id="bibtex-Wang2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2020label">
        Acoustic Scene Classification with Spectrogram Processing Strategies
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wang2020,
    author = "Wang, Helin and Zou, Yuexian and Chong, DaDing",
    title = "Acoustic Scene Classification with Spectrogram Processing Strategies",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "210--214",
    abstract = "Recently, convolutional neural networks (CNN) have achieved the state-of-the-art performance in acoustic scene classification (ASC) task. The audio data is often transformed into two-dimensional spectrogram representations, which are then fed to the neural networks. In this paper, we study the problem of efficiently taking advantage of different spectrogram representations through discriminative processing strategies. There are two main contributions. The first contribution is exploring the impact of the combination of multiple spectrogram representations at different stages, which provides a meaningful reference for the effective spectrogram fusion. The second contribution is that the processing strategies in multiple frequency bands and multiple temporal frames are proposed to make fully use of a single spectrogram representation. The proposed spectrogram processing strategies can be easily transferred to any network structures. The experiments are carried out on the DCASE 2020 Task1 datasets, and the results show that our method could achieve the accuracy of 81.8\% (official baseline: 54.1\%) and 92.1\% (official baseline: 87.3\%) on the officially provided fold 1 evaluation dataset of Task1A and Task1B, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wilkinghoff2020" style="box-shadow: none">
<div class="panel-heading" id="headingWilkinghoff2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Using Look, Listen, and Learn Embeddings for Detecting Anomalous Sounds in Machine Condition Monitoring
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kevin Wilkinghoff<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Fraunhofer Institute for Communication, Information Processing and Ergonomics FKIE
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          5 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wilkinghoff2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Wilkinghoff_1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=70zLhmQGZo4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Wilkinghoff2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wilkinghoff2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wilkinghoff2020" class="panel-collapse collapse" id="collapse-Wilkinghoff2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The goal of anomalous sound detection is to unsupervisedly train a system to distinguish normal from anomalous sounds that substantially differ from the normal sounds used for training. In this paper, a system based on Look, Listen, and Learn embeddings, which participated in task 2 “Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring” of the DCASE challenge 2020 and is adapted from an open-set machine listening system, is presented. The experimental results show that the presented system significantly outperforms the baseline system of the challenge both in detecting outliers and in recognizing the correct machine type or exact machine id. Moreover, it is shown that an ensemble consisting of the presented system and the baseline system performs even better than both of its components.
      </p>
<p>
<strong>
        Cites:
       </strong>
       5 (
       <a href="https://scholar.google.com/scholar?cites=8900677851065506714" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wilkinghoff2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Wilkinghoff_1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=70zLhmQGZo4" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wilkinghoff2020label" class="modal fade" id="bibtex-Wilkinghoff2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWilkinghoff2020label">
        Using Look, Listen, and Learn Embeddings for Detecting Anomalous Sounds in Machine Condition Monitoring
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wilkinghoff2020,
    author = "Wilkinghoff, Kevin",
    title = "Using Look, Listen, and Learn Embeddings for Detecting Anomalous Sounds in Machine Condition Monitoring",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "215--219",
    abstract = "The goal of anomalous sound detection is to unsupervisedly train a system to distinguish normal from anomalous sounds that substantially differ from the normal sounds used for training. In this paper, a system based on Look, Listen, and Learn embeddings, which participated in task 2 “Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring” of the DCASE challenge 2020 and is adapted from an open-set machine listening system, is presented. The experimental results show that the presented system significantly outperforms the baseline system of the challenge both in detecting outliers and in recognizing the correct machine type or exact machine id. Moreover, it is shown that an ensemble consisting of the presented system and the baseline system performs even better than both of its components."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wu2020" style="box-shadow: none">
<div class="panel-heading" id="headingWu2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Searching for Efficient Network Architectures for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yuzhong Wu<sup>1</sup>, and Tan Lee<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>The Chinese University of Hong Kong, Department of Electronic Engineering
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Wu_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=xToANK4B4LQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Wu2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wu2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wu2020" class="panel-collapse collapse" id="collapse-Wu2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic scene classification (ASC) is the task of classifying recorded audio signal into one of the predefined acoustic environment classes. While previous studies reported ASC systems with high accuracy, the computation cost and system complexity may not be optimal for practical mobile applications. Inspired by the success of neural architecture search (NAS) and the efficacy of MobileNets in vision applications, we propose a simple yet effective random search policy to obtain high accuracy ASC models under strict model size constraint. The search policy allows automatic discovery of the best trade-off between model depth and width, and statistical analysis of model design can be carried out using the evaluation results of randomly sampled architectures. To enable fast search, the search space is limited to several predefined efficient convolutional modules based on depth-wise convolution and swish activation function. Experimental results show that the CNN model found by this search policy gives higher accuracy compared to an AlexNet-like CNN benchmark.
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Wu_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=xToANK4B4LQ" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wu2020label" class="modal fade" id="bibtex-Wu2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWu2020label">
        Searching for Efficient Network Architectures for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wu2020,
    author = "Wu, Yuzhong and Lee, Tan",
    title = "Searching for Efficient Network Architectures for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "220--224",
    abstract = "Acoustic scene classification (ASC) is the task of classifying recorded audio signal into one of the predefined acoustic environment classes. While previous studies reported ASC systems with high accuracy, the computation cost and system complexity may not be optimal for practical mobile applications. Inspired by the success of neural architecture search (NAS) and the efficacy of MobileNets in vision applications, we propose a simple yet effective random search policy to obtain high accuracy ASC models under strict model size constraint. The search policy allows automatic discovery of the best trade-off between model depth and width, and statistical analysis of model design can be carried out using the evaluation results of randomly sampled architectures. To enable fast search, the search space is limited to several predefined efficient convolutional modules based on depth-wise convolution and swish activation function. Experimental results show that the CNN model found by this search policy gives higher accuracy compared to an AlexNet-like CNN benchmark."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2020" style="box-shadow: none">
<div class="panel-heading" id="headingXu2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A CRNN-GRU Based Reinforcement Learning Approach to Audio Captioning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Xuenan Xu<sup>1</sup>, Heinrich Dinkel<sup>1</sup>, Mengyue Wu<sup>1</sup>, and Kai Yu<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>MoE Key Lab of Artificial Intelligence, SpeechLab, Department of Computer Science and Engineering, AI Institute, Shanghai Jiao Tong University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          41 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Xu_83.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=BMP_mBXg3HI" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Xu2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2020" class="panel-collapse collapse" id="collapse-Xu2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio captioning aims at generating a natural sentence to describe the content in an audio clip. This paper proposes the use of a powerful CRNN encoder combined with a GRU decoder to tackle this multi-modal task. In addition to standard cross-entropy, reinforcement learning is also investigated for generating richer and more accurate captions. Our approach significantly improves against the baseline model on all shown metrics achieving a relative improvement of at least 34%. Results indicate that our proposed CRNN-GRU model with reinforcement learning achieves a SPIDEr of 0.190 on the Clotho evaluation set. With data augmentation, the performance is further boosted to 0.223. In the DCASE challenge Task 6 we ranked fourth based on SPIDEr, second on 5 metrics including BLEU, ROUGE-L and METEOR, without ensemble or data augmentation while maintaining a small model size (only 5 Million parameters).
      </p>
<p>
<strong>
        Cites:
       </strong>
       41 (
       <a href="https://scholar.google.com/scholar?cites=15480736018443840749" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Xu_83.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=BMP_mBXg3HI" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2020label" class="modal fade" id="bibtex-Xu2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2020label">
        A CRNN-GRU Based Reinforcement Learning Approach to Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xu2020,
    author = "Xu, Xuenan and Dinkel, Heinrich and Wu, Mengyue and Yu, Kai",
    title = "A CRNN-GRU Based Reinforcement Learning Approach to Audio Captioning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "225--229",
    abstract = "Audio captioning aims at generating a natural sentence to describe the content in an audio clip. This paper proposes the use of a powerful CRNN encoder combined with a GRU decoder to tackle this multi-modal task. In addition to standard cross-entropy, reinforcement learning is also investigated for generating richer and more accurate captions. Our approach significantly improves against the baseline model on all shown metrics achieving a relative improvement of at least 34\%. Results indicate that our proposed CRNN-GRU model with reinforcement learning achieves a SPIDEr of 0.190 on the Clotho evaluation set. With data augmentation, the performance is further boosted to 0.223. In the DCASE challenge Task 6 we ranked fourth based on SPIDEr, second on 5 metrics including BLEU, ROUGE-L and METEOR, without ensemble or data augmentation while maintaining a small model size (only 5 Million parameters)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yang2020" style="box-shadow: none">
<div class="panel-heading" id="headingYang2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Two-Stage Domain Adaptation for Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Liping Yang<sup>1</sup>, Junyong Hao<sup>1</sup>, Zhenwei Hou<sup>1</sup>, and Wang Peng<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Key Laboratory of Optoelectronic Technology and Systems, MOE, Chongqing University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          14 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Yang_23.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=hbt_nEYo5qc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Yang2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yang2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yang2020" class="panel-collapse collapse" id="collapse-Yang2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event detection under real scenarios is a challenge task. Due to the great distribution mismatch of synthetic and real audio data, the performance of sound event detection model, which is trained on strong-labeled synthetic data, degrades dramatically when it is applied in real environment. To tackle the issue and improve the robustness of sound event detection model, we propose a two-stage domain adaptation sound event detection approach in this paper. The backbone convolutional recurrent neural network (CRNN) leaned using strong-labeled synthetic data is updated by weak-label supervised adaptation and frame-level adversarial do-main adaptation. As a result, the parameters of CRNN are renewed for real audio data, and the input space distribution mismatch be-tween synthetic and real audio data is mitigated in the feature space of CRNN. Moreover, a context clip-level consistency regulariza-tion between the classification outputs of CNN and CRNN is in-troduced to improve the feature representation ability of convolu-tional layers in CRNN. Experiments on DCASE 2019 sound event detection in domestic environments task demonstrate the superiori-ty of our proposed domain adaptation approach. Our approach achieves F1 scores of 48.3% on the validation set and 49.4% on the evaluation set, which are the-state-of-art sound event detection performances of CRNN model without data augmentation.
      </p>
<p>
<strong>
        Cites:
       </strong>
       14 (
       <a href="https://scholar.google.com/scholar?cites=15120303804376065230" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Yang_23.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=hbt_nEYo5qc" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yang2020label" class="modal fade" id="bibtex-Yang2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYang2020label">
        Two-Stage Domain Adaptation for Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Yang2020,
    author = "Yang, Liping and Hao, Junyong and Hou, Zhenwei and Peng, Wang",
    title = "Two-Stage Domain Adaptation for Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "230--234",
    abstract = "Sound event detection under real scenarios is a challenge task. Due to the great distribution mismatch of synthetic and real audio data, the performance of sound event detection model, which is trained on strong-labeled synthetic data, degrades dramatically when it is applied in real environment. To tackle the issue and improve the robustness of sound event detection model, we propose a two-stage domain adaptation sound event detection approach in this paper. The backbone convolutional recurrent neural network (CRNN) leaned using strong-labeled synthetic data is updated by weak-label supervised adaptation and frame-level adversarial do-main adaptation. As a result, the parameters of CRNN are renewed for real audio data, and the input space distribution mismatch be-tween synthetic and real audio data is mitigated in the feature space of CRNN. Moreover, a context clip-level consistency regulariza-tion between the classification outputs of CNN and CRNN is in-troduced to improve the feature representation ability of convolu-tional layers in CRNN. Experiments on DCASE 2019 sound event detection in domestic environments task demonstrate the superiori-ty of our proposed domain adaptation approach. Our approach achieves F1 scores of 48.3\% on the validation set and 49.4\% on the evaluation set, which are the-state-of-art sound event detection performances of CRNN model without data augmentation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yen2020" style="box-shadow: none">
<div class="panel-heading" id="headingYen2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Joint Training of Guided Learning and Mean Teacher Models for Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Hao Yen<sup>1,2</sup>, Pin-Jui Ku<sup>1,2</sup>, Ming-Chi Yen<sup>1</sup>, Hung-Shin Lee<sup>1,2</sup>, and Hsin-Min Wang<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Information Science, Academia Sinica, <sup>2</sup>Department of Electrical Engineering, National Taiwan University
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Yen_47.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=N2PJVqhFrsk" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Yen2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yen2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yen2020" class="panel-collapse collapse" id="collapse-Yen2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present our system of sound event detection and separation in domestic environments for DCASE 2020. The task aims to determine which sound events appear in a clip and the detailed temporal ranges they occupy. The system is trained by using weakly-labeled and unlabeled real data and synthetic data with strongly annotated labels. Our proposed model structure includes a feature-level front-end based on convolution neural networks (CNN), followed by both embedding-level and instance-level back-end attention modules. In order to make full use of the large amount of unlabeled data, we jointly adopt the Guided Learning and Mean Teacher approaches to carry out weakly-supervised learning and semi-supervised learning. In addition, a set of adaptive median windows for individual sound events is used to smooth the frame-level predictions in post-processing. In the public evaluation set of DCASE 2019, the best event-based <i>F</i><sub>1</sub>-score achieved by our system is 48.50%, which is a relative improvement of 27.16% over the official baseline (38.14%). In addition, in the development set of DCASE 2020, our best system also achieves a relative improvement of 32.91% over the baseline (45.68% vs. 34.37%)
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Yen_47.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=N2PJVqhFrsk" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yen2020label" class="modal fade" id="bibtex-Yen2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYen2020label">
        Joint Training of Guided Learning and Mean Teacher Models for Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Yen2020,
    author = "Yen, Hao and Ku, Pin-Jui and Yen, Ming-Chi and Lee, Hung-Shin and Wang, Hsin-Min",
    title = "Joint Training of Guided Learning and Mean Teacher Models for Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "235--239",
    abstract = "In this paper, we present our system of sound event detection and separation in domestic environments for DCASE 2020. The task aims to determine which sound events appear in a clip and the detailed temporal ranges they occupy. The system is trained by using weakly-labeled and unlabeled real data and synthetic data with strongly annotated labels. Our proposed model structure includes a feature-level front-end based on convolution neural networks (CNN), followed by both embedding-level and instance-level back-end attention modules. In order to make full use of the large amount of unlabeled data, we jointly adopt the Guided Learning and Mean Teacher approaches to carry out weakly-supervised learning and semi-supervised learning. In addition, a set of adaptive median windows for individual sound events is used to smooth the frame-level predictions in post-processing. In the public evaluation set of DCASE 2019, the best event-based <i>F</i><sub>1</sub>-score achieved by our system is 48.50\%, which is a relative improvement of 27.16\% over the official baseline (38.14\%). In addition, in the development set of DCASE 2020, our best system also achieves a relative improvement of 32.91\% over the baseline (45.68\% vs. 34.37\%)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zinemanas2020" style="box-shadow: none">
<div class="panel-heading" id="headingZinemanas2020" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE-Models: A Python Library for Computational Environmental Sound Analysis using Deep-Learning Models
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Pablo Zinemanas<sup>1</sup>, Ignacio Hounie<sup>2</sup>, Pablo Cancela<sup>2</sup>, Frederic Font<sup>1</sup>, Martín Rocamora<sup>2</sup>, and Xavier Serra<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Music Technology Group, Universitat Pompeu Fabra, <sup>2</sup>Facultad de Ingeniería, Universidad de la República
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zinemanas2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Zinemanas_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=GSDiBMiM4Xg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
         Video
        </a>
<button aria-controls="collapse-Zinemanas2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zinemanas2020" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zinemanas2020" class="panel-collapse collapse" id="collapse-Zinemanas2020" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This document presents DCASE-models, an open-source Python library for rapid prototyping of environmental sound analysis systems, with an emphasis on deep-learning models. Together with a collection of functions for dataset handling, data preparation, feature extraction, and evaluation, it includes a model interface to standardize the interaction of machine learning methods with the other system components. This also provides an abstraction layer that allows the use of different machine learning backends. The package includes Python scripts, Jupyter Notebooks, and a web application, to illustrate its usefulness. The library seeks to alleviate the process of releasing and maintaining the code of new models, improve research reproducibility, and simplify comparison of methods. We expect it to become a valuable resource for the community.
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zinemanas2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2020/proceedings/DCASE2020Workshop_Zinemanas_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=GSDiBMiM4Xg" rel="tooltip" title="Video">
<i class="fa fa-video-camera">
</i>
        Video
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zinemanas2020label" class="modal fade" id="bibtex-Zinemanas2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZinemanas2020label">
        DCASE-Models: A Python Library for Computational Environmental Sound Analysis using Deep-Learning Models
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zinemanas2020,
    author = "Zinemanas, Pablo and Hounie, Ignacio and Cancela, Pablo and Font, Frederic and Rocamora, Martín and Serra, Xavier",
    title = "DCASE-Models: A Python Library for Computational Environmental Sound Analysis using Deep-Learning Models",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    address = "Tokyo, Japan",
    month = "November",
    year = "2020",
    pages = "240--244",
    abstract = "This document presents DCASE-models, an open-source Python library for rapid prototyping of environmental sound analysis systems, with an emphasis on deep-learning models. Together with a collection of functions for dataset handling, data preparation, feature extraction, and evaluation, it includes a model interface to standardize the interaction of machine learning methods with the other system components. This also provides an abstraction layer that allows the use of different machine learning backends. The package includes Python scripts, Jupyter Notebooks, and a web application, to illustrate its usefulness. The library seeks to alleviate the process of releasing and maintaining the code of new models, improve research reproducibility, and simplify comparison of methods. We expect it to become a valuable resource for the community."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>