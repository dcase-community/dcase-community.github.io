<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Detection and Separation in Domestic Environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. The challenge â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2021</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2021/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2021/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2021/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/tiles-13.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-domestic fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Domestic</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2021_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Detection and Separation in Domestic Environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#supplementary-metrics">Supplementary metrics</a></li>
</ul>
</li>
<li><a href="#teams-ranking">Teams ranking</a>
<ul>
<li><a href="#supplementary-metrics-1">Supplementary metrics</a></li>
</ul>
</li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps).
The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong>
given that multiple events can be present in an audio recording.
The challenge of exploring the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong>
together with a small weakly annotated training set to improve system performance remains. Isolated sound events,
background sound files and scripst to design a <strong>training set with strongly annotated synthetic data</strong> are provided.
<strong>The labels in all the annotated subsets are verified and can be considered as reliable.</strong></p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS1_all" data-scatter-y="PSDS1_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Development dataset)" data-chartable="true" data-field="PSDS1_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Development dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Development dataset)" data-chartable="true" data-field="PSDS2_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td></td>
<td>0.80</td>
<td>0.245</td>
<td>0.452</td>
<td>0.313</td>
<td>0.535</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_3</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>0.325</td>
<td>0.561</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_4</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>0.325</td>
<td>0.561</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_1</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>1.03</td>
<td>0.334</td>
<td>0.549</td>
<td>0.345</td>
<td>0.555</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>1.04</td>
<td>0.336</td>
<td>0.550</td>
<td>0.345</td>
<td>0.555</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>1.16</td>
<td>0.370</td>
<td>0.626</td>
<td>0.407</td>
<td>0.653</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_2</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>1.15</td>
<td>0.367</td>
<td>0.616</td>
<td>0.407</td>
<td>0.648</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_1</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>1.14</td>
<td>0.364</td>
<td>0.611</td>
<td>0.398</td>
<td>0.642</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td></td>
<td>1.07</td>
<td>0.327</td>
<td>0.603</td>
<td>0.524</td>
<td>0.674</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_4</td>
<td>Park_JHU_task4_SED_4</td>
<td>Park2021</td>
<td></td>
<td>0.86</td>
<td>0.237</td>
<td>0.524</td>
<td>0.446</td>
<td>0.561</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_1</td>
<td>Park_JHU_task4_SED_1</td>
<td>Park2021</td>
<td></td>
<td>1.01</td>
<td>0.305</td>
<td>0.579</td>
<td>0.508</td>
<td>0.668</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_3</td>
<td>Park_JHU_task4_SED_3</td>
<td>Park2021</td>
<td></td>
<td>0.84</td>
<td>0.222</td>
<td>0.537</td>
<td>0.456</td>
<td>0.596</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_4</td>
<td>DCASE2020 SED Mean teacher system 4</td>
<td>Zheng2021</td>
<td></td>
<td>1.30</td>
<td>0.389</td>
<td>0.742</td>
<td>0.402</td>
<td>0.786</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>DCASE2020 SED Mean teacher system 1</td>
<td>Zheng2021</td>
<td></td>
<td>1.33</td>
<td>0.452</td>
<td>0.669</td>
<td>0.454</td>
<td>0.671</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_3</td>
<td>DCASE2020 SED Mean teacher system 3</td>
<td>Zheng2021</td>
<td></td>
<td>1.29</td>
<td>0.386</td>
<td>0.746</td>
<td>0.397</td>
<td>0.788</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_2</td>
<td>DCASE2020 SED Mean teacher system 2</td>
<td>Zheng2021</td>
<td></td>
<td>1.33</td>
<td>0.447</td>
<td>0.676</td>
<td>0.454</td>
<td>0.680</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_mixupratip=0.8_nband=(2,3)_medianfilter=5</td>
<td>Nam2021</td>
<td></td>
<td>1.19</td>
<td>0.399</td>
<td>0.609</td>
<td>0.434</td>
<td>0.639</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_default</td>
<td>Nam2021</td>
<td></td>
<td>1.16</td>
<td>0.378</td>
<td>0.617</td>
<td>0.423</td>
<td>0.658</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_AFL</td>
<td>Nam2021</td>
<td></td>
<td>1.09</td>
<td>0.324</td>
<td>0.634</td>
<td>0.381</td>
<td>0.692</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Weak_SED</td>
<td>Nam2021</td>
<td></td>
<td>0.75</td>
<td>0.059</td>
<td>0.715</td>
<td>0.064</td>
<td>0.816</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_2</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.12</td>
<td>0.044</td>
<td>0.059</td>
<td>0.316</td>
<td>0.337</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_3</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.41</td>
<td>0.058</td>
<td>0.348</td>
<td>0.249</td>
<td>0.711</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.74</td>
<td>0.258</td>
<td>0.364</td>
<td>0.295</td>
<td>0.503</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>1.10</td>
<td>0.361</td>
<td>0.577</td>
<td>0.386</td>
<td>0.600</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>3-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>1.07</td>
<td>0.343</td>
<td>0.571</td>
<td>0.380</td>
<td>0.589</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>3-Resolution Mean Teacher (Higher time resolutions)</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>1.10</td>
<td>0.363</td>
<td>0.574</td>
<td>0.386</td>
<td>0.578</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_3</td>
<td>4-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>1.07</td>
<td>0.345</td>
<td>0.571</td>
<td>0.372</td>
<td>0.600</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>turpault2020b</td>
<td></td>
<td>1.11</td>
<td>0.364</td>
<td>0.580</td>
<td>0.342</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_4</td>
<td>CRNN with optimized pooling operations for scenario 2 (2)</td>
<td>Boes2021</td>
<td></td>
<td>0.60</td>
<td>0.117</td>
<td>0.457</td>
<td>0.154</td>
<td>0.729</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_3</td>
<td>CRNN with optimized pooling operations for scenario 2 (1)</td>
<td>Boes2021</td>
<td></td>
<td>0.68</td>
<td>0.121</td>
<td>0.531</td>
<td>0.158</td>
<td>0.731</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_2</td>
<td>CRNN with optimized pooling operations for scenario 1 (2)</td>
<td>Boes2021</td>
<td></td>
<td>0.77</td>
<td>0.233</td>
<td>0.440</td>
<td>0.359</td>
<td>0.601</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>CRNN with optimized pooling operations for scenario 1 (1)</td>
<td>Boes2021</td>
<td></td>
<td>0.81</td>
<td>0.253</td>
<td>0.442</td>
<td>0.361</td>
<td>0.593</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_2</td>
<td>UPB sytem 2</td>
<td>Ebbers2021</td>
<td></td>
<td>1.10</td>
<td>0.335</td>
<td>0.621</td>
<td>0.377</td>
<td>0.748</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>UPB sytem 4</td>
<td>Ebbers2021</td>
<td></td>
<td>1.16</td>
<td>0.363</td>
<td>0.637</td>
<td>0.393</td>
<td>0.758</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>UPB sytem 3</td>
<td>Ebbers2021</td>
<td></td>
<td>1.24</td>
<td>0.416</td>
<td>0.635</td>
<td>0.454</td>
<td>0.726</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>UPB sytem 1</td>
<td>Ebbers2021</td>
<td></td>
<td>1.16</td>
<td>0.373</td>
<td>0.621</td>
<td>0.429</td>
<td>0.727</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu2021</td>
<td></td>
<td>0.99</td>
<td>0.290</td>
<td>0.574</td>
<td>0.342</td>
<td>0.614</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td></td>
<td>1.04</td>
<td>0.318</td>
<td>0.583</td>
<td>0.354</td>
<td>0.613</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_4</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.37</td>
<td>0.102</td>
<td>0.231</td>
<td>0.348</td>
<td>0.551</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_1</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.30</td>
<td>0.090</td>
<td>0.169</td>
<td>0.348</td>
<td>0.551</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.54</td>
<td>0.152</td>
<td>0.322</td>
<td>0.348</td>
<td>0.551</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_3</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.24</td>
<td>0.068</td>
<td>0.146</td>
<td>0.348</td>
<td>0.551</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera2021</td>
<td></td>
<td>0.98</td>
<td>0.338</td>
<td>0.481</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_1</td>
<td>DA-SED + FG/BG</td>
<td>Olvera2021</td>
<td></td>
<td>0.95</td>
<td>0.332</td>
<td>0.462</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>1.32</td>
<td>0.442</td>
<td>0.674</td>
<td>0.457</td>
<td>0.685</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>1.31</td>
<td>0.439</td>
<td>0.667</td>
<td>0.450</td>
<td>0.682</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>1.30</td>
<td>0.434</td>
<td>0.669</td>
<td>0.451</td>
<td>0.679</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>1.29</td>
<td>0.431</td>
<td>0.661</td>
<td>0.449</td>
<td>0.675</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_1</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_1</td>
<td>Dinkel2021</td>
<td></td>
<td>1.11</td>
<td>0.361</td>
<td>0.584</td>
<td>0.375</td>
<td>0.619</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_2</td>
<td>Dinkel2021</td>
<td></td>
<td>1.13</td>
<td>0.373</td>
<td>0.585</td>
<td>0.382</td>
<td>0.622</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_3</td>
<td>Dinkel2021</td>
<td></td>
<td>1.13</td>
<td>0.370</td>
<td>0.596</td>
<td>0.381</td>
<td>0.629</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_4</td>
<td>DCASE2021_Cai_SED_CDur_Single_4</td>
<td>Dinkel2021</td>
<td></td>
<td>1.00</td>
<td>0.339</td>
<td>0.504</td>
<td>0.369</td>
<td>0.571</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYu2021</td>
<td></td>
<td>0.90</td>
<td>0.294</td>
<td>0.473</td>
<td>0.134</td>
<td>0.557</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>YuHang2021</td>
<td></td>
<td>0.61</td>
<td>0.098</td>
<td>0.496</td>
<td>0.340</td>
<td>0.523</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_1</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td></td>
<td>0.20</td>
<td>0.038</td>
<td>0.157</td>
<td>0.330</td>
<td>0.540</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td></td>
<td>0.92</td>
<td>0.301</td>
<td>0.485</td>
<td>0.110</td>
<td>0.610</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>DCASE2021 SED CRNN Model1</td>
<td>Lu2021</td>
<td></td>
<td>1.27</td>
<td>0.419</td>
<td>0.660</td>
<td>0.419</td>
<td>0.638</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_4</td>
<td>DCASE2021 SED Conformer Model2</td>
<td>Lu2021</td>
<td></td>
<td>0.88</td>
<td>0.157</td>
<td>0.685</td>
<td>0.177</td>
<td>0.749</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_3</td>
<td>DCASE2021 SED Conformer Model1</td>
<td>Lu2021</td>
<td></td>
<td>0.86</td>
<td>0.148</td>
<td>0.686</td>
<td>0.173</td>
<td>0.752</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_2</td>
<td>DCASE2021 SED CRNN Model2</td>
<td>Lu2021</td>
<td></td>
<td>1.25</td>
<td>0.412</td>
<td>0.651</td>
<td>0.418</td>
<td>0.637</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td></td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>0.360</td>
<td>0.550</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_1</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td></td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>0.360</td>
<td>0.550</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_2</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>1.19</td>
<td>0.411</td>
<td>0.585</td>
<td>0.396</td>
<td>0.587</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>1.19</td>
<td>0.413</td>
<td>0.586</td>
<td>0.401</td>
<td>0.597</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_4</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>1.19</td>
<td>0.412</td>
<td>0.586</td>
<td>0.398</td>
<td>0.599</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_3</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>1.18</td>
<td>0.409</td>
<td>0.584</td>
<td>0.392</td>
<td>0.585</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.88</td>
<td>0.279</td>
<td>0.479</td>
<td>0.328</td>
<td>0.530</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_1</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.88</td>
<td>0.277</td>
<td>0.482</td>
<td>0.332</td>
<td>0.533</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_2</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.54</td>
<td>0.056</td>
<td>0.496</td>
<td>0.060</td>
<td>0.618</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang2021</td>
<td></td>
<td>0.99</td>
<td>0.313</td>
<td>0.543</td>
<td>0.328</td>
<td>0.575</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik2021</td>
<td></td>
<td>1.02</td>
<td>0.330</td>
<td>0.544</td>
<td>0.374</td>
<td>0.586</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_1</td>
<td>CAM-based SED system</td>
<td>Bajzik2021</td>
<td></td>
<td>0.45</td>
<td>0.133</td>
<td>0.266</td>
<td>0.165</td>
<td>0.348</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_3</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>0.99</td>
<td>0.304</td>
<td>0.559</td>
<td>0.426</td>
<td>0.726</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>1.03</td>
<td>0.313</td>
<td>0.588</td>
<td>0.428</td>
<td>0.736</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>1.01</td>
<td>0.325</td>
<td>0.542</td>
<td>0.418</td>
<td>0.721</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>turpault2020a</td>
<td></td>
<td>1.00</td>
<td>0.315</td>
<td>0.547</td>
<td>0.342</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_1</td>
<td>DCASE2021_SED_A</td>
<td>Wang2021</td>
<td></td>
<td>1.13</td>
<td>0.336</td>
<td>0.646</td>
<td>0.407</td>
<td>0.703</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>DCASE2021_SED_D</td>
<td>Wang2021</td>
<td></td>
<td>1.09</td>
<td>0.304</td>
<td>0.662</td>
<td>0.370</td>
<td>0.724</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_2</td>
<td>DCASE2021_SED_B</td>
<td>Wang2021</td>
<td></td>
<td>0.69</td>
<td>0.070</td>
<td>0.636</td>
<td>0.061</td>
<td>0.808</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>DCASE2021_SED_C</td>
<td>Wang2021</td>
<td></td>
<td>1.13</td>
<td>0.339</td>
<td>0.649</td>
<td>0.388</td>
<td>0.672</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS_1_youtube" data-scatter-y="PSDS_1_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="PSDS_1_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
                PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="PSDS 1 (Public evaluation)" data-chartable="true" data-field="PSDS_1_youtube" data-sortable="true" data-value-type="float3">
                PSDS 1 <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 1 (Vimeo dataset)" data-chartable="true" data-field="PSDS_1_vimeo" data-sortable="true" data-value-type="float3">
                PSDS 1 <br/>(Vimeo dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
                PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="PSDS 2 (Public evaluation)" data-chartable="true" data-field="PSDS_2_youtube" data-sortable="true" data-value-type="float3">
                PSDS 2 <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Vimeo dataset)" data-chartable="true" data-field="PSDS_2_vimeo" data-sortable="true" data-value-type="float3">
                PSDS 2 <br/>(Vimeo dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                F-score <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="F-score 2 (Public evaluation)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                F-score <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                F-score <br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td></td>
<td>0.245</td>
<td>0.269</td>
<td>0.185</td>
<td>0.452</td>
<td>0.485</td>
<td>0.354</td>
<td>25.0</td>
<td>27.5</td>
<td>19.5</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_3</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>0.287</td>
<td>0.321</td>
<td>0.207</td>
<td>0.502</td>
<td>0.547</td>
<td>0.386</td>
<td>35.7</td>
<td>39.2</td>
<td>27.4</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_4</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>0.287</td>
<td>0.322</td>
<td>0.209</td>
<td>0.502</td>
<td>0.547</td>
<td>0.387</td>
<td>37.2</td>
<td>40.9</td>
<td>28.0</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_1</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>0.334</td>
<td>0.370</td>
<td>0.249</td>
<td>0.549</td>
<td>0.591</td>
<td>0.437</td>
<td>39.5</td>
<td>43.8</td>
<td>29.0</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>0.336</td>
<td>0.374</td>
<td>0.249</td>
<td>0.550</td>
<td>0.591</td>
<td>0.440</td>
<td>40.9</td>
<td>44.9</td>
<td>31.3</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>0.370</td>
<td>0.419</td>
<td>0.273</td>
<td>0.626</td>
<td>0.672</td>
<td>0.509</td>
<td>41.9</td>
<td>45.1</td>
<td>34.0</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_2</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>0.367</td>
<td>0.409</td>
<td>0.271</td>
<td>0.616</td>
<td>0.654</td>
<td>0.512</td>
<td>42.7</td>
<td>45.9</td>
<td>34.8</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_1</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>0.364</td>
<td>0.409</td>
<td>0.266</td>
<td>0.611</td>
<td>0.661</td>
<td>0.486</td>
<td>41.5</td>
<td>44.9</td>
<td>33.0</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td></td>
<td>0.327</td>
<td>0.371</td>
<td>0.240</td>
<td>0.603</td>
<td>0.644</td>
<td>0.492</td>
<td>38.4</td>
<td>42.2</td>
<td>28.9</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_4</td>
<td>Park_JHU_task4_SED_4</td>
<td>Park2021</td>
<td></td>
<td>0.237</td>
<td>0.267</td>
<td>0.174</td>
<td>0.524</td>
<td>0.568</td>
<td>0.417</td>
<td>36.9</td>
<td>39.8</td>
<td>29.6</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_1</td>
<td>Park_JHU_task4_SED_1</td>
<td>Park2021</td>
<td></td>
<td>0.305</td>
<td>0.344</td>
<td>0.214</td>
<td>0.579</td>
<td>0.617</td>
<td>0.471</td>
<td>34.7</td>
<td>37.8</td>
<td>26.9</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_3</td>
<td>Park_JHU_task4_SED_3</td>
<td>Park2021</td>
<td></td>
<td>0.222</td>
<td>0.244</td>
<td>0.166</td>
<td>0.537</td>
<td>0.578</td>
<td>0.430</td>
<td>33.5</td>
<td>35.4</td>
<td>28.5</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_4</td>
<td>DCASE2020 SED Mean teacher system 4</td>
<td>Zheng2021</td>
<td></td>
<td>0.389</td>
<td>0.438</td>
<td>0.261</td>
<td>0.742</td>
<td>0.775</td>
<td>0.644</td>
<td>49.5</td>
<td>54.2</td>
<td>36.9</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>DCASE2020 SED Mean teacher system 1</td>
<td>Zheng2021</td>
<td></td>
<td>0.452</td>
<td>0.517</td>
<td>0.318</td>
<td>0.669</td>
<td>0.725</td>
<td>0.530</td>
<td>52.3</td>
<td>57.4</td>
<td>39.2</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_3</td>
<td>DCASE2020 SED Mean teacher system 3</td>
<td>Zheng2021</td>
<td></td>
<td>0.386</td>
<td>0.429</td>
<td>0.270</td>
<td>0.746</td>
<td>0.778</td>
<td>0.650</td>
<td>49.7</td>
<td>55.0</td>
<td>36.3</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_2</td>
<td>DCASE2020 SED Mean teacher system 2</td>
<td>Zheng2021</td>
<td></td>
<td>0.447</td>
<td>0.506</td>
<td>0.318</td>
<td>0.676</td>
<td>0.730</td>
<td>0.546</td>
<td>52.9</td>
<td>57.7</td>
<td>40.2</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_mixupratip=0.8_nband=(2,3)_medianfilter=5</td>
<td>Nam2021</td>
<td></td>
<td>0.399</td>
<td>0.443</td>
<td>0.299</td>
<td>0.609</td>
<td>0.641</td>
<td>0.525</td>
<td>48.0</td>
<td>52.2</td>
<td>37.1</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_default</td>
<td>Nam2021</td>
<td></td>
<td>0.378</td>
<td>0.426</td>
<td>0.285</td>
<td>0.617</td>
<td>0.666</td>
<td>0.506</td>
<td>44.2</td>
<td>47.8</td>
<td>34.5</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_AFL</td>
<td>Nam2021</td>
<td></td>
<td>0.324</td>
<td>0.364</td>
<td>0.235</td>
<td>0.634</td>
<td>0.672</td>
<td>0.536</td>
<td>29.3</td>
<td>32.3</td>
<td>22.7</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Weak_SED</td>
<td>Nam2021</td>
<td></td>
<td>0.059</td>
<td>0.069</td>
<td>0.022</td>
<td>0.715</td>
<td>0.750</td>
<td>0.616</td>
<td>12.5</td>
<td>13.1</td>
<td>11.4</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_2</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.044</td>
<td>0.050</td>
<td>0.024</td>
<td>0.059</td>
<td>0.057</td>
<td>0.047</td>
<td>12.4</td>
<td>13.8</td>
<td>9.4</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_3</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.058</td>
<td>0.060</td>
<td>0.048</td>
<td>0.348</td>
<td>0.406</td>
<td>0.257</td>
<td>8.5</td>
<td>9.0</td>
<td>7.3</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.258</td>
<td>0.282</td>
<td>0.183</td>
<td>0.364</td>
<td>0.401</td>
<td>0.241</td>
<td>20.5</td>
<td>22.2</td>
<td>16.2</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>0.361</td>
<td>0.405</td>
<td>0.262</td>
<td>0.577</td>
<td>0.635</td>
<td>0.443</td>
<td>42.7</td>
<td>46.7</td>
<td>32.7</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>3-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>0.343</td>
<td>0.387</td>
<td>0.245</td>
<td>0.571</td>
<td>0.628</td>
<td>0.439</td>
<td>42.6</td>
<td>46.4</td>
<td>33.2</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>3-Resolution Mean Teacher (Higher time resolutions)</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>0.363</td>
<td>0.406</td>
<td>0.265</td>
<td>0.574</td>
<td>0.630</td>
<td>0.449</td>
<td>43.1</td>
<td>47.0</td>
<td>33.6</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_3</td>
<td>4-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>0.345</td>
<td>0.383</td>
<td>0.255</td>
<td>0.571</td>
<td>0.628</td>
<td>0.438</td>
<td>42.2</td>
<td>46.4</td>
<td>31.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>turpault2020b</td>
<td></td>
<td>0.364</td>
<td>0.407</td>
<td>0.283</td>
<td>0.580</td>
<td>0.627</td>
<td>0.471</td>
<td>42.0</td>
<td>44.9</td>
<td>34.7</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_4</td>
<td>CRNN with optimized pooling operations for scenario 2 (2)</td>
<td>Boes2021</td>
<td></td>
<td>0.117</td>
<td>0.131</td>
<td>0.078</td>
<td>0.457</td>
<td>0.500</td>
<td>0.346</td>
<td>10.6</td>
<td>11.8</td>
<td>7.9</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_3</td>
<td>CRNN with optimized pooling operations for scenario 2 (1)</td>
<td>Boes2021</td>
<td></td>
<td>0.121</td>
<td>0.139</td>
<td>0.081</td>
<td>0.531</td>
<td>0.555</td>
<td>0.435</td>
<td>14.0</td>
<td>15.9</td>
<td>9.3</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_2</td>
<td>CRNN with optimized pooling operations for scenario 1 (2)</td>
<td>Boes2021</td>
<td></td>
<td>0.233</td>
<td>0.266</td>
<td>0.143</td>
<td>0.440</td>
<td>0.489</td>
<td>0.310</td>
<td>31.2</td>
<td>34.4</td>
<td>22.6</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>CRNN with optimized pooling operations for scenario 1 (1)</td>
<td>Boes2021</td>
<td></td>
<td>0.253</td>
<td>0.290</td>
<td>0.150</td>
<td>0.442</td>
<td>0.483</td>
<td>0.319</td>
<td>31.0</td>
<td>34.7</td>
<td>21.3</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_2</td>
<td>UPB sytem 2</td>
<td>Ebbers2021</td>
<td></td>
<td>0.335</td>
<td>0.369</td>
<td>0.269</td>
<td>0.621</td>
<td>0.661</td>
<td>0.519</td>
<td>54.1</td>
<td>57.2</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>UPB sytem 4</td>
<td>Ebbers2021</td>
<td></td>
<td>0.363</td>
<td>0.407</td>
<td>0.285</td>
<td>0.637</td>
<td>0.683</td>
<td>0.533</td>
<td>56.7</td>
<td>59.6</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>UPB sytem 3</td>
<td>Ebbers2021</td>
<td></td>
<td>0.416</td>
<td>0.455</td>
<td>0.328</td>
<td>0.635</td>
<td>0.684</td>
<td>0.519</td>
<td>56.7</td>
<td>59.6</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>UPB sytem 1</td>
<td>Ebbers2021</td>
<td></td>
<td>0.373</td>
<td>0.410</td>
<td>0.300</td>
<td>0.621</td>
<td>0.661</td>
<td>0.516</td>
<td>54.1</td>
<td>57.2</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu2021</td>
<td></td>
<td>0.290</td>
<td>0.319</td>
<td>0.216</td>
<td>0.574</td>
<td>0.640</td>
<td>0.438</td>
<td>43.0</td>
<td>47.1</td>
<td>33.0</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td></td>
<td>0.318</td>
<td>0.357</td>
<td>0.238</td>
<td>0.583</td>
<td>0.641</td>
<td>0.451</td>
<td>40.2</td>
<td>43.5</td>
<td>32.3</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_4</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.102</td>
<td>0.123</td>
<td>0.043</td>
<td>0.231</td>
<td>0.244</td>
<td>0.165</td>
<td>17.5</td>
<td>19.4</td>
<td>12.9</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_1</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.090</td>
<td>0.101</td>
<td>0.040</td>
<td>0.169</td>
<td>0.176</td>
<td>0.110</td>
<td>18.1</td>
<td>19.6</td>
<td>13.8</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.152</td>
<td>0.173</td>
<td>0.099</td>
<td>0.322</td>
<td>0.347</td>
<td>0.234</td>
<td>23.6</td>
<td>25.7</td>
<td>18.2</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_3</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.068</td>
<td>0.086</td>
<td>0.012</td>
<td>0.146</td>
<td>0.152</td>
<td>0.104</td>
<td>15.1</td>
<td>16.9</td>
<td>10.8</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera2021</td>
<td></td>
<td>0.338</td>
<td>0.382</td>
<td>0.218</td>
<td>0.481</td>
<td>0.528</td>
<td>0.357</td>
<td>43.4</td>
<td>48.4</td>
<td>30.0</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_1</td>
<td>DA-SED + FG/BG</td>
<td>Olvera2021</td>
<td></td>
<td>0.332</td>
<td>0.375</td>
<td>0.205</td>
<td>0.462</td>
<td>0.506</td>
<td>0.333</td>
<td>45.5</td>
<td>50.2</td>
<td>33.1</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>0.442</td>
<td>0.492</td>
<td>0.330</td>
<td>0.674</td>
<td>0.715</td>
<td>0.573</td>
<td>50.6</td>
<td>53.3</td>
<td>43.5</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>0.439</td>
<td>0.492</td>
<td>0.319</td>
<td>0.667</td>
<td>0.710</td>
<td>0.564</td>
<td>50.5</td>
<td>53.3</td>
<td>43.0</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>0.434</td>
<td>0.481</td>
<td>0.326</td>
<td>0.669</td>
<td>0.709</td>
<td>0.570</td>
<td>49.4</td>
<td>52.4</td>
<td>41.8</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>0.431</td>
<td>0.478</td>
<td>0.320</td>
<td>0.661</td>
<td>0.702</td>
<td>0.554</td>
<td>49.9</td>
<td>52.3</td>
<td>43.6</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_1</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_1</td>
<td>Dinkel2021</td>
<td></td>
<td>0.361</td>
<td>0.406</td>
<td>0.239</td>
<td>0.584</td>
<td>0.654</td>
<td>0.418</td>
<td>37.8</td>
<td>41.4</td>
<td>28.3</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_2</td>
<td>Dinkel2021</td>
<td></td>
<td>0.373</td>
<td>0.423</td>
<td>0.243</td>
<td>0.585</td>
<td>0.652</td>
<td>0.422</td>
<td>38.8</td>
<td>41.9</td>
<td>30.3</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_3</td>
<td>Dinkel2021</td>
<td></td>
<td>0.370</td>
<td>0.419</td>
<td>0.241</td>
<td>0.596</td>
<td>0.662</td>
<td>0.433</td>
<td>38.8</td>
<td>42.0</td>
<td>30.7</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_4</td>
<td>DCASE2021_Cai_SED_CDur_Single_4</td>
<td>Dinkel2021</td>
<td></td>
<td>0.339</td>
<td>0.386</td>
<td>0.212</td>
<td>0.504</td>
<td>0.561</td>
<td>0.356</td>
<td>38.4</td>
<td>42.0</td>
<td>29.5</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYu2021</td>
<td></td>
<td>0.294</td>
<td>0.327</td>
<td>0.205</td>
<td>0.473</td>
<td>0.510</td>
<td>0.350</td>
<td>34.2</td>
<td>37.8</td>
<td>25.5</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>YuHang2021</td>
<td></td>
<td>0.098</td>
<td>0.104</td>
<td>0.090</td>
<td>0.496</td>
<td>0.515</td>
<td>0.391</td>
<td>10.7</td>
<td>11.7</td>
<td>8.7</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_1</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td></td>
<td>0.038</td>
<td>0.039</td>
<td>0.045</td>
<td>0.157</td>
<td>0.182</td>
<td>0.144</td>
<td>6.8</td>
<td>7.9</td>
<td>4.0</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td></td>
<td>0.301</td>
<td>0.341</td>
<td>0.197</td>
<td>0.485</td>
<td>0.528</td>
<td>0.360</td>
<td>34.4</td>
<td>37.8</td>
<td>25.7</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>DCASE2021 SED CRNN Model1</td>
<td>Lu2021</td>
<td></td>
<td>0.419</td>
<td>0.468</td>
<td>0.314</td>
<td>0.660</td>
<td>0.702</td>
<td>0.556</td>
<td>45.0</td>
<td>48.6</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_4</td>
<td>DCASE2021 SED Conformer Model2</td>
<td>Lu2021</td>
<td></td>
<td>0.157</td>
<td>0.177</td>
<td>0.125</td>
<td>0.685</td>
<td>0.714</td>
<td>0.598</td>
<td>15.7</td>
<td>16.6</td>
<td>14.6</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_3</td>
<td>DCASE2021 SED Conformer Model1</td>
<td>Lu2021</td>
<td></td>
<td>0.148</td>
<td>0.170</td>
<td>0.114</td>
<td>0.686</td>
<td>0.715</td>
<td>0.597</td>
<td>15.6</td>
<td>16.7</td>
<td>14.0</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_2</td>
<td>DCASE2021 SED CRNN Model2</td>
<td>Lu2021</td>
<td></td>
<td>0.412</td>
<td>0.461</td>
<td>0.313</td>
<td>0.651</td>
<td>0.694</td>
<td>0.550</td>
<td>45.5</td>
<td>48.9</td>
<td>36.9</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td></td>
<td>0.302</td>
<td>0.328</td>
<td>0.235</td>
<td>0.507</td>
<td>0.537</td>
<td>0.410</td>
<td>37.6</td>
<td>40.5</td>
<td>30.5</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_1</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td></td>
<td>0.302</td>
<td>0.328</td>
<td>0.235</td>
<td>0.507</td>
<td>0.537</td>
<td>0.410</td>
<td>38.4</td>
<td>40.9</td>
<td>32.2</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_2</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>0.411</td>
<td>0.462</td>
<td>0.307</td>
<td>0.585</td>
<td>0.639</td>
<td>0.473</td>
<td>38.3</td>
<td>41.2</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>0.413</td>
<td>0.468</td>
<td>0.306</td>
<td>0.586</td>
<td>0.640</td>
<td>0.473</td>
<td>38.3</td>
<td>41.2</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_4</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>0.412</td>
<td>0.467</td>
<td>0.306</td>
<td>0.586</td>
<td>0.639</td>
<td>0.473</td>
<td>38.3</td>
<td>41.2</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_3</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>0.409</td>
<td>0.456</td>
<td>0.307</td>
<td>0.584</td>
<td>0.637</td>
<td>0.472</td>
<td>38.3</td>
<td>41.2</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.279</td>
<td>0.312</td>
<td>0.197</td>
<td>0.479</td>
<td>0.526</td>
<td>0.357</td>
<td>34.2</td>
<td>37.1</td>
<td>27.4</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_1</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.277</td>
<td>0.305</td>
<td>0.215</td>
<td>0.482</td>
<td>0.510</td>
<td>0.388</td>
<td>31.9</td>
<td>34.2</td>
<td>26.4</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_2</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.056</td>
<td>0.064</td>
<td>0.048</td>
<td>0.496</td>
<td>0.529</td>
<td>0.389</td>
<td>8.9</td>
<td>9.5</td>
<td>7.5</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang2021</td>
<td></td>
<td>0.313</td>
<td>0.349</td>
<td>0.226</td>
<td>0.543</td>
<td>0.589</td>
<td>0.422</td>
<td>36.0</td>
<td>39.5</td>
<td>27.5</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik2021</td>
<td></td>
<td>0.330</td>
<td>0.383</td>
<td>0.216</td>
<td>0.544</td>
<td>0.602</td>
<td>0.398</td>
<td>39.8</td>
<td>43.7</td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_1</td>
<td>CAM-based SED system</td>
<td>Bajzik2021</td>
<td></td>
<td>0.133</td>
<td>0.140</td>
<td>0.081</td>
<td>0.266</td>
<td>0.259</td>
<td>0.219</td>
<td>13.7</td>
<td>15.2</td>
<td>9.7</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_3</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>0.304</td>
<td>0.345</td>
<td>0.218</td>
<td>0.559</td>
<td>0.604</td>
<td>0.441</td>
<td>34.2</td>
<td>37.0</td>
<td>27.8</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>0.313</td>
<td>0.348</td>
<td>0.235</td>
<td>0.588</td>
<td>0.639</td>
<td>0.462</td>
<td>34.6</td>
<td>38.1</td>
<td>26.5</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>0.325</td>
<td>0.371</td>
<td>0.240</td>
<td>0.542</td>
<td>0.600</td>
<td>0.408</td>
<td>37.0</td>
<td>40.5</td>
<td>28.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>turpault2020a</td>
<td></td>
<td>0.315</td>
<td>0.359</td>
<td>0.222</td>
<td>0.547</td>
<td>0.596</td>
<td>0.407</td>
<td>37.3</td>
<td>40.8</td>
<td>29.7</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_1</td>
<td>DCASE2021_SED_A</td>
<td>Wang2021</td>
<td></td>
<td>0.336</td>
<td>0.379</td>
<td>0.253</td>
<td>0.646</td>
<td>0.692</td>
<td>0.537</td>
<td>43.0</td>
<td>47.3</td>
<td>32.3</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>DCASE2021_SED_D</td>
<td>Wang2021</td>
<td></td>
<td>0.304</td>
<td>0.340</td>
<td>0.233</td>
<td>0.662</td>
<td>0.710</td>
<td>0.554</td>
<td>38.2</td>
<td>41.3</td>
<td>30.4</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_2</td>
<td>DCASE2021_SED_B</td>
<td>Wang2021</td>
<td></td>
<td>0.070</td>
<td>0.081</td>
<td>0.050</td>
<td>0.636</td>
<td>0.672</td>
<td>0.552</td>
<td>9.9</td>
<td>10.1</td>
<td>9.5</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>DCASE2021_SED_C</td>
<td>Wang2021</td>
<td></td>
<td>0.339</td>
<td>0.384</td>
<td>0.251</td>
<td>0.649</td>
<td>0.698</td>
<td>0.540</td>
<td>43.0</td>
<td>46.4</td>
<td>34.4</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best ranking score per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS_1_all" data-scatter-y="PSDS_2_all" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code_1" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 1)
            </th>
<th class="sep-right-cell sm-cell" data-field="name_1" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 1)
            </th>
<th data-field="code_2" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 2)
            </th>
<th class="sm-cell" data-field="name_2" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 2)
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td></td>
<td>0.80</td>
<td>0.245</td>
<td>0.452</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>1.04</td>
<td>0.336</td>
<td>0.550</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>1.16</td>
<td>0.370</td>
<td>0.626</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td></td>
<td>1.07</td>
<td>0.327</td>
<td>0.603</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>DCASE2020 SED Mean teacher system 1</td>
<td>Zheng_USTC_task4_SED_3</td>
<td>DCASE2020 SED Mean teacher system 3</td>
<td>Zheng2021</td>
<td></td>
<td>1.40</td>
<td>0.452</td>
<td>0.746</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_mixupratip=0.8_nband=(2,3)_medianfilter=5</td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Weak_SED</td>
<td>Nam2021</td>
<td></td>
<td>1.29</td>
<td>0.399</td>
<td>0.715</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.74</td>
<td>0.258</td>
<td>0.364</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>3-Resolution Mean Teacher (Higher time resolutions)</td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>1.10</td>
<td>0.363</td>
<td>0.577</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>turpault2020b</td>
<td></td>
<td>1.11</td>
<td>0.364</td>
<td>0.580</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>CRNN with optimized pooling operations for scenario 1 (1)</td>
<td>Boes_KUL_task4_SED_3</td>
<td>CRNN with optimized pooling operations for scenario 2 (1)</td>
<td>Boes2021</td>
<td></td>
<td>0.89</td>
<td>0.253</td>
<td>0.531</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>UPB sytem 3</td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>UPB sytem 4</td>
<td>Ebbers2021</td>
<td></td>
<td>1.24</td>
<td>0.416</td>
<td>0.637</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td></td>
<td>1.04</td>
<td>0.318</td>
<td>0.583</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.54</td>
<td>0.152</td>
<td>0.322</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera2021</td>
<td></td>
<td>0.98</td>
<td>0.338</td>
<td>0.481</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>1.32</td>
<td>0.442</td>
<td>0.674</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_2</td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_3</td>
<td>Dinkel2021</td>
<td></td>
<td>1.14</td>
<td>0.373</td>
<td>0.596</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYu2021</td>
<td></td>
<td>0.90</td>
<td>0.294</td>
<td>0.473</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>YuHang2021</td>
<td></td>
<td>0.61</td>
<td>0.098</td>
<td>0.496</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td></td>
<td>0.92</td>
<td>0.301</td>
<td>0.485</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>DCASE2021 SED CRNN Model1</td>
<td>lu_kwai_task4_SED_3</td>
<td>DCASE2021 SED Conformer Model1</td>
<td>Lu2021</td>
<td></td>
<td>1.29</td>
<td>0.419</td>
<td>0.686</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td></td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>1.19</td>
<td>0.413</td>
<td>0.586</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao_GUET_task4_SED_2</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.90</td>
<td>0.279</td>
<td>0.496</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang2021</td>
<td></td>
<td>0.99</td>
<td>0.313</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik2021</td>
<td></td>
<td>1.02</td>
<td>0.330</td>
<td>0.544</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Mean teacher system</td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>1.05</td>
<td>0.325</td>
<td>0.588</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>turpault2020a</td>
<td></td>
<td>1.00</td>
<td>0.315</td>
<td>0.547</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>DCASE2021_SED_C</td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>DCASE2021_SED_D</td>
<td>Wang2021</td>
<td></td>
<td>1.14</td>
<td>0.339</td>
<td>0.662</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics-1">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="ranking_score_youtube" data-scatter-y="ranking_score_vimeo" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code_1" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 1)
            </th>
<th class="sep-right-cell sm-cell" data-field="name_1" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 1)
            </th>
<th data-field="code_2" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 2)
            </th>
<th class="sm-cell" data-field="name_2" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 2)
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation<br/>
</th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="Ranking score (Public evaluation)" data-chartable="true" data-field="ranking_score_youtube" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="Ranking score (Vimeo dataset)" data-chartable="true" data-field="ranking_score_vimeo" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td></td>
<td>0.80</td>
<td>0.78</td>
<td>0.85</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td></td>
<td>1.04</td>
<td>1.02</td>
<td>1.10</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td></td>
<td>1.16</td>
<td>1.15</td>
<td>1.24</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td></td>
<td>1.07</td>
<td>1.06</td>
<td>1.15</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>DCASE2020 SED Mean teacher system 1</td>
<td>Zheng_USTC_task4_SED_3</td>
<td>DCASE2020 SED Mean teacher system 3</td>
<td>Zheng2021</td>
<td></td>
<td>1.40</td>
<td>1.37</td>
<td>1.52</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_mixupratip=0.8_nband=(2,3)_medianfilter=5</td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Weak_SED</td>
<td>Nam2021</td>
<td></td>
<td>1.29</td>
<td>1.25</td>
<td>1.43</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td></td>
<td>0.74</td>
<td>0.73</td>
<td>0.71</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>3-Resolution Mean Teacher (Higher time resolutions)</td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td></td>
<td>1.10</td>
<td>1.10</td>
<td>1.14</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>turpault2020b</td>
<td></td>
<td>1.11</td>
<td>1.09</td>
<td>1.22</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>CRNN with optimized pooling operations for scenario 1 (1)</td>
<td>Boes_KUL_task4_SED_3</td>
<td>CRNN with optimized pooling operations for scenario 2 (1)</td>
<td>Boes2021</td>
<td></td>
<td>0.89</td>
<td>0.87</td>
<td>0.87</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>UPB sytem 3</td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>UPB sytem 4</td>
<td>Ebbers2021</td>
<td></td>
<td>1.24</td>
<td>1.21</td>
<td>1.39</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td></td>
<td>1.04</td>
<td>1.03</td>
<td>1.09</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td></td>
<td>0.54</td>
<td>0.53</td>
<td>0.51</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera2021</td>
<td></td>
<td>0.98</td>
<td>0.97</td>
<td>0.93</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td></td>
<td>1.32</td>
<td>1.28</td>
<td>1.45</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_2</td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_3</td>
<td>Dinkel2021</td>
<td></td>
<td>1.14</td>
<td>1.14</td>
<td>1.08</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYu2021</td>
<td></td>
<td>0.90</td>
<td>0.88</td>
<td>0.89</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>YuHang2021</td>
<td></td>
<td>0.61</td>
<td>0.58</td>
<td>0.68</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td></td>
<td>0.92</td>
<td>0.92</td>
<td>0.89</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>DCASE2021 SED CRNN Model1</td>
<td>lu_kwai_task4_SED_3</td>
<td>DCASE2021 SED Conformer Model1</td>
<td>Lu2021</td>
<td></td>
<td>1.29</td>
<td>1.25</td>
<td>1.44</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td></td>
<td>0.94</td>
<td>0.91</td>
<td>1.03</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td></td>
<td>1.19</td>
<td>1.19</td>
<td>1.27</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao_GUET_task4_SED_2</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td></td>
<td>0.90</td>
<td>0.88</td>
<td>0.92</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang2021</td>
<td></td>
<td>0.99</td>
<td>0.98</td>
<td>1.03</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik2021</td>
<td></td>
<td>1.02</td>
<td>1.04</td>
<td>0.98</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Mean teacher system</td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td></td>
<td>1.05</td>
<td>1.05</td>
<td>1.11</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>turpault2020a</td>
<td></td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>DCASE2021_SED_C</td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>DCASE2021_SED_D</td>
<td>Wang2021</td>
<td></td>
<td>1.14</td>
<td>1.13</td>
<td>1.25</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="Baseline SED" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Baseline SSep_SED" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["Class_f_score_Alarm_bell_ringing", "Class_f_score_Blender", "Class_f_score_Cat", "Class_f_score_Dishes", "Class_f_score_Dog", "Class_f_score_Electric_shaver_toothbrush", "Class_f_score_Frying", "Class_f_score_Running_water", "Class_f_score_Speech", "Class_f_score_Vacuum_cleaner"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="Class_f_score_Frying" data-scatter-y="Class_f_score_Running_water" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission<br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="Class_f_score_Alarm_bell_ringing" data-sortable="true" data-value-type="float1-percentage">
                Alarm<br/>Bell<br/>Ringing
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Blender" data-sortable="true" data-value-type="float1-percentage">
                Blender
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Cat" data-sortable="true" data-value-type="float1-percentage">
                Cat
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dishes" data-sortable="true" data-value-type="float1-percentage">
                Dishes
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dog" data-sortable="true" data-value-type="float1-percentage">
                Dog
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Electric_shaver_toothbrush" data-sortable="true" data-value-type="float1-percentage">
                Electric<br/>shave<br/>toothbrush
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Frying" data-sortable="true" data-value-type="float1-percentage">
                Frying
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Running_water" data-sortable="true" data-value-type="float1-percentage">
                Running<br/>water
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Speech" data-sortable="true" data-value-type="float1-percentage">
                Speech
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Vacuum_cleaner" data-sortable="true" data-value-type="float1-percentage">
                Vacuum<br/>cleaner
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td>0.80</td>
<td>23.5</td>
<td>28.6</td>
<td>42.5</td>
<td>25.0</td>
<td>16.5</td>
<td>15.7</td>
<td>19.3</td>
<td>19.9</td>
<td>35.4</td>
<td>23.2</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_3</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>25.0</td>
<td>39.7</td>
<td>53.7</td>
<td>19.4</td>
<td>28.3</td>
<td>39.7</td>
<td>38.3</td>
<td>25.1</td>
<td>49.3</td>
<td>38.2</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_4</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>25.7</td>
<td>40.8</td>
<td>50.7</td>
<td>26.5</td>
<td>28.8</td>
<td>39.7</td>
<td>42.6</td>
<td>26.4</td>
<td>49.7</td>
<td>41.0</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_1</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td>1.03</td>
<td>30.4</td>
<td>38.1</td>
<td>63.7</td>
<td>27.6</td>
<td>29.1</td>
<td>35.6</td>
<td>37.0</td>
<td>28.4</td>
<td>52.7</td>
<td>52.3</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>TASK AWARE SOUND EVENT DETECTION BASED ON SEMI-SUPERVISED CRNN WITH SKIP CONNECTIONS DCASE 2021 CHALLENGE, TASK 4</td>
<td>Hafsati2021</td>
<td>1.04</td>
<td>32.8</td>
<td>39.0</td>
<td>63.3</td>
<td>28.9</td>
<td>32.8</td>
<td>39.7</td>
<td>41.0</td>
<td>27.9</td>
<td>51.5</td>
<td>52.6</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td>1.16</td>
<td>33.3</td>
<td>49.8</td>
<td>61.9</td>
<td>34.6</td>
<td>31.6</td>
<td>39.8</td>
<td>41.8</td>
<td>26.9</td>
<td>45.0</td>
<td>54.3</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_2</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td>1.15</td>
<td>35.0</td>
<td>48.1</td>
<td>62.1</td>
<td>33.9</td>
<td>36.3</td>
<td>40.3</td>
<td>41.4</td>
<td>28.1</td>
<td>45.8</td>
<td>55.7</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_1</td>
<td>TAL SED system</td>
<td>Gong2021</td>
<td>1.14</td>
<td>33.9</td>
<td>48.7</td>
<td>61.3</td>
<td>34.7</td>
<td>29.3</td>
<td>42.4</td>
<td>39.8</td>
<td>27.7</td>
<td>44.1</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td>1.07</td>
<td>25.7</td>
<td>41.8</td>
<td>52.2</td>
<td>10.1</td>
<td>27.2</td>
<td>40.4</td>
<td>47.7</td>
<td>36.7</td>
<td>58.0</td>
<td>44.3</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_4</td>
<td>Park_JHU_task4_SED_4</td>
<td>Park2021</td>
<td>0.86</td>
<td>25.9</td>
<td>42.1</td>
<td>33.4</td>
<td>34.0</td>
<td>17.2</td>
<td>38.9</td>
<td>50.0</td>
<td>35.9</td>
<td>50.6</td>
<td>41.5</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_1</td>
<td>Park_JHU_task4_SED_1</td>
<td>Park2021</td>
<td>1.01</td>
<td>22.8</td>
<td>40.3</td>
<td>43.6</td>
<td>8.2</td>
<td>22.9</td>
<td>33.5</td>
<td>43.2</td>
<td>34.8</td>
<td>58.5</td>
<td>39.0</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_3</td>
<td>Park_JHU_task4_SED_3</td>
<td>Park2021</td>
<td>0.84</td>
<td>21.8</td>
<td>40.2</td>
<td>24.4</td>
<td>32.2</td>
<td>13.9</td>
<td>35.1</td>
<td>44.4</td>
<td>33.3</td>
<td>51.5</td>
<td>37.8</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_4</td>
<td>DCASE2020 SED Mean teacher system 4</td>
<td>Zheng2021</td>
<td>1.30</td>
<td>36.1</td>
<td>53.3</td>
<td>70.4</td>
<td>18.8</td>
<td>45.7</td>
<td>58.2</td>
<td>40.4</td>
<td>32.5</td>
<td>70.6</td>
<td>68.7</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>DCASE2020 SED Mean teacher system 1</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>41.4</td>
<td>54.1</td>
<td>72.5</td>
<td>29.4</td>
<td>47.8</td>
<td>60.1</td>
<td>49.2</td>
<td>33.7</td>
<td>69.5</td>
<td>65.5</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_3</td>
<td>DCASE2020 SED Mean teacher system 3</td>
<td>Zheng2021</td>
<td>1.29</td>
<td>36.4</td>
<td>52.5</td>
<td>70.9</td>
<td>20.9</td>
<td>42.9</td>
<td>59.0</td>
<td>43.3</td>
<td>34.1</td>
<td>68.7</td>
<td>68.7</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_2</td>
<td>DCASE2020 SED Mean teacher system 2</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>36.6</td>
<td>55.1</td>
<td>75.3</td>
<td>29.8</td>
<td>45.6</td>
<td>55.7</td>
<td>53.6</td>
<td>38.6</td>
<td>69.3</td>
<td>69.5</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_mixupratip=0.8_nband=(2,3)_medianfilter=5</td>
<td>Nam2021</td>
<td>1.19</td>
<td>34.2</td>
<td>55.4</td>
<td>70.5</td>
<td>39.6</td>
<td>46.2</td>
<td>44.7</td>
<td>36.2</td>
<td>39.3</td>
<td>55.7</td>
<td>58.6</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_default</td>
<td>Nam2021</td>
<td>1.16</td>
<td>28.6</td>
<td>58.3</td>
<td>69.8</td>
<td>30.3</td>
<td>37.0</td>
<td>38.1</td>
<td>37.8</td>
<td>35.7</td>
<td>51.7</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_AFL</td>
<td>Nam2021</td>
<td>1.09</td>
<td>27.9</td>
<td>36.9</td>
<td>25.2</td>
<td>9.8</td>
<td>7.2</td>
<td>30.0</td>
<td>32.8</td>
<td>33.0</td>
<td>40.6</td>
<td>49.8</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Weak_SED</td>
<td>Nam2021</td>
<td>0.75</td>
<td>5.3</td>
<td>3.3</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>13.6</td>
<td>43.2</td>
<td>23.5</td>
<td>0.3</td>
<td>35.0</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_2</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td>0.12</td>
<td>0.0</td>
<td>20.5</td>
<td>9.9</td>
<td>1.0</td>
<td>2.3</td>
<td>12.5</td>
<td>20.0</td>
<td>15.7</td>
<td>26.8</td>
<td>14.8</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_3</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td>0.41</td>
<td>2.5</td>
<td>7.7</td>
<td>2.2</td>
<td>0.8</td>
<td>1.2</td>
<td>7.8</td>
<td>22.5</td>
<td>15.3</td>
<td>1.8</td>
<td>23.2</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>DCASE2021 SED system using wav2vec</td>
<td>Koo2021</td>
<td>0.74</td>
<td>15.4</td>
<td>23.5</td>
<td>30.5</td>
<td>15.1</td>
<td>20.6</td>
<td>21.1</td>
<td>21.1</td>
<td>18.5</td>
<td>19.0</td>
<td>20.0</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>37.4</td>
<td>57.1</td>
<td>63.8</td>
<td>24.2</td>
<td>34.5</td>
<td>30.0</td>
<td>46.8</td>
<td>25.9</td>
<td>49.8</td>
<td>57.3</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>3-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>37.6</td>
<td>58.1</td>
<td>63.1</td>
<td>23.9</td>
<td>34.2</td>
<td>35.4</td>
<td>43.5</td>
<td>29.8</td>
<td>49.3</td>
<td>51.3</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>3-Resolution Mean Teacher (Higher time resolutions)</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>37.1</td>
<td>51.4</td>
<td>63.9</td>
<td>26.0</td>
<td>36.9</td>
<td>28.9</td>
<td>46.9</td>
<td>30.5</td>
<td>52.0</td>
<td>57.5</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_3</td>
<td>4-Resolution Mean Teacher</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>36.2</td>
<td>57.6</td>
<td>63.1</td>
<td>24.4</td>
<td>34.8</td>
<td>35.0</td>
<td>41.9</td>
<td>27.1</td>
<td>48.2</td>
<td>53.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>DCASE2021 SSep SED baseline system</td>
<td>turpault2020b</td>
<td>1.11</td>
<td>36.7</td>
<td>47.4</td>
<td>66.3</td>
<td>33.1</td>
<td>40.5</td>
<td>34.8</td>
<td>37.2</td>
<td>21.5</td>
<td>53.0</td>
<td>49.3</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_4</td>
<td>CRNN with optimized pooling operations for scenario 2 (2)</td>
<td>Boes2021</td>
<td>0.60</td>
<td>3.7</td>
<td>24.2</td>
<td>1.4</td>
<td>0.0</td>
<td>0.6</td>
<td>13.9</td>
<td>23.7</td>
<td>12.6</td>
<td>6.2</td>
<td>19.9</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_3</td>
<td>CRNN with optimized pooling operations for scenario 2 (1)</td>
<td>Boes2021</td>
<td>0.68</td>
<td>6.6</td>
<td>16.5</td>
<td>1.4</td>
<td>0.0</td>
<td>0.3</td>
<td>21.3</td>
<td>33.0</td>
<td>18.7</td>
<td>6.6</td>
<td>35.1</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_2</td>
<td>CRNN with optimized pooling operations for scenario 1 (2)</td>
<td>Boes2021</td>
<td>0.77</td>
<td>16.9</td>
<td>32.9</td>
<td>63.1</td>
<td>7.7</td>
<td>19.4</td>
<td>25.6</td>
<td>32.6</td>
<td>14.8</td>
<td>51.8</td>
<td>47.7</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>CRNN with optimized pooling operations for scenario 1 (1)</td>
<td>Boes2021</td>
<td>0.81</td>
<td>19.0</td>
<td>29.0</td>
<td>59.1</td>
<td>7.7</td>
<td>20.9</td>
<td>34.5</td>
<td>24.8</td>
<td>13.0</td>
<td>54.0</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_2</td>
<td>UPB sytem 2</td>
<td>Ebbers2021</td>
<td>1.10</td>
<td>37.2</td>
<td>60.8</td>
<td>73.0</td>
<td>24.2</td>
<td>45.6</td>
<td>58.5</td>
<td>65.9</td>
<td>36.9</td>
<td>65.0</td>
<td>73.5</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>UPB sytem 4</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>39.2</td>
<td>61.7</td>
<td>74.2</td>
<td>33.4</td>
<td>46.6</td>
<td>57.1</td>
<td>64.0</td>
<td>45.4</td>
<td>67.6</td>
<td>77.6</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>UPB sytem 3</td>
<td>Ebbers2021</td>
<td>1.24</td>
<td>39.2</td>
<td>61.7</td>
<td>74.2</td>
<td>33.4</td>
<td>46.6</td>
<td>57.1</td>
<td>64.0</td>
<td>45.4</td>
<td>67.6</td>
<td>77.6</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>UPB sytem 1</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>37.2</td>
<td>60.8</td>
<td>73.0</td>
<td>24.2</td>
<td>45.6</td>
<td>58.5</td>
<td>65.9</td>
<td>36.9</td>
<td>65.0</td>
<td>73.5</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu2021</td>
<td>0.99</td>
<td>30.0</td>
<td>46.3</td>
<td>63.3</td>
<td>23.6</td>
<td>16.8</td>
<td>44.2</td>
<td>47.5</td>
<td>40.9</td>
<td>59.4</td>
<td>57.6</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td>1.04</td>
<td>31.8</td>
<td>48.2</td>
<td>58.3</td>
<td>28.5</td>
<td>26.7</td>
<td>37.4</td>
<td>48.6</td>
<td>36.0</td>
<td>51.1</td>
<td>35.3</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_4</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td>0.37</td>
<td>14.0</td>
<td>26.2</td>
<td>40.2</td>
<td>9.3</td>
<td>16.4</td>
<td>18.6</td>
<td>7.7</td>
<td>5.0</td>
<td>26.4</td>
<td>11.4</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_1</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td>0.30</td>
<td>13.1</td>
<td>18.1</td>
<td>49.0</td>
<td>8.6</td>
<td>19.3</td>
<td>20.0</td>
<td>6.4</td>
<td>6.2</td>
<td>28.6</td>
<td>11.4</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td>0.54</td>
<td>19.6</td>
<td>30.4</td>
<td>36.5</td>
<td>14.5</td>
<td>18.5</td>
<td>29.0</td>
<td>18.6</td>
<td>11.8</td>
<td>30.1</td>
<td>27.2</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_3</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu2021</td>
<td>0.24</td>
<td>16.5</td>
<td>19.0</td>
<td>37.2</td>
<td>6.2</td>
<td>19.3</td>
<td>13.7</td>
<td>3.5</td>
<td>6.8</td>
<td>25.0</td>
<td>3.8</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>SED ensemble 2 OT + FG/BG</td>
<td>Olvera2021</td>
<td>0.98</td>
<td>46.0</td>
<td>47.8</td>
<td>63.5</td>
<td>23.2</td>
<td>39.1</td>
<td>51.1</td>
<td>20.4</td>
<td>27.0</td>
<td>62.2</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_1</td>
<td>DA-SED + FG/BG</td>
<td>Olvera2021</td>
<td>0.95</td>
<td>43.7</td>
<td>52.3</td>
<td>63.6</td>
<td>30.0</td>
<td>40.8</td>
<td>52.6</td>
<td>24.4</td>
<td>26.6</td>
<td>63.9</td>
<td>56.9</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td>1.32</td>
<td>34.7</td>
<td>59.8</td>
<td>71.6</td>
<td>40.4</td>
<td>47.3</td>
<td>26.2</td>
<td>61.8</td>
<td>32.8</td>
<td>64.9</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td>1.31</td>
<td>37.9</td>
<td>57.4</td>
<td>72.9</td>
<td>41.8</td>
<td>46.8</td>
<td>25.2</td>
<td>60.5</td>
<td>36.9</td>
<td>64.3</td>
<td>60.8</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td>1.30</td>
<td>37.4</td>
<td>55.4</td>
<td>71.9</td>
<td>41.0</td>
<td>44.6</td>
<td>26.5</td>
<td>59.5</td>
<td>32.3</td>
<td>64.6</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>RCRNN-based noisy student SED</td>
<td>Kim2021</td>
<td>1.29</td>
<td>33.0</td>
<td>57.1</td>
<td>70.0</td>
<td>42.5</td>
<td>49.6</td>
<td>28.2</td>
<td>60.6</td>
<td>31.3</td>
<td>65.0</td>
<td>62.3</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_1</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_1</td>
<td>Dinkel2021</td>
<td>1.11</td>
<td>37.0</td>
<td>32.2</td>
<td>55.9</td>
<td>31.2</td>
<td>20.4</td>
<td>37.8</td>
<td>33.8</td>
<td>23.9</td>
<td>60.1</td>
<td>45.3</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_2</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>37.8</td>
<td>37.4</td>
<td>53.8</td>
<td>31.8</td>
<td>22.1</td>
<td>35.9</td>
<td>32.3</td>
<td>28.9</td>
<td>61.3</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>DCASE2021_Cai_SED_CDur_Ensemble_3</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>36.6</td>
<td>36.6</td>
<td>55.7</td>
<td>31.7</td>
<td>21.2</td>
<td>36.1</td>
<td>37.7</td>
<td>25.0</td>
<td>61.1</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_4</td>
<td>DCASE2021_Cai_SED_CDur_Single_4</td>
<td>Dinkel2021</td>
<td>1.00</td>
<td>34.9</td>
<td>34.1</td>
<td>52.5</td>
<td>30.8</td>
<td>28.0</td>
<td>37.6</td>
<td>35.1</td>
<td>24.9</td>
<td>61.3</td>
<td>44.4</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>DCASE2021 SED system</td>
<td>HangYu2021</td>
<td>0.90</td>
<td>29.0</td>
<td>30.7</td>
<td>59.3</td>
<td>24.5</td>
<td>31.8</td>
<td>35.3</td>
<td>30.2</td>
<td>26.0</td>
<td>49.3</td>
<td>25.9</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>DCASE2021 SED system</td>
<td>YuHang2021</td>
<td>0.61</td>
<td>5.2</td>
<td>4.8</td>
<td>5.6</td>
<td>4.3</td>
<td>2.7</td>
<td>12.5</td>
<td>26.8</td>
<td>16.0</td>
<td>5.1</td>
<td>24.0</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_1</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td>0.20</td>
<td>0.5</td>
<td>7.1</td>
<td>0.7</td>
<td>2.0</td>
<td>1.7</td>
<td>9.5</td>
<td>24.4</td>
<td>1.2</td>
<td>1.6</td>
<td>19.7</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>multi-scale CRNN</td>
<td>Yu2021</td>
<td>0.92</td>
<td>28.6</td>
<td>34.6</td>
<td>57.9</td>
<td>20.2</td>
<td>31.7</td>
<td>36.0</td>
<td>29.7</td>
<td>28.0</td>
<td>44.4</td>
<td>33.1</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>DCASE2021 SED CRNN Model1</td>
<td>Lu2021</td>
<td>1.27</td>
<td>37.1</td>
<td>41.4</td>
<td>62.5</td>
<td>40.6</td>
<td>39.7</td>
<td>46.5</td>
<td>46.5</td>
<td>34.5</td>
<td>54.5</td>
<td>46.9</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_4</td>
<td>DCASE2021 SED Conformer Model2</td>
<td>Lu2021</td>
<td>0.88</td>
<td>5.8</td>
<td>5.9</td>
<td>2.1</td>
<td>1.0</td>
<td>0.3</td>
<td>16.9</td>
<td>44.6</td>
<td>22.0</td>
<td>25.6</td>
<td>32.9</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_3</td>
<td>DCASE2021 SED Conformer Model1</td>
<td>Lu2021</td>
<td>0.86</td>
<td>6.3</td>
<td>5.1</td>
<td>1.3</td>
<td>0.8</td>
<td>0.3</td>
<td>15.7</td>
<td>43.8</td>
<td>21.8</td>
<td>29.3</td>
<td>31.9</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_2</td>
<td>DCASE2021 SED CRNN Model2</td>
<td>Lu2021</td>
<td>1.25</td>
<td>38.6</td>
<td>41.6</td>
<td>65.5</td>
<td>41.0</td>
<td>39.3</td>
<td>46.1</td>
<td>49.0</td>
<td>36.0</td>
<td>51.5</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>31.7</td>
<td>38.2</td>
<td>63.5</td>
<td>19.9</td>
<td>30.1</td>
<td>46.6</td>
<td>32.0</td>
<td>21.1</td>
<td>49.4</td>
<td>43.4</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_1</td>
<td>DCASE2020 liuliuliufangzhou system</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>34.3</td>
<td>38.8</td>
<td>63.1</td>
<td>25.7</td>
<td>27.3</td>
<td>45.3</td>
<td>31.1</td>
<td>25.8</td>
<td>49.4</td>
<td>43.7</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_2</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td>1.19</td>
<td>33.6</td>
<td>44.9</td>
<td>60.9</td>
<td>26.4</td>
<td>34.8</td>
<td>24.3</td>
<td>38.7</td>
<td>25.9</td>
<td>48.4</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td>1.19</td>
<td>33.6</td>
<td>44.9</td>
<td>60.9</td>
<td>26.4</td>
<td>34.8</td>
<td>24.3</td>
<td>38.7</td>
<td>25.9</td>
<td>48.4</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_4</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td>1.19</td>
<td>33.6</td>
<td>44.9</td>
<td>60.9</td>
<td>26.4</td>
<td>34.8</td>
<td>24.3</td>
<td>38.7</td>
<td>25.9</td>
<td>48.4</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_3</td>
<td>SOUND EVENT DETECTION USING METRIC LEARNING AND FOCAL LOSS</td>
<td>Tian2021</td>
<td>1.18</td>
<td>33.6</td>
<td>44.9</td>
<td>60.9</td>
<td>26.4</td>
<td>34.8</td>
<td>24.3</td>
<td>38.7</td>
<td>25.9</td>
<td>48.4</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td>0.88</td>
<td>32.2</td>
<td>32.4</td>
<td>58.2</td>
<td>21.7</td>
<td>17.6</td>
<td>36.2</td>
<td>26.8</td>
<td>24.6</td>
<td>49.4</td>
<td>42.9</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_1</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td>0.88</td>
<td>31.6</td>
<td>22.7</td>
<td>58.5</td>
<td>23.4</td>
<td>22.8</td>
<td>35.9</td>
<td>31.5</td>
<td>23.8</td>
<td>45.1</td>
<td>23.9</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_2</td>
<td>Adaptive Sequential Self Attention Span for Sound Event Detection</td>
<td>Yao2021</td>
<td>0.54</td>
<td>4.7</td>
<td>4.2</td>
<td>2.2</td>
<td>1.4</td>
<td>1.3</td>
<td>10.5</td>
<td>27.3</td>
<td>16.7</td>
<td>3.4</td>
<td>17.1</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Guided Learning system</td>
<td>Liang2021</td>
<td>0.99</td>
<td>38.0</td>
<td>40.7</td>
<td>48.3</td>
<td>26.0</td>
<td>24.2</td>
<td>22.6</td>
<td>35.6</td>
<td>30.0</td>
<td>44.6</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>CAM attention SED system</td>
<td>Bajzik2021</td>
<td>1.02</td>
<td>37.5</td>
<td>44.4</td>
<td>57.6</td>
<td>28.8</td>
<td>22.9</td>
<td>35.5</td>
<td>44.0</td>
<td>29.8</td>
<td>51.7</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_1</td>
<td>CAM-based SED system</td>
<td>Bajzik2021</td>
<td>0.45</td>
<td>17.1</td>
<td>7.5</td>
<td>34.2</td>
<td>7.1</td>
<td>15.2</td>
<td>8.9</td>
<td>1.2</td>
<td>4.8</td>
<td>34.1</td>
<td>7.1</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_3</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td>0.99</td>
<td>33.6</td>
<td>38.7</td>
<td>47.2</td>
<td>22.5</td>
<td>17.6</td>
<td>21.1</td>
<td>38.6</td>
<td>28.3</td>
<td>44.7</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td>1.03</td>
<td>29.4</td>
<td>25.7</td>
<td>60.7</td>
<td>20.5</td>
<td>29.1</td>
<td>30.6</td>
<td>38.3</td>
<td>24.9</td>
<td>55.0</td>
<td>32.3</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Mean teacher system</td>
<td>Liang_SS2021</td>
<td>1.01</td>
<td>33.1</td>
<td>37.1</td>
<td>52.0</td>
<td>26.8</td>
<td>32.8</td>
<td>31.7</td>
<td>41.0</td>
<td>28.0</td>
<td>49.2</td>
<td>37.8</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>DCASE2021 SED baseline system</td>
<td>turpault2020a</td>
<td>1.00</td>
<td>32.2</td>
<td>39.0</td>
<td>62.4</td>
<td>28.6</td>
<td>34.5</td>
<td>21.1</td>
<td>37.2</td>
<td>26.4</td>
<td>49.7</td>
<td>42.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_1</td>
<td>DCASE2021_SED_A</td>
<td>Wang2021</td>
<td>1.13</td>
<td>34.3</td>
<td>46.5</td>
<td>62.7</td>
<td>35.6</td>
<td>29.0</td>
<td>50.6</td>
<td>51.8</td>
<td>38.2</td>
<td>45.9</td>
<td>35.9</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>DCASE2021_SED_D</td>
<td>Wang2021</td>
<td>1.09</td>
<td>32.5</td>
<td>49.4</td>
<td>66.2</td>
<td>28.3</td>
<td>15.2</td>
<td>34.0</td>
<td>47.2</td>
<td>33.0</td>
<td>39.9</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_2</td>
<td>DCASE2021_SED_B</td>
<td>Wang2021</td>
<td>0.69</td>
<td>7.0</td>
<td>5.2</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>11.1</td>
<td>31.5</td>
<td>15.7</td>
<td>0.3</td>
<td>27.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>DCASE2021_SED_C</td>
<td>Wang2021</td>
<td>1.13</td>
<td>34.4</td>
<td>52.0</td>
<td>70.1</td>
<td>32.2</td>
<td>25.1</td>
<td>41.5</td>
<td>47.8</td>
<td>36.1</td>
<td>52.6</td>
<td>37.7</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score (Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td>0.80</td>
<td>0.245</td>
<td>0.452</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_3</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>pitch shifting, audio concatenation, volume changing</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_4</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>pitch shifting, audio concatenation, volume changing</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_1</td>
<td>Hafsati2021</td>
<td>1.03</td>
<td>0.334</td>
<td>0.549</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>Hafsati2021</td>
<td>1.04</td>
<td>0.336</td>
<td>0.550</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>Gong2021</td>
<td>1.16</td>
<td>0.370</td>
<td>0.626</td>
<td>SpecAugment, time shift, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_2</td>
<td>Gong2021</td>
<td>1.15</td>
<td>0.367</td>
<td>0.616</td>
<td>SpecAugment, time shift, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_1</td>
<td>Gong2021</td>
<td>1.14</td>
<td>0.364</td>
<td>0.611</td>
<td>SpecAugment, time shift, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td>1.07</td>
<td>0.327</td>
<td>0.603</td>
<td>mixup, frame shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_4</td>
<td>Park2021</td>
<td>0.86</td>
<td>0.237</td>
<td>0.524</td>
<td>mixup, frame shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_1</td>
<td>Park2021</td>
<td>1.01</td>
<td>0.305</td>
<td>0.579</td>
<td>mixup, frame shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_3</td>
<td>Park2021</td>
<td>0.84</td>
<td>0.222</td>
<td>0.537</td>
<td>mixup, frame shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_4</td>
<td>Zheng2021</td>
<td>1.30</td>
<td>0.389</td>
<td>0.742</td>
<td>spec-augment, time-shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>0.452</td>
<td>0.669</td>
<td>spec-augment, time-shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_3</td>
<td>Zheng2021</td>
<td>1.29</td>
<td>0.386</td>
<td>0.746</td>
<td>spec-augment, time-shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_2</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>0.447</td>
<td>0.676</td>
<td>spec-augment, time-shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>Nam2021</td>
<td>1.19</td>
<td>0.399</td>
<td>0.609</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>Nam2021</td>
<td>1.16</td>
<td>0.378</td>
<td>0.617</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>Nam2021</td>
<td>1.09</td>
<td>0.324</td>
<td>0.634</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Nam2021</td>
<td>0.75</td>
<td>0.059</td>
<td>0.715</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_2</td>
<td>Koo2021</td>
<td>0.12</td>
<td>0.044</td>
<td>0.059</td>
<td></td>
<td>raw waveform</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_3</td>
<td>Koo2021</td>
<td>0.41</td>
<td>0.058</td>
<td>0.348</td>
<td></td>
<td>raw waveform</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>Koo2021</td>
<td>0.74</td>
<td>0.258</td>
<td>0.364</td>
<td></td>
<td>raw waveform</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>0.361</td>
<td>0.577</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>0.343</td>
<td>0.571</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>0.363</td>
<td>0.574</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_3</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>0.345</td>
<td>0.571</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>turpault2020b</td>
<td>1.11</td>
<td>0.364</td>
<td>0.580</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_4</td>
<td>Boes2021</td>
<td>0.60</td>
<td>0.117</td>
<td>0.457</td>
<td>time masking, frequency masking, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_3</td>
<td>Boes2021</td>
<td>0.68</td>
<td>0.121</td>
<td>0.531</td>
<td>time masking, frequency masking, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_2</td>
<td>Boes2021</td>
<td>0.77</td>
<td>0.233</td>
<td>0.440</td>
<td>time masking, frequency masking, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>Boes2021</td>
<td>0.81</td>
<td>0.253</td>
<td>0.442</td>
<td>time masking, frequency masking, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_2</td>
<td>Ebbers2021</td>
<td>1.10</td>
<td>0.335</td>
<td>0.621</td>
<td>freuency warping, time-/frequency-masking, shifted superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>0.363</td>
<td>0.637</td>
<td>freuency warping, time-/frequency-masking, shifted superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>Ebbers2021</td>
<td>1.24</td>
<td>0.416</td>
<td>0.635</td>
<td>freuency warping, time-/frequency-masking, shifted superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>0.373</td>
<td>0.621</td>
<td>freuency warping, time-/frequency-masking, shifted superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu2021</td>
<td>0.99</td>
<td>0.290</td>
<td>0.574</td>
<td>mixup</td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td>1.04</td>
<td>0.318</td>
<td>0.583</td>
<td>mixup</td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_4</td>
<td>Liu2021</td>
<td>0.37</td>
<td>0.102</td>
<td>0.231</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_1</td>
<td>Liu2021</td>
<td>0.30</td>
<td>0.090</td>
<td>0.169</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>Liu2021</td>
<td>0.54</td>
<td>0.152</td>
<td>0.322</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_3</td>
<td>Liu2021</td>
<td>0.24</td>
<td>0.068</td>
<td>0.146</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>Olvera2021</td>
<td>0.98</td>
<td>0.338</td>
<td>0.481</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_1</td>
<td>Olvera2021</td>
<td>0.95</td>
<td>0.332</td>
<td>0.462</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Kim2021</td>
<td>1.32</td>
<td>0.442</td>
<td>0.674</td>
<td>time-frequency shift, mixup, specaugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Kim2021</td>
<td>1.31</td>
<td>0.439</td>
<td>0.667</td>
<td>time-frequency shift, mixup, specaugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Kim2021</td>
<td>1.30</td>
<td>0.434</td>
<td>0.669</td>
<td>time-frequency shift, mixup, specaugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Kim2021</td>
<td>1.29</td>
<td>0.431</td>
<td>0.661</td>
<td>time-frequency shift, mixup, specaugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_1</td>
<td>Dinkel2021</td>
<td>1.11</td>
<td>0.361</td>
<td>0.584</td>
<td>time shifting, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>0.373</td>
<td>0.585</td>
<td>time shifting, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>0.370</td>
<td>0.596</td>
<td>time shifting, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_4</td>
<td>Dinkel2021</td>
<td>1.00</td>
<td>0.339</td>
<td>0.504</td>
<td>time shifting, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>HangYu2021</td>
<td>0.90</td>
<td>0.294</td>
<td>0.473</td>
<td>minmax</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>YuHang2021</td>
<td>0.61</td>
<td>0.098</td>
<td>0.496</td>
<td>minmax</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_1</td>
<td>Yu2021</td>
<td>0.20</td>
<td>0.038</td>
<td>0.157</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>Yu2021</td>
<td>0.92</td>
<td>0.301</td>
<td>0.485</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>Lu2021</td>
<td>1.27</td>
<td>0.419</td>
<td>0.660</td>
<td>mixup, frame-shift</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_4</td>
<td>Lu2021</td>
<td>0.88</td>
<td>0.157</td>
<td>0.685</td>
<td>mixup, frame-shift</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_3</td>
<td>Lu2021</td>
<td>0.86</td>
<td>0.148</td>
<td>0.686</td>
<td>mixup, frame-shift</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_2</td>
<td>Lu2021</td>
<td>1.25</td>
<td>0.412</td>
<td>0.651</td>
<td>mixup, frame-shift</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>source augmentation, random track mixing</td>
<td>raw waveform</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_1</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>source augmentation, random track mixing</td>
<td>raw waveform</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_2</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.411</td>
<td>0.585</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.413</td>
<td>0.586</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_4</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.412</td>
<td>0.586</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_3</td>
<td>Tian2021</td>
<td>1.18</td>
<td>0.409</td>
<td>0.584</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Yao2021</td>
<td>0.88</td>
<td>0.279</td>
<td>0.479</td>
<td>MIXUP</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_1</td>
<td>Yao2021</td>
<td>0.88</td>
<td>0.277</td>
<td>0.482</td>
<td>MIXUP</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_2</td>
<td>Yao2021</td>
<td>0.54</td>
<td>0.056</td>
<td>0.496</td>
<td>MIXUP</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Liang2021</td>
<td>0.99</td>
<td>0.313</td>
<td>0.543</td>
<td>mixup, specAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>Bajzik2021</td>
<td>1.02</td>
<td>0.330</td>
<td>0.544</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_1</td>
<td>Bajzik2021</td>
<td>0.45</td>
<td>0.133</td>
<td>0.266</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_3</td>
<td>Liang_SS2021</td>
<td>0.99</td>
<td>0.304</td>
<td>0.559</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Liang_SS2021</td>
<td>1.03</td>
<td>0.313</td>
<td>0.588</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Liang_SS2021</td>
<td>1.01</td>
<td>0.325</td>
<td>0.542</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>turpault2020a</td>
<td>1.00</td>
<td>0.315</td>
<td>0.547</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_1</td>
<td>Wang2021</td>
<td>1.13</td>
<td>0.336</td>
<td>0.646</td>
<td>Mixup, Time Shift, Time Mask, Frequency Mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>Wang2021</td>
<td>1.09</td>
<td>0.304</td>
<td>0.662</td>
<td>Mixup, Time Shift, Time Mask, Frequency Mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_2</td>
<td>Wang2021</td>
<td>0.69</td>
<td>0.070</td>
<td>0.636</td>
<td>Mixup, Time Shift, Time Mask, Frequency Mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>Wang2021</td>
<td>1.13</td>
<td>0.339</td>
<td>0.649</td>
<td>Mixup, Time Shift, Time Mask, Frequency Mask</td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score (Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-field="machine_learning_semi_supervised" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Semi-supervised approach
            </th>
<th class="text-center narrow-col" data-field="post-processing" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Post-processing
            </th>
<th class="text-center narrow-col" data-field="segmentation_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Segmentation<br/>method
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td>0.80</td>
<td>0.245</td>
<td>0.452</td>
<td>CNN, conformer</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_3</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_4</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_1</td>
<td>Hafsati2021</td>
<td>1.03</td>
<td>0.334</td>
<td>0.549</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>Hafsati2021</td>
<td>1.04</td>
<td>0.336</td>
<td>0.550</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>Gong2021</td>
<td>1.16</td>
<td>0.370</td>
<td>0.626</td>
<td>CRNN</td>
<td>mean-teacher, pseudo-labelling</td>
<td>class-wise median filtering</td>
<td>attention layers</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_2</td>
<td>Gong2021</td>
<td>1.15</td>
<td>0.367</td>
<td>0.616</td>
<td>CRNN</td>
<td>mean-teacher</td>
<td>class-wise median filtering</td>
<td>attention layers</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_1</td>
<td>Gong2021</td>
<td>1.14</td>
<td>0.364</td>
<td>0.611</td>
<td>CRNN</td>
<td>mean-teacher</td>
<td>class-wise median filtering</td>
<td>attention layers</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td>1.07</td>
<td>0.327</td>
<td>0.603</td>
<td>RCRNN</td>
<td>cross-referencing self-training</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_4</td>
<td>Park2021</td>
<td>0.86</td>
<td>0.237</td>
<td>0.524</td>
<td>RCRNN</td>
<td>cross-referencing self-training</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_1</td>
<td>Park2021</td>
<td>1.01</td>
<td>0.305</td>
<td>0.579</td>
<td>RCRNN</td>
<td>cross-referencing self-training</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_3</td>
<td>Park2021</td>
<td>0.84</td>
<td>0.222</td>
<td>0.537</td>
<td>RCRNN</td>
<td>cross-referencing self-training</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_4</td>
<td>Zheng2021</td>
<td>1.30</td>
<td>0.389</td>
<td>0.742</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>0.452</td>
<td>0.669</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_3</td>
<td>Zheng2021</td>
<td>1.29</td>
<td>0.386</td>
<td>0.746</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_2</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>0.447</td>
<td>0.676</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>Nam2021</td>
<td>1.19</td>
<td>0.399</td>
<td>0.609</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (329ms), weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>Nam2021</td>
<td>1.16</td>
<td>0.378</td>
<td>0.617</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (461ms), weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>Nam2021</td>
<td>1.09</td>
<td>0.324</td>
<td>0.634</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (461ms), weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Nam2021</td>
<td>0.75</td>
<td>0.059</td>
<td>0.715</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>weak SED</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_2</td>
<td>Koo2021</td>
<td>0.12</td>
<td>0.044</td>
<td>0.059</td>
<td>Transformer, RNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_3</td>
<td>Koo2021</td>
<td>0.41</td>
<td>0.058</td>
<td>0.348</td>
<td>Transformer, RNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>Koo2021</td>
<td>0.74</td>
<td>0.258</td>
<td>0.364</td>
<td>Transformer</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>0.361</td>
<td>0.577</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (45ms)</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>0.343</td>
<td>0.571</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (45ms)</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>0.363</td>
<td>0.574</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (45ms)</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_3</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>0.345</td>
<td>0.571</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (45ms)</td>
<td></td>
<td>average</td>
</tr>
<tr data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>turpault2020b</td>
<td>1.11</td>
<td>0.364</td>
<td>0.580</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_4</td>
<td>Boes2021</td>
<td>0.60</td>
<td>0.117</td>
<td>0.457</td>
<td>CRNN</td>
<td>mean teacher</td>
<td>median filtering (3.7s)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_3</td>
<td>Boes2021</td>
<td>0.68</td>
<td>0.121</td>
<td>0.531</td>
<td>CRNN</td>
<td>mean teacher</td>
<td>median filtering (3.7s)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_2</td>
<td>Boes2021</td>
<td>0.77</td>
<td>0.233</td>
<td>0.440</td>
<td>CRNN</td>
<td>mean teacher</td>
<td>median filtering (460ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>Boes2021</td>
<td>0.81</td>
<td>0.253</td>
<td>0.442</td>
<td>CRNN</td>
<td>mean teacher</td>
<td>median filtering (460ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_2</td>
<td>Ebbers2021</td>
<td>1.10</td>
<td>0.335</td>
<td>0.621</td>
<td>FBCRNN,CRNN</td>
<td>self-training</td>
<td>median filtering (class dependent)</td>
<td>MIL</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>0.363</td>
<td>0.637</td>
<td>FBCRNN,CRNN,CTNN,CNN</td>
<td>self-training</td>
<td>median filtering (class dependent)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>Ebbers2021</td>
<td>1.24</td>
<td>0.416</td>
<td>0.635</td>
<td>FBCRNN,CRNN,CTNN,CNN</td>
<td>self-training</td>
<td>median filtering (class dependent)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>0.373</td>
<td>0.621</td>
<td>FBCRNN,CRNN</td>
<td>self-training</td>
<td>median filtering (class dependent)</td>
<td>MIL</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu2021</td>
<td>0.99</td>
<td>0.290</td>
<td>0.574</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>LinearSoftmax</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td>1.04</td>
<td>0.318</td>
<td>0.583</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>LinearSoftmax</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_4</td>
<td>Liu2021</td>
<td>0.37</td>
<td>0.102</td>
<td>0.231</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_1</td>
<td>Liu2021</td>
<td>0.30</td>
<td>0.090</td>
<td>0.169</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>Liu2021</td>
<td>0.54</td>
<td>0.152</td>
<td>0.322</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_3</td>
<td>Liu2021</td>
<td>0.24</td>
<td>0.068</td>
<td>0.146</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>Olvera2021</td>
<td>0.98</td>
<td>0.338</td>
<td>0.481</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_1</td>
<td>Olvera2021</td>
<td>0.95</td>
<td>0.332</td>
<td>0.462</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Kim2021</td>
<td>1.32</td>
<td>0.442</td>
<td>0.674</td>
<td>RCRNN</td>
<td>mean-teacher student, self-training with noisy student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Kim2021</td>
<td>1.31</td>
<td>0.439</td>
<td>0.667</td>
<td>RCRNN</td>
<td>mean-teacher student, self-training with noisy student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Kim2021</td>
<td>1.30</td>
<td>0.434</td>
<td>0.669</td>
<td>RCRNN</td>
<td>mean-teacher student, self-training with noisy student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Kim2021</td>
<td>1.29</td>
<td>0.431</td>
<td>0.661</td>
<td>RCRNN</td>
<td>mean-teacher student, self-training with noisy student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_1</td>
<td>Dinkel2021</td>
<td>1.11</td>
<td>0.361</td>
<td>0.584</td>
<td>CRNN, ensemble</td>
<td>unsupervised data augmentation</td>
<td></td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>0.373</td>
<td>0.585</td>
<td>CRNN, ensemble</td>
<td>unsupervised data augmentation</td>
<td></td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>0.370</td>
<td>0.596</td>
<td>CRNN, ensemble</td>
<td>unsupervised data augmentation</td>
<td></td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_4</td>
<td>Dinkel2021</td>
<td>1.00</td>
<td>0.339</td>
<td>0.504</td>
<td>CRNN</td>
<td>unsupervised data augmentation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>HangYu2021</td>
<td>0.90</td>
<td>0.294</td>
<td>0.473</td>
<td>Transformer,CNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>attention layers</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>YuHang2021</td>
<td>0.61</td>
<td>0.098</td>
<td>0.496</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>attention layers</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_1</td>
<td>Yu2021</td>
<td>0.20</td>
<td>0.038</td>
<td>0.157</td>
<td>Multi-scale CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>attention</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>Yu2021</td>
<td>0.92</td>
<td>0.301</td>
<td>0.485</td>
<td>Multi-scale CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>attention</td>
<td></td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>Lu2021</td>
<td>1.27</td>
<td>0.419</td>
<td>0.660</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_4</td>
<td>Lu2021</td>
<td>0.88</td>
<td>0.157</td>
<td>0.685</td>
<td>Conformer</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_3</td>
<td>Lu2021</td>
<td>0.86</td>
<td>0.148</td>
<td>0.686</td>
<td>Conformer</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_2</td>
<td>Lu2021</td>
<td>1.25</td>
<td>0.412</td>
<td>0.651</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>u-net, VGG</td>
<td></td>
<td>median filtering (93ms)</td>
<td>attention layers, d-vector</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_1</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>u-net, VGG</td>
<td></td>
<td>median filtering (93ms)</td>
<td>attention layers, d-vector</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_2</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.411</td>
<td>0.585</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering with adaptive window size</td>
<td>attention_layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.413</td>
<td>0.586</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering with adaptive window size</td>
<td>attention_layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_4</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.412</td>
<td>0.586</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering with adaptive window size</td>
<td>attention_layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_3</td>
<td>Tian2021</td>
<td>1.18</td>
<td>0.409</td>
<td>0.584</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering with adaptive window size</td>
<td>attention_layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Yao2021</td>
<td>0.88</td>
<td>0.279</td>
<td>0.479</td>
<td>CRNN,Self Attention</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_1</td>
<td>Yao2021</td>
<td>0.88</td>
<td>0.277</td>
<td>0.482</td>
<td>CRNN,Self Attention</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_2</td>
<td>Yao2021</td>
<td>0.54</td>
<td>0.056</td>
<td>0.496</td>
<td>CRNN,Self Attention</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Liang2021</td>
<td>0.99</td>
<td>0.313</td>
<td>0.543</td>
<td>CRNN</td>
<td>teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>Bajzik2021</td>
<td>1.02</td>
<td>0.330</td>
<td>0.544</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (112ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_1</td>
<td>Bajzik2021</td>
<td>0.45</td>
<td>0.133</td>
<td>0.266</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering (112ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_3</td>
<td>Liang_SS2021</td>
<td>0.99</td>
<td>0.304</td>
<td>0.559</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Liang_SS2021</td>
<td>1.03</td>
<td>0.313</td>
<td>0.588</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Liang_SS2021</td>
<td>1.01</td>
<td>0.325</td>
<td>0.542</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>turpault2020a</td>
<td>1.00</td>
<td>0.315</td>
<td>0.547</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_1</td>
<td>Wang2021</td>
<td>1.13</td>
<td>0.336</td>
<td>0.646</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layer</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>Wang2021</td>
<td>1.09</td>
<td>0.304</td>
<td>0.662</td>
<td>CRNN, CNN-Transformer</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layer, exponential softmax layer</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_2</td>
<td>Wang2021</td>
<td>0.69</td>
<td>0.070</td>
<td>0.636</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>exponential softmax layer</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>Wang2021</td>
<td>1.13</td>
<td>0.339</td>
<td>0.649</td>
<td>CRNN, CNN-Transformer</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layer</td>
<td>mean</td>
</tr>
</tbody>
</table>
<h2 id="complexity">Complexity</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="ranking_score_all" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="system_complexity" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score (Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_complexity_time" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Training time
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Na_BUPT_task4_SED_1</td>
<td>Na2021</td>
<td>0.80</td>
<td>0.245</td>
<td>0.452</td>
<td>3900000</td>
<td></td>
<td>40h (1 Quadro K1200)</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_3</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>1100000</td>
<td></td>
<td>20h (1 Tesla V100-SXM2-16GB)</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_4</td>
<td>Hafsati2021</td>
<td>0.91</td>
<td>0.287</td>
<td>0.502</td>
<td>1100000</td>
<td></td>
<td>20h (1 Tesla V100-SXM2-16GB)</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_1</td>
<td>Hafsati2021</td>
<td>1.03</td>
<td>0.334</td>
<td>0.549</td>
<td>1100000</td>
<td></td>
<td>6h (1 Tesla V100-SXM2-16GB)</td>
</tr>
<tr>
<td></td>
<td>Hafsati_TUITO_task4_SED_2</td>
<td>Hafsati2021</td>
<td>1.04</td>
<td>0.336</td>
<td>0.550</td>
<td>1100000</td>
<td></td>
<td>6h (1 Tesla V100-SXM2-16GB)</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_3</td>
<td>Gong2021</td>
<td>1.16</td>
<td>0.370</td>
<td>0.626</td>
<td>6674520</td>
<td>6</td>
<td>22.5h (1 V100)</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_2</td>
<td>Gong2021</td>
<td>1.15</td>
<td>0.367</td>
<td>0.616</td>
<td>2224840</td>
<td>2</td>
<td>7.5h (1 V100)</td>
</tr>
<tr>
<td></td>
<td>Gong_TAL_task4_SED_1</td>
<td>Gong2021</td>
<td>1.14</td>
<td>0.364</td>
<td>0.611</td>
<td>4449680</td>
<td>4</td>
<td>15h (1 V100)</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_2</td>
<td>Park2021</td>
<td>1.07</td>
<td>0.327</td>
<td>0.603</td>
<td>9000000</td>
<td></td>
<td>20h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_4</td>
<td>Park2021</td>
<td>0.86</td>
<td>0.237</td>
<td>0.524</td>
<td>9000000</td>
<td></td>
<td>20h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_1</td>
<td>Park2021</td>
<td>1.01</td>
<td>0.305</td>
<td>0.579</td>
<td>9000000</td>
<td></td>
<td>20h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Park_JHU_task4_SED_3</td>
<td>Park2021</td>
<td>0.84</td>
<td>0.222</td>
<td>0.537</td>
<td>9000000</td>
<td></td>
<td>20h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_4</td>
<td>Zheng2021</td>
<td>1.30</td>
<td>0.389</td>
<td>0.742</td>
<td>1112420</td>
<td>9</td>
<td>3h (1 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_1</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>0.452</td>
<td>0.669</td>
<td>1112420</td>
<td>3</td>
<td>3h (1 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_3</td>
<td>Zheng2021</td>
<td>1.29</td>
<td>0.386</td>
<td>0.746</td>
<td>1112420</td>
<td>10</td>
<td>3h (1 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Zheng_USTC_task4_SED_2</td>
<td>Zheng2021</td>
<td>1.33</td>
<td>0.447</td>
<td>0.676</td>
<td>1112420</td>
<td>9</td>
<td>3h (1 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>Nam2021</td>
<td>1.19</td>
<td>0.399</td>
<td>0.609</td>
<td>4427956</td>
<td>9</td>
<td>4h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>Nam2021</td>
<td>1.16</td>
<td>0.378</td>
<td>0.617</td>
<td>4427956</td>
<td>16</td>
<td>4h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>Nam2021</td>
<td>1.09</td>
<td>0.324</td>
<td>0.634</td>
<td>4427956</td>
<td>11</td>
<td>4h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Nam2021</td>
<td>0.75</td>
<td>0.059</td>
<td>0.715</td>
<td>4427956</td>
<td>9</td>
<td>4h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_2</td>
<td>Koo2021</td>
<td>0.12</td>
<td>0.044</td>
<td>0.059</td>
<td>102000000</td>
<td></td>
<td>19h (1 Tesla M40)</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_3</td>
<td>Koo2021</td>
<td>0.41</td>
<td>0.058</td>
<td>0.348</td>
<td>196000000</td>
<td>2</td>
<td>48h (1 Tesla M40)</td>
</tr>
<tr>
<td></td>
<td>Koo_SGU_task4_SED_1</td>
<td>Koo2021</td>
<td>0.74</td>
<td>0.258</td>
<td>0.364</td>
<td>95800000</td>
<td></td>
<td>19h (1 RTX 3080 Ti)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_4</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>0.361</td>
<td>0.577</td>
<td>5562100</td>
<td>5</td>
<td>20h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>0.343</td>
<td>0.571</td>
<td>3337260</td>
<td>3</td>
<td>12h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_2</td>
<td>de Benito-Gorron2021</td>
<td>1.10</td>
<td>0.363</td>
<td>0.574</td>
<td>3337260</td>
<td>3</td>
<td>12h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_3</td>
<td>de Benito-Gorron2021</td>
<td>1.07</td>
<td>0.345</td>
<td>0.571</td>
<td>4449600</td>
<td>4</td>
<td>16h (1 RTX 2080)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SSep_SED</td>
<td>turpault2020b</td>
<td>1.11</td>
<td>0.364</td>
<td>0.580</td>
<td>2200000</td>
<td></td>
<td>6h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_4</td>
<td>Boes2021</td>
<td>0.60</td>
<td>0.117</td>
<td>0.457</td>
<td>1038314</td>
<td></td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_3</td>
<td>Boes2021</td>
<td>0.68</td>
<td>0.121</td>
<td>0.531</td>
<td>1038314</td>
<td></td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_2</td>
<td>Boes2021</td>
<td>0.77</td>
<td>0.233</td>
<td>0.440</td>
<td>1038314</td>
<td></td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task4_SED_1</td>
<td>Boes2021</td>
<td>0.81</td>
<td>0.253</td>
<td>0.442</td>
<td>1038314</td>
<td></td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_2</td>
<td>Ebbers2021</td>
<td>1.10</td>
<td>0.335</td>
<td>0.621</td>
<td>9568030</td>
<td>1</td>
<td>72h (4 RTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_4</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>0.363</td>
<td>0.637</td>
<td>59853372</td>
<td>6</td>
<td>72h (4 RTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_3</td>
<td>Ebbers2021</td>
<td>1.24</td>
<td>0.416</td>
<td>0.635</td>
<td>59853372</td>
<td>6</td>
<td>72h (4 RTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Ebbers2021</td>
<td>1.16</td>
<td>0.373</td>
<td>0.621</td>
<td>9568030</td>
<td>1</td>
<td>72h (4 RTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_2</td>
<td>Zhu2021</td>
<td>0.99</td>
<td>0.290</td>
<td>0.574</td>
<td>3900000</td>
<td></td>
<td>12.5h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Zhu_AIAL-XJU_task4_SED_1</td>
<td>Zhu2021</td>
<td>1.04</td>
<td>0.318</td>
<td>0.583</td>
<td>3900000</td>
<td></td>
<td>13.5h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_4</td>
<td>Liu2021</td>
<td>0.37</td>
<td>0.102</td>
<td>0.231</td>
<td>1112420</td>
<td></td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_1</td>
<td>Liu2021</td>
<td>0.30</td>
<td>0.090</td>
<td>0.169</td>
<td>1112420</td>
<td></td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_2</td>
<td>Liu2021</td>
<td>0.54</td>
<td>0.152</td>
<td>0.322</td>
<td>1112420</td>
<td></td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_3</td>
<td>Liu2021</td>
<td>0.24</td>
<td>0.068</td>
<td>0.146</td>
<td>1112420</td>
<td></td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_2</td>
<td>Olvera2021</td>
<td>0.98</td>
<td>0.338</td>
<td>0.481</td>
<td>2225868</td>
<td>2</td>
<td>24h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Olvera_INRIA_task4_SED_1</td>
<td>Olvera2021</td>
<td>0.95</td>
<td>0.332</td>
<td>0.462</td>
<td>3338802</td>
<td>3</td>
<td>24h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Kim2021</td>
<td>1.32</td>
<td>0.442</td>
<td>0.674</td>
<td>2162412</td>
<td>10</td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Kim2021</td>
<td>1.31</td>
<td>0.439</td>
<td>0.667</td>
<td>2162412</td>
<td>5</td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Kim2021</td>
<td>1.30</td>
<td>0.434</td>
<td>0.669</td>
<td>2162412</td>
<td>5</td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Kim2021</td>
<td>1.29</td>
<td>0.431</td>
<td>0.661</td>
<td>2162412</td>
<td>5</td>
<td>5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_1</td>
<td>Dinkel2021</td>
<td>1.11</td>
<td>0.361</td>
<td>0.584</td>
<td>2043204</td>
<td>3</td>
<td>3h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_2</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>0.373</td>
<td>0.585</td>
<td>2724272</td>
<td>4</td>
<td>3h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_3</td>
<td>Dinkel2021</td>
<td>1.13</td>
<td>0.370</td>
<td>0.596</td>
<td>3405340</td>
<td>5</td>
<td>3h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Cai_SMALLRICE_task4_SED_4</td>
<td>Dinkel2021</td>
<td>1.00</td>
<td>0.339</td>
<td>0.504</td>
<td>681068</td>
<td></td>
<td>3h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_2</td>
<td>HangYu2021</td>
<td>0.90</td>
<td>0.294</td>
<td>0.473</td>
<td>11312420</td>
<td>2</td>
<td>6h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>HangYuChen_Roal_task4_SED_1</td>
<td>YuHang2021</td>
<td>0.61</td>
<td>0.098</td>
<td>0.496</td>
<td>1112420</td>
<td>2</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_1</td>
<td>Yu2021</td>
<td>0.20</td>
<td>0.038</td>
<td>0.157</td>
<td>1300000</td>
<td></td>
<td>5h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Yu_NCUT_task4_SED_2</td>
<td>Yu2021</td>
<td>0.92</td>
<td>0.301</td>
<td>0.485</td>
<td>1300000</td>
<td></td>
<td>5h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_1</td>
<td>Lu2021</td>
<td>1.27</td>
<td>0.419</td>
<td>0.660</td>
<td>10500000</td>
<td>5</td>
<td>5h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_4</td>
<td>Lu2021</td>
<td>0.88</td>
<td>0.157</td>
<td>0.685</td>
<td>39500000</td>
<td>5</td>
<td>10h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_3</td>
<td>Lu2021</td>
<td>0.86</td>
<td>0.148</td>
<td>0.686</td>
<td>39500000</td>
<td>5</td>
<td>10h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>lu_kwai_task4_SED_2</td>
<td>Lu2021</td>
<td>1.25</td>
<td>0.412</td>
<td>0.651</td>
<td>10500000</td>
<td>5</td>
<td>5h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_2</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>192905515</td>
<td></td>
<td>17h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Liu_BUPT_task4_SS_SED_1</td>
<td>Liu_SS2021</td>
<td>0.94</td>
<td>0.302</td>
<td>0.507</td>
<td>192905515</td>
<td></td>
<td>17h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_2</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.411</td>
<td>0.585</td>
<td>8471847</td>
<td>4</td>
<td>6h for each model(GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_1</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.413</td>
<td>0.586</td>
<td>8471847</td>
<td>4</td>
<td>6h for each model(GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_4</td>
<td>Tian2021</td>
<td>1.19</td>
<td>0.412</td>
<td>0.586</td>
<td>8471847</td>
<td>4</td>
<td>6h for each model(GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Tian_ICT-TOSHIBA_task4_SED_3</td>
<td>Tian2021</td>
<td>1.18</td>
<td>0.409</td>
<td>0.584</td>
<td>8471847</td>
<td>4</td>
<td>6h for each model(GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_3</td>
<td>Yao2021</td>
<td>0.88</td>
<td>0.279</td>
<td>0.479</td>
<td>2.5M</td>
<td></td>
<td>6h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_1</td>
<td>Yao2021</td>
<td>0.88</td>
<td>0.277</td>
<td>0.482</td>
<td>2.5M</td>
<td></td>
<td>6h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Yao_GUET_task4_SED_2</td>
<td>Yao2021</td>
<td>0.54</td>
<td>0.056</td>
<td>0.496</td>
<td>2.5M</td>
<td></td>
<td>6h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SED_4</td>
<td>Liang2021</td>
<td>0.99</td>
<td>0.313</td>
<td>0.543</td>
<td>1431280</td>
<td></td>
<td>16h (Tesla-V100)</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_2</td>
<td>Bajzik2021</td>
<td>1.02</td>
<td>0.330</td>
<td>0.544</td>
<td>2200000</td>
<td></td>
<td>13h (1 GeForce GTX 1650)</td>
</tr>
<tr>
<td></td>
<td>Bajzik_UNIZA_task4_SED_1</td>
<td>Bajzik2021</td>
<td>0.45</td>
<td>0.133</td>
<td>0.266</td>
<td>1200000</td>
<td></td>
<td>5h (1 GeForce GTX 1650)</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_3</td>
<td>Liang_SS2021</td>
<td>0.99</td>
<td>0.304</td>
<td>0.559</td>
<td>1112420</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_1</td>
<td>Liang_SS2021</td>
<td>1.03</td>
<td>0.313</td>
<td>0.588</td>
<td>1112420</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liang_SHNU_task4_SSep_SED_2</td>
<td>Liang_SS2021</td>
<td>1.01</td>
<td>0.325</td>
<td>0.542</td>
<td>1112420</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_SED</td>
<td>turpault2020a</td>
<td>1.00</td>
<td>0.315</td>
<td>0.547</td>
<td>2200000</td>
<td></td>
<td>6h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_1</td>
<td>Wang2021</td>
<td>1.13</td>
<td>0.336</td>
<td>0.646</td>
<td>47213260</td>
<td>10</td>
<td>480h (1 GPU 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_4</td>
<td>Wang2021</td>
<td>1.09</td>
<td>0.304</td>
<td>0.662</td>
<td>118739112</td>
<td>24</td>
<td>864h (1 GPU 1080Ti), 360h (1 GPU V100)</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_2</td>
<td>Wang2021</td>
<td>0.69</td>
<td>0.070</td>
<td>0.636</td>
<td>3350984</td>
<td>8</td>
<td>384h (1 GPU 1080Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_NSYSU_task4_SED_3</td>
<td>Wang2021</td>
<td>1.13</td>
<td>0.339</td>
<td>0.649</td>
<td>115388128</td>
<td>16</td>
<td>480h (1 GPU 1080 Ti), 360h (1 GPU V100)</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2021/technical_reports_task4.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Bajzil2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Bajzil2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection System For DCASE 2021 Challenge
       </h4>
<p style="text-align:left">
        Bajzik, Jakub
       </p>
<p style="text-align:left">
<em>
         University of Zilina, Department of Mechatronics and Electronics, Å½ilina 010 26, Slovak Republic
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bajzik_UNIZA_task4_SED_1</span> <span class="label label-primary">Bajzik_UNIZA_task4_SED_2</span><span class="clearfix"> </span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bajzil2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bajzil2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bajzil2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Bajzik_34_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bajzil2021" class="panel-collapse collapse" id="collapse-Bajzil2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection System For DCASE 2021 Challenge
      </h4>
<p style="text-align:left">
<small>
        Bajzik, Jakub
       </small>
<br/>
<small>
<em>
         University of Zilina, Department of Mechatronics and Electronics, Å½ilina 010 26, Slovak Republic
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents the systems proposal for the DCASE 2021 challenge Task 4 (Sound event detection and separation in domestic environments). The aim is to provide the event time localization timestamps in addition to event class probabilities. In this paper, the two systems are proposed. System 1 is a convolutional neural network trained for sound event classification using only weakly labeled and unlabeled data. The strong labels are obtained using the class activation mapping technique. System 1 does not reach the baseline performance. System 2 is the convolutional neural network and recurrent neural network which uses the class activation mapping technique as a part of the attention mechanism to increase the baseline performance. The second model was trained using weakly labeled, strongly labeled, and unlabeled data. Both architectures are based on the Mean Teacher baseline system 2021.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bajzil2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Bajzik_34_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bajzil2021label" class="modal fade" id="bibtex-Bajzil2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBajzil2021label">
        Sound Event Detection System For DCASE 2021 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bajzil2021,
    Author = "Bajzik, Jakub",
    title = "Sound Event Detection System For DCASE 2021 Challenge",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This paper presents the systems proposal for the DCASE 2021 challenge Task 4 (Sound event detection and separation in domestic environments). The aim is to provide the event time localization timestamps in addition to event class probabilities. In this paper, the two systems are proposed. System 1 is a convolutional neural network trained for sound event classification using only weakly labeled and unlabeled data. The strong labels are obtained using the class activation mapping technique. System 1 does not reach the baseline performance. System 2 is the convolutional neural network and recurrent neural network which uses the class activation mapping technique as a part of the attention mechanism to increase the baseline performance. The second model was trained using weakly labeled, strongly labeled, and unlabeled data. Both architectures are based on the Mean Teacher baseline system 2021."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Boes2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Boes2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Optimizing Temporal Resolution Of Convolutional Recurrent Neural Networks For Sound Event Detection
       </h4>
<p style="text-align:left">
        Boes, Wim and Van Hamme, Hugo
       </p>
<p style="text-align:left">
<em>
         ESAT, KU Leuven, Leuven, Belgium
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Boes_KUL_task4_SED_1</span> <span class="label label-primary">Boes_KUL_task4_SED_2</span> <span class="label label-primary">Boes_KUL_task4_SED_3</span> <span class="label label-primary">Boes_KUL_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Boes2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Boes2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Boes2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Boes_77_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Boes2021" class="panel-collapse collapse" id="collapse-Boes2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Optimizing Temporal Resolution Of Convolutional Recurrent Neural Networks For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Boes, Wim and Van Hamme, Hugo
       </small>
<br/>
<small>
<em>
         ESAT, KU Leuven, Leuven, Belgium
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, the systems we submitted for subtask 4 of the DCASE 2021 challenge, regarding sound event detection, are described in detail. These models are closely related to the baseline provided for this problem, as they are essentially convolutional recurrent neural networks trained in a mean teacher setting to deal with the heterogeneous annotation of the supplied data. However, the time resolution of the predictions was adapted to deal with the fact that these systems are evaluated using two intersection-based metrics involving different needs in terms of temporal localization. This was done by optimizing the pooling operations. For the first of the defined evaluation scenarios, imposing relatively strict requirements on the temporal localization accuracy, our best model achieved a PSDS score of 0.3609 on the validation data. This is only marginally better than the performance obtained by the baseline system (0.342): The amount of pooling in the baseline network already turned out to be optimal, and thus, no substantial changes were made, explaining this result. For the second evaluation scenario, imposing relatively lax restrictions on the localization accuracy, our best-performing system achieved a PSDS score of 0.7312 on the validation data. This is significantly better than the performance obtained by the baseline model (0.527), which can effectively be attributed to the changes that were applied to the pooling operations of the network.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Boes2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Boes_77_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Boes2021label" class="modal fade" id="bibtex-Boes2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBoes2021label">
        Optimizing Temporal Resolution Of Convolutional Recurrent Neural Networks For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Boes2021,
    Author = "Boes, Wim and Van Hamme, Hugo",
    title = "Optimizing Temporal Resolution Of Convolutional Recurrent Neural Networks For Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, the systems we submitted for subtask 4 of the DCASE 2021 challenge, regarding sound event detection, are described in detail. These models are closely related to the baseline provided for this problem, as they are essentially convolutional recurrent neural networks trained in a mean teacher setting to deal with the heterogeneous annotation of the supplied data. However, the time resolution of the predictions was adapted to deal with the fact that these systems are evaluated using two intersection-based metrics involving different needs in terms of temporal localization. This was done by optimizing the pooling operations. For the first of the defined evaluation scenarios, imposing relatively strict requirements on the temporal localization accuracy, our best model achieved a PSDS score of 0.3609 on the validation data. This is only marginally better than the performance obtained by the baseline system (0.342): The amount of pooling in the baseline network already turned out to be optimal, and thus, no substantial changes were made, explaining this result. For the second evaluation scenario, imposing relatively lax restrictions on the localization accuracy, our best-performing system achieved a PSDS score of 0.7312 on the validation data. This is significantly better than the performance obtained by the baseline model (0.527), which can effectively be attributed to the changes that were applied to the pooling operations of the network."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chen2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Chen2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolution-Augmented Conformer For Sound Event Detection
       </h4>
<p style="text-align:left">
        Chen, YuHang
       </p>
<p style="text-align:left">
<em>
         Royal Flush, 18, Tongshun Street, Wuchang Street, Yuhang District. HangZhou 310000, CHINA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">HangYuChen_Royal_task4_SED_1</span> <span class="label label-primary">HangYuChen_Royal_task4_SED_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chen2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chen2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chen2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Chen_51_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chen2021" class="panel-collapse collapse" id="collapse-Chen2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolution-Augmented Conformer For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Chen, YuHang
       </small>
<br/>
<small>
<em>
         Royal Flush, 18, Tongshun Street, Wuchang Street, Yuhang District. HangZhou 310000, CHINA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE2021 Task4: sound event detection and separation in domestic environments. Our model employs conformer blocks, which combine the self-attention and depth-wise convolution networks, to efficiently capture the global and local context information of an audio feature sequence. In addition to this novel architecture, we further improve the performance by utilizing a mean teacher semi-supervised learning technique, data augmentation for each sound event class. We demonstrate that the proposed method achieves the PSDS-1 and PSDS-2 score of 34%, 55.7% on the validation set, outperforming that of the baseline score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chen2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Chen_51_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chen2021label" class="modal fade" id="bibtex-Chen2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChen2021label">
        Convolution-Augmented Conformer For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chen2021,
    Author = "Chen, YuHang",
    title = "Convolution-Augmented Conformer For Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE2021 Task4: sound event detection and separation in domestic environments. Our model employs conformer blocks, which combine the self-attention and depth-wise convolution networks, to efficiently capture the global and local context information of an audio feature sequence. In addition to this novel architecture, we further improve the performance by utilizing a mean teacher semi-supervised learning technique, data augmentation for each sound event class. We demonstrate that the proposed method achieves the PSDS-1 and PSDS-2 score of 34\%, 55.7\% on the validation set, outperforming that of the baseline score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="deBenito2021" style="box-shadow: none">
<div class="panel-heading" id="heading-deBenito2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Resolution Mean Teacher For DCASE 2021 Task 4
       </h4>
<p style="text-align:left">
        de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and T. Toledano, Doroteo
       </p>
<p style="text-align:left">
<em>
         AUDIAS Research Group, Universidad AutÃ³noma de Madrid, Calle Francisco TomÃ¡s y Valiente, 11, 28049 Madrid, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">deBenito_AUDIAS_task4_SED_1</span> <span class="label label-primary">deBenito_AUDIAS_task4_SED_2</span> <span class="label label-primary">deBenito_AUDIAS_task4_SED_3</span><span class="label label-primary">deBenito_AUDIAS_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-deBenito2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-deBenito2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-deBenito2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_DeBenito_116_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-deBenito2021" class="panel-collapse collapse" id="collapse-deBenito2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Resolution Mean Teacher For DCASE 2021 Task 4
      </h4>
<p style="text-align:left">
<small>
        de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and T. Toledano, Doroteo
       </small>
<br/>
<small>
<em>
         AUDIAS Research Group, Universidad AutÃ³noma de Madrid, Calle Francisco TomÃ¡s y Valiente, 11, 28049 Madrid, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our participation in DCASE 2021 Task 4: Sound event detection and separation in domestic environments. Aiming to take advantage of the different lengths and spectral characteristics of each target category, we follow the multiresolution feature extraction approach that we proposed for last yearâ€™s edition. It is found that each one of the proposed Polyphonic Sound Detection Score (PSDS) scenarios benefits from either a higher temporal resolution or a higher frequency resolution. Furthermore, combining several time-frequency resolutions via model fusion is able to improve the PSDS results in both scenarios.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-deBenito2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_DeBenito_116_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-deBenito2021label" class="modal fade" id="bibtex-deBenito2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexdeBenito2021label">
        Multi-Resolution Mean Teacher For DCASE 2021 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{deBenito2021,
    Author = "de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and T. Toledano, Doroteo",
    title = "Multi-Resolution Mean Teacher For DCASE 2021 Task 4",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This technical report describes our participation in DCASE 2021 Task 4: Sound event detection and separation in domestic environments. Aiming to take advantage of the different lengths and spectral characteristics of each target category, we follow the multiresolution feature extraction approach that we proposed for last yearâ€™s edition. It is found that each one of the proposed Polyphonic Sound Detection Score (PSDS) scenarios benefits from either a higher temporal resolution or a higher frequency resolution. Furthermore, combining several time-frequency resolutions via model fusion is able to improve the PSDS results in both scenarios."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dinkel2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Dinkel2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The Smallrice Submission To The Dcase2021 Task 4 Challenge: A Lightweight Approach For Semi-Supervised Sound Event Detection With Unsupervised Data Augmentation
       </h4>
<p style="text-align:left">
        Dinkel, Heinrich and Cai, Xinyu and Yan, Zhiyong and Wang, Yongqing and Zhang, Junbo and Wang, Yujun
       </p>
<p style="text-align:left">
<em>
         Xiaomi Corporation, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cai_SMALLRICE_task4_SED_1</span> <span class="label label-primary">Cai_SMALLRICE_task4_SED_2</span> <span class="label label-primary">Cai_SMALLRICE_task4_SED_3</span> <span class="label label-primary">Cai_SMALLRICE_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dinkel2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dinkel2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dinkel2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Dinkel_65_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dinkel2021" class="panel-collapse collapse" id="collapse-Dinkel2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The Smallrice Submission To The Dcase2021 Task 4 Challenge: A Lightweight Approach For Semi-Supervised Sound Event Detection With Unsupervised Data Augmentation
      </h4>
<p style="text-align:left">
<small>
        Dinkel, Heinrich and Cai, Xinyu and Yan, Zhiyong and Wang, Yongqing and Zhang, Junbo and Wang, Yujun
       </small>
<br/>
<small>
<em>
         Xiaomi Corporation, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes our submission to the DCASE 2021 challenge. Different from the baseline and most other approaches, our work focuses on training a lightweight and well-performing model which can be used in real-world applications. Compared to the baseline, our model only contains 600k (15 %) parameters, resulting in a size of 2.7 Mb on disk, making it viable for applications on low-resource devices such as mobile phones. Our model is trained using unsupervised data augmentation as its consistency criterion, which we show can achieve competitive performance to the more common mean teacher paradigm. Our submitted results on the validation set result in a single model peak performance of 36.91 PSDS-1 and 57.17 PSDS2, outperforming the baseline by an absolute of 2.7 and 5.0 points respectively. Notably our approach achieves an Event-F1 score on the development set of 39.29 without post-processing. The best submitted ensemble system using a 4-way fusion achieves a PSDS-1 of 38.23 and PSDS-2 of 62.29 on the validation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dinkel2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Dinkel_65_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dinkel2021label" class="modal fade" id="bibtex-Dinkel2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDinkel2021label">
        The Smallrice Submission To The Dcase2021 Task 4 Challenge: A Lightweight Approach For Semi-Supervised Sound Event Detection With Unsupervised Data Augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dinkel2021,
    Author = "Dinkel, Heinrich and Cai, Xinyu and Yan, Zhiyong and Wang, Yongqing and Zhang, Junbo and Wang, Yujun",
    title = "The Smallrice Submission To The Dcase2021 Task 4 Challenge: A Lightweight Approach For Semi-Supervised Sound Event Detection With Unsupervised Data Augmentation",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This paper describes our submission to the DCASE 2021 challenge. Different from the baseline and most other approaches, our work focuses on training a lightweight and well-performing model which can be used in real-world applications. Compared to the baseline, our model only contains 600k (15 \%) parameters, resulting in a size of 2.7 Mb on disk, making it viable for applications on low-resource devices such as mobile phones. Our model is trained using unsupervised data augmentation as its consistency criterion, which we show can achieve competitive performance to the more common mean teacher paradigm. Our submitted results on the validation set result in a single model peak performance of 36.91 PSDS-1 and 57.17 PSDS2, outperforming the baseline by an absolute of 2.7 and 5.0 points respectively. Notably our approach achieves an Event-F1 score on the development set of 39.29 without post-processing. The best submitted ensemble system using a 4-way fusion achieves a PSDS-1 of 38.23 and PSDS-2 of 62.29 on the validation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ebbers2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Ebbers2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Self-Trained Audio Tagging And Sound Event Detection In Domestic Environments
       </h4>
<p style="text-align:left">
        Ebbers, Janek Haeb-Umbach, Reinhold
       </p>
<p style="text-align:left">
<em>
         Paderborn University, Department of Communications Engineering, Paderborn, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ebbers_UPB_task4_SED_1</span><span class="label label-primary">Ebbers_UPB_task4_SED_2</span><span class="label label-primary">Ebbers_UPB_task4_SED_3</span><span class="label label-primary">Ebbers_UPB_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ebbers2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ebbers2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ebbers2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Ebbers_131_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Ebbers2021').collapse('show');window.location.hash='#Ebbers2021';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ebbers2021" class="panel-collapse collapse" id="collapse-Ebbers2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Self-Trained Audio Tagging And Sound Event Detection In Domestic Environments
      </h4>
<p style="text-align:left">
<small>
        Ebbers, Janek Haeb-Umbach, Reinhold
       </small>
<br/>
<small>
<em>
         Paderborn University, Department of Communications Engineering, Paderborn, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report we present our system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 Challenge Task 4: Sound Event Detection and Separation in Domestic Environments. Our presented solution is an advancement of our system used in the previous edition of the task.We use our previously proposed forward-backward convolutional recurrent neural network (FBCRNN) for tagging and pseudo labeling and tag-conditioned sound event detection (SED) models which are trained using the strong pseudo labels provided by the FBCRNN. Our advancement over our previous model is threefold. Firstly, we introduce a strong label loss in the objective of the FBCRNN to take advantage of the strongly labeled synthetic data during training, which leads to both better tagging and detection performance. Secondly, we perform multiple iterations of self-training for both the FBCRNN and tag-conditioned SED models. Thirdly, while we used only tag-conditioned CNNs as our SED model in the last edition we here explore sophisticated SED model architectures, namely, tag-conditioned bidirectional CRNNs and tag-conditioned bidirectional convolutional transformer neural networks (CTNNs) and combine them. With scenario and class dependent tuning of median filter lengths for post-processing, our final SED model, consisting of 6 submodels (2 of each architecture), is able to achieve validation poly-phonic sound event detection scores (PSDS) of 0.454 for scenario 1 and 0.758 for scenario 2 as well as a collar-based F1-score of 0.602 outperforming the baselines and our model from the last edition by far. Source code will be made publicly available at https://github.com/fgnt/pb_sed.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ebbers2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Ebbers_131_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/fgnt/pb_sed" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ebbers2021label" class="modal fade" id="bibtex-Ebbers2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEbbers2021label">
        Self-Trained Audio Tagging And Sound Event Detection In Domestic Environments
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ebbers2021,
    Author = "Ebbers, Janek Haeb-Umbach, Reinhold",
    title = "Self-Trained Audio Tagging And Sound Event Detection In Domestic Environments",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this report we present our system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 Challenge Task 4: Sound Event Detection and Separation in Domestic Environments. Our presented solution is an advancement of our system used in the previous edition of the task.We use our previously proposed forward-backward convolutional recurrent neural network (FBCRNN) for tagging and pseudo labeling and tag-conditioned sound event detection (SED) models which are trained using the strong pseudo labels provided by the FBCRNN. Our advancement over our previous model is threefold. Firstly, we introduce a strong label loss in the objective of the FBCRNN to take advantage of the strongly labeled synthetic data during training, which leads to both better tagging and detection performance. Secondly, we perform multiple iterations of self-training for both the FBCRNN and tag-conditioned SED models. Thirdly, while we used only tag-conditioned CNNs as our SED model in the last edition we here explore sophisticated SED model architectures, namely, tag-conditioned bidirectional CRNNs and tag-conditioned bidirectional convolutional transformer neural networks (CTNNs) and combine them. With scenario and class dependent tuning of median filter lengths for post-processing, our final SED model, consisting of 6 submodels (2 of each architecture), is able to achieve validation poly-phonic sound event detection scores (PSDS) of 0.454 for scenario 1 and 0.758 for scenario 2 as well as a collar-based F1-score of 0.602 outperforming the baselines and our model from the last edition by far. Source code will be made publicly available at https://github.com/fgnt/pb\_sed."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gong2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Gong2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Improved Pseudo-Labeling Method For Semi-Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Gong, Yaguang and Li, Changlong and Wang, Xintian and Ma, Lu and Yang, Song and Wu, Zhongqin Wu
       </p>
<p style="text-align:left">
<em>
         TAL Education Group, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Gong_TAL_SED_task4_1</span> <span class="label label-primary">Gong_TAL_SED_task4_2</span> <span class="label label-primary">Gong_TAL_SED_task4_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Gong2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gong2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gong2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Gong_104_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gong2021" class="panel-collapse collapse" id="collapse-Gong2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Improved Pseudo-Labeling Method For Semi-Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Gong, Yaguang and Li, Changlong and Wang, Xintian and Ma, Lu and Yang, Song and Wu, Zhongqin Wu
       </small>
<br/>
<small>
<em>
         TAL Education Group, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report illustrates a framework for the DCASE2021 task4 - Sound Event Detection. The proposed framework is built on the pseudo-labeling method widely applied for semi-supervised learning(SSL) tasks. The proposed method synthesizes weak pseudo-labels for the large amount of unlabeled data by utilizing the modelâ€™s predictions on weakly augmented spectrograms. Weak pseudo-labels are then used as supervision for strongly augmented spectrograms of the same sample. Along to this main contribution, this work introduces data augmentation techniques including random frequency masking and time shifting, training techniques such as class-specific weighted loss, and model ensemble techniques. Experimental results demonstrate that the proposed method achieves PSDS of 0.407/0.653(scenario1/scenario2) on the validation set, which presents superior performance against the baseline score 0.342/0.527.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gong2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Gong_104_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gong2021label" class="modal fade" id="bibtex-Gong2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGong2021label">
        Improved Pseudo-Labeling Method For Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Gong2021,
    Author = "Gong, Yaguang and Li, Changlong and Wang, Xintian and Ma, Lu and Yang, Song and Wu, Zhongqin Wu",
    title = "Improved Pseudo-Labeling Method For Semi-Supervised Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This report illustrates a framework for the DCASE2021 task4 - Sound Event Detection. The proposed framework is built on the pseudo-labeling method widely applied for semi-supervised learning(SSL) tasks. The proposed method synthesizes weak pseudo-labels for the large amount of unlabeled data by utilizing the modelâ€™s predictions on weakly augmented spectrograms. Weak pseudo-labels are then used as supervision for strongly augmented spectrograms of the same sample. Along to this main contribution, this work introduces data augmentation techniques including random frequency masking and time shifting, training techniques such as class-specific weighted loss, and model ensemble techniques. Experimental results demonstrate that the proposed method achieves PSDS of 0.407/0.653(scenario1/scenario2) on the validation set, which presents superior performance against the baseline score 0.342/0.527."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hafsati2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Hafsati2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Task Aware Sound Event Detection Based On Semi-Supervised CRNNWith Skip Connections: DCASE 2021 Challenge, Task 4
       </h4>
<p style="text-align:left">
        Hafsati, Mohammed and Bentounes, Kamil
       </p>
<p style="text-align:left">
<em>
<sup>1</sup>Beijing Kuaishou Technology Co., Ltd, China <sup>2</sup>The State Key Laboratory of Automotive Satefy and Energy, Tsinghua University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hafsati_TUITO_task4_SED_1</span> <span class="label label-primary">Hafsati_TUITO_task4_SED_2</span> <span class="label label-primary">Hafsati_TUITO_task4_SED_3</span> <span class="label label-primary">Hafsati_TUITO_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hafsati2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hafsati2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hafsati2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Hafsati_87_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hafsati2021" class="panel-collapse collapse" id="collapse-Hafsati2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Task Aware Sound Event Detection Based On Semi-Supervised CRNNWith Skip Connections: DCASE 2021 Challenge, Task 4
      </h4>
<p style="text-align:left">
<small>
        Hafsati, Mohammed and Bentounes, Kamil
       </small>
<br/>
<small>
<em>
<sup>1</sup>Beijing Kuaishou Technology Co., Ltd, China <sup>2</sup>The State Key Laboratory of Automotive Satefy and Energy, Tsinghua University, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound Event Detection (SED) is the task of classifying different sounds occurring in a recorded environment and their onset and offset times. This assignment is the primary goal of the fourth task of the DCASE challenge using some strongly labeled, partially labeled, and unlabeled datasets. In this paper, we describe our submitted approach for this challenge. Our neural network is based on sequential convolutional neural networks with skipping some layers and a recurrent neural network. To overcome the challenge of using unlabeled data, we used semi-supervised learning, and to improve the performance further, we propose to use data augmentation techniques. With our model, we can slightly outperform the baseline with fewer filters and therefore fewer parameters. Moreover, similar amount of parameters as the baseline, we significantly outperform it.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hafsati2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Hafsati_87_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hafsati2021label" class="modal fade" id="bibtex-Hafsati2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHafsati2021label">
        Task Aware Sound Event Detection Based On Semi-Supervised CRNNWith Skip Connections: DCASE 2021 Challenge, Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hafsati2021,
    Author = "Hafsati, Mohammed and Bentounes, Kamil",
    title = "Task Aware Sound Event Detection Based On Semi-Supervised CRNNWith Skip Connections: DCASE 2021 Challenge, Task 4",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Sound Event Detection (SED) is the task of classifying different sounds occurring in a recorded environment and their onset and offset times. This assignment is the primary goal of the fourth task of the DCASE challenge using some strongly labeled, partially labeled, and unlabeled datasets. In this paper, we describe our submitted approach for this challenge. Our neural network is based on sequential convolutional neural networks with skipping some layers and a recurrent neural network. To overcome the challenge of using unlabeled data, we used semi-supervised learning, and to improve the performance further, we propose to use data augmentation techniques. With our model, we can slightly outperform the baseline with fewer filters and therefore fewer parameters. Moreover, similar amount of parameters as the baseline, we significantly outperform it."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Self-Training With Noisy Student Model And Semi-Supervised Loss Function For DCASE 2021 Challenge Task 4
       </h4>
<p style="text-align:left">
        Kim, Nam Kyun <sup>1</sup> and Kim, Hong Kook <sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Electrical Engineering and Computer Science, 123 Cheomdangwagi-ro, Gwangju 61005, Republic of Korea <sup>2</sup> AI Graduate School Gwangju Institute of Science and Technology, 123 Cheomdangwagi-ro, Gwangju 61005, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_AiTeR_GIST_SED_1</span> <span class="label label-primary">Kim_AiTeR_GIST_SED_2</span> <span class="label label-primary">Kim_AiTeR_GIST_SED_3</span> <span class="label label-primary">Kim_AiTeR_GIST_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Kim_23_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2021" class="panel-collapse collapse" id="collapse-Kim2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Self-Training With Noisy Student Model And Semi-Supervised Loss Function For DCASE 2021 Challenge Task 4
      </h4>
<p style="text-align:left">
<small>
        Kim, Nam Kyun <sup>1</sup> and Kim, Hong Kook <sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>School of Electrical Engineering and Computer Science, 123 Cheomdangwagi-ro, Gwangju 61005, Republic of Korea <sup>2</sup> AI Graduate School Gwangju Institute of Science and Technology, 123 Cheomdangwagi-ro, Gwangju 61005, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report proposes a polyphonic sound event detection (SED) method for the DCASE 2021 Challenge Task 4. The proposed SED model consists of two stages: a mean-teacher model for providing target labels regarding weakly labeled or unlabeled data and a self-training-based noisy student model for predicting strong labels for sound events. The mean-teacher model, which is based on the residual convolutional recurrent neural network (RCRNN) for the teacher and student model, is first trained using all the training data from a weakly labeled dataset, an unlabeled dataset, and a strongly labeled synthetic dataset. Then, the trained mean-teacher model predicts the strong label to each of the weakly labeled and unlabeled datasets, which is brought to the noisy student model in the second stage of the proposed SED model. Here, the structure of the noisy student model is identical to the RCRNN-based student model of the mean-teacher model in the first stage. Then, it is self-trained by adding feature noises, such as time-frequency shift, mixup, SpecAugment, and dropout-based model noise. In addition, a semi-supervised loss function is applied to train the noisy student model, which acts as label noise injection. The performance of the proposed SED model is evaluated on the validation set of the DCASE 2021 Challenge Task 4, and then, several ensemble models that combine five-fold validation models with different hyperparameters of the semi-supervised loss function are finally selected as our final models.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Kim_23_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2021label" class="modal fade" id="bibtex-Kim2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2021label">
        Self-Training With Noisy Student Model And Semi-Supervised Loss Function For DCASE 2021 Challenge Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2021,
    Author = "Kim, Nam Kyun and Kim, Hong Kook",
    title = "Self-Training With Noisy Student Model And Semi-Supervised Loss Function For DCASE 2021 Challenge Task 4",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This report proposes a polyphonic sound event detection (SED) method for the DCASE 2021 Challenge Task 4. The proposed SED model consists of two stages: a mean-teacher model for providing target labels regarding weakly labeled or unlabeled data and a self-training-based noisy student model for predicting strong labels for sound events. The mean-teacher model, which is based on the residual convolutional recurrent neural network (RCRNN) for the teacher and student model, is first trained using all the training data from a weakly labeled dataset, an unlabeled dataset, and a strongly labeled synthetic dataset. Then, the trained mean-teacher model predicts the strong label to each of the weakly labeled and unlabeled datasets, which is brought to the noisy student model in the second stage of the proposed SED model. Here, the structure of the noisy student model is identical to the RCRNN-based student model of the mean-teacher model in the first stage. Then, it is self-trained by adding feature noises, such as time-frequency shift, mixup, SpecAugment, and dropout-based model noise. In addition, a semi-supervised loss function is applied to train the noisy student model, which acts as label noise injection. The performance of the proposed SED model is evaluated on the validation set of the DCASE 2021 Challenge Task 4, and then, several ensemble models that combine five-fold validation models with different hyperparameters of the semi-supervised loss function are finally selected as our final models."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koo2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Koo2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection Based On Self-Supervised Learning Of Wav2vec 2.0
       </h4>
<p style="text-align:left">
        Koo, Hyejin<sup>1</sup> and Park, Hyung-Min<sup>1</sup> and Park, Jonghyeon<sup>2</sup> and Oh, Myungwoo<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Dept. of Electronic Engineering, Sogang University, Seoul 04107, South Korea, <sup>2</sup>NAVER Corp. Gyeonggi-do 13561, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Koo_SGU_task4_SED_1</span> <span class="label label-primary">Koo_SGU_task4_SED_2</span> <span class="label label-primary">Koo_SGU_task4_SED_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Koo2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koo2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koo2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Koo_120_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koo2021" class="panel-collapse collapse" id="collapse-Koo2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection Based On Self-Supervised Learning Of Wav2vec 2.0
      </h4>
<p style="text-align:left">
<small>
        Koo, Hyejin<sup>1</sup> and Park, Hyung-Min<sup>1</sup> and Park, Jonghyeon<sup>2</sup> and Oh, Myungwoo<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Dept. of Electronic Engineering, Sogang University, Seoul 04107, South Korea, <sup>2</sup>NAVER Corp. Gyeonggi-do 13561, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our system for DCASE2021 Task4: Sound Event Detection (SED) and Separation in Domain Environments. This task evaluates how to capture information of SED with a relatively small amount of labeled data in addition to lots of unlabeled data. We apply wav2vec 2.0 on the SED for the first time. Even though wav2vec 2.0 pre-training using the DCASE2021 Taksk4 dataset spends long time to train audio representations, the presented model achieved higher intersection F1 and PSDS2. The baselineâ€™s mean-teacher model and dataset was used to compare wav2vec 2.0 and log-mel features. Under the same conditions, we present how wav2vec 2.0 features work on the SED task.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koo2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Koo_120_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koo2021label" class="modal fade" id="bibtex-Koo2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoo2021label">
        Sound Event Detection Based On Self-Supervised Learning Of Wav2vec 2.0
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Koo2021,
    Author = "Koo, Hyejin and Park, Hyung-Min and Park, Jonghyeon and Oh, Myungwoo",
    title = "Sound Event Detection Based On Self-Supervised Learning Of Wav2vec 2.0",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this report, we present our system for DCASE2021 Task4: Sound Event Detection (SED) and Separation in Domain Environments. This task evaluates how to capture information of SED with a relatively small amount of labeled data in addition to lots of unlabeled data. We apply wav2vec 2.0 on the SED for the first time. Even though wav2vec 2.0 pre-training using the DCASE2021 Taksk4 dataset spends long time to train audio representations, the presented model achieved higher intersection F1 and PSDS2. The baselineâ€™s mean-teacher model and dataset was used to compare wav2vec 2.0 and log-mel features. Under the same conditions, we present how wav2vec 2.0 features work on the SED task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liang2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Liang2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Adaptive Focal Loss With Data Augmentation For Semi-Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Liang, Yunhao and Tang, Tiantian and Long, Yanhua
       </p>
<p style="text-align:left">
<em>
         Shanghai Normal University, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liang_SHNU_task4_SSep_SED_1</span> <span class="label label-primary">Liang_SHNU_task4_SSep_SED_2</span> <span class="label label-primary">Liang_SHNU_task4_SSep_SED_3</span> <span class="label label-primary">Liang_SHNU_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liang2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liang2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Liang_44_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liang2021" class="panel-collapse collapse" id="collapse-Liang2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Adaptive Focal Loss With Data Augmentation For Semi-Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Liang, Yunhao and Tang, Tiantian and Long, Yanhua
       </small>
<br/>
<small>
<em>
         Shanghai Normal University, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE2021 Task4: sound event detection and separation in domestic environments. In our submissions, two different deep models are investigated. The first one is a mean-teacher model with convolutional recurrent neural network (CRNN). The second one is a joint framework with adaptive focal loss based on the Guided Learning architecture. To improve the performance of system, we propose to use various methods such as the specaugment data augmentation method, adaptive focal loss, event specific post-processing. To combine sound separation with sound event detection, we train models using the outputs of the sound separation baseline system. We demonstrate that the proposed method achieves the event-based macro F1 score of 44.4%, 0.428 in PSDS1 and 0.736 in PSDS2 on the validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Liang_44_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liang2021label" class="modal fade" id="bibtex-Liang2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiang2021label">
        Adaptive Focal Loss With Data Augmentation For Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liang2021,
    Author = "Liang, Yunhao and Tang, Tiantian and Long, Yanhua",
    title = "Adaptive Focal Loss With Data Augmentation For Semi-Supervised Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE2021 Task4: sound event detection and separation in domestic environments. In our submissions, two different deep models are investigated. The first one is a mean-teacher model with convolutional recurrent neural network (CRNN). The second one is a joint framework with adaptive focal loss based on the Guided Learning architecture. To improve the performance of system, we propose to use various methods such as the specaugment data augmentation method, adaptive focal loss, event specific post-processing. To combine sound separation with sound event detection, we train models using the outputs of the sound separation baseline system. We demonstrate that the proposed method achieves the event-based macro F1 score of 44.4\%, 0.428 in PSDS1 and 0.736 in PSDS2 on the validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2021 Task 4
       </h4>
<p style="text-align:left">
        Liu, Gang and Liu, Zhuang Zhuang and Fang, Jun Yan and Liu, Yi Liu and Zhou, Ming Kun
       </p>
<p style="text-align:left">
<em>
         Beijing University of Posts and Telecommunications, Beijing,China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_BUPT_task4_SED_1</span> <span class="label label-primary">Liu_BUPT_task4_SED_2</span> <span class="label label-primary">Liu_BUPT_task4_SED_3</span> <span class="label label-primary">Liu_BUPT_task4_SED_4</span> <span class="label label-primary">Liu_BUPT_task4_SS_SED_1</span> <span class="label label-primary">Liu_BUPT_task4_SS_SED_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Liu_100_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2021" class="panel-collapse collapse" id="collapse-Liu2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2021 Task 4
      </h4>
<p style="text-align:left">
<small>
        Liu, Gang and Liu, Zhuang Zhuang and Fang, Jun Yan and Liu, Yi Liu and Zhou, Ming Kun
       </small>
<br/>
<small>
<em>
         Beijing University of Posts and Telecommunications, Beijing,China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Audio tagging aims to assign one or more labels to the audio clip. In this paper, we proposed our solutions applied to our submission for DCASE2021 Task4. The target of the systems is to provide not only the event class but also the event time localization given that multiple events can be present in an audio recording. We present a convolutional recurrent neural network (CRNN) with two recurrent neural network (RNN) classifiers sharing the same preprocessing convolutional neural network (CNN). Both recurrent networks perform audio tagging. One is processing the input audio signal in forward direction and the other in backward direction. We also use a spatial attention layer which called Fcanet to improve our system. We also make an independent system to achieve sound event speration.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Liu_100_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2021label" class="modal fade" id="bibtex-Liu2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2021label">
        Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2021 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2021,
    Author = "Liu, Gang and Liu, Zhuang Zhuang and Fang, Jun Yan and Liu, Yi Liu and Zhou, Ming Kun",
    title = "Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2021 Task 4",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Audio tagging aims to assign one or more labels to the audio clip. In this paper, we proposed our solutions applied to our submission for DCASE2021 Task4. The target of the systems is to provide not only the event class but also the event time localization given that multiple events can be present in an audio recording. We present a convolutional recurrent neural network (CRNN) with two recurrent neural network (RNN) classifiers sharing the same preprocessing convolutional neural network (CNN). Both recurrent networks perform audio tagging. One is processing the input audio signal in forward direction and the other in backward direction. We also use a spatial attention layer which called Fcanet to improve our system. We also make an independent system to achieve sound event speration."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lu2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Lu2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Integrating Advantages Of Recurrent And Transformer Structures For Sound Event Detection In Multiple Scenarios
       </h4>
<p style="text-align:left">
        Lu, Rui <sup>1</sup> and Hu, Wenzheng <sup>2</sup> and Duan Zhiyao <sup>1</sup> and Liu, Ji <sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Beijing Kuaishou Technology Co., Ltd, China <sup>2</sup>The State Key Laboratory of Automotive Satefy and Energy, Tsinghua University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">lu_kwai_task4_SED_1</span> <span class="label label-primary">lu_kwai_task4_SED_2</span> <span class="label label-primary">lu_kwai_task4_SED_3</span> <span class="label label-primary">lu_kwai_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lu2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lu2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Lu_8_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lu2021" class="panel-collapse collapse" id="collapse-Lu2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Integrating Advantages Of Recurrent And Transformer Structures For Sound Event Detection In Multiple Scenarios
      </h4>
<p style="text-align:left">
<small>
        Lu, Rui <sup>1</sup> and Hu, Wenzheng <sup>2</sup> and Duan Zhiyao <sup>1</sup> and Liu, Ji <sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Beijing Kuaishou Technology Co., Ltd, China <sup>2</sup>The State Key Laboratory of Automotive Satefy and Energy, Tsinghua University, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we detail our submitted systems for task4 of DCASE2021: Sound Event Detection and Separation in Domestic Environments. Our systems exploit both recurrent structure and transformer structure to model the complicated dynamics in real life domestic audio data. In addition to prevalent tricks such as semi-supervised mean-teacher learning, data augmentation and ensemble, we find that different models exhibit differently under the two scenarios, which emphasize different system properties. By integrating advantages of both the recurrent and transformer structures, our proposed systems achieve an overall poly-phonic sound event detection scores (PSDS-scores) of 1.171 (PSDS-scenario1 + PSDS-scenario2) on the hold-out test set of the development dataset, outperforming the baseline system by 34.8%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Lu_8_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lu2021label" class="modal fade" id="bibtex-Lu2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLu2021label">
        Integrating Advantages Of Recurrent And Transformer Structures For Sound Event Detection In Multiple Scenarios
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lu2021,
    Author = "Lu, Rui and Hu, Wenzheng and Zhiyao, Duan and Liu, Ji",
    title = "Integrating Advantages Of Recurrent And Transformer Structures For Sound Event Detection In Multiple Scenarios",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we detail our submitted systems for task4 of DCASE2021: Sound Event Detection and Separation in Domestic Environments. Our systems exploit both recurrent structure and transformer structure to model the complicated dynamics in real life domestic audio data. In addition to prevalent tricks such as semi-supervised mean-teacher learning, data augmentation and ensemble, we find that different models exhibit differently under the two scenarios, which emphasize different system properties. By integrating advantages of both the recurrent and transformer structures, our proposed systems achieve an overall poly-phonic sound event detection scores (PSDS-scores) of 1.171 (PSDS-scenario1 + PSDS-scenario2) on the hold-out test set of the development dataset, outperforming the baseline system by 34.8\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Na2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Na2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolutional Network With Conformer For Semi-Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Na, Tong and Zhang, Qinyi
       </p>
<p style="text-align:left">
<em>
         Beijing University of Posts and Telecommunications, Beijing,China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Na_BUPT_task4_SED_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Na2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Na2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Na2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Na_122_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Na2021" class="panel-collapse collapse" id="collapse-Na2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolutional Network With Conformer For Semi-Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Na, Tong and Zhang, Qinyi
       </small>
<br/>
<small>
<em>
         Beijing University of Posts and Telecommunications, Beijing,China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our system submission for DCASE 2021 Task 4. Our model employs a convolutional network in conjunction with conformer blocks and utilizes the Mean-Teacher semi-supervised learning technique for further improvement.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Na2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Na_122_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Na2021label" class="modal fade" id="bibtex-Na2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNa2021label">
        Convolutional Network With Conformer For Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Na2021,
    Author = "Na, Tong and Zhang, Qinyi",
    title = "Convolutional Network With Conformer For Semi-Supervised Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe our system submission for DCASE 2021 Task 4. Our model employs a convolutional network in conjunction with conformer blocks and utilizes the Mean-Teacher semi-supervised learning technique for further improvement."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nam2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Nam2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Heavily Augmented Sound Event Detection utilizing Weak Predictions
       </h4>
<p style="text-align:left">
        Nam, Hyeonuk and Ko, Byeong-Yun and Lee, Gyeong-Tae and Kim, Seong-Hu and Jung, Won-Ho and Choi, Sang-Min and Park, Yong-Hwa
       </p>
<p style="text-align:left">
<em>
         Korea Advanced Institute of Science and Technology, Department of Mechanical Engineering, 291 Daehak-ro, Yuseong-gu, Daejeon 34141, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nam_KAIST_task4_SED_1</span> <span class="label label-primary">Nam_KAIST_task4_SED_2</span> <span class="label label-primary">Nam_KAIST_task4_SED_3</span> <span class="label label-primary">Nam_KAIST_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nam2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nam2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nam2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Nam_41_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Nam2021').collapse('show');window.location.hash='#Nam2021';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nam2021" class="panel-collapse collapse" id="collapse-Nam2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Heavily Augmented Sound Event Detection utilizing Weak Predictions
      </h4>
<p style="text-align:left">
<small>
        Nam, Hyeonuk and Ko, Byeong-Yun and Lee, Gyeong-Tae and Kim, Seong-Hu and Jung, Won-Ho and Choi, Sang-Min and Park, Yong-Hwa
       </small>
<br/>
<small>
<em>
         Korea Advanced Institute of Science and Technology, Department of Mechanical Engineering, 291 Daehak-ro, Yuseong-gu, Daejeon 34141, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The performance of Sound Event Detection (SED) systems are greatly limited by the difficulty in generating large strongly labeled dataset. In this work, we used two main approaches to overcome the lack of strongly labeled data. First, we applied heavy data augmentation on input features. Data augmentation methods used include not only conventional methods used in speech/audio domains but also our proposed method named FilterAugment. Second, we propose two methods to utilize weak predictions to enhance weakly supervised SED performance. As a result, we obtained the best PSDS1 of 0.4336 and best PSDS2 of 0.8161 on the DESED real validation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nam2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Nam_41_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/frednam93/FilterAugSED" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nam2021label" class="modal fade" id="bibtex-Nam2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNam2021label">
        Heavily Augmented Sound Event Detection utilizing Weak Predictions
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nam2021,
    Author = "Nam, Hyeonuk and Ko, Byeong-Yun and Lee, Gyeong-Tae and Kim, Seong-Hu and Jung, Won-Ho and Choi, Sang-Min and Park, Yong-Hwa",
    title = "Heavily Augmented Sound Event Detection utilizing Weak Predictions",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "The performance of Sound Event Detection (SED) systems are greatly limited by the difficulty in generating large strongly labeled dataset. In this work, we used two main approaches to overcome the lack of strongly labeled data. First, we applied heavy data augmentation on input features. Data augmentation methods used include not only conventional methods used in speech/audio domains but also our proposed method named FilterAugment. Second, we propose two methods to utilize weak predictions to enhance weakly supervised SED performance. As a result, we obtained the best PSDS1 of 0.4336 and best PSDS2 of 0.8161 on the DESED real validation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Olvera2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Olvera2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Domain-Adapted Sound Event Detection System With Auxiliary Foreground-Background Classifier
       </h4>
<p style="text-align:left">
        Olvera, Michel<sup>1</sup> and Vincent, Emmanuel<sup>1</sup> and Gasso, Gilles<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>UniversitÃ© de Lorraine, Inria, Loria, F-54000 Nancy, France, <sup>2</sup>LITIS EA 4108, UniversitÃ© &amp; INSA Rouen Normandie, 76800 Saint-Ã‰tienne du Rouvray, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Olvera_INRIA_task4_SED_1</span> <span class="label label-primary">Olvera_INRIA_task4_SED_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Olvera2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Olvera2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Olvera2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Olvera_121_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Olvera2021" class="panel-collapse collapse" id="collapse-Olvera2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Domain-Adapted Sound Event Detection System With Auxiliary Foreground-Background Classifier
      </h4>
<p style="text-align:left">
<small>
        Olvera, Michel<sup>1</sup> and Vincent, Emmanuel<sup>1</sup> and Gasso, Gilles<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>UniversitÃ© de Lorraine, Inria, Loria, F-54000 Nancy, France, <sup>2</sup>LITIS EA 4108, UniversitÃ© &amp; INSA Rouen Normandie, 76800 Saint-Ã‰tienne du Rouvray, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we propose a sound event detection system for the DCASE 2021 task 4 challenge, which consists of a foreground-background classification branch that is jointly trained with the baseline architecture. Furthermore, to account for the mismatch between synthetic annotated data and real unlabeled data used for training, we also propose a frame-level domain adaptation scheme to improve detection performance over real soundscapes. We show that these improvements to the baseline method help in the generalization of the sound event detection task.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Olvera2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Olvera_121_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Olvera2021label" class="modal fade" id="bibtex-Olvera2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexOlvera2021label">
        Domain-Adapted Sound Event Detection System With Auxiliary Foreground-Background Classifier
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Olvera2021,
    Author = "Olvera, Michel and Vincent, Emmanuel and Gasso, Gilles",
    title = "Domain-Adapted Sound Event Detection System With Auxiliary Foreground-Background Classifier",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we propose a sound event detection system for the DCASE 2021 task 4 challenge, which consists of a foreground-background classification branch that is jointly trained with the baseline architecture. Furthermore, to account for the mismatch between synthetic annotated data and real unlabeled data used for training, we also propose a frame-level domain adaptation scheme to improve detection performance over real soundscapes. We show that these improvements to the baseline method help in the generalization of the sound event detection task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Park2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection with Cross-Referencing Self-Training
       </h4>
<p style="text-align:left">
        Park Sangwook<sup>1</sup> and Choi, Woohyun<sup>2</sup> and Elhilali, Mounya<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Electrical and Computer Engineering, Johns Hopkins University, United States, <sup>2</sup>LG electronics
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Park_task4_SED_1</span> <span class="label label-primary">Park_task4_SED_2</span> <span class="label label-primary">Park_task4_SED_3</span> <span class="label label-primary">Park_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Park2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Park_101_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Park2021').collapse('show');window.location.hash='#Park2021';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2021" class="panel-collapse collapse" id="collapse-Park2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection with Cross-Referencing Self-Training
      </h4>
<p style="text-align:left">
<small>
        Park Sangwook<sup>1</sup> and Choi, Woohyun<sup>2</sup> and Elhilali, Mounya<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Department of Electrical and Computer Engineering, Johns Hopkins University, United States, <sup>2</sup>LG electronics
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes a sound event detection method submitted to the DCASE2021 challenge, task 4. In this approach, we design a residual convolutional recurrent neural network and train this network with a cross-referencing self-training approach that leverages an extensive unlabeled data in combination with labeled data. This approach takes advantage of semi-supervised training using pseudo-labels from a balanced student-teacher model, and outperforms DCASE2021 challenge baseline in terms of Poly-phonic Sound event Detection Score. Additionally, the proposed network has more accurate predictions in class-wise collar-based-F1, compared to the baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Park_101_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/JHU-LCAP/CRSTmodel" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2021label" class="modal fade" id="bibtex-Park2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2021label">
        Sound Event Detection with Cross-Referencing Self-Training
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Park2021,
    Author = "Sangwook, Park and Choi, Woohyun and Elhilali, Mounya",
    title = "Sound Event Detection with Cross-Referencing Self-Training",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This report describes a sound event detection method submitted to the DCASE2021 challenge, task 4. In this approach, we design a residual convolutional recurrent neural network and train this network with a cross-referencing self-training approach that leverages an extensive unlabeled data in combination with labeled data. This approach takes advantage of semi-supervised training using pseudo-labels from a balanced student-teacher model, and outperforms DCASE2021 challenge baseline in terms of Poly-phonic Sound event Detection Score. Additionally, the proposed network has more accurate predictions in class-wise collar-based-F1, compared to the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tian2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Tian2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection Using Metric Learning And Focal Loss For DCASE 2021 Task 4
       </h4>
<p style="text-align:left">
        Tian, Gangyi<sup>1</sup> and Huang, Yuxin<sup>1,2</sup> and Ye, Zhirong<sup>1,2</sup> and Ma, Shuo<sup>1,2</sup> and Wang, Xiangdong<sup>1</sup> and Liu, Hong<sup>1</sup> and Qian, Yueliang<sup>1</sup> and Tao, Rui<sup>3</sup> and Yan, Long<sup>3</sup> and Ouchi, Kazushige<sup>3</sup> and Ebbers, Janek<sup>4</sup> Haeb-Umbach, Reinhold<sup>4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China,Beijing University of Posts and Telecommunications, Beijing,China, <sup>4</sup>Paderborn University, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Tian_ICT-TOSHIBA_task4_SED_1</span><span class="label label-primary">Tian_ICT-TOSHIBA_task4_SED_2</span><span class="label label-primary">Tian_ICT-TOSHIBA_task4_SED_3</span><span class="label label-primary">Tian_ICT-TOSHIBA_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Tian2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tian2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tian2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Tian_130_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tian2021" class="panel-collapse collapse" id="collapse-Tian2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection Using Metric Learning And Focal Loss For DCASE 2021 Task 4
      </h4>
<p style="text-align:left">
<small>
        Tian, Gangyi<sup>1</sup> and Huang, Yuxin<sup>1,2</sup> and Ye, Zhirong<sup>1,2</sup> and Ma, Shuo<sup>1,2</sup> and Wang, Xiangdong<sup>1</sup> and Liu, Hong<sup>1</sup> and Qian, Yueliang<sup>1</sup> and Tao, Rui<sup>3</sup> and Yan, Long<sup>3</sup> and Ouchi, Kazushige<sup>3</sup> and Ebbers, Janek<sup>4</sup> Haeb-Umbach, Reinhold<sup>4</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China,Beijing University of Posts and Telecommunications, Beijing,China, <sup>4</sup>Paderborn University, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our system submission for DCASE 2021 Task 4. Our model employs a convolutional network in conjunction with conformer blocks and utilizes the Mean-Teacher semi-supervised learning technique for further improvement.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tian2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Tian_130_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tian2021label" class="modal fade" id="bibtex-Tian2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTian2021label">
        Sound Event Detection Using Metric Learning And Focal Loss For DCASE 2021 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Tian2021,
    Author = "Tian, Gangyi and Huang, Yuxin and Ye, Zhirong and Ma, Shuo and Wang, Xiangdong and Liu, Hong and Qian, Yueliang and Tao, Rui and Yan, Long and Ouchi, Kazushige and Ebbers, Janek Haeb-Umbach, Reinhold",
    title = "Sound Event Detection Using Metric Learning And Focal Loss For DCASE 2021 Task 4",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe our system submission for DCASE 2021 Task 4. Our model employs a convolutional network in conjunction with conformer blocks and utilizes the Mean-Teacher semi-supervised learning technique for further improvement."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="turpault2020a" style="box-shadow: none">
<div class="panel-heading" id="heading-turpault2020a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Training Sound Event Detection On A Heterogeneous Dataset
       </h4>
<p style="text-align:left">
        Turpault, Nicolas and Serizel, Romain
       </p>
<p style="text-align:left">
<em>
         UniversitÃ© de Lorraine, CNRS, Inria, Loria, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2020_SED_baseline_system</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-turpault2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-turpault2020a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-turpault2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891665/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-turpault2020a').collapse('show');window.location.hash='#turpault2020a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-turpault2020a" class="panel-collapse collapse" id="collapse-turpault2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Training Sound Event Detection On A Heterogeneous Dataset
      </h4>
<p style="text-align:left">
<small>
        Turpault, Nicolas and Serizel, Romain
       </small>
<br/>
<small>
<em>
         UniversitÃ© de Lorraine, CNRS, Inria, Loria, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Acoustic features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         p-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-turpault2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891665/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-turpault2020alabel" class="modal fade" id="bibtex-turpault2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexturpault2020alabel">
        Training Sound Event Detection On A Heterogeneous Dataset
       </h4>
</div>
<div class="modal-body">
<pre>@unpublished{turpault2020a,
    AUTHOR = "Turpault, Nicolas and Serizel, Romain",
    title = "Training Sound Event Detection On A Heterogeneous Dataset",
    note = "working paper or preprint",
    institution = "DCASE2020 Challenge",
    year = "2020",
    abstract = "Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="turpault2020b" style="box-shadow: none">
<div class="panel-heading" id="heading-turpault2020b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Improving Sound Event Detection In Domestic Environments Using Sound Separation
       </h4>
<p style="text-align:left">
        Turpault, Nicolas<sup>1</sup> and Wisdom, Scott<sup>2</sup> and Erdogan, Hakan<sup>2</sup> and Herhey, John R.<sup>2</sup> and Serizel, Romain<sup>1</sup> and Fonseca, Eduardo<sup>3</sup> and Seetharaman, Prem<sup>4</sup> and Salomon, Justin<sup>5</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Universite de Lorraine, CNRS, Inria, Loria, Nancy, France, <sup>2</sup>Google Research, AI Perception, Cambridge, United States, <sup>3</sup>Music Technology Group, Universitat Pompeu Fabra, Barcelona, <sup>4</sup>Interactive Audio Lab, Northwestern University, Evanston, United States<sup>5</sup>Adobe Research, San Francisco, United States
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2020_SS_SED_baseline_system</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-turpault2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-turpault2020b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-turpault2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891700/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-turpault2020b').collapse('show');window.location.hash='#turpault2020b';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-turpault2020b" class="panel-collapse collapse" id="collapse-turpault2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Improving Sound Event Detection In Domestic Environments Using Sound Separation
      </h4>
<p style="text-align:left">
<small>
        Turpault, Nicolas<sup>1</sup> and Wisdom, Scott<sup>2</sup> and Erdogan, Hakan<sup>2</sup> and Herhey, John R.<sup>2</sup> and Serizel, Romain<sup>1</sup> and Fonseca, Eduardo<sup>3</sup> and Seetharaman, Prem<sup>4</sup> and Salomon, Justin<sup>5</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Universite de Lorraine, CNRS, Inria, Loria, Nancy, France, <sup>2</sup>Google Research, AI Perception, Cambridge, United States, <sup>3</sup>Music Technology Group, Universitat Pompeu Fabra, Barcelona, <sup>4</sup>Interactive Audio Lab, Northwestern University, Evanston, United States<sup>5</sup>Adobe Research, San Francisco, United States
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Acoustic features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         p-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-turpault2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891700/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-turpault2020blabel" class="modal fade" id="bibtex-turpault2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexturpault2020blabel">
        Improving Sound Event Detection In Domestic Environments Using Sound Separation
       </h4>
</div>
<div class="modal-body">
<pre>@unpublished{turpault2020b,
    AUTHOR = "Turpault, Nicolas and Wisdom, Scott and Erdogan, Hakan and Herhey, John R. and Serizel, Romain and Fonseca, Eduardo and Seetharaman, Prem and Salomon, Justin",
    title = "Improving Sound Event Detection In Domestic Environments Using Sound Separation",
    note = "working paper or preprint",
    institution = "DCASE2020 Challenge",
    year = "2020",
    abstract = "Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CHT+NSYSU Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For DCASE 2021 Task 4
       </h4>
<p style="text-align:left">
        Wang, Yih-Wen <sup>1</sup> and Chen, Chia-Ping <sup>1</sup> and Lu, Chung-Li <sup>2</sup> and Chan, Bo-Cheng <sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>National Sun Yat-Sen University, Taiwan <sup>2</sup> Chunghwa Telecom Laboratories, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_NSYSU_task4_SED_1</span> <span class="label label-primary">Wang_NSYSU_task4_SED_2</span> <span class="label label-primary">Wang_NSYSU_task4_SED_3</span> <span class="label label-primary">Wang_NSYSU_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Wang_25_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2021" class="panel-collapse collapse" id="collapse-Wang2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CHT+NSYSU Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For DCASE 2021 Task 4
      </h4>
<p style="text-align:left">
<small>
        Wang, Yih-Wen <sup>1</sup> and Chen, Chia-Ping <sup>1</sup> and Lu, Chung-Li <sup>2</sup> and Chan, Bo-Cheng <sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>National Sun Yat-Sen University, Taiwan <sup>2</sup> Chunghwa Telecom Laboratories, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE 2021 Task4: sound event detection and separation in domestic environments. The proposed system is based on mean-teacher framework of semi-supervised learning and neural networks of CRNN and CNN-Transformer. We employ consistency training of interpolation (ICT), shift (SCT), and clip-level (CCT) to enhance the generalization and representation. A multiscale CNN block is applied to extract various features to mitigate the influence of the event length diversity for the network. An efficient channel attention network (ECA-Net) and exponential softmax pooling enable the model to obtain definite sound event predictions. To further improve the performance, we use data augmentation including mixup, time shift, and time-frequency masks. Our ensemble system achieves the PSDS-scenario1 of 40.72% and PSDS-scenario2 of 80.80% on the validation set, significantly outperforming that of the baseline score of 34.2% and 52.7%, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Wang_25_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2021label" class="modal fade" id="bibtex-Wang2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2021label">
        CHT+NSYSU Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For DCASE 2021 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2021,
    Author = "Wang, Yih-Wen and Chen, Chia-Ping and Lu, Chung-Li and Chan, Bo-Cheng",
    title = "CHT+NSYSU Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For DCASE 2021 Task 4",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE 2021 Task4: sound event detection and separation in domestic environments. The proposed system is based on mean-teacher framework of semi-supervised learning and neural networks of CRNN and CNN-Transformer. We employ consistency training of interpolation (ICT), shift (SCT), and clip-level (CCT) to enhance the generalization and representation. A multiscale CNN block is applied to extract various features to mitigate the influence of the event length diversity for the network. An efficient channel attention network (ECA-Net) and exponential softmax pooling enable the model to obtain definite sound event predictions. To further improve the performance, we use data augmentation including mixup, time shift, and time-frequency masks. Our ensemble system achieves the PSDS-scenario1 of 40.72\% and PSDS-scenario2 of 80.80\% on the validation set, significantly outperforming that of the baseline score of 34.2\% and 52.7\%, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yao2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Yao2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Adaptive Memory Controlled Self Attention For Sound Event Detection
       </h4>
<p style="text-align:left">
        Yoa, Yu and Song, Xiyu
       </p>
<p style="text-align:left">
<em>
         Guilin university of Electronic Technology, Guilin, 541004, Guangxi, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yao_GUET_task4_SED_1</span> <span class="label label-primary">Yao_GUET_task4_SED_2</span> <span class="label label-primary">Yao_GUET_task4_SED_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yao2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yao2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yao2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yao_9_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yao2021" class="panel-collapse collapse" id="collapse-Yao2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Adaptive Memory Controlled Self Attention For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Yoa, Yu and Song, Xiyu
       </small>
<br/>
<small>
<em>
         Guilin university of Electronic Technology, Guilin, 541004, Guangxi, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event detection is a task to detect the time stamps and the class of sound event occurred in a recording. Real life sound events overlap in recording and the duration varies dramatically than synthetic data, making it even harder to recognize. In this paper we investigate how well that attention mechanism could improve for real life sound event detection (SED). Convolutional Recurrent Neural Networks (CRNN) have recently shown improved performances over established methods in various sound recognition tasks. In our work we use CRNN to extract hidden state feature representations; then, self-attention mechanism is introduced to memorize long-range dependencies of features that CRNN extract. Furthermore, we proposed to used adaptive memory controlled self-attention to explicitly compute the relations between time steps in audio representation embedding. The proposed method is evaluated on the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 challenge Task4 dataset, which contains different overlapping sound events from real life and synthetic. We develop a self attention SED model that used memory-controlled strategy with heuristically choose a fix attention width achieving a PSDS-scenario2 of 60.72% in average which indicating that attention mechanism is able to improve sound event detection. We show that proposed adaptive memory-controlled model reaches the same level result as fix attention width memory-controlled model.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yao2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yao_9_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yao2021label" class="modal fade" id="bibtex-Yao2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYao2021label">
        Adaptive Memory Controlled Self Attention For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yao2021,
    Author = "Yoa, Yu and Song, Xiyu",
    title = "Adaptive Memory Controlled Self Attention For Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Sound event detection is a task to detect the time stamps and the class of sound event occurred in a recording. Real life sound events overlap in recording and the duration varies dramatically than synthetic data, making it even harder to recognize. In this paper we investigate how well that attention mechanism could improve for real life sound event detection (SED). Convolutional Recurrent Neural Networks (CRNN) have recently shown improved performances over established methods in various sound recognition tasks. In our work we use CRNN to extract hidden state feature representations; then, self-attention mechanism is introduced to memorize long-range dependencies of features that CRNN extract. Furthermore, we proposed to used adaptive memory controlled self-attention to explicitly compute the relations between time steps in audio representation embedding. The proposed method is evaluated on the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 challenge Task4 dataset, which contains different overlapping sound events from real life and synthetic. We develop a self attention SED model that used memory-controlled strategy with heuristically choose a fix attention width achieving a PSDS-scenario2 of 60.72\% in average which indicating that attention mechanism is able to improve sound event detection. We show that proposed adaptive memory-controlled model reaches the same level result as fix attention width memory-controlled model."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yu2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Yu2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised Sound Event Detection Using Multi-Scale Convolutional Recurrent Neural Network And Weighted Pooling
       </h4>
<p style="text-align:left">
        Yu, Dongchi and Cai, Xichang and Liu, Duxin and Liu, Zihan
       </p>
<p style="text-align:left">
<em>
         North China University of Technology, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yu_NCUT_task4_SED_1</span> <span class="label label-primary">Yu_NCUT_task4_SED_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yu2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yu2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yu_56_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yu2021" class="panel-collapse collapse" id="collapse-Yu2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised Sound Event Detection Using Multi-Scale Convolutional Recurrent Neural Network And Weighted Pooling
      </h4>
<p style="text-align:left">
<small>
        Yu, Dongchi and Cai, Xichang and Liu, Duxin and Liu, Zihan
       </small>
<br/>
<small>
<em>
         North China University of Technology, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE2021 Task4: sound event detection and separation in domestic environment. We mainly focus on the scenario that recognizes sound events without source separation. Since the duration of different sound events could be quite different, our model employs a multi-scale convolution recurrent network to extract the multi-scale features of an audio sequence. For more efficiently utilizing weak label training data, a global weighted pooling strategy is introduced to aggregate frame level predictions to generate clip level prediction. Additionally, our model also use mean teacher semi-supervised learning technique and data augmentation. We demonstrate that the proposed method achieves the PSDS2 score of 0.61 and the event-based macro F1 score of 42.15% on the validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yu_56_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yu2021label" class="modal fade" id="bibtex-Yu2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYu2021label">
        Semi-Supervised Sound Event Detection Using Multi-Scale Convolutional Recurrent Neural Network And Weighted Pooling
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yu2021,
    Author = "Yu, Dongchi and Cai, Xichang and Liu, Duxin and Liu, Zihan",
    title = "Semi-Supervised Sound Event Detection Using Multi-Scale Convolutional Recurrent Neural Network And Weighted Pooling",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE2021 Task4: sound event detection and separation in domestic environment. We mainly focus on the scenario that recognizes sound events without source separation. Since the duration of different sound events could be quite different, our model employs a multi-scale convolution recurrent network to extract the multi-scale features of an audio sequence. For more efficiently utilizing weak label training data, a global weighted pooling strategy is introduced to aggregate frame level predictions to generate clip level prediction. Additionally, our model also use mean teacher semi-supervised learning technique and data augmentation. We demonstrate that the proposed method achieves the PSDS2 score of 0.61 and the event-based macro F1 score of 42.15\% on the validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zheng2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Zheng2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Zheng USTC Teamâ€™s Submission For DCASE2021 Task4 â€“ Semi-Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Zheng, Xu and Chen, Han and Song, Yan
       </p>
<p style="text-align:left">
<em>
         National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zheng_USTC_task4_SED_1</span> <span class="label label-primary">Zheng_USTC_task4_SED_2</span> <span class="label label-primary">Zheng_USTC_task4_SED_3</span><span class="label label-primary">Zheng_USTC_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zheng2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zheng2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zheng2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zheng_110_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zheng2021" class="panel-collapse collapse" id="collapse-Zheng2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Zheng USTC Teamâ€™s Submission For DCASE2021 Task4 â€“ Semi-Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Zheng, Xu and Chen, Han and Song, Yan
       </small>
<br/>
<small>
<em>
         National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our submitted system for DCASE2021 Task4: sound event detection and separation in domestic environments. Specifically, three main techniques are applied to improve the performance of the official baseline system with both synthetic and real data (weakly labeled and unlabeled). Firstly, in order to improve the localization ability of CRNN model, we propose to use the selective kernel(SK) unit. By stacking the SK unit, each neuron can adaptively adjust its receptive field for both short- and long- duration events. Secondly, based on the fact that detection outputs are dominated by the high-confidence predictions(lower than 0.1 or higher than 0.9), we propose to use soft detection output by setting proper temperature parameter in sigmoid, which can effectively improve the PSDS2 score. Thirdly, several data augmentation techniques and score fusion mechanisms are applied to improve the stability and robustness of the system performance. Experiments on the DCASE2021 task4 validation dataset demonstrate the effectiveness of the techniques used in our system. Specifically, PSDS scores of 0.45 and 0.78 are achieved for scenario1 and scenario2 respectively, outperforming the result of 0.34 and 0.53 in baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zheng2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zheng_110_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zheng2021label" class="modal fade" id="bibtex-Zheng2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZheng2021label">
        Zheng USTC Teamâ€™s Submission For DCASE2021 Task4 â€“ Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zheng2021,
    Author = "Zheng, Xu and Chen, Han and Song, Yan",
    title = "Zheng USTC Teamâ€™s Submission For DCASE2021 Task4 â€“ Semi-Supervised Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we present our submitted system for DCASE2021 Task4: sound event detection and separation in domestic environments. Specifically, three main techniques are applied to improve the performance of the official baseline system with both synthetic and real data (weakly labeled and unlabeled). Firstly, in order to improve the localization ability of CRNN model, we propose to use the selective kernel(SK) unit. By stacking the SK unit, each neuron can adaptively adjust its receptive field for both short- and long- duration events. Secondly, based on the fact that detection outputs are dominated by the high-confidence predictions(lower than 0.1 or higher than 0.9), we propose to use soft detection output by setting proper temperature parameter in sigmoid, which can effectively improve the PSDS2 score. Thirdly, several data augmentation techniques and score fusion mechanisms are applied to improve the stability and robustness of the system performance. Experiments on the DCASE2021 task4 validation dataset demonstrate the effectiveness of the techniques used in our system. Specifically, PSDS scores of 0.45 and 0.78 are achieved for scenario1 and scenario2 respectively, outperforming the result of 0.34 and 0.53 in baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhu2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhu2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Scale Convolution Based Attention Network For Semi-Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Zhu, Xiujuan<sup>1,3</sup> and Sun, Xinghao<sup>1,3</sup> and Hu, Ying<sup>1,3</sup> and Chen, Yadong<sup>1,3</sup> and Qiu, Wenbo<sup>1,3</sup> and Tang, Yuwu<sup>1,3</sup> and He, Liang<sup>1,2</sup> and Xu, Minqiang<sup>4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Information Science and Engineering, Xinjiang University, Urumqi, China, <sup>2</sup>Department of Electronic Engineering, Tsinghua University, China, <sup>3</sup>Key Laboratory of Signal Detection and Processing in Xinjiang, China, <sup>4</sup>SpeakIn Technology
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhu_AIAL-XJU_task4_SED_1</span> <span class="label label-primary">Zhu_AIAL-XJU_task4_SED_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhu2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhu2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zhu_98_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Zhu2021').collapse('show');window.location.hash='#Zhu2021';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhu2021" class="panel-collapse collapse" id="collapse-Zhu2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Scale Convolution Based Attention Network For Semi-Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Zhu, Xiujuan<sup>1,3</sup> and Sun, Xinghao<sup>1,3</sup> and Hu, Ying<sup>1,3</sup> and Chen, Yadong<sup>1,3</sup> and Qiu, Wenbo<sup>1,3</sup> and Tang, Yuwu<sup>1,3</sup> and He, Liang<sup>1,2</sup> and Xu, Minqiang<sup>4</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>School of Information Science and Engineering, Xinjiang University, Urumqi, China, <sup>2</sup>Department of Electronic Engineering, Tsinghua University, China, <sup>3</sup>Key Laboratory of Signal Detection and Processing in Xinjiang, China, <sup>4</sup>SpeakIn Technology
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Deep Convolutional Recurrent Neural Networks (CRNN) have drawn great attention in sound event detection (SED). Due to the variation in duration for acoustic events is relatively large, It is critcally important to design a good operator that can extract multiscale feature more efficiently for SED. However, most CRNN-based models lack discriminative ability for different types of acoustic events and deal with them equally, which results in the representational capacity of the models being limited. Inspired by this, We proposed a Multi-Scale Convolution based Attention Network(MSCA). By using Multi-Scale Convolution, a more effective feature representation ability can be obtained, Which can naturally learn coarse-to-fine multi-scale features to helps the model recognize different sound events. On the other hand, a channel-wise attention module is designed, which can adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhu2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zhu_98_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/zhuxiujuan-maker/Zhu_AIAL-XJU_task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhu2021label" class="modal fade" id="bibtex-Zhu2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhu2021label">
        Multi-Scale Convolution Based Attention Network For Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhu2021,
    Author = "Zhu, Xiujuan and Sun, Xinghao and Hu, Yin and Chen, Yadong and Qiu, Wenbo and Tang, Yuwu and He, Liang and Xu, Minqiang",
    title = "Multi-Scale Convolution Based Attention Network For Semi-Supervised Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Deep Convolutional Recurrent Neural Networks (CRNN) have drawn great attention in sound event detection (SED). Due to the variation in duration for acoustic events is relatively large, It is critcally important to design a good operator that can extract multiscale feature more efficiently for SED. However, most CRNN-based models lack discriminative ability for different types of acoustic events and deal with them equally, which results in the representational capacity of the models being limited. Inspired by this, We proposed a Multi-Scale Convolution based Attention Network(MSCA). By using Multi-Scale Convolution, a more effective feature representation ability can be obtained, Which can naturally learn coarse-to-fine multi-scale features to helps the model recognize different sound events. On the other hand, a channel-wise attention module is designed, which can adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>