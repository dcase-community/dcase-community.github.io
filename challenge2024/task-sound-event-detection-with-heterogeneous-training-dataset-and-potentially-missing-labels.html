<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Detection with Heterogeneous Training Dataset and Potentially Missing Labels - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of this task is to evaluate systems for the detection of sound events using real data, with different types of annotations data and corresponding labels available for training. Challenge has ended. Full results for this task can be found in the Results page. If you are interested in …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right navbar-tighter" id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2024</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2024/"><i class="fa fa-home"></i>&nbsp;Intro</a>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-t1"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-t2"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-t3"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-t4"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-t5"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-automated-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-t6"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-scene-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-t7"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-retrieval text-t8"></i>&nbsp;Task8&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-queried-audio-source-separation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-separation text-t9"></i>&nbsp;Task9&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-traffic text-t10"></i>&nbsp;Task10&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2024/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2024/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/ceil-05.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-t4"></i><i class="fa dc-domestic fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Events</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2024_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Detection with Heterogeneous Training Dataset and Potentially Missing Labels</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Samuele Cornell" class="img img-circle" src="/images/person/samuele_cornell.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Samuele Cornell</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.cmu.edu/">
                                Carnegie Mellon University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Janek Ebbers" class="img img-circle" src="/images/person/janek_ebbers.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Janek Ebbers</strong>
<a class="icon" href="mailto:ebbers@merl.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.merl.com/">
                                Mitsubishi Electric Research Lab
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Manu Harju" class="img img-circle" src="/images/person/default.png" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Manu Harju</strong>
<a class="icon" href="mailto:manu.harju@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Irene Martin Morato" class="img img-circle" src="/images/person/irene_martin_morato.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Irene Martin Morato</strong>
<a class="icon" href="mailto:irene.martinmorato@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Constance Douwes" class="img img-circle" src="/images/person/constance_douwes.png" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Constance Douwes</strong>
<a class="icon" href="mailto:constance.douwes@inria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.inria.fr/fr">
                                Inria
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Annamaria Mesaros" class="img img-circle" src="/images/person/annamaria_mesaros.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Annamaria Mesaros</strong>
<a class="icon" href="mailto:annamaria.mesaros@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Romain Serizel" class="img img-circle" src="/images/person/romain_serizel.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Romain Serizel</strong>
<a class="icon" href="mailto:romain.serizel@loria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.loria.fr/en/">
                                University of Lorraine
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a>
<ul>
<li><a href="#novelties-for-2024-edition">Novelties for 2024 edition</a></li>
<li><a href="#scientific-questions">Scientific questions</a></li>
</ul>
</li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#desed-dataset">DESED dataset</a></li>
<li><a href="#maestro-real">MAESTRO Real</a></li>
<li><a href="#dataset-overlap">Dataset overlap</a></li>
<li><a href="#reference-labels">Reference labels</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-set">Development set</a></li>
</ul>
</li>
<li><a href="#external-data-resources">External data resources</a></li>
<li><a href="#task-rules">Task rules</a>
<ul>
<li><a href="#evaluation-set">Evaluation set</a></li>
</ul>
</li>
<li><a href="#submission">Submission</a></li>
<li><a href="#evaluation">Evaluation</a>
<ul>
<li><a href="#segment-based-metrics">Segment based metrics</a></li>
<li><a href="#polyphonic-sound-event-detection-scores">Polyphonic sound event detection scores</a></li>
<li><a href="#collar-based-f1-score">Collar-based F1-score</a></li>
<li><a href="#ranking-metric">Ranking metric</a></li>
<li><a href="#multi-runs-evaluation">Multi-runs Evaluation</a></li>
<li><a href="#energy-consumption-mandatory-this-year-!">Energy Consumption (mandatory this year !)</a></li>
<li><a href="#multiplyaccumulate-mac-operations">Multiply–accumulate (MAC) operations</a></li>
<li><a href="#evaluation-toolboxes">Evaluation toolboxes</a></li>
</ul>
</li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#baseline-novelties-short-description">Baseline Novelties Short Description</a></li>
<li><a href="#results-for-the-development-dataset">Results for the development dataset</a></li>
<li><a href="#repositories">Repositories</a></li>
</ul>
</li>
<li><a href="#citations">Citations</a></li>
<li><a href="#results">Results</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of this task is to evaluate systems for the detection of sound events using real data, with different types of annotations data and corresponding labels available for training.</p>
<p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found in the <a class="btn btn-default btn-xs" href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels-results">Results <i class="fa fa-caret-right"></i></a> page.
</p>
<div class="alert alert-info">
    If you are interested in the task, you can join us on the <strong><a href="https://dcase.slack.com/archives/C01NR59KAS3">dedicated slack channel</a></strong>
</div>
<h1 id="description">Description</h1>
<p>This task is the follow up to <a href="https://dcase.community/challenge2023/task-sound-event-detection-with-weak-labels-and-synthetic-soundscapes">task 4 A</a> and <a href="https://dcase.community/challenge2023/task-sound-event-detection-with-soft-labels">task 4 B</a> in 2023. We propose to unify the setup of both subtasks. The target of this task is to provide the event class together with the event time boundaries, given that multiple events can be present and may be overlapping in an audio recording. This task aims at exploring how to leverage training data with varying annotation granularity (temporal resolution, soft/hard labels). </p>
<h2 id="novelties-for-2024-edition">Novelties for 2024 edition</h2>
<ul>
<li>Systems will be evaluated on <strong>labels with different granularity</strong> in order to get a broader view of the systems behavior and to assess their robustness for different applications. </li>
<li>The target classes in the different datasets are different, and hence sound labels of one dataset <strong>may be present but not annotated</strong> in the other one. The systems will have to cope with potentially missing target labels at training.</li>
<li>The SED system will have to perform <strong>without knowing the origin of the audio clips</strong> at evaluation time.</li>
</ul>
<h2 id="scientific-questions">Scientific questions</h2>
<p>This task highlights a number of specific research questions:</p>
<ul>
<li>What is the most efficient way to exploit different sources of data to train a sound event detection system? </li>
<li>Is annotation uncertainty useful in learning models for SED? </li>
<li>How to exploit training data with partially missing annotations? How can we evaluate SED systems in a robust way?</li>
<li>How can we train SED systems that should perform well under various sound event distribution with potentail domain mismatch?</li>
</ul>
<h1 id="audio-dataset">Audio dataset</h1>
<p>This task is based on the <strong>DESED dataset</strong> and the <strong>MAESTRO Real</strong> dataset. </p>
<h2 id="desed-dataset">DESED dataset</h2>
<p>DESED dataset has been used since <a href="https://dcase.community/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments">DCASE 2020 Task 4</a>. DESED is composed of 10 sec audio clips recorded in domestic environments (taken from <a href="https://research.google.com/AudioSet/">AudioSet</a>) or synthesized using <a href="https://github.com/justinsalamon/Scaper">Scaper</a> to simulate a domestic environment. DESED focuses on 10 classes of sound events that represent a subset of AudioSet (note that not all the classes in DESED correspond to classes in AudioSet; for example, some classes in DESED group several classes from AudioSet).</p>
<div class="btex-item" data-item="Turpault2019_DCASE" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2019_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault, Romain Serizel, Ankit Parag Shah, and Justin Salamon.
<em>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis.</em>
In <span class="bibtex-protected">Workshop on Detection and Classification of Acoustic Scenes and Events</span>. New York City, United States, October 2019.
URL: <a href="https://hal.inria.fr/hal-02160855">https://hal.inria.fr/hal-02160855</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" class="panel-collapse collapse" id="collapseTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" role="tabpanel">
<h4>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10% points in terms of F-measure.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2flabel" class="modal fade" id="bibtexTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2f" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2019_DCASEcd6ecc91d6364da085931514ae3fbc2flabel">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Turpault2019_DCASE,
    Author = "Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in domestic environments with weakly labeled data and soundscape synthesis}",
    booktitle = "{Workshop on Detection and Classification of Acoustic Scenes and Events}",
    address = "New York City, United States",
    year = "2019",
    month = "October",
    keywords = "Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02160855",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02160855"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Serizel2020_ICASSP" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Serizel2020_ICASSP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Romain Serizel, Nicolas Turpault, Ankit Shah, and Justin Salamon.
<em>Sound event detection in synthetic domestic environments.</em>
In <span class="bibtex-protected">ICASSP 2020 - 45th International Conference on Acoustics, Speech, and Signal Processing</span>. Barcelona, Spain, 2020.
URL: <a href="https://hal.inria.fr/hal-02355573">https://hal.inria.fr/hal-02355573</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02355573/file/Sound_event_detection_in_domestic_environments_on_synthetic_soundscapes.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" class="panel-collapse collapse" id="collapseSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" role="tabpanel">
<h4>Sound event detection in synthetic domestic environments</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10% points in terms of F-measure.</p>
<h5>Keywords</h5>
<p class="text-justify">semi-supervised learning ; weakly labeled data ; synthetic data ; Sound event detection ; Index Terms-Sound event detection</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02355573/file/Sound_event_detection_in_domestic_environments_on_synthetic_soundscapes.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90label" class="modal fade" id="bibtexSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexSerizel2020_ICASSP7e573dd60e3e4eff84fae0a5eeee7b90label">Sound event detection in synthetic domestic environments</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Serizel2020_ICASSP,
    Author = "Serizel, Romain and Turpault, Nicolas and Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in synthetic domestic environments}",
    booktitle = "{ICASSP 2020 - 45th International Conference on Acoustics, Speech, and Signal Processing}",
    address = "Barcelona, Spain",
    year = "2020",
    keywords = "semi-supervised learning ; weakly labeled data ; synthetic data ; Sound event detection ; Index Terms-Sound event detection",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02355573",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02355573"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="maestro-real">MAESTRO Real</h2>
<p>The dataset consists of real-life recordings with a length of approximately 3 minutes each, recorded in a few different acoustic scenes. The audio was annotated using Amazon Mechanical Turk, with a procedure that allows estimating soft labels from multiple annotator opinions. </p>
<div class="btex-item" data-item="Martinmorato2023" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Martinmorato2023"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Irene Martín-Morató and Annamaria Mesaros.
<em>Strong labeling of sound events using crowdsourced weak labels and annotator competence estimation.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 31():902–914, 2023.
<a href="https://doi.org/10.1109/TASLP.2022.3233468">doi:10.1109/TASLP.2022.3233468</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10016759" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" class="panel-collapse collapse" id="collapseMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" role="tabpanel">
<h4>Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation</h4>
<h5>Abstract</h5>
<p class="text-justify">Crowdsourcing is a popular tool for collecting large amounts of annotated data, but the specific format of the strong labels necessary for sound event detection is not easily obtainable through crowdsourcing. In this work, we propose a novel annotation workflow that leverages the efficiency of crowdsourcing weak labels, and uses a high number of annotators to produce reliable and objective strong labels. The weak labels are collected in a highly redundant setup, to allow reconstruction of the temporal information. To obtain reliable labels, the annotators' competence is estimated using MACE (Multi-Annotator Competence Estimation) and incorporated into the strong labels estimation through weighing of individual opinions. We show that the proposed method produces consistently reliable strong annotations not only for synthetic audio mixtures, but also for audio recordings of real everyday environments. While only a maximum 80% coincidence with the complete and correct reference annotations was obtained for synthetic data, these results are explained by an extended study of how polyphony and SNR levels affect the identification rate of the sound events by the annotators. On real data, even though the estimated annotators' competence is significantly lower and the coincidence with reference labels is under 69%, the proposed majority opinion approach produces reliable aggregated strong labels in comparison with the more difficult task of crowdsourcing directly strong labels.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10016759" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMartinmorato2023af24ec5d40e0415a9899dd8dc5232783label" class="modal fade" id="bibtexMartinmorato2023af24ec5d40e0415a9899dd8dc5232783" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMartinmorato2023af24ec5d40e0415a9899dd8dc5232783label">Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation</h4>
</div>
<div class="modal-body">
<pre>@article{Martinmorato2023,
    author = "Martín-Morató, Irene and Mesaros, Annamaria",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    title = "Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation",
    year = "2023",
    volume = "31",
    number = "",
    pages = "902-914",
    doi = "10.1109/TASLP.2022.3233468",
    abstract = "Crowdsourcing is a popular tool for collecting large amounts of annotated data, but the specific format of the strong labels necessary for sound event detection is not easily obtainable through crowdsourcing. In this work, we propose a novel annotation workflow that leverages the efficiency of crowdsourcing weak labels, and uses a high number of annotators to produce reliable and objective strong labels. The weak labels are collected in a highly redundant setup, to allow reconstruction of the temporal information. To obtain reliable labels, the annotators' competence is estimated using MACE (Multi-Annotator Competence Estimation) and incorporated into the strong labels estimation through weighing of individual opinions. We show that the proposed method produces consistently reliable strong annotations not only for synthetic audio mixtures, but also for audio recordings of real everyday environments. While only a maximum 80\% coincidence with the complete and correct reference annotations was obtained for synthetic data, these results are explained by an extended study of how polyphony and SNR levels affect the identification rate of the sound events by the annotators. On real data, even though the estimated annotators' competence is significantly lower and the coincidence with reference labels is under 69\%, the proposed majority opinion approach produces reliable aggregated strong labels in comparison with the more difficult task of crowdsourcing directly strong labels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<h2 id="dataset-overlap">Dataset overlap</h2>
<p>The original training datasets have not been reannotated. Therefore, sound labels of one dataset may be present but not annotated in the other one. The systems will have to cope with potentially missing target labels at training. Additionnaly, there is some overlap in terms of classes between the datasets. The following classes have been collapsed:</p>
<ul>
<li>People talking (MAESTRO) -&gt; Speech (DESED)</li>
<li>Cutlery &amp; dishes (MAESTRO) -&gt; dishes (DESED)</li>
</ul>
<h2 id="reference-labels">Reference labels</h2>
<p>One of the challenge of the task is to train systems exploiting annotations at different granularity. We describe here the different annotations available.</p>
<h3>Weak annotations</h3>
<p>The weak annotations in DESED have been verified manually for a small subset of the training set.
The weak annotations are provided in a tab separated csv file under the following format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event_labels (strings)</span><span class="o">]</span>
</code></pre></div>
<p>For example:
    <code>Y-BJNMHMZDcU_50.000_60.000.wav Alarm_bell_ringing,Dog</code></p>
<h3>Strong annotations</h3>
<p>A subset of DESED development has been annotated manually with strong annotations, to be used as the <strong>test set</strong>
(see also below for a detailed explanation about the development set).</p>
<p>The <strong>synthetic subset</strong> of DESED development set is generated and labeled with strong annotations using the
<a href="https://github.com/justinsalamon/Scaper">Scaper soundscape synthesis and augmentation library</a>.
Each sound clip from FSD50K was verified by humans in order to check the event class present in FSD50K annotation was indeed dominant in the audio clip.</p>
<div class="btex-item" data-item="fonseca2020fsd50k" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="fonseca2020fsd50k"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra.
<em>FSD50K: an open dataset of human-labeled sound events.</em>
In arXiv:2010.00475. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexfonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2010.00475.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsefonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsefonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingfonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" class="panel-collapse collapse" id="collapsefonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" role="tabpanel">
<h4>FSD50K: an Open Dataset of Human-Labeled Sound Events</h4>
<h5>Abstract</h5>
<p class="text-justify">Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexfonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2010.00475.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexfonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1label" class="modal fade" id="bibtexfonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexfonseca2020fsd50ke5b5195956d042bf8ab6112b0de5bdb1label">FSD50K: an Open Dataset of Human-Labeled Sound Events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{fonseca2020fsd50k,
    author = "Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier",
    title = "{FSD50K}: an Open Dataset of Human-Labeled Sound Events",
    year = "2020",
    booktitle = "arXiv:2010.00475",
    abstract = "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="font2013freesound" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="font2013freesound"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Frederic Font, Gerard Roma, and Xavier Serra.
<em>Freesound technical demo.</em>
In Proceedings of the 21st ACM international conference on Multimedia, 411–412. ACM, 2013.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexfont2013freesoundfa3a761e956e4d86b2d2336a99d39464" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://mtg.upf.edu/system/files/publications/Font-Roma-Serra-ACMM-2013.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsefont2013freesoundfa3a761e956e4d86b2d2336a99d39464" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsefont2013freesoundfa3a761e956e4d86b2d2336a99d39464" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingfont2013freesoundfa3a761e956e4d86b2d2336a99d39464" class="panel-collapse collapse" id="collapsefont2013freesoundfa3a761e956e4d86b2d2336a99d39464" role="tabpanel">
<h4>Freesound technical demo</h4>
<h5>Abstract</h5>
<p class="text-justify">Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexfont2013freesoundfa3a761e956e4d86b2d2336a99d39464" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://mtg.upf.edu/system/files/publications/Font-Roma-Serra-ACMM-2013.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexfont2013freesoundfa3a761e956e4d86b2d2336a99d39464label" class="modal fade" id="bibtexfont2013freesoundfa3a761e956e4d86b2d2336a99d39464" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexfont2013freesoundfa3a761e956e4d86b2d2336a99d39464label">Freesound technical demo</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{font2013freesound,
    author = "Font, Frederic and Roma, Gerard and Serra, Xavier",
    title = "Freesound technical demo",
    booktitle = "Proceedings of the 21st ACM international conference on Multimedia",
    pages = "411--412",
    year = "2013",
    organization = "ACM",
    abstract = "Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>In both cases, the minimum length for an event is 250ms. The minimum duration of the pause between two events from the same class is 150ms.
When the silence between two consecutive events from the same class was less than 150ms the events have been merged to a single event.</p>
<p>The strong annotations are provided in a tab separated csv file under the following format:</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">onset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">offset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event_label (string)</span><span class="o">]</span>
</code></pre></div>
<p>For example:</p>
<p><code>YOTsn73eqbfc_10.000_20.000.wav   0.163   0.665   Alarm_bell_ringing</code></p>
<h3>Soft labels</h3>
<p>The reference labels in MAESTRO development data are available as soft labels.</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">onset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">offset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event_label (string)[tab</span><span class="o">][</span><span class="n">soft label (float)</span><span class="o">]</span><span class="err">]</span>
</code></pre></div>
<p>Example: </p>
<div class="highlight"><pre><span></span><code>a1.wav       0  1   footsteps   0.6
a1.wav       0  1   people_talking      0.9
a1.wav       1  2   footsteps   0.8
</code></pre></div>
<p>These labels can be transformed into hard (binary) labels, using the 0.5 threshold, and the equivalent annotation would be </p>
<p><strong>Hard labels</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">onset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">offset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">]</span><span class="err">[</span><span class="n">event_label</span><span class="w"> </span><span class="p">(</span><span class="n">string</span><span class="p">)</span>
</code></pre></div>
<p>Example: </p>
<div class="highlight"><pre><span></span><code>a1.wav       0  2   footsteps
a1.wav       0  1   people_talking
</code></pre></div>
<p>In the provided dataset there are 17 sound classes. Of them, only 15 classes have values over 0.5, out of which another 4 are very rare. For this reason, the <strong>evaluation is conducted only against the following 11 classes</strong>: </p>
<ul>
<li>Birds singing</li>
<li>Car</li>
<li>People talking</li>
<li>Footsteps</li>
<li>Children voices</li>
<li>Wind blowing</li>
<li>Brakes squeaking</li>
<li>Large vehicle</li>
<li>Cutlery and dishes</li>
<li>Metro approaching</li>
<li>Metro leaving</li>
</ul>
<h2 id="download">Download</h2>
<p>The DESED dataset is composed of several subsets that can be downloaded independently from the respective repositories or automatically with the Task 4 data generation script.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/DESED_task/blob/master/recipes/dcase2021_task4_baseline/generate_dcase_task4_2021.py" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/DESED_task/blob/master/recipes/dcase2021_task4_baseline/generate_dcase_task4_2021.py" target="_blank">
<span style="font-size:20px;">Data generation script for SED training <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>To access the datasets separately please refer to <a href="https://dcase.community/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments#download">task 4 2021 instructions</a></p>
<p>The MAESTRO dataset can be downloaded at the following repository.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/7244360" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-database fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/7244360" target="_blank">
<span style="font-size:20px;">MAESTRO Real - Multi-Annotator Estimated Strong Labels <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(2.6 GB)</span>
<br/>
<a href="10.5281/zenodo.7244360">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7244360.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="task-setup">Task setup</h1>
<p>The challenge consists of detecting sound events within audio using training with varying types of annotation (weak hard label, strong hard labels and strong soft labels). The detection within an audio clip should be performed with start and end timestamps. Note that an audio clip may contain to more than one sound event.</p>
<p>Participants are allowed to use external datasets and embeddings extracted from pre-trained models. Lists of the eligible datasets and pre-trained models are provided below. </p>
<h2 id="development-set">Development set</h2>
<p>We provide 4 different splits of the training data in our development set:
"Weakly labeled training set", "Unlabeled in domain training set", "Synthetic strongly labeled set" and "Soft labeled training set".</p>
<p><strong>Weakly labeled training set</strong>:<br/>
This set contains <strong>1578 clips</strong> (2244 class occurrences) for which weak annotations have been verified and cross-checked.</p>
<p><strong>Unlabeled in domain training set</strong>:<br/>
This set is considerably larger than the previous one. It contains <strong>14412 clips</strong>.
The clips are selected such that the distribution per class (based on AudioSet annotations)
is close to the distribution in the labeled set.
Note however that given the uncertainty on AudioSet labels this distribution might not be exactly similar.</p>
<p><strong>Synthetic strongly labeled set</strong>:<br/>
This set is composed of <strong>10000 clips</strong> generated with the <a href="https://github.com/justinsalamon/Scaper">Scaper soundscape synthesis and augmentation library</a>.
The clips are generated such that the distribution per event is close to the one of the validation set.</p>
<ul>
<li>We used all the foreground files from the DESED synthetic soundbank (multiple times).</li>
<li>We used background files annotated as "other" from <a href="https://zenodo.org/record/1247102">the subpart of SINS dataset</a> and files from the <a href="https://zenodo.org/record/400515">TUT Acoustic scenes 2017, development dataset</a>.</li>
<li>We used the clips from <a href="https://zenodo.org/record/4012661">FUSS</a> containing the non-target classes. The clip selection is based on FSD50K annotations. The clips selected are in the training for <strong>both</strong> FSD50K and FUSS. We provide tsv files corresponding to these splits.</li>
<li>Event distribution statistics for both target event classes and non target event classes are computed on annotations obtained by human for <a href="https://research.google.com/audioset/download_strong.html">~90k clips from AudioSet</a>.</li>
</ul>
<div class="btex-item" data-item="hershey2021benefit" data-source="content/data/challenge2022/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="hershey2021benefit"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Shawn Hershey, Daniel PW Ellis, Eduardo Fonseca, Aren Jansen, Caroline Liu, R Channing Moore, and Manoj Plakal.
<em>The benefit of temporally-strong labels in audio event classification.</em>
In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 366–370. IEEE, 2021.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexhershey2021benefit7f2bd341d70d4c78a778cc6975753021" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2105.07031" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsehershey2021benefit7f2bd341d70d4c78a778cc6975753021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsehershey2021benefit7f2bd341d70d4c78a778cc6975753021" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headinghershey2021benefit7f2bd341d70d4c78a778cc6975753021" class="panel-collapse collapse" id="collapsehershey2021benefit7f2bd341d70d4c78a778cc6975753021" role="tabpanel">
<h4>The benefit of temporally-strong labels in audio event classification</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexhershey2021benefit7f2bd341d70d4c78a778cc6975753021" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2105.07031" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexhershey2021benefit7f2bd341d70d4c78a778cc6975753021label" class="modal fade" id="bibtexhershey2021benefit7f2bd341d70d4c78a778cc6975753021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexhershey2021benefit7f2bd341d70d4c78a778cc6975753021label">The benefit of temporally-strong labels in audio event classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{hershey2021benefit,
    author = "Hershey, Shawn and Ellis, Daniel PW and Fonseca, Eduardo and Jansen, Aren and Liu, Caroline and Moore, R Channing and Plakal, Manoj",
    title = "The benefit of temporally-strong labels in audio event classification",
    booktitle = "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "366--370",
    year = "2021",
    organization = "IEEE"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>We share the original data and scripts to generate soundscapes and encourage participants to create their own subsets.
See <a href="https://github.com/turpaultn/DESED">DESED github repo</a> and <a href="https://Scaper.readthedocs.io/en/latest/">Scaper documentation</a> for more information about how to create new soundscapes.</p>
<p><strong>Soft labeled training set</strong>
This year the MAESTRO dataset is provided with a fixed training/validation split in which approximately 70% of the data (per class) is used in training, and the rest is used for testing. Participants are required to report results on the validation set results using this setup. </p>
<h3>Sound event detection validation set</h3>
<p>The validation set is the combination of DESED validation set and MAESTRO validation set split described above. 
The overall validation set contains <strong>1184 clips</strong>.
DESED validation set is annotated with strong labels, with timestamps (obtained by human annotators) while MAESTRO validation set is annotated with soft labels.</p>
<h1 id="external-data-resources">External data resources</h1>
<p>List of external data resources allowed:</p>
<table class="datatable table table-hover table-condensed" data-filter-control="false" data-filter-show-clear="false" data-id-field="name" data-pagination="true" data-show-pagination-switch="true" data-sort-name="name" data-sort-order="asc">
<thead>
<tr>
<th data-field="name" data-sortable="true">Dataset name</th>
<th data-field="type" data-filter-control="select" data-sortable="true" data-tag="true">Type</th>
<th data-field="date" data-sortable="true">Added</th>
<th data-field="link" data-value-type="url">Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>YAMNet</td>
<td>model</td>
<td>20.05.2021</td>
<td>https://github.com/tensorflow/models/tree/master/research/audioset/yamnet</td>
</tr>
<tr>
<td>PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</td>
<td>model</td>
<td>31.03.2021</td>
<td>https://zenodo.org/record/3987831</td>
</tr>
<tr>
<td>OpenL3</td>
<td>model</td>
<td>12.02.2020</td>
<td>https://openl3.readthedocs.io/</td>
</tr>
<tr>
<td>VGGish</td>
<td>model</td>
<td>12.02.2020</td>
<td>https://github.com/tensorflow/models/tree/master/research/audioset/vggish</td>
</tr>
<tr>
<td>COLA</td>
<td>model</td>
<td>25.02.2023</td>
<td>https://github.com/google-research/google-research/tree/master/cola</td>
</tr>
<tr>
<td>BYOL-A</td>
<td>model</td>
<td>25.02.2023</td>
<td>https://github.com/nttcslab/byol-a</td>
</tr>
<tr>
<td>AST: Audio Spectrogram Transformer</td>
<td>model</td>
<td>25.02.2023</td>
<td>https://github.com/YuanGongND/ast</td>
</tr>
<tr>
<td>PaSST: Efficient Training of Audio Transformers with Patchout</td>
<td>model</td>
<td>13.05.2022</td>
<td>https://github.com/kkoutini/PaSST</td>
</tr>
<tr>
<td>BEATs: Audio Pre-Training with Acoustic Tokenizers</td>
<td>model</td>
<td>01.03.2023</td>
<td>https://github.com/microsoft/unilm/tree/master/beats</td>
</tr>
<tr>
<td>AudioSet</td>
<td>audio, video</td>
<td>04.03.2019</td>
<td>https://research.google.com/audioset/</td>
</tr>
<tr>
<td>FSD50K</td>
<td>audio</td>
<td>10.03.2022</td>
<td>https://zenodo.org/record/4060432</td>
</tr>
<tr>
<td>ImageNet</td>
<td>image</td>
<td>01.03.2021</td>
<td>http://www.image-net.org/</td>
</tr>
<tr>
<td>MUSAN</td>
<td>audio</td>
<td>25.02.2023</td>
<td>https://www.openslr.org/17/</td>
</tr>
<tr>
<td>DCASE 2018, Task 5: Monitoring of domestic activities based on multi-channel acoustics - Development dataset</td>
<td>audio</td>
<td>25.02.2023</td>
<td>https://zenodo.org/record/1247102#.Y_oyRIBBx8s</td>
</tr>
<tr>
<td>Pre-trained desed embeddings (Panns, AST part 1)</td>
<td>model</td>
<td>25.02.2023</td>
<td>https://zenodo.org/record/6642806#.Y_oy_oBBx8s</td>
</tr>
<tr>
<td>Audio Teacher-Student Transformer</td>
<td>model</td>
<td>22.04.2024</td>
<td>https://drive.google.com/file/d/1_xb0_n3UNbUG_pH1vLHTviLfsaSfCzxz/view</td>
</tr>
<tr>
<td>TUT Acoustic scenes dataset</td>
<td>audio</td>
<td>22.04.2024</td>
<td>https://zenodo.org/records/45739</td>
</tr>
<tr>
<td>MicIRP</td>
<td>IR</td>
<td>28.03.2023</td>
<td>http://micirp.blogspot.com/?m=1</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Datasets and models can be added to the list upon request until May 15th (as long as the corresponding resources are publicly available).</p>
<h1 id="task-rules">Task rules</h1>
<p>There are general rules valid for all tasks; these, along with information on technical report and submission requirements can be found <a href="http://dcase.community/challenge2023/rules">here</a>.</p>
<p>Task specific rules:</p>
<ul>
<li>Participants are allowed to submit up to <strong>4 different systems</strong> .</li>
<li>Participants <strong>have to submit at least one system without ensembling</strong>.</li>
<li>Participants <strong>have to submit</strong> (post-processed and unprocessed) output scores from <strong>three independent model trainings</strong> with different initialization to be able to evaluate the model performance's standard deviation.</li>
<li>Participants <strong>are allowed</strong> to use external data for system development. </li>
<li>Data from other task is considered external data.</li>
<li>Embeddings extracted from models pre-trained on external data is considered as external data</li>
<li>Another example of external data is other materials related to the video such as the rest of audio from where the 10-sec clip was extracted, the video frames and metadata.</li>
<li>Datasets and models can be added to the list upon request <strong>until May 1st</strong> (as long as the corresponding resources are publicly available).</li>
<li>The external dataset used during training should be listed in the YAML file describing the submission.</li>
<li>Manipulation of provided training data <strong>is allowed</strong>.</li>
<li>Participants <strong>are not allowed</strong> to use the <strong>public evaluation dataset</strong> and <strong>synthetic evaluation dataset</strong> (or part of them) to train their systems or tune hyper-parameters.</li>
<li><strong>Domain identification is prohibited</strong>: participant <strong>are not allowed</strong> to leverage domain information in inference whether the audio comes from MAESTRO or DESED.</li>
</ul>
<h2 id="evaluation-set">Evaluation set</h2>
<p>The evaluation dataset is composed of the DESED evaluation set and the MAESTRO evaluation set. The audio clips will be randomized and there will be <strong>no indication of the origin (DESED or MAESTRO) of the clips</strong>.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/records/11425124" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-database fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/records/11425124" target="_blank">
<span style="font-size:20px;">Evaluation set DCASE 2024 task 4 <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(2.5 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.11425124">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.11425124.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="submission">Submission</h1>
<p>Instructions regarding the output submission format and the required metadata can be found in the example submission package.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2024/dcase2024_challenge_submission_package_example.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2024/dcase2024_challenge_submission_package_example.zip" target="_blank">
<span style="font-size:20px;">DCASE2024 challenge submission example package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(21.1 MB)</span>
<br/>
<span class="text-muted">
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/></p>
<h1 id="evaluation">Evaluation</h1>
<h2 id="segment-based-metrics">Segment based metrics</h2>
<p>System evaluation will be based on the following metrics, calculated in 1s-segments: </p>
<ul>
<li>micro-average F1 score <span class="math">\(F1_m\)</span>, calculated using <a href="https://github.com/TUT-ARG/sed_eval">sed_eval</a>, with a decision threshold of 0.5 applied to the system output provided by participants</li>
<li>micro-average error rate <span class="math">\(ER_M\)</span>, calculated using <a href="https://github.com/TUT-ARG/sed_eval">sed_eval</a>, with a decision threshold of 0.5 applied to the system output provided by participants</li>
<li>macro-average F1 score <span class="math">\(F1_M\)</span>, calculated using <a href="https://github.com/TUT-ARG/sed_eval">sed_eval</a>, with a decision threshold of 0.5 applied to the system output provided by participants</li>
<li>macro-average pAUC score <span class="math">\(pAUC_M\)</span>, calculated using <a href="https://github.com/fgnt/sed_scores_eval">sed_scores_eval</a>, with a false positive rate threshold of 0.1</li>
<li>macro-average F1 score with optimum threshold per class <span class="math">\(F1_{MO}\)</span> calculated using <a href="https://github.com/fgnt/sed_scores_eval">sed_scores_eval</a>, based on the best F1 score per class obtained with a class-specific threshold </li>
</ul>
<h2 id="polyphonic-sound-event-detection-scores">Polyphonic sound event detection scores</h2>
<p>Submissions will also be evaluated with polyphonic sound event detection scores (PSDS) computed over the real recordings in the evaluation set (the performance on synthetic recordings is not taken into account in the metric). This metric is based on the intersection between events. The PSDS parameters used for evaluation are the following:</p>
<ul>
<li>Detection Tolerance criterion (DTC): 0.7</li>
<li>Ground Truth intersection criterion (GTC): 0.7</li>
<li>Cost of instability across class (<span class="math">\(\alpha_{ST}\)</span>): 1</li>
<li>Cost of CTs on user experience (<span class="math">\(\alpha_{CT}\)</span>): 0</li>
<li>Maximum False Positive rate (e_max): 100</li>
</ul>
<h2 id="collar-based-f1-score">Collar-based F1-score</h2>
<p>Additionally, event-based measures with a 200 ms collar on onsets and a 200 ms / 20% of the events length collar on offsets will be provided as a contrastive measure. System will be evaluated with threshold fixed at 0.5 unless participant explicitly provide another operating point to be evaluated with F1-score.</p>
<h2 id="ranking-metric">Ranking metric</h2>
<p>The systems will be ranked according to the sum of <span class="math">\(pAUC_M\)</span> (computed on the MAESTRO evaluation set, on MAESTRO classes) and <span class="math">\(PSDS\)</span> (computed on DESED evaluation set only, on DESED classes).</p>
<h2 id="multi-runs-evaluation">Multi-runs Evaluation</h2>
<p>Further we kindly ask participants to provide (post-processed and unprocessed) output scores from three independent model trainings with different initialization to be able to evaluate the model performance's standard deviation.</p>
<h2 id="energy-consumption-mandatory-this-year-!">Energy Consumption (mandatory this year !)</h2>
<p>Since last year, energy consumption (kWh) is going to be considered as additional metric to rank the submitted systems, therefore it is mandatory to report the energy consumption of the submitted models [11]. </p>
<p>Participants need to provide, for each submitted system (or at least the best one), the following energy consumption figures in kWh using <a href="https://github.com/mlco2/codecarbon">CodeCarbon</a>:</p>
<p>1) whole system training
2) devtest inference</p>
<p>You can refer to <a href="https://github.com/mlco2/codecarbon">Codecarbon</a> on how to accomplish this (it's super simple! 😉). </p>
<h3>Important Steps</h3>
<ol>
<li><strong>Initialize the Tracker</strong>:</li>
<li>When initializing the tracker, make sure to specify the GPU IDs that you want to track.</li>
<li>
<p>You can do this by using the following parameter: <code>gpu_ids=[torch.cuda.current_device()]</code>.</p>
</li>
<li>
<p><strong>Example</strong>:
   ```python
   from codecarbon import OfflineEmissionsTracker
   tracker = OfflineEmissionsTracker(gpu_ids=[torch.cuda.current_device()], country_iso_code="CAN")
   tracker.start()
   # Your code here
   tracker.stop()</p>
</li>
<li>
<p><strong>Additional Resources</strong>:</p>
</li>
<li>For more hints on how we do this on the baseline system, check out <code>local/sed_trainer_pretrained.py</code>.</li>
</ol>
<p>⚠️ In addition to this, we kindly suggest the participants to
provide the energy consumption in kWh (using the same hardware used for 2) and 3)) of:</p>
<p>1) training the baseline system for 10 epochs
2) devtest inference for the baseline system</p>
<p>Both are computed by the <code>python train_pretrained.py</code> command. You just need to set 10 epochs in the <code>confs/default.yaml</code>. <br/> 
You can find the energy consumed in kWh in <code>./exp/2024_baseline/version_X/codecarbon/emissions_baseline_training.csv</code> for training and <code>./exp/2024_baseline/version_X/codecarbon/emissions_baseline_test.csv</code> for devtest inference. </p>
<p>Energy consumption depends on hardware, and each participant uses different hardware. To account for this difference, we use the baseline training and inference kWh energy consumption as a common reference. It is important that the inference energy consumption figures for both submitted systems and baseline are computed on the same hardware under similar loading.</p>
<div class="alert alert-info">
<strong>(NEW)</strong> This year, we ask participants submit the whole .csv files that provide the details of consumption for GPU, CPU and RAM usage. For more information, please refer to the submission package example.
</div>
<h2 id="multiplyaccumulate-mac-operations">Multiply–accumulate (MAC) operations</h2>
<p>This year we are introducing a new metric, complementary to the energy consumption metric.
We are considering the Multiply–accumulate operations (MACs) for 10 seconds of audio prediction, so to have information regarding the computational complexity of the network in terms of multiply-accumulate (MAC) operations.</p>
<p>We use <a href="https://github.com/Lyken17/pytorch-OpCounter">THOP: PyTorch-OpCounter</a> as framework to compute the number of multiply-accumulate operations (MACs). For more information regarding how to install and use THOP, the reader is referred to <a href="https://github.com/Lyken17/pytorch-OpCounter">THOP documentation</a>.</p>
<h2 id="evaluation-toolboxes">Evaluation toolboxes</h2>
<p>Evaluation is done using <code>sed_eval</code> and <code>sed_scores_eval</code> toolboxes:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span style="font-size:20px;">sed_eval - Evaluation toolbox for Sound Event Detection <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/fgnt/sed_scores_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/fgnt/sed_scores_eval" target="_blank">
<span style="font-size:20px;">sed_scores_eval - Evaluation toolbox for efficient threshold-independent evaluation of Sound Event Detection <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/>
<br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/mlco2/codecarbon" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/mlco2/codecarbon" target="_blank">
<span style="font-size:20px;">CodeCarbon, Track emissions from Compute <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/>
<br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank">
<span style="font-size:20px;">THOP: PyTorch-OpCounter <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/>
<br/></p>
<h1 id="baseline-system">Baseline system</h1>
<p>We provide one baseline system for the task which uses pre-trained BEATS embeddings and Audioset strong-annotated data together with DESED and MAESTRO data. <br/></p>
<p>This baseline is built upon the 2023 pre-trained embedding baseline. 
It exploits the pre-trained model <a href="https://arxiv.org/abs/2212.09058">BEATs</a>, the current state-of-the-art on the <a href="https://paperswithcode.com/sota/audio-classification-on-audioset">Audioset classification task</a>. In addition it uses by default the Audioset strong-annotated data. <br/>
<br/></p>
<p>🆕 We made some changes in the loss computation as well as in the attention pooling to make sure that the baseline can handle 
now multiple datasets with potentially missing information.</p>
<p>In the proposed baseline, the frame-level embeddings are used in a late-fusion fashion with the existing CRNN baseline classifier. The temporal resolution of the frame-level embeddings is matched to that of the CNN output using Adaptative Average Pooling. We then feed their frame-level concatenation to the RNN + MLP classifier. See <code>desed_tasl/nnet/CRNN.py</code> for details. </p>
<p>See the configuration file: <code>./confs/pretrained.yaml</code>:</p>
<div class="highlight"><pre><span></span><code><span class="nt">pretrained</span><span class="p">:</span>
<span class="w">  </span><span class="nt">pretrained</span><span class="p">:</span>
<span class="w">  </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">beats</span>
<span class="w">  </span><span class="nt">e2e</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">freezed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">extracted_embeddings_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./embeddings</span>
<span class="nt">net</span><span class="p">:</span>
<span class="w">  </span><span class="nt">use_embeddings</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">embedding_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">768</span>
<span class="w">  </span><span class="nt">embedding_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">frame</span>
<span class="w">  </span><span class="nt">aggregation_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pool1d</span>
</code></pre></div>
<p>The embeddings can be integrated using several aggregation methods : <strong>frame</strong> (method from 2022 year : taking the last state of an RNN fed with the embeddings sequence), <strong>interpolate</strong> (nearest-neighbour interpolation to adapt the temporal resolution) and <strong>pool1d</strong> (adaptative average pooling as described before).</p>
<p>We provide [pretrained checkpoints][zenodo_pretrained_models]. The baseline can be tested on the development set of the dataset using the following command:
<code>python train_pretrained.py --test_from_checkpoint /path/to/downloaded.ckpt</code></p>
<p>More detailed info about the baseline on <a href="">DCASE 2024 Task 4 baseline repo</a>.</p>
<h2 id="baseline-novelties-short-description">Baseline Novelties Short Description</h2>
<p>The baseline is the same as the pre-trained embedding <a href="https://github.com/DCASE-REPO/DESED_task/tree/master/recipes/dcase2023_task4_baseline">DCASE 2023 Task 4 baseline</a>, based on a Mean-Teacher model [1]. <br/>
We made some changes here in order to handle both DESED and MAESTRO which can have partially missing labels (e.g. DESED events may not be annotated in MAESTRO and vice-versa). <br/> 
In detail: </p>
<ol>
<li>We map certain classes in MAESTRO to some DESED classes (but not vice-versa) when training on MAESTRO data.</li>
<li>See <code>local/classes_dict.py</code> and the function <code>process_tsvs</code> used in <code>train_pretrained.py</code>.</li>
<li>When computing losses on MAESTRO and DESED we mask the output logits which corresponds to classes for which we do miss annotation for the current dataset.</li>
<li>This masking is also applied to the attention pooling layer see <code>desed_task/nnet/CRNN.py</code>. </li>
<li>Mixup is performed only within the same dataset (e.g. only within MAESTRO and DESED). </li>
<li>To handle MAESTRO, which is long form, we perform overlap add at the logit level over sliding windows, see <code>local/sed_trained_pretrained.py</code></li>
</ol>
<h2 id="results-for-the-development-dataset">Results for the development dataset</h2>
<table class="table table-striped">
<thead>
<tr>
<td>Dataset</td>
<td>PSDS-scenario1</td>
<td><strong>PSDS-scenario1 (sed score)</strong> </td>
<td><strong>mean pAUC</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>Dev-test</td>
<td> 0.50 +/- 0.01  </td>
<td><strong> 0.52 +/- 0.007  </strong></td>
<td><strong> 0.637 +/- 0.04 </strong></td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> The Dev-test <span class="math">\(pAUC_M\)</span> is computed only on MAESTRO data (and on MAESTRO classes) while the Dev-test <span class="math">\(PSDS_1\)</span> is computed only on DESED data (and on DESED classes).</p>
<p><strong>Note:</strong> The performance might not be exactly reproducible on a GPU based system.
That is why, you can download the checkpoint of the network along with the TensorBoard events.
Launch <code>python train_sed.py --test_from_checkpoint /path/to/downloaded.ckpt</code> to test this model.</p>
<h3>Energy consumption during the training and evaluation phase</h3>
<p>Energy consumption for 1 run on a NVIDIA A100 40Gb on a single DGX A100 machine for a training phase and an inference phase on the development set.</p>
<table class="table table-striped">
<thead>
<tr>
<td>Dataset</td>
<td><strong>Training</strong> (kWh)</td>
<td><strong>Dev-test</strong> (kWh)</td>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><strong> 1.542 +/- 0.341  </strong></td>
<td><strong> 0.133 +/- 0.03 </strong></td>
</tr>
</tbody>
</table>
<h2 id="repositories">Repositories</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/DESED_task/tree/master/recipes/dcase2024_task4_baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/DESED_task/tree/master/recipes/dcase2024_task4_baseline" target="_blank">
<span style="font-size:20px;">DCASE2024 Task 4 <strong>baseline</strong>, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/records/11034682" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-table fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/records/11034682" target="_blank">
<span style="font-size:20px;">Task 4 - <strong>SED baseline</strong> checkpoint of the network <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(115.5 MB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.11034681">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.11034681.svg"/>
</a>
<span class="text-muted">
                
                version 1
                
                
                </span>
</div>
</div>
<p><br/></p>
<h1 id="citations">Citations</h1>
<div class="btex-item" data-item="cornell2024dcase" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="cornell2024dcase"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Samuele Cornell, Janek Ebbers, Constance Douwes, Irene Martín-Morató, Manu Harju, Annamaria Mesaros, and Romain Serizel.
<em>Dcase 2024 task 4: sound event detection with heterogeneous data and missing labels.</em>
2024.
<a href="https://arxiv.org/abs/2406.08056">arXiv:2406.08056</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexcornell2024dcase7e38b25d08224c88b09bdabc84a91288" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapsecornell2024dcase7e38b25d08224c88b09bdabc84a91288" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsecornell2024dcase7e38b25d08224c88b09bdabc84a91288" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingcornell2024dcase7e38b25d08224c88b09bdabc84a91288" class="panel-collapse collapse" id="collapsecornell2024dcase7e38b25d08224c88b09bdabc84a91288" role="tabpanel">
<h4>DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and Missing Labels</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexcornell2024dcase7e38b25d08224c88b09bdabc84a91288" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexcornell2024dcase7e38b25d08224c88b09bdabc84a91288label" class="modal fade" id="bibtexcornell2024dcase7e38b25d08224c88b09bdabc84a91288" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexcornell2024dcase7e38b25d08224c88b09bdabc84a91288label">DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and Missing Labels</h4>
</div>
<div class="modal-body">
<pre>@misc{cornell2024dcase,
    author = "Cornell, Samuele and Ebbers, Janek and Douwes, Constance and Martín-Morató, Irene and Harju, Manu and Mesaros, Annamaria and Serizel, Romain",
    title = "DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and Missing Labels",
    year = "2024",
    eprint = "2406.08056",
    archiveprefix = "arXiv",
    primaryclass = "eess.AS"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Martinmorato2023" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Martinmorato2023"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Irene Martín-Morató and Annamaria Mesaros.
<em>Strong labeling of sound events using crowdsourced weak labels and annotator competence estimation.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 31():902–914, 2023.
<a href="https://doi.org/10.1109/TASLP.2022.3233468">doi:10.1109/TASLP.2022.3233468</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMartinmorato2023077a83bb78aa48518806a0bc54d2abec" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10016759" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMartinmorato2023077a83bb78aa48518806a0bc54d2abec" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMartinmorato2023077a83bb78aa48518806a0bc54d2abec" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMartinmorato2023077a83bb78aa48518806a0bc54d2abec" class="panel-collapse collapse" id="collapseMartinmorato2023077a83bb78aa48518806a0bc54d2abec" role="tabpanel">
<h4>Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation</h4>
<h5>Abstract</h5>
<p class="text-justify">Crowdsourcing is a popular tool for collecting large amounts of annotated data, but the specific format of the strong labels necessary for sound event detection is not easily obtainable through crowdsourcing. In this work, we propose a novel annotation workflow that leverages the efficiency of crowdsourcing weak labels, and uses a high number of annotators to produce reliable and objective strong labels. The weak labels are collected in a highly redundant setup, to allow reconstruction of the temporal information. To obtain reliable labels, the annotators' competence is estimated using MACE (Multi-Annotator Competence Estimation) and incorporated into the strong labels estimation through weighing of individual opinions. We show that the proposed method produces consistently reliable strong annotations not only for synthetic audio mixtures, but also for audio recordings of real everyday environments. While only a maximum 80% coincidence with the complete and correct reference annotations was obtained for synthetic data, these results are explained by an extended study of how polyphony and SNR levels affect the identification rate of the sound events by the annotators. On real data, even though the estimated annotators' competence is significantly lower and the coincidence with reference labels is under 69%, the proposed majority opinion approach produces reliable aggregated strong labels in comparison with the more difficult task of crowdsourcing directly strong labels.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMartinmorato2023077a83bb78aa48518806a0bc54d2abec" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10016759" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMartinmorato2023077a83bb78aa48518806a0bc54d2abeclabel" class="modal fade" id="bibtexMartinmorato2023077a83bb78aa48518806a0bc54d2abec" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMartinmorato2023077a83bb78aa48518806a0bc54d2abeclabel">Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation</h4>
</div>
<div class="modal-body">
<pre>@article{Martinmorato2023,
    author = "Martín-Morató, Irene and Mesaros, Annamaria",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    title = "Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation",
    year = "2023",
    volume = "31",
    number = "",
    pages = "902-914",
    doi = "10.1109/TASLP.2022.3233468",
    abstract = "Crowdsourcing is a popular tool for collecting large amounts of annotated data, but the specific format of the strong labels necessary for sound event detection is not easily obtainable through crowdsourcing. In this work, we propose a novel annotation workflow that leverages the efficiency of crowdsourcing weak labels, and uses a high number of annotators to produce reliable and objective strong labels. The weak labels are collected in a highly redundant setup, to allow reconstruction of the temporal information. To obtain reliable labels, the annotators' competence is estimated using MACE (Multi-Annotator Competence Estimation) and incorporated into the strong labels estimation through weighing of individual opinions. We show that the proposed method produces consistently reliable strong annotations not only for synthetic audio mixtures, but also for audio recordings of real everyday environments. While only a maximum 80\% coincidence with the complete and correct reference annotations was obtained for synthetic data, these results are explained by an extended study of how polyphony and SNR levels affect the identification rate of the sound events by the annotators. On real data, even though the estimated annotators' competence is significantly lower and the coincidence with reference labels is under 69\%, the proposed majority opinion approach produces reliable aggregated strong labels in comparison with the more difficult task of crowdsourcing directly strong labels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Ronchini2021" data-source="content/data/challenge2022/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Ronchini2021"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Francesca Ronchini, Romain Serizel, Nicolas Turpault, and Samuele Cornell.
<em>The impact of non-target events in synthetic soundscapes for sound event detection.</em>
In Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021), 115–119. Barcelona, Spain, November 2021.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexRonchini2021b7f06a362b05464eb975102a47f1e077" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/abs/2109.14061v1" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseRonchini2021b7f06a362b05464eb975102a47f1e077" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseRonchini2021b7f06a362b05464eb975102a47f1e077" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingRonchini2021b7f06a362b05464eb975102a47f1e077" class="panel-collapse collapse" id="collapseRonchini2021b7f06a362b05464eb975102a47f1e077" role="tabpanel">
<h4>The Impact of Non-Target Events in Synthetic Soundscapes for Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">Detection and Classification Acoustic Scene and Events Challenge 2021 Task 4 uses a heterogeneous dataset that includes both recorded and synthetic soundscapes. Until recently only target sound events were considered when synthesizing the soundscapes. However, recorded soundscapes often contain a substantial amount of non-target events that may affect the performance. In this paper, we focus on the impact of these non-target events in the synthetic soundscapes. Firstly, we investigate to what extent using non-target events alternatively during the training or validation phase (or none of them) helps the system to correctly detect target events. Secondly, we analyze to what extend adjusting the signal-to-noise ratio between target and non-target events at training improves the sound event detection performance. The results show that using both target and non-target events for only one of the phases (validation or training) helps the system to properly detect sound events, outperforming the baseline (which uses non-target events in both phases). The paper also reports the results of a preliminary study on evaluating the system on clips that contain only non-target events. This opens questions for future work on non-target subset and acoustic similarity between target and non-target events which might confuse the system.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexRonchini2021b7f06a362b05464eb975102a47f1e077" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/abs/2109.14061v1" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexRonchini2021b7f06a362b05464eb975102a47f1e077label" class="modal fade" id="bibtexRonchini2021b7f06a362b05464eb975102a47f1e077" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexRonchini2021b7f06a362b05464eb975102a47f1e077label">The Impact of Non-Target Events in Synthetic Soundscapes for Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ronchini2021,
    author = "Ronchini, Francesca and Serizel, Romain and Turpault, Nicolas and Cornell, Samuele",
    title = "The Impact of Non-Target Events in Synthetic Soundscapes for Sound Event Detection",
    booktitle = "Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)",
    address = "Barcelona, Spain",
    month = "November",
    year = "2021",
    pages = "115--119",
    abstract = "Detection and Classification Acoustic Scene and Events Challenge 2021 Task 4 uses a heterogeneous dataset that includes both recorded and synthetic soundscapes. Until recently only target sound events were considered when synthesizing the soundscapes. However, recorded soundscapes often contain a substantial amount of non-target events that may affect the performance. In this paper, we focus on the impact of these non-target events in the synthetic soundscapes. Firstly, we investigate to what extent using non-target events alternatively during the training or validation phase (or none of them) helps the system to correctly detect target events. Secondly, we analyze to what extend adjusting the signal-to-noise ratio between target and non-target events at training improves the sound event detection performance. The results show that using both target and non-target events for only one of the phases (validation or training) helps the system to properly detect sound events, outperforming the baseline (which uses non-target events in both phases). The paper also reports the results of a preliminary study on evaluating the system on clips that contain only non-target events. This opens questions for future work on non-target subset and acoustic similarity between target and non-target events which might confuse the system.",
    isbn = "978-84-09-36072-7",
    doi. = "10.5281/zenodo.5770113"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Ebbers2022" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Ebbers2022"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Janek Ebbers, Reinhold Haeb-Umbach, and Romain Serizel.
<em>Threshold independent evaluation of sound event detection scores.</em>
In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1021–1025. IEEE, 2022.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexEbbers20224fa67cf15a6e41b6a251e9226116f3d5" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2201.13148" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseEbbers20224fa67cf15a6e41b6a251e9226116f3d5" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseEbbers20224fa67cf15a6e41b6a251e9226116f3d5" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingEbbers20224fa67cf15a6e41b6a251e9226116f3d5" class="panel-collapse collapse" id="collapseEbbers20224fa67cf15a6e41b6a251e9226116f3d5" role="tabpanel">
<h4>Threshold Independent Evaluation of Sound Event Detection Scores</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexEbbers20224fa67cf15a6e41b6a251e9226116f3d5" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2201.13148" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexEbbers20224fa67cf15a6e41b6a251e9226116f3d5label" class="modal fade" id="bibtexEbbers20224fa67cf15a6e41b6a251e9226116f3d5" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexEbbers20224fa67cf15a6e41b6a251e9226116f3d5label">Threshold Independent Evaluation of Sound Event Detection Scores</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ebbers2022,
    author = "Ebbers, Janek and Haeb-Umbach, Reinhold and Serizel, Romain",
    title = "Threshold Independent Evaluation of Sound Event Detection Scores",
    booktitle = "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "1021--1025",
    year = "2022",
    organization = "IEEE"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Turpault2019_DCASE" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2019_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault, Romain Serizel, Ankit Parag Shah, and Justin Salamon.
<em>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis.</em>
In <span class="bibtex-protected">Workshop on Detection and Classification of Acoustic Scenes and Events</span>. New York City, United States, October 2019.
URL: <a href="https://hal.inria.fr/hal-02160855">https://hal.inria.fr/hal-02160855</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" class="panel-collapse collapse" id="collapseTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" role="tabpanel">
<h4>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10% points in terms of F-measure.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106label" class="modal fade" id="bibtexTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2019_DCASE3cc63aa5fb4646aaa023e8f75c7f5106label">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Turpault2019_DCASE,
    Author = "Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in domestic environments with weakly labeled data and soundscape synthesis}",
    booktitle = "{Workshop on Detection and Classification of Acoustic Scenes and Events}",
    address = "New York City, United States",
    year = "2019",
    month = "October",
    keywords = "Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02160855",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02160855"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h1 id="results">Results</h1>
<p>All confindence intervals are computed based on the three runs per systems and bootstrapping on the evaluation set. The table below includes only the best ranking score per submitting team <strong>without ensembling</strong>.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="psds1_desed_eval_bootstrap" data-scatter-y="mean_sbpauc_maestro_eval_fullset" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="system_name" data-sortable="true">
                Submission <br/>code<br/>
</th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS (Evaluation dataset)" data-chartable="true" data-field="psds1_desed_eval_bootstrap" data-sortable="true" data-value-type="float3-interval-muted">
<br/>PSDS <br/>(DESED evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="mpAUC (Evaluation dataset)" data-chartable="true" data-field="mean_sbpauc_maestro_eval_fullset" data-sortable="true" data-value-type="float3-interval-muted">
<br/>mpAUC <br/>(MAESTRO evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Schmid_CPJKU_task4_2</td>
<td>1.35</td>
<td>0.642 (0.612 - 0.675)</td>
<td>0.711 (0.704 - 0.717)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_2</td>
<td>1.32</td>
<td>0.583 (0.560 - 0.601)</td>
<td>0.738 (0.732 - 0.745)</td>
</tr>
<tr>
<td></td>
<td>Zhang_BUPT_task4_1</td>
<td>1.23</td>
<td>0.528 (0.502 - 0.549)</td>
<td>0.704 (0.704 - 0.705)</td>
</tr>
<tr>
<td></td>
<td>Chen_CHT_task4_1</td>
<td>1.23</td>
<td>0.499 (0.474 - 0.523)</td>
<td>0.733 (0.730 - 0.739)</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST-HanwhaVision_task4_1</td>
<td>1.23</td>
<td>0.564 (0.545 - 0.586)</td>
<td>0.665 (0.646 - 0.677)</td>
</tr>
<tr>
<td></td>
<td>Chen_NCUT_task4_3</td>
<td>1.20</td>
<td>0.529 (0.510 - 0.555)</td>
<td>0.675 (0.675 - 0.675)</td>
</tr>
<tr>
<td></td>
<td>LEE_KT_task4_1</td>
<td>1.19</td>
<td>0.503 (0.458 - 0.562)</td>
<td>0.684 (0.672 - 0.693)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>1.13</td>
<td>0.481 (0.456 - 0.505)</td>
<td>0.646 (0.641 - 0.653)</td>
</tr>
<tr>
<td></td>
<td>XIAO_FMSG-JLESS_task4_3</td>
<td>1.12</td>
<td>0.571 (0.540 - 0.587)</td>
<td>0.553 (0.553 - 0.553)</td>
</tr>
<tr>
<td></td>
<td>Lyu_SCUT_task4_2</td>
<td>1.10</td>
<td>0.484 (0.461 - 0.505)</td>
<td>0.612 (0.596 - 0.624)</td>
</tr>
<tr>
<td></td>
<td>Niu_XJU_task4_1</td>
<td>1.07</td>
<td>0.469 (0.444 - 0.499)</td>
<td>0.603 (0.599 - 0.610)</td>
</tr>
<tr>
<td></td>
<td>Cai_USTC_task4_2</td>
<td>0.63</td>
<td>0.577 (0.559 - 0.598)</td>
<td>0.050 (0.050 - 0.050)</td>
</tr>
<tr>
<td></td>
<td>Huang_SJTU_task4_4</td>
<td>1.20</td>
<td>0.523 (0.500 - 0.552)</td>
<td>0.678 (0.669 - 0.685)</td>
</tr>
</tbody>
</table>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>