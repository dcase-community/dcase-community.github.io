<!DOCTYPE html><html lang="en">
<head>
    <title>Proceedings - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2017/proceedings">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="The proceedings of the DCASE2017 Workshop have been published as electronic publication of Tampere University of Technology series: Virtanen, T., Mesaros, A., Heittola, T., Diment, A., Vincent, E., Benetos, E. &amp; Elizalde, B. (Eds.) (2017). Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017). ISBN (Electronic …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2017</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2017/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Technical program">
        <a href="/workshop2017/technical-program"><i class="fa fa-list fa-fw"></i>&nbsp;Program</a>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2017/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2017/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2017/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2017/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metro-brussels-01.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-right"><object class="img img-responsive sr-header-overlay" type="image/svg+xml" data="..//images/overlays/wave.svg"></object></div><h1 class="bold">Proceedings</h1><span class="subheading">Workshop on Detection and Classification of Acoustic Scenes and Events</span><hr class="small right bold">
                        <span class="subheading subheading-secondary">16 - 17 November 2017, Munich, Germany</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <div class="row">
<div class="col-xs-10">
<p>The proceedings of the DCASE2017 Workshop have been published as electronic publication of Tampere University of Technology series:</p>
</div>
<div class="col-xs-2"></div>
</div>
<div class="row" style="display: -webkit-box !important;display: -webkit-flex !important;display: -ms-flexbox !important;display: flex !important;">
<div class="col-xs-2">
<a data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/116766/DCASE_2017.pdf" rel="tooltip" title="PDF"><img class="img-responsive img-thumbnail" src="../images/covers/DCASE2017Workshop_proceedings_cover.png"/></a>
</div>
<div class="col-xs-8 bg-light-gray">
<p>Virtanen, T., Mesaros, A., Heittola, T., Diment, A., Vincent, E., Benetos, E. &amp; Elizalde, B. (Eds.) (2017). <em>Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017).</em>
</p>
<p>ISBN (Electronic): 978-952-15-4042-4</p>
<br/>
<div class="btn-group">
<a class="btn btn-xs btn-primary" data-placement="bottom" href="https://urn.fi/URN:ISBN:978-952-15-4042-4" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" target="_blank" title="Permanent link"><i class="fa fa-link fa-1x"></i> Link</a>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/116766/DCASE_2017.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="PDF"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button class="btn btn-xs btn-danger" data-target="#bibtex_proceedings" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
</div>
<dic class="col-xs-2">
</dic></div>
<div aria-hidden="true" aria-labelledby="bibtex_proceedingslabel" class="modal fade" id="bibtex_proceedings" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtex_proceedingslabel">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)</h4>
</div>
<div class="modal-body">
<pre>
@book{DCASE2017Workshop,
    title = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    author = "Tuomas Virtanen and Annamaria Mesaros and Toni Heittola and Aleksandr Diment and Emmanuel Vincent and Emmanouil Benetos and Benjamin Martinez Elizalde",
    year = "2017",
    publisher = "Tampere University of Technology. Laboratory of Signal Processing",
}
                </pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
<div class="clearfix"></div>
<div class="btex" data-scholar-cite-counts="true" data-source="content/data/workshop2017/proceedings.bib" data-stats="true">
<em>
  Total cites: 1995 (updated 30.11.2023)
 </em>
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Abesser2017" style="box-shadow: none">
<div class="panel-heading" id="headingAbesser2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification by Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jakob Abeßer, Stylianos Ioannis Mimilakis, Robert Grafe, and Hanna Lukashevich
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Fraunhofer IDMT, Ilmenau, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          9 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Abesser2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Abesser_165.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Abesser2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Abesser2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Abesser2017" class="panel-collapse collapse" id="collapse-Abesser2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban environments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-theart CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configuration leads to a classification accuracy of 75% for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic Scene Classification, Convolutional Neural Networks, Stacked Denoising Autoencoder, Smart City
      </p>
<p>
<strong>
        Cites:
       </strong>
       9 (
       <a href="https://scholar.google.com/scholar?cites=5585762483094500966" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Abesser2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Abesser_165.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Abesser2017label" class="modal fade" id="bibtex-Abesser2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAbesser2017label">
        Acoustic Scene Classification by Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Abesser2017,
    author = "Abeßer, Jakob and Mimilakis, Stylianos Ioannis and Gräfe, Robert and Lukashevich, Hanna",
    title = "Acoustic Scene Classification by Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "7--11",
    keywords = "Acoustic Scene Classification, Convolutional Neural Networks, Stacked Denoising Autoencoder, Smart City",
    abstract = "Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban environments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-theart CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configuration leads to a classification accuracy of 75\% for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Adavanne2017" style="box-shadow: none">
<div class="panel-heading" id="headingAdavanne2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound Event Detection Using Weakly Labeled Dataset with Stacked Convolutional and Recurrent Neural Network
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sharath Adavanne and Tuomas Virtanen
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          67 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Adavanne2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Adavanne_129.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Adavanne_129_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Adavanne2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Adavanne2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Adavanne2017" class="panel-collapse collapse" id="collapse-Adavanne2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We achieve this by using a stacked convolutional and recurrent neural network with two prediction layers in sequence one for the strong followed by the weak label. The network is trained using frame-wise log melband energy as the input audio feature, and weak labels provided in the dataset as labels for the weak label prediction layer. Strong labels are generated by replicating the weak labels as many number of times as the frames in the input audio feature, and used for strong label layer during training. We propose to control what the network learns from the weak and strong labels by different weighting for the loss computed in the two prediction layers. The proposed method is evaluated on a publicly available dataset of 155 hours with 17 sound event classes. The method achieves the best error rate of 0.84 for strong labels and F-score of 43.3% for weak labels on the unseen test split.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       sound event detection, weak labels, deep neural network, CNN, GRU
      </p>
<p>
<strong>
        Cites:
       </strong>
       67 (
       <a href="https://scholar.google.com/scholar?cites=16322973848465432759" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Adavanne2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Adavanne_129.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Adavanne_129_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Adavanne2017label" class="modal fade" id="bibtex-Adavanne2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAdavanne2017label">
        Sound Event Detection Using Weakly Labeled Dataset with Stacked Convolutional and Recurrent Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Adavanne2017,
    author = "Adavanne, Sharath and Virtanen, Tuomas",
    title = "Sound Event Detection Using Weakly Labeled Dataset with Stacked Convolutional and Recurrent Neural Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "12--16",
    keywords = "sound event detection, weak labels, deep neural network, CNN, GRU",
    abstract = "This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We achieve this by using a stacked convolutional and recurrent neural network with two prediction layers in sequence one for the strong followed by the weak label. The network is trained using frame-wise log melband energy as the input audio feature, and weak labels provided in the dataset as labels for the weak label prediction layer. Strong labels are generated by replicating the weak labels as many number of times as the frames in the input audio feature, and used for strong label layer during training. We propose to control what the network learns from the weak and strong labels by different weighting for the loss computed in the two prediction layers. The proposed method is evaluated on a publicly available dataset of 155 hours with 17 sound event classes. The method achieves the best error rate of 0.84 for strong labels and F-score of 43.3\% for weak labels on the unseen test split."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Amiriparian2017" style="box-shadow: none">
<div class="panel-heading" id="headingAmiriparian2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Shahin Amiriparian<sup>1,2,3</sup>, Michael Freitag<sup>1</sup>, Nicholas Cummins<sup>1,2</sup> and Björn Schuller<sup>2,4</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Chair of Complex &amp; Intelligent Systems, Universität Passau, Passau, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care, Augsburg University, Augsburg, Germany, <sup>3</sup>Machine Intelligence &amp; Signal Processing Group, Technische Universität München, München, Germany, <sup>4</sup>Group of Language, Audio &amp; Music, Imperial Collage London, London, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          101 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Amiriparian2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Amiriparian_172.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Amiriparian_172_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Amiriparian2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Amiriparian2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Amiriparian2017" class="panel-collapse collapse" id="collapse-Amiriparian2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). We propose a system for this task using a recurrent sequence to sequence autoencoder for unsupervised representation learning from raw audio files. First, we extract mel-spectrograms from the raw audio files. Second, we train a recurrent sequence to sequence autoencoder on these spectrograms, that are considered as time-dependent frequency vectors. Then, we extract, from a fully connected layer between the decoder and encoder units, the learnt representations of spectrograms as the feature vectors for the corresponding audio instances. Finally, we train a multilayer perceptron neural network on these feature vectors to predict the class labels. In comparison to the baseline, the accuracy is increased from 74:8% to 88:0% on the development set, and from 61:0% to 67:5% on the test set.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       deep feature learning, sequence to sequence learning, recurrent autoencoders, audio processing acoustic scene classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       101 (
       <a href="https://scholar.google.com/scholar?cites=9638091853851638935" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Amiriparian2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Amiriparian_172.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Amiriparian_172_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Amiriparian2017label" class="modal fade" id="bibtex-Amiriparian2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAmiriparian2017label">
        Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Amiriparian2017,
    author = "Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Schuller, Björn",
    title = "Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "17--21",
    keywords = "deep feature learning, sequence to sequence learning, recurrent autoencoders, audio processing acoustic scene classification",
    abstract = "This paper describes our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). We propose a system for this task using a recurrent sequence to sequence autoencoder for unsupervised representation learning from raw audio files. First, we extract mel-spectrograms from the raw audio files. Second, we train a recurrent sequence to sequence autoencoder on these spectrograms, that are considered as time-dependent frequency vectors. Then, we extract, from a fully connected layer between the decoder and encoder units, the learnt representations of spectrograms as the feature vectors for the corresponding audio instances. Finally, we train a multilayer perceptron neural network on these feature vectors to predict the class labels. In comparison to the baseline, the accuracy is increased from 74:8\% to 88:0\% on the development set, and from 61:0\% to 67:5\% on the test set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bisot2017" style="box-shadow: none">
<div class="panel-heading" id="headingBisot2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Nonnegative Feature Learning Methods for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Victor Bisot<sup>1</sup>, Romain Serizel<sup>2,3,4</sup>, Slim Essid<sup>1</sup> and Gaël Richard<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Image Data and Signal, Telecom ParisTech, Paris, France, <sup>2</sup>Université de Lorraine, Loria, Nancy, France, <sup>3</sup>Inria, Nancy, France, <sup>4</sup>CNRS, LORIA, Nancy, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          10 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bisot2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Bisot_194.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Bisot2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bisot2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bisot2017" class="panel-collapse collapse" id="collapse-Bisot2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper introduces improvements to nonnegative feature learning-based methods for acoustic scene classification. We start by introducing modifications to the task-driven nonnegative matrix factorization algorithm. The proposed adapted scaling algorithm improves the generalization capability of task-driven nonnegative matrix factorization for the task. We then propose to exploit simple deep neural network architecture to classify both low level time-frequency representations and unsupervised nonnegative matrix factorization activation features independently. Moreover, we also propose a deep neural network architecture that exploits jointly unsupervised nonnegative matrix factorization activation features and low-level time frequency representations as inputs. Finally, we present a fusion of proposed systems in order to further improve performance. The resulting systems are our submission for the task 1 of the DCASE 2017 challenge.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Feature learning, Nonnegative Matrix Factorization, Deep Neural Networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       10 (
       <a href="https://scholar.google.com/scholar?cites=107380518118783527" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bisot2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Bisot_194.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bisot2017label" class="modal fade" id="bibtex-Bisot2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBisot2017label">
        Nonnegative Feature Learning Methods for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Bisot2017,
    author = "Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gaël",
    title = "Nonnegative Feature Learning Methods for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "22--26",
    keywords = "Feature learning, Nonnegative Matrix Factorization, Deep Neural Networks",
    abstract = "This paper introduces improvements to nonnegative feature learning-based methods for acoustic scene classification. We start by introducing modifications to the task-driven nonnegative matrix factorization algorithm. The proposed adapted scaling algorithm improves the generalization capability of task-driven nonnegative matrix factorization for the task. We then propose to exploit simple deep neural network architecture to classify both low level time-frequency representations and unsupervised nonnegative matrix factorization activation features independently. Moreover, we also propose a deep neural network architecture that exploits jointly unsupervised nonnegative matrix factorization activation features and low-level time frequency representations as inputs. Finally, we present a fusion of proposed systems in order to further improve performance. The resulting systems are our submission for the task 1 of the DCASE 2017 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cakir2017" style="box-shadow: none">
<div class="panel-heading" id="headingCakir2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Convolutional Recurrent Neural Networks for Rare Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Emre Cakir and Tuomas Virtanen
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          66 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cakir2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Cakir_105.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Cakir2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cakir2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cakir2017" class="panel-collapse collapse" id="collapse-Cakir2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound events possess certain temporal and spectral structure in their time-frequency representations. The spectral content for the samples of the same sound event class may exhibit small shifts due to intra-class acoustic variability. Convolutional layers can be used to learn high-level, shift invariant features from time-frequency representations of acoustic samples, while recurrent layers can be used to learn the longer term temporal context from the extracted high-level features. In this paper, we propose combining these two in a convolutional recurrent neural network (CRNN) for rare sound event detection. The proposed method is evaluated over DCASE 2017 challenge dataset of individual sound event samples mixed with everyday acoustic scene samples. CRNN provides significant performance improvement over two other deep learning based methods mainly due to its capability of longer term temporal modeling.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound Event Detection, Convolutional Neural Network, Recurrent Neural Network, Machine learning
      </p>
<p>
<strong>
        Cites:
       </strong>
       66 (
       <a href="https://scholar.google.com/scholar?cites=13305212892014403441" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cakir2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Cakir_105.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cakir2017label" class="modal fade" id="bibtex-Cakir2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCakir2017label">
        Convolutional Recurrent Neural Networks for Rare Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cakir2017,
    author = "Cakir, Emre and Virtanen, Tuomas",
    title = "Convolutional Recurrent Neural Networks for Rare Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "27--31",
    keywords = "Sound Event Detection, Convolutional Neural Network, Recurrent Neural Network, Machine learning",
    abstract = "Sound events possess certain temporal and spectral structure in their time-frequency representations. The spectral content for the samples of the same sound event class may exhibit small shifts due to intra-class acoustic variability. Convolutional layers can be used to learn high-level, shift invariant features from time-frequency representations of acoustic samples, while recurrent layers can be used to learn the longer term temporal context from the extracted high-level features. In this paper, we propose combining these two in a convolutional recurrent neural network (CRNN) for rare sound event detection. The proposed method is evaluated over DCASE 2017 challenge dataset of individual sound event samples mixed with everyday acoustic scene samples. CRNN provides significant performance improvement over two other deep learning based methods mainly due to its capability of longer term temporal modeling."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dekkers2017" style="box-shadow: none">
<div class="panel-heading" id="headingDekkers2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Gert Dekkers<sup>1,2</sup>, Steven Lauwereins<sup>2</sup>, Bart Thoen<sup>1</sup>, Mulu Weldegebreal Adhana<sup>1</sup>, Henk Brouckxon<sup>3</sup>, Bertold Van den Bergh<sup>2</sup>, Toon van Waterschoot<sup>1,2</sup>, Bart Vanrumste<sup>1,2,4</sup>, Marian Verhelst<sup>2</sup>, Peter Karsmakers<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup> KU Leuven, Department of Electrical Engineering, Engineering Technology Cluster, Geel, Belgium, <sup>2</sup> KU Leuven, Department of Electrical Engineering, Leuven, Belgium, <sup>3</sup> Vrije Universiteit Brussel, Department ETRO-DSSP, Brussels, Belgium, <sup>4</sup> IMEC, Leuven, Belgium
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          128 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dekkers2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Dekkers2017').collapse('show');window.location.hash='#Dekkers2017';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Dekkers2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dekkers2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dekkers2017" class="panel-collapse collapse" id="collapse-Dekkers2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       128 (
       <a href="https://scholar.google.com/scholar?cites=4411865755195938803" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dekkers2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://iiw.kuleuven.be/onderzoek/advise/datasets#SINS%20database" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dekkers2017label" class="modal fade" id="bibtex-Dekkers2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDekkers2017label">
        The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dekkers2017,
    author = "Dekkers, Gert and Lauwereins, Steven and Thoen, Bart and Adhana, Mulu Weldegebreal and Brouckxon, Henk and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marian and Karsmakers, Peter and",
    title = "The {SINS} Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "32--36",
    keywords = "Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks",
    abstract = "There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fonseca2017" style="box-shadow: none">
<div class="panel-heading" id="headingFonseca2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Eduardo Fonseca, Rong Gong, Dmitry Bogdanov, Olga Slizovskaia, Emilia Gomez and Xavier Serra
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          26 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fonseca2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Fonseca_181.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Fonseca_181_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Fonseca2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fonseca2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fonseca2017" class="panel-collapse collapse" id="collapse-Fonseca2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This work describes our contribution to the acoustic scene classification task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection of hand-crafted features is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled melspectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8.2%.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       acoustic scene classification, gradient boosting machine, convolutional neural networks, ensembling
      </p>
<p>
<strong>
        Cites:
       </strong>
       26 (
       <a href="https://scholar.google.com/scholar?cites=14885374962050905611" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fonseca2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Fonseca_181.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Fonseca_181_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fonseca2017label" class="modal fade" id="bibtex-Fonseca2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFonseca2017label">
        Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Fonseca2017,
    author = "Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier",
    title = "Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "37--41",
    keywords = "acoustic scene classification, gradient boosting machine, convolutional neural networks, ensembling",
    abstract = "This work describes our contribution to the acoustic scene classification task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection of hand-crafted features is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled melspectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8.2\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Green2017" style="box-shadow: none">
<div class="panel-heading" id="headingGreen2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification Using Spatial Features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Marc C. Green and Damian Murphy
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Audio Lab, Department of Electonic Engineering, University of York, York, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          15 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Green2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Green_126.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Green_126_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Green2017').collapse('show');window.location.hash='#Green2017';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Green2017" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Green2017" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Green2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Green2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Green2017" class="panel-collapse collapse" id="collapse-Green2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or binaural datasets. This paper introduces EigenScape - a new dataset of 4th-order Ambisonic acoustic scene recordings - and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient - Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Directional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The differences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, MFCC, gaussian mixture model, ambisonics, directional audio coding, multichannel, eigenmike
      </p>
<p>
<strong>
        Cites:
       </strong>
       15 (
       <a href="https://scholar.google.com/scholar?cites=7499261875180261096" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Green2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Green_126.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Green_126_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://zenodo.org/record/1012809#.WhU8KRZLeoo" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        EigenScape
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/marc1701/EigenScape" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="EigenScape">
<i class="fa fa-git">
</i>
        EigenScape
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Green2017label" class="modal fade" id="bibtex-Green2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGreen2017label">
        Acoustic Scene Classification Using Spatial Features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Green2017,
    author = "Green, Marc Ciufo and Murphy, Damian",
    title = "Acoustic Scene Classification Using Spatial Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "42--45",
    keywords = "Acoustic scene classification, MFCC, gaussian mixture model, ambisonics, directional audio coding, multichannel, eigenmike",
    abstract = "Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or binaural datasets. This paper introduces EigenScape - a new dataset of 4th-order Ambisonic acoustic scene recordings - and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient - Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Directional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The differences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Han2017" style="box-shadow: none">
<div class="panel-heading" id="headingHan2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yoonchang Han<sup>1</sup> and Jeongsoo Park<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Cochlear.ai, Seoul, Korea, <sup>2</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          165 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Han2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Han_206.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Han_206_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Han2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Han2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Han2017" class="panel-collapse collapse" id="collapse-Han2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we demonstrate how we applied convolutional neural network for DCASE 2017 task 1, acoustic scene classification. We propose a variety of preprocessing methods that emphasise different acoustic characteristics such as binaural representations, harmonicpercussive source separation, and background subtraction. We also present a network structure designed for paired input to make the most of the spatial information contained in the stereo. The experimental results show that the proposed network structures and the preprocessing methods effectively learn acoustic characteristics from the audio recordings, and their ensemble model significantly reduces the error rate further, exhibiting an accuracy of 0.917 for 4-fold cross-validation on the development. The proposed system achieved second place in DCASE 2017 task 1 with an accuracy of 0.804 on the evaluation set.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2017, acoustic scene classification, convolutional neural network, binaural representations, harmonicpercussive source separation, background subtraction
      </p>
<p>
<strong>
        Cites:
       </strong>
       165 (
       <a href="https://scholar.google.com/scholar?cites=6231226620215168504" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Han2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Han_206.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Han_206_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Han2017label" class="modal fade" id="bibtex-Han2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHan2017label">
        Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Han2017,
    author = "Han, Yoonchang and Park, Jeongsoo",
    title = "Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "46--50",
    keywords = "DCASE 2017, acoustic scene classification, convolutional neural network, binaural representations, harmonicpercussive source separation, background subtraction",
    abstract = "In this paper, we demonstrate how we applied convolutional neural network for DCASE 2017 task 1, acoustic scene classification. We propose a variety of preprocessing methods that emphasise different acoustic characteristics such as binaural representations, harmonicpercussive source separation, and background subtraction. We also present a network structure designed for paired input to make the most of the spatial information contained in the stereo. The experimental results show that the proposed network structures and the preprocessing methods effectively learn acoustic characteristics from the audio recordings, and their ensemble model significantly reduces the error rate further, exhibiting an accuracy of 0.917 for 4-fold cross-validation on the development. The proposed system achieved second place in DCASE 2017 task 1 with an accuracy of 0.804 on the evaluation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jeong2017" style="box-shadow: none">
<div class="panel-heading" id="headingJeong2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Audio Event Detection Using Multiple-Input Convolutional Neural Network
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Il-Young Jeong<sup>1,2</sup>, Subin Lee<sup>1,2</sup>, Yoonchang Han<sup>2</sup> and Kyogu Lee<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea, <sup>2</sup>Cochlear.ai, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          61 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jeong2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jeong_202.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Jeong_202_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Jeong2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jeong2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jeong2017" class="panel-collapse collapse" id="collapse-Jeong2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes the model and training framework from our submission for DCASE 2017 task 3: sound event detection in real life audio. Extending the basic convolutional neural network architecture, we use both short- and long-term audio signal simultaneously as input data. In the training stage, we calculated validation errors more frequently than one epoch with adaptive thresholds. We also used class-wise early-stopping strategy to find the best model for each class. The proposed model showed meaningful improvements in cross-validation experiments compared to the baseline system.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2017, Sound event detection, Convolutional neural networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       61 (
       <a href="https://scholar.google.com/scholar?cites=12089201817674036143" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jeong2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jeong_202.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Jeong_202_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jeong2017label" class="modal fade" id="bibtex-Jeong2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJeong2017label">
        Audio Event Detection Using Multiple-Input Convolutional Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jeong2017,
    author = "Jeong, Il-Young and Lee, Subin and Han, Yoonchang and Lee, Kyogu",
    title = "Audio Event Detection Using Multiple-Input Convolutional Neural Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "51--54",
    keywords = "DCASE 2017, Sound event detection, Convolutional neural networks",
    abstract = "This paper describes the model and training framework from our submission for DCASE 2017 task 3: sound event detection in real life audio. Extending the basic convolutional neural network architecture, we use both short- and long-term audio signal simultaneously as input data. In the training stage, we calculated validation errors more frequently than one epoch with adaptive thresholds. We also used class-wise early-stopping strategy to find the best model for each class. The proposed model showed meaningful improvements in cross-validation experiments compared to the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jimenez2017" style="box-shadow: none">
<div class="panel-heading" id="headingJimenez2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Abelino Jimenez, Benjamin Elizalde and Bhiksha Raj
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          13 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jimenez2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jimenez_195.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jimenez_195_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Jimenez2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jimenez2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jimenez2017" class="panel-collapse collapse" id="collapse-Jimenez2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic scene recordings are represented by different types of handcrafted or Neural Network features. These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). However, the complexity of training these methods increases with the dimensionality of these input features and the size of the dataset. A solution is to map the input features to a randomized low-dimensional feature space. The resulting random features can approximate non-linear kernels with faster linear kernel computation. In this work, we computed a set of 6,553 input features and used them to compute random features to approximate three types of kernels, Guassian, Laplacian and Cauchy. We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4%. Moreover, the random features reduced the dimensionality of the input by more than three times with minimal loss of performance and by more than six times and still outperformed the baseline. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic Scene Classification, Laplacian Kernel, Kernel Machines, Random Features
      </p>
<p>
<strong>
        Cites:
       </strong>
       13 (
       <a href="https://scholar.google.com/scholar?cites=2533868160392305751" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jimenez2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jimenez_195.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jimenez_195_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jimenez2017label" class="modal fade" id="bibtex-Jimenez2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJimenez2017label">
        DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jimenez2017,
    author = "Jimenez, Abelino and Elizalde, Benjamin and Raj, Bhiksha",
    title = "{DCASE} 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "55--58",
    keywords = "Acoustic Scene Classification, Laplacian Kernel, Kernel Machines, Random Features",
    abstract = "Acoustic scene recordings are represented by different types of handcrafted or Neural Network features. These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). However, the complexity of training these methods increases with the dimensionality of these input features and the size of the dataset. A solution is to map the input features to a randomized low-dimensional feature space. The resulting random features can approximate non-linear kernels with faster linear kernel computation. In this work, we computed a set of 6,553 input features and used them to compute random features to approximate three types of kernels, Guassian, Laplacian and Cauchy. We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4\%. Moreover, the random features reduced the dimensionality of the input by more than three times with minimal loss of performance and by more than six times and still outperformed the baseline. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jung2017" style="box-shadow: none">
<div class="panel-heading" id="headingJung2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DNN-Based Audio Scene Classification for DCASE2017: Dual Input Features, Balancing Cost, and Stochastic Data Duplication
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jung Jee-Weon, Heo Hee-Soo, Yang IL-Ho, Yoon Sung-Hyun, Shim Hye-Jin and Yu Ha-Jin
        </em>
</p>
<p class="text-muted">
<small>
<em>
          School of Computer Science, University of Seoul, Seoul, Republic of South Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          15 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jung2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jung_187.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jung_187_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Jung2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jung2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jung2017" class="panel-collapse collapse" id="collapse-Jung2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this study, we explored DNN-based audio scene classification systems with dual input features. Dual input features take advantage of simultaneously utilizing two features with different levels of abstraction as inputs: a frame-level mel-filterbank feature and segment-level identity vector. A new fine-tune cost that solves the drawback of dual input features was developed, as well as a data duplication method that enables DNN to clearly discriminate frequently misclassified classes. Combining the proposed methods with the latest DNN techniques such as residual learning achieved a fold-wise accuracy of 95.9% for the validation set and 70.6% for the evaluation set provided by the Detection and Classification of Acoustic Scenes and Events community.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio scene classification, DNN, dual input feature, balancing cost, data duplication, residual learning
      </p>
<p>
<strong>
        Cites:
       </strong>
       15 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jung2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jung_187.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jung_187_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jung2017label" class="modal fade" id="bibtex-Jung2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJung2017label">
        DNN-Based Audio Scene Classification for DCASE2017: Dual Input Features, Balancing Cost, and Stochastic Data Duplication
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jung2017,
    author = "Jung, Jee-Weon and Heo, Hee-Soo and Yang, IL-Ho and Yoon, Sung-Hyun and Shim, Hye-Jin and Yu, Ha-Jin",
    title = "{DNN}-Based Audio Scene Classification for {DCASE2017}: Dual Input Features, Balancing Cost, and Stochastic Data Duplication",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "59--63",
    keywords = "audio scene classification, DNN, dual input feature, balancing cost, data duplication, residual learning",
    abstract = "In this study, we explored DNN-based audio scene classification systems with dual input features. Dual input features take advantage of simultaneously utilizing two features with different levels of abstraction as inputs: a frame-level mel-filterbank feature and segment-level identity vector. A new fine-tune cost that solves the drawback of dual input features was developed, as well as a data duplication method that enables DNN to clearly discriminate frequently misclassified classes. Combining the proposed methods with the latest DNN techniques such as residual learning achieved a fold-wise accuracy of 95.9\% for the validation set and 70.6\% for the evaluation set provided by the Detection and Classification of Acoustic Scenes and Events community."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kroos2017" style="box-shadow: none">
<div class="panel-heading" id="headingKroos2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Neuroevolution for Sound Event Detection in Real Life Audio: A Pilot Study
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Christian Kroos and Mark D. Plumbley
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          15 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kroos2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Kroos_191.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Kroos_191_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Kroos2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kroos2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kroos2017" class="panel-collapse collapse" id="collapse-Kroos2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Neuroevolution techniques combine genetic algorithms with artificial neural networks, some of them evolving network topology along with the network weights. One of these latter techniques is the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. For this pilot study we devised an extended variant (joint NEAT, J-NEAT), introducing dynamic cooperative co-evolution, and applied it to sound event detection in real life audio (Task 3) in the DCASE 2017 challenge. Our research question was whether small networks could be evolved that would be able to compete with the much larger networks now typical for classification and detection tasks. We used the wavelet-based deep scattering transform and k-means clustering across the resulting scales (not across samples) to provide J-NEAT with a compact representation of the acoustic input. The results show that for the development data set J-NEAT was capable of evolving small networks that match the performance of the baseline system in terms of the segment-based error metrics, while exhibiting a substantially better event-related error rate. In the challenge, J-NEAT took first place overall according to the F1 error metric with an F1 of 44:9% and achieved rank 15 out of 34 on the ER error metric with a value of 0:891. We discuss the question of evolving versus learning for supervised tasks.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, neuroevolution, NEAT, deep scattering transform, wavelets, clustering, co-evolution
      </p>
<p>
<strong>
        Cites:
       </strong>
       15 (
       <a href="https://scholar.google.com/scholar?cites=1171917506310779895" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kroos2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Kroos_191.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Kroos_191_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kroos2017label" class="modal fade" id="bibtex-Kroos2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKroos2017label">
        Neuroevolution for Sound Event Detection in Real Life Audio: A Pilot Study
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kroos2017,
    author = "Kroos, Christian and Plumbley, Mark D.",
    title = "Neuroevolution for Sound Event Detection in Real Life Audio: A Pilot Study",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "64--68",
    keywords = "Sound event detection, neuroevolution, NEAT, deep scattering transform, wavelets, clustering, co-evolution",
    abstract = "Neuroevolution techniques combine genetic algorithms with artificial neural networks, some of them evolving network topology along with the network weights. One of these latter techniques is the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. For this pilot study we devised an extended variant (joint NEAT, J-NEAT), introducing dynamic cooperative co-evolution, and applied it to sound event detection in real life audio (Task 3) in the DCASE 2017 challenge. Our research question was whether small networks could be evolved that would be able to compete with the much larger networks now typical for classification and detection tasks. We used the wavelet-based deep scattering transform and k-means clustering across the resulting scales (not across samples) to provide J-NEAT with a compact representation of the acoustic input. The results show that for the development data set J-NEAT was capable of evolving small networks that match the performance of the baseline system in terms of the segment-based error metrics, while exhibiting a substantially better event-related error rate. In the challenge, J-NEAT took first place overall according to the F1 error metric with an F1 of 44:9\% and achieved rank 15 out of 34 on the ER error metric with a value of 0:891. We discuss the question of evolving versus learning for supervised tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2017a" style="box-shadow: none">
<div class="panel-heading" id="headingLee2017a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Combining Multi-Scale Features Using Sample-Level Deep Convolutional Neural Networks for Weakly Supervised Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jongpil Lee<sup>1</sup>, Jiyoung Park<sup>1</sup>, Sangeun Kum<sup>1</sup>, Youngho Jeong<sup>2</sup>, Juhan Nam<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Graduate School of Culture Technology, KAIST, Korea, <sup>2</sup>Realistic AV Research Group, ETRI, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_119.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_119_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Lee2017a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2017a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2017a" class="panel-collapse collapse" id="collapse-Lee2017a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes our method submitted to large-scale weakly supervised sound event detection for smart cars in the DCASE Challenge 2017. It is based on two deep neural network methods suggested for music auto-tagging. One is training sample-level Deep Convolutional Neural Networks (DCNN) using raw waveforms as a feature extractor. The other is aggregating features on multiscaled models of the DCNNs and making final predictions from them. With this approach, we achieved the best results, 47.3% in F-score on subtask A (audio tagging) and 0.75 in error rate on subtask B (sound event detection) in the evaluation. These results show that the waveform-based models can be comparable to spectrogrambased models when compared to other DCASE Task 4 submissions. Finally, we visualize hierarchically learned filters from the challenge dataset in each layer of the waveform-based model to explain how they discriminate the events.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, audio tagging, weakly supervised learning, multi-scale features, sample-level, convolutional neural networks, raw waveforms
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=11544837818995381701" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_119.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_119_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2017alabel" class="modal fade" id="bibtex-Lee2017a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2017alabel">
        Combining Multi-Scale Features Using Sample-Level Deep Convolutional Neural Networks for Weakly Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2017a,
    author = "Lee, Jongpil and Park, Jiyoung and Kum, Sangeun and Jeong, Youngho and Nam, Juhan",
    title = "Combining Multi-Scale Features Using Sample-Level Deep Convolutional Neural Networks for Weakly Supervised Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "69--73",
    keywords = "Sound event detection, audio tagging, weakly supervised learning, multi-scale features, sample-level, convolutional neural networks, raw waveforms",
    abstract = "This paper describes our method submitted to large-scale weakly supervised sound event detection for smart cars in the DCASE Challenge 2017. It is based on two deep neural network methods suggested for music auto-tagging. One is training sample-level Deep Convolutional Neural Networks (DCNN) using raw waveforms as a feature extractor. The other is aggregating features on multiscaled models of the DCNNs and making final predictions from them. With this approach, we achieved the best results, 47.3\% in F-score on subtask A (audio tagging) and 0.75 in error rate on subtask B (sound event detection) in the evaluation. These results show that the waveform-based models can be comparable to spectrogrambased models when compared to other DCASE Task 4 submissions. Finally, we visualize hierarchically learned filters from the challenge dataset in each layer of the waveform-based model to explain how they discriminate the events."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2017b" style="box-shadow: none">
<div class="panel-heading" id="headingLee2017b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Ensemble of Convolutional Neural Networks for Weakly-supervised Sound Event Detection Using Multiple Scale Input
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Donmoon Lee<sup>1,2</sup>, Subin Lee<sup>1,2</sup>, Yoonchang Han<sup>2</sup> and Kyogu Lee<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea, <sup>2</sup>Cochlear.ai, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          82 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2017b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_200.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_200_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Lee2017b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2017b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2017b" class="panel-collapse collapse" id="collapse-Lee2017b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we use ensemble of convolutional neural network models that use the various analysis window to detect audio events in the automotive environment. When detecting the presence of audio events, global input based model that uses the entire audio clip works better. On the other hand, segmented input based models works better in finding the accurate position of the event. Experimental results for weakly-labeled audio data confirm the performance trade-off between the two tasks, depending on the length of input audio. By combining the predictions of various models, the proposed system achieved 0.4762 in the clip-based F1-score and 0.7167 in the segment-based error rate.
      </p>
<p>
<strong>
        Cites:
       </strong>
       82 (
       <a href="https://scholar.google.com/scholar?cites=7148163314101910553" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2017b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_200.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_200_presentation.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2017blabel" class="modal fade" id="bibtex-Lee2017b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2017blabel">
        Ensemble of Convolutional Neural Networks for Weakly-supervised Sound Event Detection Using Multiple Scale Input
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2017b,
    author = "Lee, Donmoon and Lee, Subin and Han, Yoonchang and Lee, Kyogu",
    title = "Ensemble of Convolutional Neural Networks for Weakly-supervised Sound Event Detection Using Multiple Scale Input",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    pages = "74--79",
    abstract = "In this paper, we use ensemble of convolutional neural network models that use the various analysis window to detect audio events in the automotive environment. When detecting the presence of audio events, global input based model that uses the entire audio clip works better. On the other hand, segmented input based models works better in finding the accurate position of the event. Experimental results for weakly-labeled audio data confirm the performance trade-off between the two tasks, depending on the length of input audio. By combining the predictions of various models, the proposed system achieved 0.4762 in the clip-based F1-score and 0.7167 in the segment-based error rate."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lim2017" style="box-shadow: none">
<div class="panel-heading" id="headingLim2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Rare Sound Event Detection Using 1D Convolutional Recurrent Neural Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Hyungui Lim<sup>1</sup>, Jeongsoo Park<sup>2,3</sup> and Yoonchang Han<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Cochlear.ai, Seoul, Korea, <sup>2</sup>N/A, Cochlear.ai, Seoul, Korea, <sup>3</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          158 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lim2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lim_205.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Lim_205_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Lim2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lim2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lim2017" class="panel-collapse collapse" id="collapse-Lim2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Rare sound event detection is a newly proposed task in IEEE DCASE 2017 to identify the presence of monophonic sound event that is classified as an emergency and to detect the onset time of the event. In this paper, we introduce a rare sound event detection system using combination of 1D convolutional neural network (1D ConvNet) and recurrent neural network (RNN) with long shortterm memory units (LSTM). A log-amplitude mel-spectrogram is used as an input acoustic feature and the 1D ConvNet is applied in each time-frequency frame to convert the spectral feature. Then the RNN-LSTM is utilized to incorporate the temporal dependency of the extracted features. The system is evaluated using DCASE 2017 Challenge Task 2 Dataset. Our best result on the test set of the development dataset shows 0.07 and 96.26 of error rate and F-score on the event-based metric, respectively. The proposed system has achieved the 1st place in the challenge with an error rate of 0.13 and an F-Score of 93.1 on the evaluation dataset.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Rare sound event detection, deep learning, convolutional neural network, recurrent neural network, long short-term memory
      </p>
<p>
<strong>
        Cites:
       </strong>
       158 (
       <a href="https://scholar.google.com/scholar?cites=5448010273932832629" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lim2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lim_205.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Lim_205_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lim2017label" class="modal fade" id="bibtex-Lim2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLim2017label">
        Rare Sound Event Detection Using 1D Convolutional Recurrent Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lim2017,
    author = "Lim, Hyungui and Park, Jeongsoo and Han, Yoonchang",
    title = "Rare Sound Event Detection Using {1D} Convolutional Recurrent Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "80--84",
    keywords = "Rare sound event detection, deep learning, convolutional neural network, recurrent neural network, long short-term memory",
    abstract = "Rare sound event detection is a newly proposed task in IEEE DCASE 2017 to identify the presence of monophonic sound event that is classified as an emergency and to detect the onset time of the event. In this paper, we introduce a rare sound event detection system using combination of 1D convolutional neural network (1D ConvNet) and recurrent neural network (RNN) with long shortterm memory units (LSTM). A log-amplitude mel-spectrogram is used as an input acoustic feature and the 1D ConvNet is applied in each time-frequency frame to convert the spectral feature. Then the RNN-LSTM is utilized to incorporate the temporal dependency of the extracted features. The system is evaluated using DCASE 2017 Challenge Task 2 Dataset. Our best result on the test set of the development dataset shows 0.07 and 96.26 of error rate and F-score on the event-based metric, respectively. The proposed system has achieved the 1st place in the challenge with an error rate of 0.13 and an F-Score of 93.1 on the evaluation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mesaros2017" style="box-shadow: none">
<div class="panel-heading" id="headingMesaros2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE2017 Challenge Setup: Tasks, Datasets and Baseline System
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Annamaria Mesaros<sup>1</sup>, Toni Heittola<sup>1</sup>, Aleksandr Diment<sup>1</sup>, Benjamin Elizalde<sup>2</sup>, Ankit Shah<sup>2</sup>, Emmanuel Vincent<sup>3</sup>, Bhiksha Raj<sup>2</sup> and Tuomas Virtanen <sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Tampere University of Technology, Laboratory of Signal Processing, Tampere, Finland, <sup>2</sup>Carnegie Mellon University, Department of Electrical and Computer Engineering, &amp; Department of Language Technologies Institute, Pittsburgh, USA, <sup>3</sup>Inria, F-54600 Villers-les-Nancy, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          549 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mesaros2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Mesaros2017" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mesaros2017" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Mesaros2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mesaros2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mesaros2017" class="panel-collapse collapse" id="collapse-Mesaros2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events, Weak Labels
      </p>
<p>
<strong>
        Cites:
       </strong>
       549 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mesaros2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/TUT-ARG/DCASE2017-baseline-system" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-git">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mesaros2017label" class="modal fade" id="bibtex-Mesaros2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMesaros2017label">
        DCASE2017 Challenge Setup: Tasks, Datasets and Baseline System
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2017,
    author = "Mesaros, Annamaria and Heittola, Toni and Diment, Aleksandr and Elizalde, Benjamin and Shah, Ankit and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas",
    title = "{DCASE2017} Challenge Setup: Tasks, Datasets and Baseline System",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "85--92",
    keywords = "Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events, Weak Labels",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mun2017" style="box-shadow: none">
<div class="panel-heading" id="headingMun2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Seongkyu Mun<sup>1</sup>, Sangwook Park<sup>1</sup>, David Han<sup>2</sup> and Hanseok Ko<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Intelligent Signal Processing Laboratory, Korea University, Seoul, South Korea, <sup>2</sup>Office of Naval Research, Office of Naval Research, Arlington VA, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          190 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mun2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mun_215.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Mun2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mun2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mun2017" class="panel-collapse collapse" id="collapse-Mun2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the crossvalidated experiments on development DB, the usage of the generated features could improve ASC performance.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       acoustic scene classification, generative adversarial networks, support vector machine, data augmentation, decision hyper-plane
      </p>
<p>
<strong>
        Cites:
       </strong>
       190 (
       <a href="https://scholar.google.com/scholar?cites=14067028002649002953" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mun2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mun_215.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mun2017label" class="modal fade" id="bibtex-Mun2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMun2017label">
        Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mun2017,
    author = "Mun, Seongkyu and Park, Sangwook and Han, David K and Ko, Hanseok",
    title = "Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using {SVM} Hyper-Plane",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "93--102",
    keywords = "acoustic scene classification, generative adversarial networks, support vector machine, data augmentation, decision hyper-plane",
    abstract = "Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the crossvalidated experiments on development DB, the usage of the generated features could improve ASC performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2017" style="box-shadow: none">
<div class="panel-heading" id="headingPark2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sangwook Park<sup>1</sup>, Seongkyu Mun<sup>2</sup>, Younglo Lee<sup>1</sup> and Hanseok Ko<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Electrical Engineering, Korea University, Seoul, Republic of Korea, <sup>2</sup>Department of Visual Information Processing, Korea University, Seoul, Republic of Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          35 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Park_214.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Park2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2017" class="panel-collapse collapse" id="collapse-Park2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes new image features for the acoustic scene classification task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In classification of acoustic scenes, identical sounds being observed in different places may affect performance. To resolve this issue, a covariance matrix, which represents energy density for each subband, and a double Fourier transform image, which represents energy variation for each subband, were defined as features. To classify the acoustic scenes with these features, Convolutional Neural Network has been applied with several techniques to reduce training time and to resolve initialization and local optimum problems. According to the experiments which were performed with the DCASE2017 challenge development dataset it is claimed that the proposed method outperformed several baseline methods. Specifically, the class average accuracy is shown as 83.6%, which is an improvement of 8.8%, 9.5%, 8.2% compared to MFCC-MLP, MFCC-GMM, and CepsCom-GMM, respectively.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, covariance learning, double FFT, convolutional neural network
      </p>
<p>
<strong>
        Cites:
       </strong>
       35 (
       <a href="https://scholar.google.com/scholar?cites=15474066980561754674" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Park_214.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2017label" class="modal fade" id="bibtex-Park2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2017label">
        Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Park2017,
    author = "Park, Sangwook and Mun, Seongkyu and Lee, Younglo and Ko, Hanseok",
    title = "Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "98--102",
    keywords = "Acoustic scene classification, covariance learning, double FFT, convolutional neural network",
    abstract = "This paper proposes new image features for the acoustic scene classification task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In classification of acoustic scenes, identical sounds being observed in different places may affect performance. To resolve this issue, a covariance matrix, which represents energy density for each subband, and a double Fourier transform image, which represents energy variation for each subband, were defined as features. To classify the acoustic scenes with these features, Convolutional Neural Network has been applied with several techniques to reduce training time and to resolve initialization and local optimum problems. According to the experiments which were performed with the DCASE2017 challenge development dataset it is claimed that the proposed method outperformed several baseline methods. Specifically, the class average accuracy is shown as 83.6\%, which is an improvement of 8.8\%, 9.5\%, 8.2\% compared to MFCC-MLP, MFCC-GMM, and CepsCom-GMM, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Piczak2017" style="box-shadow: none">
<div class="panel-heading" id="headingPiczak2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Karol Piczak
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          36 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Piczak2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Piczak_210.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Piczak_210_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Piczak2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Piczak2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Piczak2017" class="panel-collapse collapse" id="collapse-Piczak2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This study describes a convolutional neural network model submitted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with different frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolutional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       acoustic scene classification, spectrogram, frequency resolution, convolutional neural network, DCASE 2017
      </p>
<p>
<strong>
        Cites:
       </strong>
       36 (
       <a href="https://scholar.google.com/scholar?cites=9872322517840027717" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Piczak2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Piczak_210.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Piczak_210_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Piczak2017label" class="modal fade" id="bibtex-Piczak2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPiczak2017label">
        The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Piczak2017,
    author = "Piczak, Karol Jerzy",
    title = "The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "103--107",
    keywords = "acoustic scene classification, spectrogram, frequency resolution, convolutional neural network, DCASE 2017",
    abstract = "This study describes a convolutional neural network model submitted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with different frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolutional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Qian2017" style="box-shadow: none">
<div class="panel-heading" id="headingQian2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Wavelets Revisited for the Classification of Acoustic Scenes
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Qian Kun<sup>1,2,3</sup>, Ren Zhao<sup>2,3</sup>, Pandit Vedhas<sup>2,3</sup>, Yang Zijiang<sup>1,2</sup>, Zhang Zixing<sup>2</sup>, and Schuller Björn<sup>2,3,4</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>MISP group, Technische Universität München, Munich, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>3</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>4</sup>GLAM - Group on Language, Audio and Music, Imperial College London, London, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          41 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Qian2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Qian_132.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Qian_132_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Qian2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Qian2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Qian2017" class="panel-collapse collapse" id="collapse-Qian2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We investigate the effectiveness of wavelet features for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). On the back-end side, gated recurrent neural networks (GRNNs) are compared against traditional support vector machines (SVMs). We observe that, the proposed wavelet features behave comparable to the typically-used temporal and spectral features in the classification of acoustic scenes. Further, a late fusion of trained models with wavelets and typical acoustic features reach the best averaged 4-fold cross validation accuracy of 83.2 \%, and 82.6 \% by SVMs, and GRNNs, respectively; both significantly outperform the baseline (74.8 \%) of the official development set (p &lt; 0:001, one-tailed z-test).
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic Scene Classification, Wavelets, Support Vector Machines, Sequence Modelling, Gated Recurrent Neural Networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       41 (
       <a href="https://scholar.google.com/scholar?cites=1393538104037651417" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Qian2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Qian_132.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Qian_132_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Qian2017label" class="modal fade" id="bibtex-Qian2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexQian2017label">
        Wavelets Revisited for the Classification of Acoustic Scenes
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Qian2017,
    author = "Qian, Kun and Ren, Zhao and Pandit, Vedhas and Yang, Zijiang and Zhang, Zixing and Schuller, Björn",
    title = "Wavelets Revisited for the Classification of Acoustic Scenes",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "108--112",
    keywords = "Acoustic Scene Classification, Wavelets, Support Vector Machines, Sequence Modelling, Gated Recurrent Neural Networks",
    abstract = "We investigate the effectiveness of wavelet features for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). On the back-end side, gated recurrent neural networks (GRNNs) are compared against traditional support vector machines (SVMs). We observe that, the proposed wavelet features behave comparable to the typically-used temporal and spectral features in the classification of acoustic scenes. Further, a late fusion of trained models with wavelets and typical acoustic features reach the best averaged 4-fold cross validation accuracy of 83.2 \\%, and 82.6 \\% by SVMs, and GRNNs, respectively; both significantly outperform the baseline (74.8 \\%) of the official development set (p &lt; 0:001, one-tailed z-test)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ren2017" style="box-shadow: none">
<div class="panel-heading" id="headingRen2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Deep Sequential Image Features on Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ren Zhao<sup>1,2</sup>, Pandit Vedhas<sup>1,2</sup>, Qian Kun<sup>1,2,3</sup>, Yang Zijiang<sup>1,2</sup>, Zhang Zixing<sup>2</sup>, and Schuller Björn<sup>1,2,4</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>3</sup>MISP group, Technische Universität München, Munich, Germany, <sup>4</sup>GLAM - Group on Language, Audio and Music, Imperial College London, London, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          52 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ren2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Ren_133.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Ren_133_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Ren2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ren2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ren2017" class="panel-collapse collapse" id="collapse-Ren2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       For the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017), we propose a novel method to classify 15 different acoustic scenes using deep sequential learning, based on features extracted from Short-Time Fourier Transform and scalogram of the audio scenes using Convolutional Neural Networks. It is the first time to investigate the performance of bump and morse scalograms for acoustic scene classification in an according context. First, segmented audio waves are transformed into a spectrogram and two types of scalograms; then, ‘deep features’ are extracted from these using the pre-trained VGG16 model by probing at the fully connected layer. These representations are then fed into Gated Recurrent Neural Networks for classification separately. Predictions from the three systems are finally combined by a margin sampling value strategy. On the official development set of the challenge, the best accuracy on a four-fold cross-validation setup is 80:9%, which increases by 6:1% when compared with the official baseline (p &lt; :001 by one-tailed z-test).
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio Scene Classification, Deep Sequential Learning, Scalogram, Convolutional Neural Networks, Gated Recurrent Neural Networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       52 (
       <a href="https://scholar.google.com/scholar?cites=7756205325355086085" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ren2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Ren_133.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Ren_133_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ren2017label" class="modal fade" id="bibtex-Ren2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRen2017label">
        Deep Sequential Image Features on Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ren2017,
    author = "Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Björn",
    title = "Deep Sequential Image Features on Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "113--117",
    keywords = "Audio Scene Classification, Deep Sequential Learning, Scalogram, Convolutional Neural Networks, Gated Recurrent Neural Networks",
    abstract = "For the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017), we propose a novel method to classify 15 different acoustic scenes using deep sequential learning, based on features extracted from Short-Time Fourier Transform and scalogram of the audio scenes using Convolutional Neural Networks. It is the first time to investigate the performance of bump and morse scalograms for acoustic scene classification in an according context. First, segmented audio waves are transformed into a spectrogram and two types of scalograms; then, ‘deep features’ are extracted from these using the pre-trained VGG16 model by probing at the fully connected layer. These representations are then fed into Gated Recurrent Neural Networks for classification separately. Predictions from the three systems are finally combined by a margin sampling value strategy. On the official development set of the challenge, the best accuracy on a four-fold cross-validation setup is 80:9\%, which increases by 6:1\% when compared with the official baseline (p &lt; :001 by one-tailed z-test)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schindler2017" style="box-shadow: none">
<div class="panel-heading" id="headingSchindler2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Alexander Schindler<sup>1</sup>, Thomas Lidy<sup>2</sup> and Andreas Rauber<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Center for Digital Safety and Security, Austrian Institute of Technology, Vienna, Austria, <sup>2</sup>Institute for Software and Interactive Systems, Technical University of Vienna, Vienna, Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          15 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schindler2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Schindler_152.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Schindler_152_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Schindler2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schindler2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schindler2017" class="panel-collapse collapse" id="collapse-Schindler2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper we present a Deep Neural Network architecture for the task of acoustic scene classification which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolutions are chosen to cover fine-grained characteristics of a scene’s spectral texture as well as its distribution of acoustic events. The proposed model shows a 3.56% absolute improvement of the best performing single resolution model and 12.49% of the DCASE 2017 Acoustic Scenes Classification task baseline [1].
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Deep Learning, Convolutional Neural Networks, Acoustic Scene Classification, Audio Analysis
      </p>
<p>
<strong>
        Cites:
       </strong>
       15 (
       <a href="https://scholar.google.com/scholar?cites=6176277944699167989" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schindler2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Schindler_152.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Schindler_152_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schindler2017label" class="modal fade" id="bibtex-Schindler2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchindler2017label">
        Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schindler2017,
    author = "Schindler, Alexander and Lidy, Thomas and Rauber, Andreas",
    title = "Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "118--122",
    keywords = "Deep Learning, Convolutional Neural Networks, Acoustic Scene Classification, Audio Analysis",
    abstract = "In this paper we present a Deep Neural Network architecture for the task of acoustic scene classification which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolutions are chosen to cover fine-grained characteristics of a scene’s spectral texture as well as its distribution of acoustic events. The proposed model shows a 3.56\% absolute improvement of the best performing single resolution model and 12.49\% of the DCASE 2017 Acoustic Scenes Classification task baseline [1]."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vafeiadis2017" style="box-shadow: none">
<div class="panel-heading" id="headingVafeiadis2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Anastasios Vafeiadis<sup>1</sup>, Dimitrios Kalatzis<sup>1</sup>, Konstantinos Votis<sup>1</sup>, Dimitrios Giakoumis<sup>1</sup>, Dimitrios Tzovaras<sup>1</sup>, Liming Chen<sup>2</sup> and Raouf Hamzaoui<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Information Technologies Institute, Center for Research &amp; Technology Hellas, Thessaloniki, Greece, <sup>2</sup>Faculty of Technology, De Montfort University, Leicester, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          24 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vafeiadis2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Vafeiadis_135.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Vafeiadis_135_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Vafeiadis2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vafeiadis2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vafeiadis2017" class="panel-collapse collapse" id="collapse-Vafeiadis2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes our contribution to the 2017 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. We investigated two approaches for the acoustic scene classification task. Firstly, we used a combination of features in the time and frequency domain and a hybrid Support Vector Machines - Hidden Markov Model (SVM-HMM) classifier to achieve an average accuracy over 4-folds of 80.9% on the development dataset and 61.0% on the evaluation dataset. Secondly, by exploiting dataaugmentation techniques and using the whole segment (as opposed to splitting into sub-sequences) as an input, the accuracy of our CNN system was boosted to 95.9%. However, due to the small number of kernels used for the CNN and a failure of capturing the global information of the audio signals, it achieved an accuracy of 49.5% on the evaluation dataset. Our two approaches outperformed the DCASE baseline method, which uses log-mel band energies for feature extraction and a Multi-Layer Perceptron (MLP) to achieve an average accuracy over 4-folds of 74.8%.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, feature extraction, deep learning, spectral features, data augmentation
      </p>
<p>
<strong>
        Cites:
       </strong>
       24 (
       <a href="https://scholar.google.com/scholar?cites=370788048014212072" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vafeiadis2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Vafeiadis_135.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Vafeiadis_135_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vafeiadis2017label" class="modal fade" id="bibtex-Vafeiadis2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVafeiadis2017label">
        Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Vafeiadis2017,
    author = "Vafeiadis, Anastasios and Kalatzis, Dimitrios and Votis, Konstantinos and Giakoumis, Dimitrios and Tzovaras, Dimitrios and Chen, Liming and Hamzaoui, Raouf",
    title = "Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "123--127",
    keywords = "Acoustic scene classification, feature extraction, deep learning, spectral features, data augmentation",
    abstract = "This report describes our contribution to the 2017 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. We investigated two approaches for the acoustic scene classification task. Firstly, we used a combination of features in the time and frequency domain and a hybrid Support Vector Machines - Hidden Markov Model (SVM-HMM) classifier to achieve an average accuracy over 4-folds of 80.9\% on the development dataset and 61.0\% on the evaluation dataset. Secondly, by exploiting dataaugmentation techniques and using the whole segment (as opposed to splitting into sub-sequences) as an input, the accuracy of our CNN system was boosted to 95.9\%. However, due to the small number of kernels used for the CNN and a failure of capturing the global information of the audio signals, it achieved an accuracy of 49.5\% on the evaluation dataset. Our two approaches outperformed the DCASE baseline method, which uses log-mel band energies for feature extraction and a Multi-Layer Perceptron (MLP) to achieve an average accuracy over 4-folds of 74.8\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2017" style="box-shadow: none">
<div class="panel-heading" id="headingWang2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Audio Events Detection and classification using extended R-FCN Approach
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Wang Kaiwu, Yang Liping and Yang Bin
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Key Laboratory of Optoelectronic Technology and Systems (Chongqing University), Ministry of Education, ChongQing University, ChongQing, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Wang_121.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wang2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2017" class="panel-collapse collapse" id="collapse-Wang2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this study, we present a new audio event detection and classification approach based on R-FCN—a state-of-the-art fully convolutional network framework for visual object detection. Spectrogram features of audio signals are used as the input of the approach. The proposed approach consists of two stages like R-FCN network. In the first stage, we detect whether there are audio events by sliding convolutional kernel in time axis, and then proposals which possibly contain audio events are generated by RPN (Region Proposal Networks). In the second stage, time and frequency domain information are integrated to classify these proposals and refine their boundaries. Our approach can output the positions of audio events directly which can input a two-dimensional representation of arbitrary length sound without any size regularization.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio events detection, Convolutional Neural Network, spectrogram feature
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Wang_121.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2017label" class="modal fade" id="bibtex-Wang2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2017label">
        Audio Events Detection and classification using extended R-FCN Approach
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wang2017,
    author = "Wang, Kaiwu and Yang, Liping and Yang, Bin",
    title = "Audio Events Detection and classification using extended {R-FCN} Approach",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "128--132",
    keywords = "audio events detection, Convolutional Neural Network, spectrogram feature",
    abstract = "In this study, we present a new audio event detection and classification approach based on R-FCN—a state-of-the-art fully convolutional network framework for visual object detection. Spectrogram features of audio signals are used as the input of the approach. The proposed approach consists of two stages like R-FCN network. In the first stage, we detect whether there are audio events by sliding convolutional kernel in time axis, and then proposals which possibly contain audio events are generated by RPN (Region Proposal Networks). In the second stage, time and frequency domain information are integrated to classify these proposals and refine their boundaries. Our approach can output the positions of audio events directly which can input a two-dimensional representation of arbitrary length sound without any size regularization."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zheng2017" style="box-shadow: none">
<div class="panel-heading" id="headingZheng2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Zheng Weiping<sup>1</sup>, Yi Jiantao<sup>1</sup>, Xing Xiaotao<sup>1</sup>, Liu Xiangtao<sup>2</sup> and Peng Shaohu<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Computer, South China Normal University, Guangzhou, China, <sup>2</sup>Shenzhen Chinasfan Information Technology Co., Ltd., Shenzhen Chinasfan Information Technology Co., Ltd., Shenzhen, China, <sup>3</sup>School of Mechanical and Electrical Engineering,, Guangzhou University, Guangzhou, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          72 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zheng2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zheng_159.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017_Xing_158_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download poster">
<i class="fa fa-file-text-o fa-1x">
</i>
         Poster
        </a>
<button aria-controls="collapse-Zheng2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zheng2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zheng2017" class="panel-collapse collapse" id="collapse-Zheng2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Making sense of the environment by sounds is an important research in machine learning community. In this work, a Deep Convolutional Neural Network (DCNN) model is presented to classify acoustic scenes along with a multiple spectrograms fusion method. Firstly, the generations of standard spectrogram and CQT spectrogram are introduced separately. Corresponding features can then be extracted by feeding these spectrogram data into the proposed DCNN model. To fuse these multiple spectrogram features, two fusing mechanisms, namely the voting and the SVM methods, are designed. By fusing DCNN features of the standard and CQT spectrograms, the accuracy is significantly improved in our experiments, comparing with the single spectrogram schemes. This proves the effectiveness of the proposed multi-spectrograms fusion method.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Deep convolutional neural network, spectrogram, feature fusion, acoustic scene classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       72 (
       <a href="https://scholar.google.com/scholar?cites=11077330979788972912" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zheng2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zheng_159.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017_Xing_158_poster.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Poster
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zheng2017label" class="modal fade" id="bibtex-Zheng2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZheng2017label">
        Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zheng2017,
    author = "Zheng, Weiping and Yi, Jiantao and Xing, Xiaotao and Liu, Xiangtao and Peng, Shaohu",
    title = "Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "133--137",
    keywords = "Deep convolutional neural network, spectrogram, feature fusion, acoustic scene classification",
    abstract = "Making sense of the environment by sounds is an important research in machine learning community. In this work, a Deep Convolutional Neural Network (DCNN) model is presented to classify acoustic scenes along with a multiple spectrograms fusion method. Firstly, the generations of standard spectrogram and CQT spectrogram are introduced separately. Corresponding features can then be extracted by feeding these spectrogram data into the proposed DCNN model. To fuse these multiple spectrogram features, two fusing mechanisms, namely the voting and the SVM methods, are designed. By fusing DCNN features of the standard and CQT spectrograms, the accuracy is significantly improved in our experiments, comparing with the single spectrogram schemes. This proves the effectiveness of the proposed multi-spectrograms fusion method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhou2017" style="box-shadow: none">
<div class="panel-heading" id="headingZhou2017" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Robust Sound Event Detection Through Noise Estimation and Source Separation Using NMF
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Qing Zhou and Zuren Feng
        </em>
</p>
<p class="text-muted">
<small>
<em>
          School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi'an, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          12 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhou2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zhou_113.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Zhou2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhou2017" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhou2017" class="panel-collapse collapse" id="collapse-Zhou2017" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper addresses the problem of sound event detection under non-stationary noises and various real-world acoustic scenes. An effective noise reduction strategy is proposed in this paper which can automatically adapt to background variations. The proposed method is based on supervised non-negative matrix factorization (NMF) for separating target events from noise. The event dictionary is trained offline using the training data of the target event class while the noise dictionary is learned online from the input signal by sparse and low-rank decomposition. Incorporating the estimated noise bases, this method can produce accurate source separation results by reducing noise residue and signal distortion of the reconstructed event spectrogram. Experimental results on DCASE 2017 task 2 dataset show that the proposed method outperforms the baseline system based on multi-layer perceptron classifiers and also another NMF-based method which employs a semi-supervised strategy for noise reduction.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, non-negative matrix factorization, sparse and low-rank decomposition, source separation
      </p>
<p>
<strong>
        Cites:
       </strong>
       12 (
       <a href="https://scholar.google.com/scholar?cites=17159375819196978342" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhou2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zhou_113.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhou2017label" class="modal fade" id="bibtex-Zhou2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhou2017label">
        Robust Sound Event Detection Through Noise Estimation and Source Separation Using NMF
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zhou2017,
    author = "Zhou, Qing and Feng, Zuren",
    title = "Robust Sound Event Detection Through Noise Estimation and Source Separation Using {NMF}",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "138--142",
    keywords = "Sound event detection, non-negative matrix factorization, sparse and low-rank decomposition, source separation",
    abstract = "This paper addresses the problem of sound event detection under non-stationary noises and various real-world acoustic scenes. An effective noise reduction strategy is proposed in this paper which can automatically adapt to background variations. The proposed method is based on supervised non-negative matrix factorization (NMF) for separating target events from noise. The event dictionary is trained offline using the training data of the target event class while the noise dictionary is learned online from the input signal by sparse and low-rank decomposition. Incorporating the estimated noise bases, this method can produce accurate source separation results by reducing noise residue and signal distortion of the reconstructed event spectrogram. Experimental results on DCASE 2017 task 2 dataset show that the proposed method outperforms the baseline system based on multi-layer perceptron classifiers and also another NMF-based method which employs a semi-supervised strategy for noise reduction."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>