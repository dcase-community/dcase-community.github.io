<!DOCTYPE html><html lang="en">
<head>
    <title>Monitoring of domestic activities based on multi-channel acoustics - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2018/task-monitoring-domestic-activities">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of this task is to classify multi-channel audio segments (i.e. segmented data is given), acquired by a microphone array, into one of the provided predefined classes. These classes are daily activities performed in a home environment (e.g. &#34;Cooking&#34;). updated 14/09/2018 The challenge has finished …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2018/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-general-purpose-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-bird-audio-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2018/task-monitoring-domestic-activities" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-home text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2018/task-monitoring-domestic-activities"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2018/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2018/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/port-malaga-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-danger"></i><i class="fa dc-domestic fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Monitor</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 5</span></span><img src="../images/logos/dcase/dcase2018_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Monitoring of domestic activities based on multi-channel acoustics</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Gert Dekkers" class="img img-circle" src="/images/person/gert_dekkers.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Gert Dekkers</strong>
<a class="icon" href="mailto:gert.dekkers@kuleuven.be"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.kuleuven.be/english/">
                                KU Leuven
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Peter Karsmakers" class="img img-circle" src="/images/person/peters_karsmakers.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Peter Karsmakers</strong>
<a class="icon" href="mailto:peter.karsmakers@kuleuven.be"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.kuleuven.be/english/">
                                KU Leuven
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Lode Vuegen" class="img img-circle" src="/images/person/lode_vuegen.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Lode Vuegen</strong>
<a class="icon" href="mailto:lode.vuegen@kuleuven.be"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.kuleuven.be/english/">
                                KU Leuven
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#description">Description</a></li>
<li><a href="#dataset">Dataset</a>
<ul>
<li><a href="#content">Content</a></li>
<li><a href="#recording-and-annotation-procedure">Recording and annotation procedure</a></li>
<li><a href="#download-development-dataset">Download: Development dataset</a></li>
<li><a href="#download-evaluation-dataset">Download: Evaluation dataset</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-set">Development set</a></li>
<li><a href="#external-data-sourcespre-trained-models">External data sources/pre-trained models</a></li>
<li><a href="#evaluation-set">Evaluation set</a></li>
<li><a href="#submission">Submission</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#repository">Repository</a></li>
<li><a href="#results-for-the-development-dataset">Results for the development dataset</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of this task is to classify multi-channel audio segments (i.e. segmented data is given), acquired by a microphone array, into one of the provided predefined classes. These classes are daily activities performed in a home environment (e.g. "Cooking").</p>
<p><em><div style="text-align: right"> updated 14/09/2018 </div></em></p>
<p class="bg-success">
The challenge has finished. Results are available at the <a href="/challenge2018/task-monitoring-domestic-activities-results">task results page</a>. Ground truth annotations for the <a href="https://zenodo.org/record/1964758">evaluation dataset</a> have also been released.</p>
<h1 id="introduction">Introduction</h1>
<p>There is a rising interest in smart environments that enhance the quality of live for humans in terms of e.g. safety, security, comfort, and home care. In order to have smart functionality, situational awareness is required, which might be obtained by interpreting a multitude of sensing modalities including acoustics. The latter is already used in vocal assistants such as Google Home, Apple HomePod, and Amazon Echo. While these devices focus on speech, they could be extended to identify domestic activities carried out by humans. In the literature, this recognition of activities based on acoustics is already touched upon. Yet, the acoustic models are typically based on single channel and single location recordings. In this task, it is investigated <strong>to which extend multi-channel acoustic recordings are beneficial for the purpose of detecting domestic activities.</strong></p>
<h1 id="description">Description</h1>
<p>The goal of this task is to <strong>classify multi-channel audio segments</strong> (i.e. segmented data is given), acquired by a microphone array, into one of the provided predefined classes as illustrated by Figure 1. These classes are <strong>daily activities performed in a home environment</strong>. For example, “Cooking”, “Watching TV” and “Working”.  As they can be composed out of different sound events such activities are considered as acoustic scenes.  The difference with <em>Task 1: Acoustic scene classification</em> is the type of scenes and the possibility to use multi-channel audio.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-6 col-centered">
<img class="img img-responsive" src="../images/tasks/challenge2018/task5_multichannel_activity_classification.png"/>
<figcaption>Figure 1: Conceptual overview of the task.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>In this challenge a person living alone at home is considered.  This reduces the complexity of the problem since the number of overlapping activities is expected to be small. In fact, in the considered data set no overlapping activities are present. 
These conditions were chosen to focus on the main goal of this task which is to investigate to which extend multi-channel acoustic recordings are beneficial for the purpose of detecting domestic activities. This means that spatial properties can be exploited to serve as input features to the classification problem. However, using absolute localization of sound sources as input for the detection model is doomed to not generalize well to cases where the position of the microphone array is altered. Therefore, in this task <strong>the focus is on systems which can exploit spatial cues independent of sensor location using multi-channel audio</strong>.</p>
<h1 id="dataset">Dataset</h1>
<h2 id="content">Content</h2>
<p>The dataset used in this task is a derivative of the <strong>SINS dataset</strong>. It contains a continuous recording of one person living in a vacation home over a period of one week.  It was collected using a network of 13 microphone arrays distributed over the entire home. The microphone array consists of 4 linearly arranged microphones. For this task 7 microphone arrays in the combined living room and kitchen area are used. Figure 2 shows the floorplan of the recorded environment along with the position of the used sensor nodes.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-6 col-centered">
<img class="img img-responsive" src="../images/tasks/challenge2018/task5_2dplan.png"/>
<figcaption>Figure 2: 2D floorplan of the combined kitchen and living room with the used sensor nodes.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>The continuous recordings were split into audio segments of 10s. Segments containing more then one active class (e.g. a transition of two actitivies) were left out. This means that each segments represents one activity. Subsampling was then performed starting from the largest classes to make the dataset easier to use for a challenge. These audio segments are provided as individual files along with the ground truth. Each audio segment contains 4 channels (e.g. the 4 microphone channels from a particular node).
The daily activities for this task (9) are shown in Table 1 along with the available 10s multi-channel segments in the development set and the amount of full sessions of a certain activity (e.g. a cooking session).</p>
<div class="table table-responsive">
<table class="table table-striped">
<thead>
<tr>
<th>Activity</th>
<th class="col-md-3"># 10s segments</th>
<th class="col-md-3"># sessions</th>
</tr>
</thead>
<tbody>
<tr>
<td>Absence (nobody present in the room)</td>
<td>18860</td>
<td>42</td>
</tr>
<tr>
<td>Cooking</td>
<td>5124</td>
<td>13</td>
</tr>
<tr>
<td>Dishwashing</td>
<td>1424</td>
<td>10</td>
</tr>
<tr>
<td>Eating</td>
<td>2308</td>
<td>13</td>
</tr>
<tr>
<td>Other (present but not doing any relevant activity)</td>
<td>2060</td>
<td>118</td>
</tr>
<tr>
<td>Social activity (visit, phone call)</td>
<td>4944</td>
<td>21</td>
</tr>
<tr>
<td>Vacuum cleaning</td>
<td>972</td>
<td>9</td>
</tr>
<tr>
<td>Watching TV</td>
<td>18648</td>
<td>9</td>
</tr>
<tr>
<td>Working (typing, mouse click, ...)</td>
<td>18644</td>
<td>33</td>
</tr>
</tbody>
<tfoot>
<tr>
<td><strong>Total</strong></td>
<td><strong>72984</strong></td>
<td><strong>268</strong></td>
</tr>
</tfoot>
</table>
</div>
<div class="clearfix"></div>
<p>As <strong>development set, approximately 200 hours of data from 4 sensor nodes</strong> along with the ground truth is given. As <strong>evaluation set</strong>, data is provided from <strong>all the sensor nodes</strong> (i.e. also sensor nodes not present in the development set). The <strong>evaluation will be based on the sensor nodes not present in the development set</strong>. The data from the same nodes as in training are provided to give insights about the overfitting on those positions. The partitioning of the data was done randomly. The segments belonging to one particular consecutive activity (e.g. a full session of cooking) were kept together. The data provided for each sensor node contain recordings of the same time period. This means that the performed activities are observed from multiple microphone arrays at the same time instant. Due to the subsampling on the segments of the largest classes, there is not a full time-wise overlap by all sensor nodes for a particular consecutive activity of those classes. </p>
<h2 id="recording-and-annotation-procedure">Recording and annotation procedure</h2>
<p>The sensor node configuration used in this setup is a control board together with a linear microphone array. The control board contains an EFM32 ARM cortex M4 microcontroller from Silicon Labs (EFM32WG980) used for sampling the analog audio. The microphone array contains four Sonion N8AC03 MEMS low-power (±17µW) microphones with an inter-microphone distance of 5 cm. The sampling for each audio channel is done sequentially at a rate of 16 kHz with a bit depth of 12.
The annotation was performed in two phases. First, during the data collection a smartphone application was used to let the monitored person(s) annotate the activities while being recorded. The person could only select a fixed set of activities. The application was easy to use and did not significantly influence the transition between activities. Secondly, the start and stop timestamps of each activity were refined by using our own annotation software. Postprocessing and sharing the database involves privacy-related aspects. Besides the person(s) living there, multiple people visited the home. Moreover, during a phone call, one can partially hear the person on the other end. A written informed consent was obtained from all participants.</p>
<p>More information about the full dataset can be found in:</p>
<div class="btex-item" data-item="Dekkers2017" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Dekkers2017"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Gert Dekkers, Steven Lauwereins, Bart Thoen, Mulu Weldegebreal Adhana, Henk Brouckxon, Toon van Waterschoot, Bart Vanrumste, Marian Verhelst, and Peter Karsmakers.
<em>The SINS database for detection of daily activities in a home environment using an acoustic sensor network.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), 32–36. November 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDekkers20175f65be1c437b445ba31a499018be04d3" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://dcase.community/documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDekkers20175f65be1c437b445ba31a499018be04d3" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDekkers20175f65be1c437b445ba31a499018be04d3" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDekkers20175f65be1c437b445ba31a499018be04d3" class="panel-collapse collapse" id="collapseDekkers20175f65be1c437b445ba31a499018be04d3" role="tabpanel">
<h4>The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
<h5>Abstract</h5>
<p class="text-justify">There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.</p>
<h5>Keywords</h5>
<p class="text-justify">Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDekkers20175f65be1c437b445ba31a499018be04d3" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://dcase.community/documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDekkers20175f65be1c437b445ba31a499018be04d3label" class="modal fade" id="bibtexDekkers20175f65be1c437b445ba31a499018be04d3" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDekkers20175f65be1c437b445ba31a499018be04d3label">The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dekkers2017,
    Author = "Dekkers, Gert and Lauwereins, Steven and Thoen, Bart and Adhana, Mulu Weldegebreal and Brouckxon, Henk and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marian and Karsmakers, Peter",
    title = "The {SINS} Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "32--36",
    abstract = "There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.",
    keywords = "Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="download-development-dataset">Download: Development dataset</h2>
<p><em>In case you are using the provided baseline system, there is no need to download the dataset as the system will automatically download needed dataset for you.</em></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/1247102" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/1247102" target="_blank">
<span style="font-size:20px;">Task 5, <strong>development dataset</strong> <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(42.6 GB download - 87.0 GB when extracted)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.1247102">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.1247102.svg"/>
</a>
</div>
</div>
<p class="bg-danger">
An inconsistency in the dataset was reported <a href="https://groups.google.com/forum/#!topic/dcase-discussions/iDh4M-RMy8U">here</a>. The issue is fixed in the current release of the dataset (v1.0.3, 15/05/2018). If you have an earlier version you can either download the entire dataset again or overwrite a subset of the files using <a href="https://www.dropbox.com/s/309gqi58549v7yu/task5_dataset_fix_20180514.zip?dl=0">this archive</a>
</p>
<p>The content of the development set is structured in the following manner:</p>
<div class="highlight"><pre><span></span><code><span class="n">dataset</span><span class="w"> </span><span class="n">root</span>
<span class="err">│</span><span class="w">   </span><span class="n">EULA</span><span class="p">.</span><span class="n">pdf</span><span class="w">                </span><span class="k">End</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="n">license</span><span class="w"> </span><span class="n">agreement</span>
<span class="err">│</span><span class="w">   </span><span class="n">meta</span><span class="p">.</span><span class="n">txt</span><span class="w">                </span><span class="n">meta</span><span class="w"> </span><span class="k">data</span><span class="p">,</span><span class="w"> </span><span class="n">tsv</span><span class="o">-</span><span class="nf">format</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">audio file (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">label (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">session (str)</span><span class="o">]</span>
<span class="err">│</span><span class="w">   </span><span class="n">README</span><span class="p">.</span><span class="n">md</span><span class="w">               </span><span class="n">Dataset</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="p">(</span><span class="n">markdown</span><span class="p">)</span>
<span class="err">│</span><span class="w">   </span><span class="n">README</span><span class="p">.</span><span class="n">html</span><span class="w">             </span><span class="n">Dataset</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="p">(</span><span class="n">HTML</span><span class="p">)</span>
<span class="err">│</span>
<span class="err">└───</span><span class="n">audio</span><span class="w">                   </span><span class="mi">72984</span><span class="w"> </span><span class="n">audio</span><span class="w"> </span><span class="n">segments</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="o">-</span><span class="nc">bit</span><span class="w"> </span><span class="mi">16</span><span class="n">kHz</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span><span class="w">   </span><span class="n">DevNode1_ex1_1</span><span class="p">.</span><span class="n">wav</span><span class="w">  </span><span class="n">name</span><span class="w"> </span><span class="nf">format</span><span class="w"> </span><span class="n">DevNode</span><span class="err">{</span><span class="n">NodeID</span><span class="err">}</span><span class="n">_ex</span><span class="err">{</span><span class="n">sessionID</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">segmentID</span><span class="err">}</span><span class="p">.</span><span class="n">wav</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span><span class="w">   </span><span class="n">DevNode2_ex1_2</span><span class="p">.</span><span class="n">wav</span>
<span class="err">│</span><span class="w">   </span><span class="err">│</span><span class="w">   </span><span class="p">...</span>
<span class="err">│</span>
<span class="err">└───</span><span class="n">evaluation_setup</span><span class="w">        </span><span class="k">cross</span><span class="o">-</span><span class="n">validation</span><span class="w"> </span><span class="n">setup</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="n">folds</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="n">fold1_train</span><span class="p">.</span><span class="n">txt</span><span class="w">     </span><span class="n">training</span><span class="w"> </span><span class="k">file</span><span class="w"> </span><span class="n">list</span><span class="p">,</span><span class="w"> </span><span class="n">tsv</span><span class="o">-</span><span class="nf">format</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">audio file (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">label (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">session (str)</span><span class="o">]</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="n">fold1_test</span><span class="p">.</span><span class="n">txt</span><span class="w">      </span><span class="n">test</span><span class="w"> </span><span class="k">file</span><span class="w"> </span><span class="n">list</span><span class="p">,</span><span class="w"> </span><span class="n">tsv</span><span class="o">-</span><span class="nf">format</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">audio file (str)</span><span class="o">]</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="n">fold1_evaluate</span><span class="p">.</span><span class="n">txt</span><span class="w">  </span><span class="n">evaluation</span><span class="w"> </span><span class="k">file</span><span class="w"> </span><span class="n">list</span><span class="p">,</span><span class="w"> </span><span class="n">tsv</span><span class="o">-</span><span class="nf">format</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">audio file (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">label (str)</span><span class="o">]</span><span class="w">  </span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="p">...</span><span class="w">        </span>
</code></pre></div>
<p>The multi-channel audio files can be found under directory <code>audio</code> and are formatted in the following manner:</p>
<div class="highlight"><pre><span></span><code>DevNode{NodeID}_ex{sessionID}_{segmentID}.wav
</code></pre></div>
<ul>
<li><em>{NodeID}</em> (1-4) is an identifier to indicate which segments belong to a specific node. In total 4 nodes are given (1-4). It is unknown what the location of the node is to the participant.</li>
<li><em>{sessionID}</em> indicates a full session of a certain activity.</li>
<li><em>{segmentID}</em> indicates a segment belonging to a certain {sessionID}. A session of a certain activity (e.g. cooking) can have multiple 10s segments. Keep in mind that segmentIDs are not shared between nodes (e.g. DevNode1_ex1_1 is not necessarely recorded at the same time range as DevNode2_ex1_1 but it surely belongs to the same session). </li>
</ul>
<p>The file <code>meta.txt</code> and the content of the folder <code>evaluation_setup</code> contain filenames with optionally ground truth labels and an identifier of to which session the segment belongs. These are arranged in the following manner:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">activity label (str)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">session (str)</span><span class="o">]</span>
</code></pre></div>
<p>The directory <code>evaluation_setup</code> provides cross-validation folds for the development dataset. More information on the usage can be read <a href="#task-setup">here</a></p>
<h2 id="download-evaluation-dataset">Download: Evaluation dataset</h2>
<p><em>In case you are using the provided baseline system, there is no need to download the dataset as the system will automatically download needed dataset for you.</em></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/1964758" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/1964758" target="_blank">
<span style="font-size:20px;">Task 5, <strong>evaluation dataset</strong> <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(42.2 GB download - 87.0 GB when extracted)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.1964758">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.1964758.svg"/>
</a>
<span class="text-muted">
                
                version 2.0
                
                
                </span>
</div>
</div>
<p>The content of the dataset is structured in the following manner:</p>
<div class="highlight"><pre><span></span><code>dataset root
│   EULA.pdf                End user license agreement
│   meta.txt                meta data, tsv-format, [audio file (str)]\n
│   readme.md               Dataset description (markdown)
│   readme.html             Dataset description (HTML)
│
└───audio                   72972 audio segments, 16-bit 16kHz
│   │   1.wav               name format {segmentID}.wav
│   │   100.wav
│   │   ...
│
└───evaluation_setup        evaluation files
    │   test.txt            test file list, tsv-format, [audio file (str)]\n
</code></pre></div>
<p>The multi-channel audio files can be found under directory <code>audio</code> and are formatted in the following manner:</p>
<div class="highlight"><pre><span></span><code>{segmentID}.wav
</code></pre></div>
<p>The file <code>meta.txt</code> and the content of the folder <code>evaluation_setup</code> contain filenames. Ground truth will be made available after the challenge results have been made public. Additionally, a filename mapping will be made available that will map the filenames to a filename similar as the development dataset.</p>
<h1 id="task-setup">Task setup</h1>
<p>The task is split up in two phases. First the development dataset is provided. A month before the challenge deadline the evaluation sets are provided. At the challenge deadline submissions include the system output on the evaluation set, system meta information and one technical report. The goal of the task to obtain the highest score on the evaluation set.</p>
<h2 id="development-set">Development set</h2>
<p>The development set includes multi-channel audio segments, recorded by four different sensor nodes, along with the ground truth and cross-validation folds.
Cross-validation folds are provided for the development dataset in order to make results reported with this dataset uniform. 
Results on these subsets are used for comparison in the initial internal report and also need to be reported in the outputted meta information. 
The setup consists of four folds distributing the available files. 
Segments belonging to a particular session of an activity (e.g. a session of cooking collected by multiple sensor nodes) are kept together to minimize leakage between folds. 
The folds are provided with the dataset in the directory <code>evaluation setup</code>. For each fold a training, testing and evaluation subset is provided. </p>
<p class="bg-danger">
<strong>Important:</strong> Important: If you are not using the provided cross-validation setup, pay attention to the segments extracted from the same sessions. Make sure that for each given fold, ALL segments from the same session must be either in the training subset OR in the test subset.
</p>
<h2 id="external-data-sourcespre-trained-models">External data sources/pre-trained models</h2>
<p>List of external datasets/pre-trained models allowed:</p>
<table class="datatable table table-hover table-condensed" data-filter-control="false" data-filter-show-clear="false" data-id-field="name" data-pagination="false" data-show-pagination-switch="false" data-sort-name="name" data-sort-order="asc">
<thead>
<tr>
<th data-field="name" data-sortable="true">Dataset name</th>
<th data-field="type" data-filter-control="select" data-sortable="true" data-tag="true">Type</th>
<th data-field="date" data-sortable="true">Added</th>
<th data-field="link" data-value-type="url">Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>AudioSet</td>
<td>audio</td>
<td>29.6.2018</td>
<td>https://research.google.com/audioset/</td>
</tr>
<tr>
<td>VGGish</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://research.google.com/audioset/</td>
</tr>
<tr>
<td>Xception</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://keras.io/applications/</td>
</tr>
<tr>
<td>VGG16</td>
<td>model</td>
<td>5.7.2018</td>
<td>http://www.robots.ox.ac.uk/~vgg/research/very_deep/</td>
</tr>
<tr>
<td>VGG19</td>
<td>model</td>
<td>5.7.2018</td>
<td>http://www.robots.ox.ac.uk/~vgg/research/very_deep/</td>
</tr>
<tr>
<td>ResNet50</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://github.com/KaimingHe/deep-residual-networks</td>
</tr>
<tr>
<td>InceptionV3</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://keras.io/applications/</td>
</tr>
<tr>
<td>InceptionResNetV2</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://keras.io/applications/</td>
</tr>
<tr>
<td>MobileNet</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://keras.io/applications/</td>
</tr>
<tr>
<td>DenseNet</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://keras.io/applications/</td>
</tr>
<tr>
<td>MobileNetV2</td>
<td>model</td>
<td>5.7.2018</td>
<td>https://keras.io/applications/</td>
</tr>
</tbody>
</table>
<p><br/></p>
<h2 id="evaluation-set">Evaluation set</h2>
<p>The evaluation set includes multi-channel audio segments, recorded by seven different sensor nodes. Three sensor nodes are not in the development set and will be used for the final evaluation score. The other segments obtained by the same nodes as in the development set are used to check overfitting. Participants should run their system on this dataset, and submit the classification results (system output) to DCASE2018 Challenge. The evaluation dataset is provided without ground truth. </p>
<h2 id="submission">Submission</h2>
<p>Challenge submissions consists of one zip-package containing the <strong>system outputs and system meta information</strong> and one <strong>technical report</strong> (pdf file). Detailed information for the challenge submission can found on the <a href="/challenge2018/submission">submission page</a>.
System output should be presented as a single text-file (in tab-seperated format) containing classification result for each audio file in the evaluation set. Result items can be in any order. The format is as follows:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">activity label (string)</span><span class="o">]</span>
</code></pre></div>
<p>The filename should be formatted such that it includes the <code>audio</code> folder. An example of the output file:</p>
<div class="highlight"><pre><span></span><code>audio/1.wav other
audio/2.wav social_activity
audio/3.wav eating
audio/4.wav working
audio/5.wav absence
audio/6.wav vacuum_cleaner
audio/7.wav dishwashing
audio/8.wav watching_tv
audio/9.wav cooking
</code></pre></div>
<p>A template for the system meta information (.yaml file) is available on the submissions page. </p>
<p>Multiple system outputs can be submitted (maximum 4 per participant). If submitting multiple systems, the individual text-files should be packaged into a zip file for submission. Please carefully mark the connection between the submitted files and the corresponding system or system parameters (for example by naming the text file appropriately).  </p>
<h1 id="task-rules">Task rules</h1>
<p>These are the general rules valid for all tasks. The same rules and additional information on technical report and submission requirements can be found <a href="/challenge2018/rules">here</a>. Task specific rules are highlighted in bold.</p>
<ul>
<li><strong>Participants are allowed to use external data for system development taking into account the following principles:</strong><ul>
<li>The used data must be publicly available without cost before 29th of March 2018</li>
<li>External data includes pre-trained models</li>
<li>Participants should inform/suggest such data to be listed on the task webpage, so all competitors know about them and have equal opportunity to use them</li>
<li>Once the evaluation set is published, the list of external datasets allowed is locked (no further external sources allowed)</li>
</ul>
</li>
<li><strong>Manipulation of provided training data is allowed (e.g. data augmentation).</strong></li>
<li>Participants are not allowed to make subjective judgements on the evaluation data, nor to annotate it. </li>
<li>The evaluation dataset cannot be used to train the submitted system; the use of statistics about the evaluation data in the decision making is also forbidden.</li>
<li>The system outputs that do not respect the challenge rules will be evaluated on request, but they will not be officially included in the challenge rankings.</li>
</ul>
<h1 id="evaluation">Evaluation</h1>
<p>The scoring of this task will be based on <strong>macro-averaged F1-score</strong>. The F1-score is calculated for each class seperately and averaged over all classes. A full 10s multi-channel audio segment is considered to be one sample. The winner is of the task is the submission with the highest macro-averaged F1-score on the evaluation set. The output is send through and the scores are calculated by the task coordinators. </p>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="10" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f1_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="1"></th>
</tr>
<tr>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="F1-score (Eval. set | unknown mic.)" data-chartable="true" data-field="f1_eval" data-sortable="true" data-value-type="float1-percentage">
                F1-score <br/>on Eval. set <br/>(Unknown mic.)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Gert Dekkers</td>
<td>Computer Science, KU Leuven - ADVISE, Geel, Belgium</td>
<td>task-monitoring-domestic-activities-results#Dekkers2018</td>
<td>83.1</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task5_1</td>
<td>Lionel Delphin-Poulat</td>
<td>HOME/CONTENT, Orange Labs, Lannion, France</td>
<td>task-monitoring-domestic-activities-results#Delphin-Poulat2018</td>
<td>80.7</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task5_2</td>
<td>Lionel Delphin-Poulat</td>
<td>HOME/CONTENT, Orange Labs, Lannion, France</td>
<td>task-monitoring-domestic-activities-results#Delphin-Poulat2018</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task5_3</td>
<td>Lionel Delphin-Poulat</td>
<td>HOME/CONTENT, Orange Labs, Lannion, France</td>
<td>task-monitoring-domestic-activities-results#Delphin-Poulat2018</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task5_4</td>
<td>Lionel Delphin-Poulat</td>
<td>HOME/CONTENT, Orange Labs, Lannion, France</td>
<td>task-monitoring-domestic-activities-results#Delphin-Poulat2018</td>
<td>82.5</td>
</tr>
<tr>
<td></td>
<td>Inoue_IBM_task5_1</td>
<td>Tadanobu Inoue</td>
<td>AI, IBM Research, Tokyo, Japan</td>
<td>task-monitoring-domestic-activities-results#Inoue2018</td>
<td>88.4</td>
</tr>
<tr>
<td></td>
<td>Inoue_IBM_task5_2</td>
<td>Tadanobu Inoue</td>
<td>AI, Research, Tokyo, Japan</td>
<td>task-monitoring-domestic-activities-results#Inoue2018</td>
<td>88.3</td>
</tr>
<tr>
<td></td>
<td>Kong_Surrey_task5_1</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK</td>
<td>task-monitoring-domestic-activities-results#Kong2018</td>
<td>83.2</td>
</tr>
<tr>
<td></td>
<td>Kong_Surrey_task5_2</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK</td>
<td>task-monitoring-domestic-activities-results#Kong2018</td>
<td>82.4</td>
</tr>
<tr>
<td></td>
<td>Li_NPU_task5_1</td>
<td>Dexin Li</td>
<td>Speech Signal Processing, CIAIC, Xi'an, China</td>
<td>task-monitoring-domestic-activities-results#Li2018</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Li_NPU_task5_2</td>
<td>Dexin Li</td>
<td>Speech Signal Processing, CIAIC, Xi'an, China</td>
<td>task-monitoring-domestic-activities-results#Li2018</td>
<td>78.6</td>
</tr>
<tr>
<td></td>
<td>Li_NPU_task5_3</td>
<td>Dexin Li</td>
<td>Speech Signal Processing, CIAIC, Xi'an, China</td>
<td>task-monitoring-domestic-activities-results#Li2018</td>
<td>84.8</td>
</tr>
<tr>
<td></td>
<td>Li_NPU_task5_4</td>
<td>Dexin Li</td>
<td>Speech Signal Processing, CIAIC, Xi'an, China</td>
<td>task-monitoring-domestic-activities-results#Li2018</td>
<td>85.1</td>
</tr>
<tr>
<td></td>
<td>Liao_NTHU_task5_1</td>
<td>Hsueh-Wei Liao</td>
<td>Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan</td>
<td>task-monitoring-domestic-activities-results#Liao2018</td>
<td>86.7</td>
</tr>
<tr>
<td></td>
<td>Liao_NTHU_task5_2</td>
<td>Hsueh-Wei Liao</td>
<td>Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan</td>
<td>task-monitoring-domestic-activities-results#Liao2018</td>
<td>72.1</td>
</tr>
<tr>
<td></td>
<td>Liao_NTHU_task5_3</td>
<td>Hsueh-Wei Liao</td>
<td>Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan</td>
<td>task-monitoring-domestic-activities-results#Liao2018</td>
<td>76.7</td>
</tr>
<tr>
<td></td>
<td>Liu_THU_task5_1</td>
<td>Huaping Liu</td>
<td>Computer Science and Technology, Computer Science and Technology, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Liu2018</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>Liu_THU_task5_2</td>
<td>Huaping Liu</td>
<td>Computer Science and Technology, Computer Science and Technology, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Liu2018</td>
<td>87.4</td>
</tr>
<tr>
<td></td>
<td>Liu_THU_task5_3</td>
<td>Huaping Liu</td>
<td>Computer Science and Technology, Computer Science and Technology, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Liu2018</td>
<td>86.8</td>
</tr>
<tr>
<td></td>
<td>Nakadai_HRI-JP_task5_1</td>
<td>Kazuhiro Nakadai</td>
<td>Research Div., Honda Research Institute Japan, Wako, Japan</td>
<td>task-monitoring-domestic-activities-results#Nakadai2018</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>Raveh_INRC_task5_1</td>
<td>Alon Raveh</td>
<td>Signal Processing Department, National Research Center, Haifa, Israel</td>
<td>task-monitoring-domestic-activities-results#Raveh2018</td>
<td>80.4</td>
</tr>
<tr>
<td></td>
<td>Raveh_INRC_task5_2</td>
<td>Alon Raveh</td>
<td>Signal Processing Department, National Research Center, Haifa, Israel</td>
<td>task-monitoring-domestic-activities-results#Raveh2018</td>
<td>80.2</td>
</tr>
<tr>
<td></td>
<td>Raveh_INRC_task5_3</td>
<td>Alon Raveh</td>
<td>Signal Processing Department, National Research Center, Haifa, Israel</td>
<td>task-monitoring-domestic-activities-results#Raveh2018</td>
<td>81.7</td>
</tr>
<tr>
<td></td>
<td>Raveh_INRC_task5_4</td>
<td>Alon Raveh</td>
<td>Signal Processing Department, National Research Center, Haifa, Israel</td>
<td>task-monitoring-domestic-activities-results#Raveh2018</td>
<td>81.2</td>
</tr>
<tr>
<td></td>
<td>Sun_SUTD_task5_1</td>
<td>Yingxiang Sun</td>
<td>Engineering Product Development, Singapore University of Technology and Design, Singapore</td>
<td>task-monitoring-domestic-activities-results#Chew2018</td>
<td>76.8</td>
</tr>
<tr>
<td></td>
<td>Tanabe_HIT_task5_1</td>
<td>Ryo Tanabe</td>
<td>R&amp;D Group, Hitachi, Ltd., Tokyo, Japan</td>
<td>task-monitoring-domestic-activities-results#Tanabe2018</td>
<td>88.4</td>
</tr>
<tr>
<td></td>
<td>Tanabe_HIT_task5_2</td>
<td>Ryo Tanabe</td>
<td>R&amp;D Group, Hitachi, Ltd., Tokyo, Japan</td>
<td>task-monitoring-domestic-activities-results#Tanabe2018</td>
<td>82.2</td>
</tr>
<tr>
<td></td>
<td>Tanabe_HIT_task5_3</td>
<td>Ryo Tanabe</td>
<td>R&amp;D Group, Hitachi, Ltd., Tokyo, Japan</td>
<td>task-monitoring-domestic-activities-results#Tanabe2018</td>
<td>86.3</td>
</tr>
<tr>
<td></td>
<td>Tanabe_HIT_task5_4</td>
<td>Ryo Tanabe</td>
<td>R&amp;D Group, Hitachi, Ltd., Tokyo, Japan</td>
<td>task-monitoring-domestic-activities-results#Tanabe2018</td>
<td>88.4</td>
</tr>
<tr>
<td></td>
<td>Tiraboschi_UNIMI_task5_1</td>
<td>Marco Tiraboschi</td>
<td>Computer Science, UniversitÃ  degli Studi di Milano, Milan, Italy</td>
<td>task-monitoring-domestic-activities-results#Tiraboschi2018</td>
<td>76.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_THU_task5_1</td>
<td>Weiqiang Zhang</td>
<td>Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Shen2018</td>
<td>85.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_THU_task5_2</td>
<td>Weiqiang Zhang</td>
<td>Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Shen2018</td>
<td>84.3</td>
</tr>
<tr>
<td></td>
<td>Zhang_THU_task5_3</td>
<td>Weiqiang Zhang</td>
<td>Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Shen2018</td>
<td>86.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_THU_task5_4</td>
<td>Weiqiang Zhang</td>
<td>Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-monitoring-domestic-activities-results#Shen2018</td>
<td>85.9</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2018/task-monitoring-domestic-activities-results">results page</a></p>
<h1 id="baseline-system">Baseline system</h1>
<h2 id="setup">Setup</h2>
<p>The baseline system is intended to lower the hurdle to participate the challenge. It provides an entry-level approach which is simple but relatively close to the state of the art systems. High-end performance is left for the challenge participants to find. Participants are allowed to build their system on top of the given baseline system. The system has all needed functionality for dataset handling, storing / accessing features and models, and evaluating the results, making the adaptation for one's needs rather easy. The baseline system is also a good starting point for entry level researchers.</p>
<p>If participants plan to publish their code to the DCASE community after the challenge, by building their approach on the baseline system will make their code more accessible to the community. DCASE organizers encourage participants strongly to share their code in any form after the challenge to push the research further.</p>
<p>During the recording campaign, data was measured simultaneously using multiple microphone arrays (nodes) each containing 4 microphones.  Hence, each domestic activity is recorded as many times as there were microphones.  The baseline system trains a single classifier model that takes a single channel as input.  Each parallel recording of a single activity is considered as a different example during training. The learner in the baseline system is based on a Neural Network architecture using convolutional and dense layers. As input, log mel-band energies are provided to the network for each microphone channel separately. In the prediction stage a single outcome is computed for each node by averaging the 4 model outcomes (posteriors) that were computed by evaluating the trained classifier model on all 4 microphones.</p>
<p>The baseline system parameters are as follows:</p>
<ul>
<li>Frame size: 40 ms (50% hop size)</li>
<li>Feature matrix: <ul>
<li>40 log mel-band energies in 501 successive frames (10 s)</li>
</ul>
</li>
<li>Neural Network:<ul>
<li>Input data: 40x501 (each microphone channel is considered to be a separate example for the learner)</li>
<li>Architecture:<ul>
<li>1D Convolutional layer (filters: 32, kernel size: 5, stride: 1, axis: time) + Batch Normalization + ReLU activation</li>
<li>1D Max Pooling (pool size: 5, stride: 5) + Dropout (rate: 20%)</li>
<li>1D Convolutional layer (filters, 64, kernel size: 3, stride: 1, axis: time) + Batch Normalization + ReLU activation</li>
<li>1D Global Max Pooling + Dropout (rate: 20%)</li>
<li>Dense layer (neurons: 64) + ReLU activation + Dropout (rate: 20%)</li>
<li>Softmax output layer (classes: 9)</li>
</ul>
</li>
<li>Learning:<ul>
<li>Optimizer: Adam (learning rate: 0.0001)</li>
<li>Epochs: 500</li>
<li>On each epoch, the training dataset is randomly subsampled so that the number of examples for each class match the size of the smallest class</li>
<li>Batch size: 256 * 4 channels (each channel is considered as a different example for the learner)</li>
</ul>
</li>
</ul>
</li>
<li>Fusion: Output probabilities from the four microphones in a particular node under test are averaged to obtain the final posterior probability.</li>
<li>Model selection: The performance of the model is evaluated every 10 epochs on a validation subset (30% subsampled from the training set). The model with the highest Macro-averaged F1-score is picked.</li>
</ul>
<p>The baseline system is build on <a href="https://github.com/DCASE-REPO/dcase_util">dcase_util</a> toolbox. The machine learning part of the code in build on <a href="https://keras.io/">Keras (v2.1.5)</a> while using <a href="https://www.tensorflow.org/">TensorFlow (v1.4.0)</a> as backend.</p>
<h2 id="repository">Repository</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task5" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task5" target="_blank">
<span style="font-size:20px;">DCASE2018 Task 5 <strong>Baseline</strong> <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p class="bg-danger">
An inconsistency in the dataset was reported <a href="https://groups.google.com/forum/#!topic/dcase-discussions/iDh4M-RMy8U">here</a>. The issue is fixed in the current release of the dataset (v1.0.3, 15/05/2018). The new repository is updated on the latest release of the [dcase_util library (v0.2.3)](https://github.com/DCASE-REPO/dcase_util). Using an older version will download an older version of the dataset. If you prefer to not download all files again, you can overwrite a subset of the files using <a href="https://www.dropbox.com/s/309gqi58549v7yu/task5_dataset_fix_20180514.zip?dl=0">this archive</a>.
</p>
<h2 id="results-for-the-development-dataset">Results for the development dataset</h2>
<p>When running the code in development mode the baseline system provides results for the 4-fold cross-validation setup. The table below shows the averaged <code>Macro-averaged F1-score</code> over these 4 folds. </p>
<div class="table-responsive col-md-6">
<table class="table table-striped">
<thead>
<tr>
<th>Activity</th>
<th class="col-md-3">F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Absence</td>
<td>85.41 %</td>
</tr>
<tr>
<td>Cooking</td>
<td>95.14 %</td>
</tr>
<tr>
<td>Dishwashing</td>
<td>76.73 %</td>
</tr>
<tr>
<td>Eating</td>
<td>83.64 %</td>
</tr>
<tr>
<td>Other</td>
<td>44.76 %</td>
</tr>
<tr>
<td>Social activity</td>
<td>93.92 %</td>
</tr>
<tr>
<td>Vacuum cleaning</td>
<td>99.31 %</td>
</tr>
<tr>
<td>Watching TV</td>
<td>99.59 %</td>
</tr>
<tr>
<td>Working</td>
<td>82.03 %</td>
</tr>
</tbody>
<tfoot>
<tr>
<td><strong>Macro-averaged F1-score</strong></td>
<td><strong>84.50 %</strong></td>
</tr>
</tfoot>
</table>
</div>
<div class="clearfix"></div>
<p><strong>Note:</strong> The performance might not be exactly reproducible but similar results should be obtainable.</p>
<h1 id="citation">Citation</h1>
<p>If you are using the <strong>dataset</strong> please cite the following paper:</p>
<div class="btex-item" data-item="Dekkers2017" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Dekkers2017"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Gert Dekkers, Steven Lauwereins, Bart Thoen, Mulu Weldegebreal Adhana, Henk Brouckxon, Toon van Waterschoot, Bart Vanrumste, Marian Verhelst, and Peter Karsmakers.
<em>The SINS database for detection of daily activities in a home environment using an acoustic sensor network.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), 32–36. November 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDekkers20178cde3cc233424b1abd70d0eeba0f2550" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://dcase.community/documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDekkers20178cde3cc233424b1abd70d0eeba0f2550" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDekkers20178cde3cc233424b1abd70d0eeba0f2550" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDekkers20178cde3cc233424b1abd70d0eeba0f2550" class="panel-collapse collapse" id="collapseDekkers20178cde3cc233424b1abd70d0eeba0f2550" role="tabpanel">
<h4>The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
<h5>Abstract</h5>
<p class="text-justify">There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.</p>
<h5>Keywords</h5>
<p class="text-justify">Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDekkers20178cde3cc233424b1abd70d0eeba0f2550" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://dcase.community/documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDekkers20178cde3cc233424b1abd70d0eeba0f2550label" class="modal fade" id="bibtexDekkers20178cde3cc233424b1abd70d0eeba0f2550" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDekkers20178cde3cc233424b1abd70d0eeba0f2550label">The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dekkers2017,
    Author = "Dekkers, Gert and Lauwereins, Steven and Thoen, Bart and Adhana, Mulu Weldegebreal and Brouckxon, Henk and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marian and Karsmakers, Peter",
    title = "The {SINS} Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "32--36",
    abstract = "There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.",
    keywords = "Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>If you are using the <strong>baseline</strong> code, or want to refer <strong>challenge task</strong> please cite the following papers:</p>
<div class="btex-item" data-item="Dekkers2018_DCASE" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Dekkers2018_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Gert Dekkers, Lode Vuegen, Toon van Waterschoot, Bart Vanrumste, and Peter Karsmakers.
<em>DCASE 2018 Challenge - Task 5: Monitoring of domestic activities based on multi-channel acoustics.</em>
Technical Report, KU Leuven, 2018.
URL: <a href="https://arxiv.org/abs/1807.11246">https://arxiv.org/abs/1807.11246</a>, <a href="https://arxiv.org/abs/1807.11246">arXiv:1807.11246</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/abs/1807.11246" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" class="panel-collapse collapse" id="collapseDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" role="tabpanel">
<h4>DCASE 2018 Challenge - Task 5: Monitoring of domestic activities based on multi-channel acoustics</h4>
<h5>Abstract</h5>
<p class="text-justify">The DCASE 2018 Challenge consists of five tasks related to automatic classification and detection of sound events and scenes. This paper presents the setup of Task 5 which includes the description of the task, dataset and the baseline system. In this task, it is investigated to which extend multi-channel acoustic recordings are beneficial for the purpose of classifying domestic activities. The goal is to exploit spectral and spatial cues independent of sensor location using multi-channel audio. For this purpose we provided a development and evaluation dataset which are derivatives of the SINS database and contain domestic activities recorded by multiple microphone arrays. The baseline system, based on a Neural Network architecture using convolutional and dense layer(s), is intended to lower the hurdle to participate the challenge and to provide a reference performance.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic scene classification, Multi-channel, Activities of the Daily Living</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/abs/1807.11246" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5label" class="modal fade" id="bibtexDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDekkers2018_DCASE72c40e7e5d6d4be4be8d6881be6fbad5label">DCASE 2018 Challenge - Task 5: Monitoring of domestic activities based on multi-channel acoustics</h4>
</div>
<div class="modal-body">
<pre>@techreport{Dekkers2018_DCASE,
    Author = "Dekkers, Gert and Vuegen, Lode and van Waterschoot, Toon and Vanrumste, Bart and Karsmakers, Peter",
    title = "{DCASE 2018 Challenge - Task 5: Monitoring of domestic activities based on multi-channel acoustics}",
    archiveprefix = "arXiv",
    eprint = "1807.11246",
    year = "2018",
    institution = "KU Leuven",
    url = "https://arxiv.org/abs/1807.11246",
    keywords = "Acoustic scene classification, Multi-channel, Activities of the Daily Living",
    abstract = "The DCASE 2018 Challenge consists of five tasks related to automatic classification and detection of sound events and scenes. This paper presents the setup of Task 5 which includes the description of the task, dataset and the baseline system. In this task, it is investigated to which extend multi-channel acoustic recordings are beneficial for the purpose of classifying domestic activities. The goal is to exploit spectral and spatial cues independent of sensor location using multi-channel audio. For this purpose we provided a development and evaluation dataset which are derivatives of the SINS database and contain domestic activities recorded by multiple microphone arrays. The baseline system, based on a Neural Network architecture using convolutional and dense layer(s), is intended to lower the hurdle to participate the challenge and to provide a reference performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/>
<br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>