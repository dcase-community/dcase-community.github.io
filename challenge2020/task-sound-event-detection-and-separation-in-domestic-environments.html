<!DOCTYPE html><html lang="en">
<head>
    <title>Sound event detection and separation in domestic environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled (with time stamps). Challenge has ended. Full results for this task can be found in the Results page. Description This …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2020</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2020/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2020/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2020/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2020/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/roof-tiles-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-domestic fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Domestic</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2020_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound event detection and separation in domestic environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Romain Serizel" class="img img-circle" src="/images/person/romain_serizel.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Romain Serizel</strong>
<a class="icon" href="mailto:romain.serizel@loria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.loria.fr/en/">
                                University of Lorraine
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Nicolas Turpault" class="img img-circle" src="/images/person/nicolas_turpault.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Nicolas Turpault</strong>
<a class="icon" href="mailto:nicolas.turpault@inria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.inria.fr/">
                                Inria Nancy Grand-Est
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="John Hershey" class="img img-circle" src="/images/person/john_hershey.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>John Hershey</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.google.com/">
                                Google, Inc.
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Scott Wisdom" class="img img-circle" src="/images/person/scott_wisdom.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Scott Wisdom</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.google.com/">
                                Google, Inc.
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Hakan Erdogan" class="img img-circle" src="/images/person/hakan_erdogan.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Hakan Erdogan</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.google.com/">
                                Google, Inc.
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Justin Salamon" class="img img-circle" src="/images/person/justin_salamon.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Justin Salamon</strong>
<a class="icon" href="mailto:justin.salamon@nyu.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.adobe.com/">
                                Adobe Research
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Ankit Parag Shah" class="img img-circle" src="/images/person/ankit_parag_shah.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Ankit Parag Shah</strong>
<a class="icon" href="mailto:aps1@andrew.cmu.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.cmu.edu/">
                                Carnegie Mellon University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Daniel P. W. Ellis" class="img img-circle" src="/images/person/dan_ellis.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Daniel P. W. Ellis</strong>
<a class="icon" href="mailto:dpwe@google.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.google.com/">
                                Google, Inc.
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Prem Seetharaman" class="img img-circle" src="/images/person/prem_seetharaman.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Prem Seetharaman</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.northwestern.edu/">
                                Northwestern University
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#audio-material">Audio material</a></li>
<li><a href="#reference-labels">Reference labels</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-datasets">Evaluation datasets</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#evaluation">Evaluation</a>
<ul>
<li><a href="#sound-event-detection-evaluation">Sound event detection evaluation</a></li>
<li><a href="#source-separation-evaluation-optional">Source separation evaluation (optional)</a></li>
</ul>
</li>
<li><a href="#results">Results</a></li>
<li><a href="#baselines">Baselines</a>
<ul>
<li><a href="#sound-event-detection-baseline">Sound event detection baseline</a></li>
<li><a href="#sound-separation-baseline">Sound separation baseline</a></li>
<li><a href="#sound-event-detection-and-separation-baseline">Sound event detection and separation baseline</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a>
<ul>
<li><a href="#task-and-datasets">Task and datasets</a></li>
<li><a href="#baselines-1">Baselines</a></li>
</ul>
</li></ul></div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled (with time stamps). </p>
<p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found in the <a class="btn btn-default btn-xs" href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments-results">Results <i class="fa fa-caret-right"></i></a> page.
</p>
<h1 id="description">Description</h1>
<p>This task is the follow-up to <a href="http://dcase.community/challenge2019/task-sound-event-detection-in-domestic-environments">DCASE 2019 task 4</a>.
The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps).
The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong>
given that multiple events can be present in an audio recording (see also <a href="#fig1">Fig 1</a>).
The challenge of exploring the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong>
together with a small weakly annotated training set to improve system performance remains. Isolated sound events,
background sound files and scripst to design a <strong>training set with strongly annotated synthetic data</strong> are provided.
<strong>The labels in all the annotated subsets are verified and can be considered as reliable.</strong></p>
<figure name="fig1">
<div class="row row-centered">
<div class="col-xs-10 col-md-6 col-centered">
<img class="img img-responsive" src="../images/tasks/challenge2020/task4_sound_event_detection.png"/>
<figcaption>Figure 1: Overview of a sound event detection system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>This year, we also encourage participants to propose systems that use sound separation jointly with sound event detection.
This step can be used to <strong>separate overlapping sound events</strong> and <strong>extract foreground sound events from the background sound</strong>.
To motivate participants to explore that direction, we provide a baseline <strong>sound separation model</strong> that can be used for pre-processing (see also <a href="#fig2">Fig 2</a>).</p>
<figure name="fig2">
<div class="row row-centered">
<div class="col-xs-10 col-md-6 col-centered">
<img class="img img-responsive" src="../images/tasks/challenge2020/task4_sound_event_detection_and_separation.png"/>
<figcaption>Figure 2: Example of a sound event detection system with a sound separation pre-processing.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>Compared to previous years, this task aims to investigate how we can optimally exploit synthetic data.
An additional scientific question is to what extent can sound separation improve sound event detection, and vice-versa?</p>
<h1 id="audio-dataset">Audio dataset</h1>
<p>The data for the DCASE 2020 task 4 consist of several datasets designed for sound event detection and/or sound separation.
The datasets are described below.</p>
<h2 id="audio-material">Audio material</h2>
<table class="table table-striped">
<thead>
<tr>
<th>Dataset</th>
<th>Subset</th>
<th>Type</th>
<th>Usage</th>
<th>Annotations</th>
<th>Sampling frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="6" valign="middle">DESED</td>
<td>Real: weakly labeled</td>
<td>Real soundscapes</td>
<td>Training</td>
<td>Weak labels (no timestamps)</td>
<td>44.1kHz</td>
</tr>
<tr>
<td>Real: unlabeled</td>
<td>Real soundscapes</td>
<td>Training</td>
<td>No annotations</td>
<td>44.1kHz</td>
</tr>
<tr>
<td>Real: validation</td>
<td>Real soundscapes</td>
<td>Validation</td>
<td>Strong labels (with timestamps)</td>
<td>44.1kHz</td>
</tr>
<tr>
<td>Real: public evaluation</td>
<td>Real soundscapes</td>
<td>Evaluation **(do not use this subset to tune hyperparamters)**</td>
<td>Strong labels (with timestamps)</td>
<td>44.1kHz</td>
</tr>
<tr>
<td>Synthetic: training</td>
<td>Isolated events + synthetic soundscapes</td>
<td>Training/validation</td>
<td>Strong labels (with timestamps)</td>
<td>16kHz</td>
</tr>
<tr>
<td>Synthetic: evaluation</td>
<td>Isolated events + backgrounds</td>
<td>Evaluation **(do not use this subset to tune hyperparamters)**</td>
<td>Event level labels (no timestamps)</td>
<td>16kHz</td>
</tr>
<tr>
<td colspan="2">SINS</td>
<td>Background</td>
<td>Training/validation</td>
<td>No annotations</td>
<td>16kHz</td>
</tr>
<tr>
<td colspan="2">TUT Acoustic scenes 2017, development dataset</td>
<td>Background</td>
<td>Training/validation</td>
<td>No annotations</td>
<td>44.1kHz</td>
</tr>
<tr>
<td colspan="2">Source separation dataset</td>
<td>Isolated events + synthetic soundscapes</td>
<td>Training/validation</td>
<td>No annotations</td>
<td>16kHz</td>
</tr>
</tbody>
</table>
<p class="alert bg-danger">
    If you plan to perform source separation (or to use backgrounds from the SINS dataset) please resample your recorded data in <strong>16kHz</strong>. If you are using only recorded data and perform only sound event detection you can use sampling rates up to <strong>44.1kHz</strong>.<br/>
    Please note that the <strong>baselines work on 16kHz</strong> data.
</p>
<h3>DESED dataset</h3>
<p>DESED dataset is the dataset that was used in <a href="http://dcase.community/challenge2019/task-sound-event-detection-in-domestic-environments">DCASE 2019 task 4</a>.
The dataset for this task is composed of 10 sec audio clips recorded in domestic environment or synthesized using <a href="https://github.com/justinsalamon/scaper">Scaper</a> to simulate a domestic environment.
The task focuses on 10 class of sound events that represent a subset of <a href="https://research.google.com/audioset/">Audioset</a>
(not all the classes are present in Audioset, some classes of sound events are including several classes from Audioset):</p>
<ul>
<li>Speech <code>Speech</code></li>
<li>Dog <code>Dog</code></li>
<li>Cat <code>Cat</code></li>
<li>Alarm/bell/ringing <code>Alarm_bell_ringing</code></li>
<li>Dishes <code>Dishes</code></li>
<li>Frying <code>Frying</code></li>
<li>Blender <code>Blender</code></li>
<li>Running water <code>Running_water</code></li>
<li>Vacuum cleaner <code>Vacuum_cleaner</code></li>
<li>Electric shaver/toothbrush <code>Electric_shaver_toothbrush</code></li>
</ul>
<p>More information about this dataset and how to generate synthetic soundscapes can be found on <a href="https://project.inria.fr/desed/">DESED website</a>.</p>
<div class="btex-item" data-item="Turpault2019_DCASE" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2019_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault, Romain Serizel, Ankit Parag Shah, and Justin Salamon.
<em>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis.</em>
In <span class="bibtex-protected">Workshop on Detection and Classification of Acoustic Scenes and Events</span>. New York City, United States, October 2019.
URL: <a href="https://hal.inria.fr/hal-02160855">https://hal.inria.fr/hal-02160855</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" class="panel-collapse collapse" id="collapseTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" role="tabpanel">
<h4>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10% points in terms of F-measure.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945clabel" class="modal fade" id="bibtexTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945c" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2019_DCASE20b1c0cb5cd342bd9601947ea373945clabel">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Turpault2019_DCASE,
    Author = "Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in domestic environments with weakly labeled data and soundscape synthesis}",
    booktitle = "{Workshop on Detection and Classification of Acoustic Scenes and Events}",
    address = "New York City, United States",
    year = "2019",
    month = "October",
    keywords = "Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02160855",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02160855"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Serizel2020_ICASSP" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Serizel2020_ICASSP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Romain Serizel, Nicolas Turpault, Ankit Shah, and Justin Salamon.
<em>Sound event detection in synthetic domestic environments.</em>
In <span class="bibtex-protected">ICASSP 2020 - 45th International Conference on Acoustics, Speech, and Signal Processing</span>. Barcelona, Spain, 2020.
URL: <a href="https://hal.inria.fr/hal-02355573">https://hal.inria.fr/hal-02355573</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02355573/file/Sound_event_detection_in_domestic_environments_on_synthetic_soundscapes.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" class="panel-collapse collapse" id="collapseSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" role="tabpanel">
<h4>Sound event detection in synthetic domestic environments</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10% points in terms of F-measure.</p>
<h5>Keywords</h5>
<p class="text-justify">semi-supervised learning ; weakly labeled data ; synthetic data ; Sound event detection ; Index Terms-Sound event detection</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02355573/file/Sound_event_detection_in_domestic_environments_on_synthetic_soundscapes.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879eclabel" class="modal fade" id="bibtexSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879ec" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexSerizel2020_ICASSP332fdd8d15d941b7821b5b7c319879eclabel">Sound event detection in synthetic domestic environments</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Serizel2020_ICASSP,
    Author = "Serizel, Romain and Turpault, Nicolas and Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in synthetic domestic environments}",
    booktitle = "{ICASSP 2020 - 45th International Conference on Acoustics, Speech, and Signal Processing}",
    address = "Barcelona, Spain",
    year = "2020",
    keywords = "semi-supervised learning ; weakly labeled data ; synthetic data ; Sound event detection ; Index Terms-Sound event detection",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02355573",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02355573"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h3>Sound separation dataset (FUSS)</h3>
<p>The Free Universal Sound Separation (FUSS) Dataset is a database of arbitrary sound mixtures and source-level references, for use in experiments on arbitrary sound separation.</p>
<p><strong>Overview:</strong>
The audio data is sourced from a subset of a prerelease of FSD50k, a sound event dataset composed of Freesound content annotated with labels from the AudioSet Ontology.
Using the FSD50k labels, these sound source files have been screened such that they likely only contain a single type of sound.
Labels are not provided for these sound source files, and are not considered part of the challenge.
Thus, official challenge results should not use FSD50k labels, even though they may become available upon FSD50k release.
To create mixtures, 10 second clips of sounds are convolved with simulated room impulse responses and added together.
Each 10 second mixture contains between 1 and 4 sounds. Sound source files longer than 10s are considered "background" sources. Every mixture contains one background source, which is active for the entire duration.
We provide: a software recipe to create the dataset, the room impulse responses, and access to the original source audio.</p>
<div class="btex-item" data-item="fonseca2020fsd50k" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="fonseca2020fsd50k"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra.
<em>FSD50K: an open dataset of human-labeled sound events.</em>
In arXiv:2010.00475. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexfonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2010.00475.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsefonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsefonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingfonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" class="panel-collapse collapse" id="collapsefonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" role="tabpanel">
<h4>FSD50K: an Open Dataset of Human-Labeled Sound Events</h4>
<h5>Abstract</h5>
<p class="text-justify">Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexfonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2010.00475.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexfonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7label" class="modal fade" id="bibtexfonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexfonseca2020fsd50k9e37f0a309ce429e90e50e6cd94b77f7label">FSD50K: an Open Dataset of Human-Labeled Sound Events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{fonseca2020fsd50k,
    author = "Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier",
    title = "{FSD50K}: an Open Dataset of Human-Labeled Sound Events",
    year = "2020",
    booktitle = "arXiv:2010.00475",
    abstract = "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="font2013freesound" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="font2013freesound"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Frederic Font, Gerard Roma, and Xavier Serra.
<em>Freesound technical demo.</em>
In Proceedings of the 21st ACM international conference on Multimedia, 411–412. ACM, 2013.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexfont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://mtg.upf.edu/system/files/publications/Font-Roma-Serra-ACMM-2013.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsefont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsefont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingfont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" class="panel-collapse collapse" id="collapsefont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" role="tabpanel">
<h4>Freesound technical demo</h4>
<h5>Abstract</h5>
<p class="text-justify">Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexfont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://mtg.upf.edu/system/files/publications/Font-Roma-Serra-ACMM-2013.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexfont2013freesound3c07365efab6447aa1b5e4882cd4e8b8label" class="modal fade" id="bibtexfont2013freesound3c07365efab6447aa1b5e4882cd4e8b8" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexfont2013freesound3c07365efab6447aa1b5e4882cd4e8b8label">Freesound technical demo</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{font2013freesound,
    author = "Font, Frederic and Roma, Gerard and Serra, Xavier",
    title = "Freesound technical demo",
    booktitle = "Proceedings of the 21st ACM international conference on Multimedia",
    pages = "411--412",
    year = "2013",
    organization = "ACM",
    abstract = "Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Wisdom_InPrep2020" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Wisdom_InPrep2020"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Scott Wisdom, Hakan Erdogan, Daniel P. W. Ellis, Romain Serizel, Nicolas Turpault, Eduardo Fonseca, Justin Salamon, Prem Seetharaman, and John R. Hershey.
<em>What's all the fuss about free universal sound separation data?</em>
In in preparation. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexWisdom_InPrep2020c3123a084048450e86aee11b95038b64" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseWisdom_InPrep2020c3123a084048450e86aee11b95038b64" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseWisdom_InPrep2020c3123a084048450e86aee11b95038b64" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingWisdom_InPrep2020c3123a084048450e86aee11b95038b64" class="panel-collapse collapse" id="collapseWisdom_InPrep2020c3123a084048450e86aee11b95038b64" role="tabpanel">
<h4>What's All the FUSS About Free Universal Sound Separation Data?</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexWisdom_InPrep2020c3123a084048450e86aee11b95038b64" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexWisdom_InPrep2020c3123a084048450e86aee11b95038b64label" class="modal fade" id="bibtexWisdom_InPrep2020c3123a084048450e86aee11b95038b64" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexWisdom_InPrep2020c3123a084048450e86aee11b95038b64label">What's All the FUSS About Free Universal Sound Separation Data?</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wisdom_InPrep2020,
    Author = "Wisdom, Scott and Erdogan, Hakan and Ellis, Daniel P. W. and Serizel, Romain and Turpault, Nicolas and Fonseca, Eduardo and Salamon, Justin and Seetharaman, Prem and Hershey, John R.",
    title = "What's All the FUSS About Free Universal Sound Separation Data?",
    year = "2020",
    booktitle = "in preparation"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><strong>Motivation:</strong>
This dataset provides a platform to investigate how sound separation may help with event detection and vice versa.
Event detection is more difficult in noisy environments, and so separation could be a useful pre-processing step.
Data with strong labels for event detection are relatively scarce, especially when restricted to specific classes within a domain.
In contrast, sound separation data needs no event labels for training, and may be more plentiful.
In this setting, the idea is to utilize larger unlabeled separation data to train separation systems, which can serve as a front-end to event-detection systems trained on more limited data.  </p>
<p><strong>Room simulation:</strong>
Room impulse responses are simulated using the image method with frequency-dependent walls.
Each impulse corresponds to a rectangular room of random size with random wall materials, where a single microphone and up to 4 sound sources are placed at random spatial locations.</p>
<p><strong>Recipe for data creation:</strong>
The data creation recipe starts with scripts, based on <a href="https://github.com/justinsalamon/scaper">Scaper</a>, to generate mixtures of events with random timing of sound events,
along with a background sound that spans the duration of the mixture clip.<br/>
The constituent sound files for each mixture are also generated for use as references for training and evaluation.<br/>
The data are reverberated using a different room simulation for each mixture. <br/>
In this simulation each sound source has its own reverberation corresponding to a different spatial location. <br/>
The reverberated mixtures are created by summing over the reverberated sound sources. <br/>
The data creation scripts support modification, so that participants may remix and augment the training data as desired.</p>
<h3>Additional (background) datasets</h3>
<p><strong>SINS dataset:</strong>
A part of the derivative of the SINS dataset used for <a href="http://dcase.community/challenge2018/task-monitoring-domestic-activities">DCASE2018 task 5</a> is used as background for the
synthetic subset of the dataset for DCASE 2019 task 4.
The SINS dataset contains a continuous recording of one person living in a vacation home over a period of one week.<br/>
It was collected using a network of 13 microphone arrays distributed over the entire home.
The microphone array consists of 4 linearly arranged microphones.</p>
<div class="btex-item" data-item="Dekkers2017" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Dekkers2017"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Gert Dekkers, Steven Lauwereins, Bart Thoen, Mulu Weldegebreal Adhana, Henk Brouckxon, Toon van Waterschoot, Bart Vanrumste, Marian Verhelst, and Peter Karsmakers.
<em>The SINS database for detection of daily activities in a home environment using an acoustic sensor network.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), 32–36. November 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://dcase.community/documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" class="panel-collapse collapse" id="collapseDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" role="tabpanel">
<h4>The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
<h5>Abstract</h5>
<p class="text-justify">There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.</p>
<h5>Keywords</h5>
<p class="text-justify">Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://dcase.community/documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDekkers2017d1d603b79a78442baf5414dfa4dd4ce9label" class="modal fade" id="bibtexDekkers2017d1d603b79a78442baf5414dfa4dd4ce9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDekkers2017d1d603b79a78442baf5414dfa4dd4ce9label">The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dekkers2017,
    Author = "Dekkers, Gert and Lauwereins, Steven and Thoen, Bart and Adhana, Mulu Weldegebreal and Brouckxon, Henk and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marian and Karsmakers, Peter",
    title = "The {SINS} Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "32--36",
    abstract = "There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.",
    keywords = "Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><strong>TUT Acoustic scenes 2017, development dataset:</strong>
TUT Acoustic Scenes 2017 dataset consists of recordings from various acoustic scenes, all having distinct recording locations.
For each recording location, 3-5 minute long audio recording was captured. The original recordings were then split into segments with a length of 10 seconds.
These audio segments are provided in individual files.</p>
<div class="btex-item" data-item="Mesaros2016_EUSIPCO" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_EUSIPCO"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>TUT database for acoustic scene classification and sound event detection.</em>
In 24th European Signal Processing Conference 2016 (EUSIPCO 2016). Budapest, Hungary, 2016.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" class="panel-collapse collapse" id="collapseMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" role="tabpanel">
<h4>TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79label" class="modal fade" id="bibtexMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_EUSIPCO857befedc41d4d66866662e5a306dd79label">TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2016_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "{TUT} Database for Acoustic Scene Classification and Sound Event Detection",
    abstract = "We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.",
    year = "2016",
    address = "Budapest, Hungary",
    booktitle = "24th European Signal Processing Conference 2016 (EUSIPCO 2016)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h3>Generating your own training data using Scaper</h3>
<p>Participants can use the provided isolated foreground and background sounds, in combination with the <a href="https://github.com/justinsalamon/scaper">Scaper</a> soundscape synthesis and augmentation library, to generate additional (potentially infinite!) training data.</p>
<p>Resources for getting started include:</p>
<ul>
<li>The Scaper scripts provided with the DESED dataset (<a href="https://project.inria.fr/desed/download/synthetic-data/#gendata">link</a>)</li>
<li>The canonical Scaper script used to create the source separation dataset (<a href="https://github.com/google-research/sound-separation/datasets/fuss">link</a>)</li>
<li>The <a href="https://scaper.readthedocs.io/en/latest/tutorial.html">Scaper tutorial</a></li>
</ul>
<div class="btex-item" data-item="Salamon2017_WASPAA" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Salamon2017_WASPAA"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            J. Salamon, D. MacConnell, M. Cartwright, P. Li, and J. P. Bello.
<em>Scaper: a library for soundscape synthesis and augmentation.</em>
In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 344–348. New Paltz, NY, USA, Oct. 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_scaper_waspaa_2017.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" class="panel-collapse collapse" id="collapseSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" role="tabpanel">
<h4>Scaper: A Library for Soundscape Synthesis and Augmentation</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_scaper_waspaa_2017.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227label" class="modal fade" id="bibtexSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexSalamon2017_WASPAAadf206dc0e304179a07f85b973df5227label">Scaper: A Library for Soundscape Synthesis and Augmentation</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Salamon2017_WASPAA,
    Author = "Salamon, J. and MacConnell, D. and Cartwright, M. and Li, P. and Bello, J. P.",
    address = "New Paltz, NY, USA",
    booktitle = "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    month = "Oct.",
    pages = "344--348",
    title = "Scaper: A Library for Soundscape Synthesis and Augmentation",
    year = "2017"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="reference-labels">Reference labels</h2>
<p>Audioset provides annotations at clip level (without time boundaries for the events). Therefore, the original annotations are considered as weak labels.
Google researchers conducted a quality assessment task where experts were exposed to 10 randomly selected clips for each class and discovered that a in most of the cases not all the clips contains the event related to the given annotation.</p>
<h3>Weak annotations</h3>
<p>The weak annotations have been verified manually for a small subset of the training set. The weak annotations are provided in a tab separated csv file under the following format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event_labels (strings)</span><span class="o">]</span>
</code></pre></div>
<p>For example:</p>
<p><code>Y-BJNMHMZDcU_50.000_60.000.wav   Alarm_bell_ringing,Dog</code></p>
<p>The first column, <code>Y-BJNMHMZDcU_50.000_60.000.wav</code>, is the name of the audio file downloaded from Youtube
(<code>Y-BJNMHMZDcU</code> is Youtube ID of the video from where the 10-second clips was extracted t=50 sec to t=60 sec,
correspond to the clip boundaries within the full video) and the last column,
<code>Alarm_bell_ringing;Dog</code> corresponds to the sound classes present in the clip separated by a coma.</p>
<h3>Strong annotations</h3>
<p>Another subset of the development has been annotated manually with strong annotations, to be used as the test set
(see also below for a detailed explanation about the development set).</p>
<p>The synthetic subset of the development set is generated and labeled with strong annotations using the
<a href="https://github.com/justinsalamon/scaper">Scaper soundscape synthesis and augmentation library</a>.
Each sound clip from FSD50k was verified by humans in order to check the event class present in FSD50k annotation was indeed dominant in the audio clip.</p>
<div class="btex-item" data-item="fonseca2020fsd50k" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="fonseca2020fsd50k"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra.
<em>FSD50K: an open dataset of human-labeled sound events.</em>
In arXiv:2010.00475. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexfonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2010.00475.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsefonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsefonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingfonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" class="panel-collapse collapse" id="collapsefonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" role="tabpanel">
<h4>FSD50K: an Open Dataset of Human-Labeled Sound Events</h4>
<h5>Abstract</h5>
<p class="text-justify">Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexfonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2010.00475.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexfonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408elabel" class="modal fade" id="bibtexfonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408e" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexfonseca2020fsd50k4d6dc24f777c4c5aa644a4d0088b408elabel">FSD50K: an Open Dataset of Human-Labeled Sound Events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{fonseca2020fsd50k,
    author = "Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier",
    title = "{FSD50K}: an Open Dataset of Human-Labeled Sound Events",
    year = "2020",
    booktitle = "arXiv:2010.00475",
    abstract = "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="font2013freesound" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="font2013freesound"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Frederic Font, Gerard Roma, and Xavier Serra.
<em>Freesound technical demo.</em>
In Proceedings of the 21st ACM international conference on Multimedia, 411–412. ACM, 2013.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexfont2013freesound4e2fd425174e4387bd4125a98020acf2" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://mtg.upf.edu/system/files/publications/Font-Roma-Serra-ACMM-2013.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsefont2013freesound4e2fd425174e4387bd4125a98020acf2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsefont2013freesound4e2fd425174e4387bd4125a98020acf2" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingfont2013freesound4e2fd425174e4387bd4125a98020acf2" class="panel-collapse collapse" id="collapsefont2013freesound4e2fd425174e4387bd4125a98020acf2" role="tabpanel">
<h4>Freesound technical demo</h4>
<h5>Abstract</h5>
<p class="text-justify">Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexfont2013freesound4e2fd425174e4387bd4125a98020acf2" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://mtg.upf.edu/system/files/publications/Font-Roma-Serra-ACMM-2013.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexfont2013freesound4e2fd425174e4387bd4125a98020acf2label" class="modal fade" id="bibtexfont2013freesound4e2fd425174e4387bd4125a98020acf2" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexfont2013freesound4e2fd425174e4387bd4125a98020acf2label">Freesound technical demo</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{font2013freesound,
    author = "Font, Frederic and Roma, Gerard and Serra, Xavier",
    title = "Freesound technical demo",
    booktitle = "Proceedings of the 21st ACM international conference on Multimedia",
    pages = "411--412",
    year = "2013",
    organization = "ACM",
    abstract = "Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>In both cases, the minimum length for an event is 250ms. The minimum duration of the pause between two events from the same class is 150ms.
When the silence between two consecutive events from the same class was less than 150ms the events have been merged to a single event.
The strong annotations are provided in a tab separated csv file under the following format:</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">onset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">offset (in seconds) (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event_label (string)</span><span class="o">]</span>
</code></pre></div>
<p>For example:</p>
<p><code>YOTsn73eqbfc_10.000_20.000.wav   0.163   0.665   Alarm_bell_ringing</code></p>
<p>The first column, <code>YOTsn73eqbfc_10.000_20.000.wav</code>, is the name of the audio file, the second column <code>0.163</code> is the onset time in seconds,
the third column <code>0.665</code> is the offset time in seconds and the last column, <code>Alarm_bell_ringing</code> corresponds to the class of the sound event.</p>
<h2 id="download">Download</h2>
<p>The dataset is composed of two subset that can be downloaded independently. The procedure to download each subset is described below.</p>
<h3>DESED dataset</h3>
<p>The instructions to download the audio files, the annotations and example scripts to generate synthetic soundscapes
can be found on the <a href="https://project.inria.fr/desed/dcase-challenge/dcase-2020-task-4/">DCASE 2020 section of DESED website</a>.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://project.inria.fr/desed/dcase-challenge/dcase-2020-task-4/" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://project.inria.fr/desed/dcase-challenge/dcase-2020-task-4/" target="_blank">
<span style="font-size:20px;">DESED dataset for <strong>DCASE 2020 task 4</strong> <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p class="bg-danger">
If you experience problems during the download of the recorded soundscapes please contact the task organizers.
(Nicolas Turpault and Romain Serizel in priority)
</p>
<h3>Source separation (FUSS) dataset</h3>
<p>Instructions to download the original audio data, the model parameters, the audio mixtures and the recipes to generate them is available on the FUSS repositories (<a href="https://zenodo.org/record/3710392">zenodo</a> and <a href="https://github.com/google-research/sound-separation/datasets/fuss">github</a>).</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3694384" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3694384" target="_blank">
<span style="font-size:20px;">FUSS dataset - <strong>audio data and model parameters</strong> <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(24.6 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3694384">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3694384.svg"/>
</a>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/datasets/fuss" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/datasets/fuss" target="_blank">
<span style="font-size:20px;">FUSS dataset - <strong> recipes</strong> for generating data <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h1 id="task-setup"><a name="task_setup">Task setup</a></h1>
<p>The challenge consists of detecting sound events within web videos using training data from real recordings both weakly labeled and unlabeled and synthetic audio clips that are strongly labeled. The detection within a 10-seconds clip should be performed with start and end timestamps. As we encourage participants to use a source separation algorithm together with the sound event detection, there are three possible scenarios:</p>
<ul>
<li>You are working sound event detection without source separation pre-processing (this is a direct follow-up to last year task 4)</li>
<li>You are are working on both source separation and sound event detection (this includes the case when reusing the source separation baseline)</li>
<li>You are working only on source separation and use the sound event detection baseline</li>
</ul>
<p>In each case, participants are expect to provide the output of a sound event detection system (<a href="#sed_eval">see also below</a>). Note that an additional (optional) separate evaluation set designed to evaluate source separation performance will also be provided (<a href="#ss_eval">see also below</a>).</p>
<h2 id="development-dataset">Development dataset</h2>
<p>The development set is divided into six main partitions (four for sound event detection, 2 for source separation) that are detailed below.</p>
<h3>Sound event detection training set</h3>
<p>This dataset is a subset of DESED. We provide 3 different splits of training data in our training set:
Labeled training set, Unlabeled in domain training set and Synthetic set with strong annotations.
The first two set are the same as in DCASE2019 task 4.</p>
<p><strong>Labeled training set</strong>:<br/>
This set contains <strong>1578 clips</strong> (2244 class occurrences) for which weak annotations have been verified and cross-checked.
The amount of clips per class is the following:</p>
<p><strong>Unlabeled in domain training set</strong>:<br/>
This set is considerably larger than the previous one. It contains <strong>14412 clips</strong>.
The clips are selected such that the distribution per class (based on Audioset annotations)
is close to the distribution in the labeled set.
Note however that given the uncertainty on Audioset labels this distribution might not be exactly similar.</p>
<p><strong>Synthetic strongly labeled set</strong>:<br/>
This set is composed of <strong>2584 clips</strong> generated with the <a href="https://github.com/justinsalamon/scaper">Scaper soundscape synthesis and augmentation library</a>.
The clips are generated such that the distribution per event is close to that of the validation set.
We used all the foreground files from DESED synthetic soundbank.
We used background files annotated as "other" from <a href="https://zenodo.org/record/1247102">the subpart of SINS dataset</a>.
The default parameters and the Room Impulse Responses are the same as the source separation training set (but the events distribution is different).
Note that a 10-seconds clip may correspond to more than one sound event but the polyphony maximum is limited to 2.</p>
<p>This year we share the original data and scripts to generate soundscapes and encourage participants to create their own subsets.
See <a href="https://github.com/turpaultn/DESED">DESED github repo</a> and <a href="https://scaper.readthedocs.io/en/latest/">Scaper documentation</a> for more information about how to create new soundscapes.</p>
<h3>Sound event detection validation set</h3>
<p>The validation set is designed such that the distribution in term of clips per class is similar to that of the weakly labeled training set.
It is the same as <strong>DCASE 2019 task 4 validation set</strong>.
The validation set contains <strong>1168 clips</strong> (4093 events).
The validation set is annotated with strong labels, with timestamps (obtained by human annotators).
Note that a 10-seconds clip may correspond to more than one sound event.
The amount of events per class is the following:</p>
<h3>Source separation training set</h3>
<p>The source separation training set consists of <strong>20000 mixture clips</strong>.
The ground-truth reference sources are provided for each of these mixtures.
Each 10 second mixture contains between 1 and 4 sources.
Every mixture contains one background source, which is active for the entire duration.</p>
<h3>Source separation validation set</h3>
<p>The source separation validation set is generated in the same way as the training set, and consists of <strong>1000 mixture clips</strong>,
along with corresponding ground-truth reference sources. Raw source clips for this set are sourced from different
Freesound uploaders than those for the raw source clips used to generate training.</p>
<h2 id="evaluation-datasets">Evaluation datasets</h2>
<p>As we encourage participants to use a source separation algorithm together with the sound event detection, there are three possible scenarios:</p>
<ol>
<li>You are working sound event detection without source separation pre-processing</li>
<li>You are are working on both source separation and sound event detection</li>
<li>You are working only on source separation and use the sound event detection baseline</li>
</ol>
<p>Participants are allowed to submit up to 4 different systems for each scenario listed above. <strong>In each case, participants are expected to provide at least the output of a sound event detection system.</strong> Participants who wants to get their sound separation submission evaluated can download the specific sound separation evaluation dataset (see below).</p>
<p>PSDS submissions are optional. If you want your system to be evaluated in PSDS please follow strictly the format illustrated in the example submission package.</p>
<p><strong>Before submission, please make sure you check that your submission package is correct with the validation script enclosed in the submission package:</strong>
<code>python validate_submissions.py -i /Users/nturpaul/Documents/code/dcase2020/task4_test</code></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2020/dcase2020_challenge_submission_package_example.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2020/dcase2020_challenge_submission_package_example.zip" target="_blank">
<span style="font-size:20px;">DCASE2020 challenge submission example package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(185 kB)</span>
<br/>
<span class="text-muted">
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/></p>
<h3>Sound event detection evaluation dataset</h3>
<p>The sound event detection evaluation dataset is composed of 10 seconds and 5 minutes audio clips.</p>
<ul>
<li>
<p>A first subset is composed of audio clips extracted from youtube and vimeo videos under creative common licenses. This subset is used for ranking purposes. <strong>This subset includes the <a href="https://zenodo.org/record/3588172">public evaluation dataset</a>.</strong></p>
</li>
<li>
<p>A second subset is composed on synthetic clips generated with scaper. This subset is used for analysis purposes.</p>
</li>
</ul>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3866455" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3866455" target="_blank">
<span style="font-size:20px;">DCASE2020 Task 4 <strong>Sound event detection</strong> evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(9.1 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3866455">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3866455.svg"/>
</a>
</div>
</div>
<h3>Sound separation evaluation evaluation dataset</h3>
<p>The sound separation evalaution dataset is the evaluation part of the FUSS dataset.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3694384" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3694384" target="_blank">
<span style="font-size:20px;">FUSS dataset - <strong>audio data and model parameters</strong> <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(24.6 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3694384">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3694384.svg"/>
</a>
</div>
</div>
<h1 id="task-rules">Task rules</h1>
<p>There are general rules valid for all tasks; these, along with information on technical report and submission
requirements can be found <a href="http://dcase.community/challenge2020/rules">here</a>.</p>
<p>Task specific rules:</p>
<ul>
<li>Participants are allowed to submit up to <strong>4 different systems for each scenario listed in the <a href="#task_setup">task setup section</a></strong>.</li>
<li>Participants <strong>are not allowed</strong> to use external data for system development. Data from other task is considered external data.</li>
<li>Another example of external data is other materials related to the video such as the rest of audio from where the 10-sec clip was extracted, the video frames and metadata.</li>
<li>Participants <strong>are not allowed</strong> to use the embeddings provided by Audioset or any other features that indirectly use external data.</li>
<li><strong>For the real recordings (from Audioset), only weak labels and none of the strong labels (timestamps) or original
(Audioset) labels can be used for the training of the submitted system.</strong>
For the synthetic clips, strong labels provided can be used.</li>
<li>Manipulation of provided training data <strong>is allowed</strong>.</li>
<li>The development dataset can be augmented without the use of external data
(e.g. by mixing data sampled from a PDF or using techniques such as pitch shifting or time stretching).</li>
<li>Participants <strong>are not allowed</strong> to use the <strong>public evaluation dataset</strong> and <strong>synthetic evaluation dataset</strong>
(or part of them) to train their systems or tune hyper-parameters.</li>
</ul>
<h1 id="evaluation">Evaluation</h1>
<h2 id="sound-event-detection-evaluation"><a name="sed_eval">Sound event detection evaluation</a></h2>
<p>All submissions will be evaluated with event-based measures with a 200 ms collar on onsets and a 200 ms / 20% of the events length collar on offsets.
Submissions will be ranked according to the event-based F1-score computed over the real recordings in the evaluation set
(the performance on synthetic recordings is not taken into account in the ranking).
Additionally, multiple poly-phonic sound event detection scores will be provided as a contrastive measure.</p>
<p>F-scores are computed using a single operating point (decision thresholds=0.5) while other PSDS values are computed using 50 operating points (linearly distributed from 0.01 to 0.99). <em>The evaluation with the PSDS metric is optional.</em></p>
<p>The parameters used for PSDS performances are:</p>
<ul>
<li>Detection Tolerance parameter (dtc): 0.5</li>
<li>Ground Truth intersection parameter (gtc): 0.5</li>
<li>Cross-Trigger Tolerance parameter (cttc): 0.3</li>
<li>maximum False Positive rate (e_max): 100</li>
</ul>
<p>The difference between the 3 PSDS reported:</p>
<table class="table table-striped">
<thead>
<tr>
<td></td>
<td>alpha_ct</td>
<td>alpha_st</td>
</tr>
</thead>
<tr>
<td>PSDS</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>PSDS cross-trigger</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>PSDS macro</td>
<td>0</td>
<td>1</td>
</tr>
</table>
<p>alpha_ct is the weight related to the cost of cross-trigger, alpha_st is the weight related to the cost of instability across classes.</p>
<p>Evaluation is done using sed_eval and psds_eval toolboxes:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span style="font-size:20px;">sed_eval - Evaluation toolbox for Sound Event Detection <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/audioanalytic/psds_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/audioanalytic/psds_eval" target="_blank">
<span style="font-size:20px;">psds_eval - Polyphonic Sound Detection Score <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>You can find more information on how to use PSDS for task 4 on the dedicated notebook:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/audioanalytic/psds_eval/blob/master/jupyter/psds.ipynb" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/audioanalytic/psds_eval/blob/master/jupyter/psds.ipynb" target="_blank">
<span style="font-size:20px;">Notebook - PSDS evaluation for DCASE 2020 Task 4 <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>Detailed information on metrics calculation is available in:</p>
<div class="btex-item" data-item="Mesaros2016_MDPI" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_MDPI"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>Metrics for polyphonic sound event detection.</em>
<em>Applied Sciences</em>, 6(6):162, 2016.
URL: <a href="http://www.mdpi.com/2076-3417/6/6/162">http://www.mdpi.com/2076-3417/6/6/162</a>, <a href="https://doi.org/10.3390/app6060162">doi:10.3390/app6060162</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" class="panel-collapse collapse" id="collapseMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" role="tabpanel">
<h4>Metrics for Polyphonic Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_MDPI69b16868bfb74a6196fc803e4430794alabel" class="modal fade" id="bibtexMesaros2016_MDPI69b16868bfb74a6196fc803e4430794a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_MDPI69b16868bfb74a6196fc803e4430794alabel">Metrics for Polyphonic Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2016_MDPI,
    Author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "Metrics for Polyphonic Sound Event Detection",
    journal = "Applied Sciences",
    year = "2016",
    number = "6",
    pages = "162",
    volume = "6",
    abstract = "This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.",
    doi = "10.3390/app6060162",
    issn = "2076-3417",
    url = "http://www.mdpi.com/2076-3417/6/6/162"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Bilen2020" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Bilen2020"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Cagdas Bilen, Giacomo Ferroni, Francesco Tuveri, Juan Azcarreta, and Sacha Krstulovic.
<em>A framework for the robust evaluation of sound event detection.</em>
<em>arXiv preprint arXiv:1910.08440</em>, 2019.
URL: <a href="https://arxiv.org/abs/1910.08440">https://arxiv.org/abs/1910.08440</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexBilen20208c58d366b2d34f5382df0fb7cc90b5b7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1910.08440" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseBilen20208c58d366b2d34f5382df0fb7cc90b5b7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseBilen20208c58d366b2d34f5382df0fb7cc90b5b7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingBilen20208c58d366b2d34f5382df0fb7cc90b5b7" class="panel-collapse collapse" id="collapseBilen20208c58d366b2d34f5382df0fb7cc90b5b7" role="tabpanel">
<h4>A Framework for the Robust Evaluation of Sound Event Detection</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexBilen20208c58d366b2d34f5382df0fb7cc90b5b7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1910.08440" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexBilen20208c58d366b2d34f5382df0fb7cc90b5b7label" class="modal fade" id="bibtexBilen20208c58d366b2d34f5382df0fb7cc90b5b7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexBilen20208c58d366b2d34f5382df0fb7cc90b5b7label">A Framework for the Robust Evaluation of Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@article{Bilen2020,
    author = "Bilen, Cagdas and Ferroni, Giacomo and Tuveri, Francesco and Azcarreta, Juan and Krstulovic, Sacha",
    title = "A Framework for the Robust Evaluation of Sound Event Detection",
    journal = "arXiv preprint arXiv:1910.08440",
    year = "2019",
    url = "https://arxiv.org/abs/1910.08440"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="source-separation-evaluation-optional"><a name="ss_eval">Source separation evaluation (optional)</a></h2>
<p>Source separation approaches will be optionally evaluated on an additional evaluation with standard source separation metrics as follows.
For each example mixture x containing J sources, performance will be measured with permutation-invariant scale-invariant signal-to-noise ratio improvement (SI-SNRi).</p>
<ul>
<li>SNR is defined as 10 * log10 of the ratio of source power to error power.</li>
<li>Scale-invariant SNR (SI-SNR) allows scaling of the reference to best match the estimate. The optimal scaling factor given an estimate signal e and reference signal r is <e, r=""> / <r, r="">, where &lt; , &gt; indicates inner-product.</r,></e,></li>
<li>Permutation invariance allows the estimates to be permuted to match reference signals such that the mean SI-SNR across sources is maximized.</li>
<li>SI-SNR improvement (SI-SNRi) is the SI-SNR of the estimate with respect to the reference, minus SI-SNR of the mixture with respect to the reference.</li>
</ul>
<p>A python evaluation script for source separation can be found on:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/blob/master/models/dcase2020_fuss_baseline/evaluate.py" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/blob/master/models/dcase2020_fuss_baseline/evaluate.py" target="_blank">
<span style="font-size:20px;">Evaluation scripts for source separation <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="10" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="2"></th>
</tr>
<tr>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound Separation
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>Chuming Liang</td>
<td>Xiaomi Co., AI Lab, Wuhan, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Liang2020</td>
<td>36.0 (35.3 - 36.8)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>Krzysztof Rykaczewski</td>
<td>Samsung R&amp;D Institute Poland - Samsung Research, Audio Intelligence, Warsaw, Poland</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Rykaczewski2020</td>
<td>21.6 (21.0 - 22.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>Krzysztof Rykaczewski</td>
<td>Samsung R&amp;D Institute Poland - Samsung Research, Audio Intelligence, Warsaw, Poland</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Rykaczewski2020</td>
<td>21.9 (21.3 - 22.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>Krzysztof Rykaczewski</td>
<td>Samsung R&amp;D Institute Poland - Samsung Research, Audio Intelligence, Warsaw, Poland</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Rykaczewski2020</td>
<td>10.4 (9.7 - 11.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>Krzysztof Rykaczewski</td>
<td>Samsung R&amp;D Institute Poland - Samsung Research, Audio Intelligence, Warsaw, Poland</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Rykaczewski2020</td>
<td>21.6 (20.8 - 22.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>Bowei Hou</td>
<td>Waseda University, Graduate School of Information, Production and Systems, Kitakyushu, Japan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#HouB2020</td>
<td>34.9 (34.0 - 35.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Koichi Miyazaki</td>
<td>Nagoya University, a, Japan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Miyazaki2020</td>
<td>51.1 (50.1 - 52.3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Koichi Miyazaki</td>
<td>Nagoya University, Japan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Miyazaki2020</td>
<td>46.4 (45.5 - 47.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>Koichi Miyazaki</td>
<td>Nagoya University, Japan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Miyazaki2020</td>
<td>50.7 (49.6 - 51.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.3 (43.4 - 45.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.6 (43.5 - 46.0)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>Yuxin Huang</td>
<td>Institute of Computing Technology,Chinese Academy of Sciences, Bejing Key Laboratory of Mobile Computing and Pervasive Device, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>Abigail Copiaco</td>
<td>University of Wollongong, Department of Engineering and Information Sciences, Wollongong, Australia</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Copiaco2020a</td>
<td>7.8 (7.3 - 8.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>Abigail Copiaco</td>
<td>University of Wollongong, Department of Engineering and Information Sciences, Wollongong, Australia</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Copiaco2020a</td>
<td>7.5 (7.0 - 8.0)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Nam Kyun Kim</td>
<td>Gwnagju Institute of Science and Technology, School of Electrical Engineering and Computer Science, Gwnagju, South Korea</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Kim2020</td>
<td>43.7 (42.8 - 44.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Nam Kyun Kim</td>
<td>Gwnagju Institute of Science and Technology, School of Electrical Engineering and Computer Science, Gwnagju, South Korea</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Kim2020</td>
<td>43.9 (43.0 - 44.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Nam Kyun Kim</td>
<td>Gwnagju Institute of Science and Technology, School of Electrical Engineering and Computer Science, Gwnagju, South Korea</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Kim2020</td>
<td>44.4 (43.5 - 45.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Nam Kyun Kim</td>
<td>Gwnagju Institute of Science and Technology, School of Electrical Engineering and Computer Science, Gwnagju, South Korea</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Kim2020</td>
<td>44.2 (43.4 - 45.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>Abigail Copiaco</td>
<td>University of Wollongong, Department of Engineering and Information Sciences, Wollongong, Australia</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>Lu JiaKai</td>
<td>PFU SHANGHAI Co., LTD, 1T3K, Shanghai, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#JiaKai2020</td>
<td>38.6 (37.7 - 39.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>Lu JiaKai</td>
<td>PFU SHANGHAI Co., LTD, 1T3K, Shanghai, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#JiaKai2020</td>
<td>39.3 (38.4 - 40.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>Lu JiaKai</td>
<td>PFU SHANGHAI Co., LTD, 1T3K, Shanghai, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#JiaKai2020</td>
<td>41.2 (40.1 - 42.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>Lu JiaKai</td>
<td>PFU SHANGHAI Co., LTD, 1T3K, Shanghai, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#JiaKai2020</td>
<td>40.6 (39.6 - 41.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>junyong Hao</td>
<td>CHONGQING UNIVERSITY, Intelligent Information Technology and System Lab, Chongqing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Hao2020</td>
<td>47.0 (46.0 - 48.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>junyong Hao</td>
<td>CHONGQING UNIVERSITY, Intelligent Information Technology and System Lab, Chongqing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Hao2020</td>
<td>46.3 (45.5 - 47.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>junyong Hao</td>
<td>CHONGQING UNIVERSITY, Intelligent Information Technology and System Lab, Chongqing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Hao2020</td>
<td>44.9 (43.9 - 45.8)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>junyong Hao</td>
<td>CHONGQING UNIVERSITY, Intelligent Information Technology and System Lab, Chongqing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Hao2020</td>
<td>47.8 (46.9 - 49.0)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>Hou Zhenwei</td>
<td>CHONGQING UNIVERSITY, Intelligent Information Technology and System Lab, Chongqing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#HouZ2020</td>
<td>45.1 (44.2 - 45.8)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>Diego de Benito-Gorron</td>
<td>Universidad Autónoma de Madrid, Escuela Politécnica Superior, Madrid, Spain</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#deBenito2020</td>
<td>38.2 (37.5 - 39.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>Diego de Benito-Gorron</td>
<td>Universidad Autónoma de Madrid, Escuela Politécnica Superior, Madrid, Spain</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#de Benito-Gorron2020</td>
<td>37.9 (37.0 - 39.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Chih-Yuan Koh</td>
<td>National Tsing Hua University, Department of Electrical Engineering, Hsinchu, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Koh2020</td>
<td>46.6 (45.8 - 47.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Chih-Yuan Koh</td>
<td>National Tsing Hua University, Department of Electrical Engineering, Hsinchu, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Koh2020</td>
<td>45.2 (44.3 - 46.3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Chih-Yuan Koh</td>
<td>National Tsing Hua University, Department of Electrical Engineering, Hsinchu, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Koh2020</td>
<td>45.2 (44.2 - 46.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Chih-Yuan Koh</td>
<td>National Tsing Hua University, Department of Electrical Engineering, Hsinchu, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Koh2020</td>
<td>46.3 (45.4 - 47.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>Samuele Cornell</td>
<td>Università Politecnica delle Marche, Department of Information Engineering, Ancona, Italy</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Cornell2020</td>
<td>42.0 (40.9 - 43.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>Samuele Cornell</td>
<td>Università  Politecnica delle Marche, Department of Information Engineering, Ancona, Italy</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Cornell2020</td>
<td>44.4 (43.3 - 45.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>Samuele Cornell</td>
<td>Università Politecnica delle Marche, Department of Information Engineering, Ancona, Italy</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Cornell2020</td>
<td>43.2 (42.1 - 44.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>Samuele Cornell</td>
<td>Università Politecnica delle Marche, Department of Information Engineering, Ancona, Italy</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>Samuele Cornell</td>
<td>Università Politecnica delle Marche, Department of Information Engineering, Ancona, Italy</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Cornell2020</td>
<td>42.6 (41.6 - 43.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Tianchu Yao</td>
<td>University of Electronic Science and Technology of China, School of Information and Communication Engineering, Chengdu, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yao2020</td>
<td>44.1 (43.1 - 45.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Tianchu Yao</td>
<td>University of Electronic Science and Technology of China, School of Information and Communication Engineering, Chengdu, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yao2020</td>
<td>46.4 (45.3 - 47.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Tianchu Yao</td>
<td>University of Electronic Science and Technology of China, School of Information and Communication Engineering, Chengdu, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yao2020</td>
<td>45.7 (44.7 - 47.0)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Tianchu Yao</td>
<td>University of Electronic Science and Technology of China, School of Information and Communication Engineering, Chengdu, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yao2020</td>
<td>46.2 (45.2 - 47.0)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Yuzhuo Liu</td>
<td>The Institute of Acoustics of the Chinese Academy of Sciences, The Key Lab of Speech Acoustics and Content Understanding, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Liu2020</td>
<td>40.7 (39.7 - 41.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Yuzhuo Liu</td>
<td>The Institute of Acoustics of the Chinese Academy of Sciences, The Key Lab of Speech Acoustics and Content Understanding, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Liu2020</td>
<td>41.8 (40.7 - 42.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Yuzhuo Liu</td>
<td>The Institute of Acoustics of the Chinese Academy of Sciences, The Key Lab of Speech Acoustics and Content Understanding, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Liu2020</td>
<td>45.2 (44.2 - 46.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>Yuzhuo Liu</td>
<td>The Institute of Acoustics of the Chinese Academy of Sciences, The Key Lab of Speech Acoustics and Content Understanding, Beijing, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Liu2020</td>
<td>43.1 (42.1 - 44.2)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Sangwook Park</td>
<td>Johns Hopkins University., Electrical and Computer Engineering, Baltimore, MD, USA</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Park2020</td>
<td>35.8 (35.0 - 36.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Sangwook Park</td>
<td>Johns Hopkins University., Electrical and Computer Engineering, Baltimore, MD, USA</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Park2020</td>
<td>26.5 (25.7 - 27.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>Sangwook Park</td>
<td>Johns Hopkins University., Electrical and Computer Engineering, Baltimore, MD, USA</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Park2020</td>
<td>36.9 (36.1 - 37.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>Sangwook Park</td>
<td>Johns Hopkins University., Electrical and Computer Engineering, Baltimore, MD, USA</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Park2020</td>
<td>34.7 (34.1 - 35.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>You Siang Chen</td>
<td>National Tsing Hua University, Department of Power Mechanical Engineering, Hsinchu, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>Teck Kai Chan</td>
<td>Newcastle University Singapore, Faculty of Science, Agriculture, and Engineering, Singapore, Singapore</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Chan2020</td>
<td>44.4 (43.5 - 45.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>Teck Kai Chan</td>
<td>Newcastle University Singapore, Faculty of Science, Agriculture, and Engineering, Singapore, Singapore</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Chan2020</td>
<td>46.3 (45.3 - 47.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>Teck Kai Chan</td>
<td>Newcastle University Singapore, Faculty of Science, Agriculture, and Engineering, Singapore, Singapore</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Chan2020</td>
<td>45.8 (45.0 - 47.0)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>Teck Kai Chan</td>
<td>Newcastle University Singapore, Faculty of Science, Agriculture, and Engineering, Singapore, Singapore</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Chan2020</td>
<td>43.5 (42.6 - 44.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>Hao Yen</td>
<td>National Taiwan University, Department of Electrical Engineering, Taipei, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yen2020</td>
<td>42.7 (41.6 - 43.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>Hao Yen</td>
<td>National Taiwan University, Department of Electrical Engineering, Taipei, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yen2020</td>
<td>42.6 (41.8 - 43.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>Hao Yen</td>
<td>National Taiwan University, Department of Electrical Engineering, Taipei, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yen2020</td>
<td>41.6 (40.6 - 42.7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>Hao Yen</td>
<td>National Taiwan University, Department of Electrical Engineering, Taipei, Taiwan</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Yen2020</td>
<td>43.6 (42.4 - 44.6)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Maolin Tang</td>
<td>Sichuan University, Computer Science and Technology, Sichuan, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Tang2020</td>
<td>43.1 (42.3 - 44.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Maolin Tang</td>
<td>Sichuan University, Computer Science and Technology, Sichuan, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Tang2020</td>
<td>44.1 (43.4 - 44.8)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Maolin Tang</td>
<td>Sichuan University, Computer Science and Technology, Sichuan, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Tang2020</td>
<td>42.4 (41.4 - 43.4)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Maolin Tang</td>
<td>Sichuan University, Computer Science and Technology, Sichuan, China</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Tang2020</td>
<td>44.1 (43.3 - 45.0)</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>Nicolas Turpault</td>
<td>Inria Nancy Grand-Est, Department of Natural Language Processing &amp; Knowledge Discovery, Nancy, France</td>
<td>task-sound-event-detection-in-domestic-environments#Turpault2020a</td>
<td>34.9 (34.0 - 35.7)</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>Nicolas Turpault</td>
<td>Inria Nancy Grand-Est, Department of Natural Language Processing &amp; Knowledge Discovery, Nancy, France</td>
<td>task-sound-event-detection-in-domestic-environments#Turpault2020b</td>
<td>36.5 (35.6 - 37.2)</td>
<td>Sound Separation</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Janek Ebbers</td>
<td>Paderborn University, Department of Communications Engineering, Paderborn, Germany</td>
<td>task-sound-event-detection-and-separation-in-domestic-environments-results#Ebbers2020</td>
<td>47.2 (46.5 - 48.1)</td>
<td></td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments-results">Task 4 results page</a></p>
<h1 id="baselines">Baselines</h1>
<p>There are three different baselines: one for sound event detection, one for sound separation and one combing sound separation and sound event detection.</p>
<h2 id="sound-event-detection-baseline"><a name="sed_base"> Sound event detection baseline</a></h2>
<p>The baseline model is inspired by last year 2nd best submission system of DCASE 2019 task 4. It is an improvement of <a href="https://github.com/turpaultn/DCASE2019_task4">dcase 2019 baseline</a>. The model is a mean-teacher model.</p>
<p>The main differences of the baseline system (without source separation) compared to DCASE 2019:</p>
<ul>
<li>The sampling rate becomes 16kHz.</li>
<li>Features:<ul>
<li>2048 fft window, 255 hop size, 8000 max frequency for mel, 128 mel bins.</li>
</ul>
</li>
<li>Different synthetic dataset is used.</li>
<li>The architecture (number of layers) is taken from L. Delphin-Poulat &amp; C. Plapous.</li>
<li>There is rampup for the learning rate for 50 epochs.</li>
<li>Median window of 0.45s.</li>
<li>Early stopping (10 epochs).</li>
</ul>
<h3>Performance</h3>
<table class="table table-striped">
<thead>
<tr>
<td></td>
<td><strong>Macro F-score Event-based</strong></td>
<td><strong>PSDS macro F-score</strong></td>
<td>PSDS </td>
<td>PSDS cross-trigger</td>
<td>PSDS macro</td>
</tr>
</thead>
<tbody>
<tr>
<td>Validation</td>
<td><strong> 34.8 % </strong></td>
<td><strong> 60.0% </strong></td>
<td> 0.610 </td>
<td> 0.524 </td>
<td> 0.433 </td>
</tr>
</tbody>
</table>
<h3>Download</h3>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" target="_blank">
<span style="font-size:20px;">Task 4 - <strong>sound event detection</strong> baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3745475" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-table fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3745475" target="_blank">
<span style="font-size:20px;">SED baseline DNN pre-trained weights <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(18.3 MB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3726376">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3726376.svg"/>
</a>
<span class="text-muted">
                
                version 2
                
                
                </span>
</div>
</div>
<p><br/></p>
<p><strong>Note:</strong> The performance might not be exactly reproducible on a GPU based system.
That is why, you can download the weights of the networks used for the experiments and run <code>TestModel.py --model_path="Path_of_model"</code> to reproduce the results.</p>
<div class="btex-item" data-item="Turpault2020a" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2020a"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault and Romain Serizel.
<em>Training sound event detection on a heterogeneous dataset.</em>
working paper or preprint, 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" class="panel-collapse collapse" id="collapseTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" role="tabpanel">
<h4>Training Sound Event Detection On A Heterogeneous Dataset</h4>
<h5>Abstract</h5>
<p class="text-justify">Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1label" class="modal fade" id="bibtexTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2020a85a00e1eed1543f4b92612ccd7f19eb1label">Training Sound Event Detection On A Heterogeneous Dataset</h4>
</div>
<div class="modal-body">
<pre>@unpublished{Turpault2020a,
    AUTHOR = "Turpault, Nicolas and Serizel, Romain",
    title = "Training Sound Event Detection On A Heterogeneous Dataset",
    note = "working paper or preprint",
    institution = "Universit{\'e} de Lorraine, CNRS, Inria, Loria, France",
    year = "2020",
    abstract = "Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Delphin-Poulat2019" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Delphin-Poulat2019"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Lionel Delphin-Poulat and Cyril Plapous.
<em>Mean teacher with data augmentation for dcase 2019 task 4.</em>
Technical Report, Orange Labs Lannion, France, June 2019.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Delphin_15.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" class="panel-collapse collapse" id="collapseDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" role="tabpanel">
<h4>MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4</h4>
<h5>Abstract</h5>
<p class="text-justify">In this paper, we present our neural network for the DCASE 2019 challenge’s Task 4 (Sound event detection in domestic environments) [1]. The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled. We propose a mean-teacher model with convolutional neural network (CNN) and recurrent neural network (RNN) together with data augmentation and a median window tuned for each class based on prior knowledge.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Delphin_15.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDelphin-Poulat2019e520f3c56677474f82b06463a3b062bblabel" class="modal fade" id="bibtexDelphin-Poulat2019e520f3c56677474f82b06463a3b062bb" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDelphin-Poulat2019e520f3c56677474f82b06463a3b062bblabel">MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4</h4>
</div>
<div class="modal-body">
<pre>@techreport{Delphin-Poulat2019,
    Author = "Delphin-Poulat, Lionel and Plapous, Cyril",
    title = "MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4",
    institution = "Orange Labs Lannion, France",
    year = "2019",
    month = "June",
    abstract = "In this paper, we present our neural network for the DCASE 2019 challenge’s Task 4 (Sound event detection in domestic environments) [1]. The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled. We propose a mean-teacher model with convolutional neural network (CNN) and recurrent neural network (RNN) together with data augmentation and a median window tuned for each class based on prior knowledge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="tarvainen2017mean" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="tarvainen2017mean"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Antti Tarvainen and Harri Valpola.
<em>Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results.</em>
In Advances in neural information processing systems, 1195–1204. 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtextarvainen2017mean9082c4899b704fb8b9cad6351a462848" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsetarvainen2017mean9082c4899b704fb8b9cad6351a462848" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsetarvainen2017mean9082c4899b704fb8b9cad6351a462848" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingtarvainen2017mean9082c4899b704fb8b9cad6351a462848" class="panel-collapse collapse" id="collapsetarvainen2017mean9082c4899b704fb8b9cad6351a462848" role="tabpanel">
<h4>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</h4>
<h5>Abstract</h5>
<p class="text-justify">The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtextarvainen2017mean9082c4899b704fb8b9cad6351a462848" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtextarvainen2017mean9082c4899b704fb8b9cad6351a462848label" class="modal fade" id="bibtextarvainen2017mean9082c4899b704fb8b9cad6351a462848" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtextarvainen2017mean9082c4899b704fb8b9cad6351a462848label">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{tarvainen2017mean,
    author = "Tarvainen, Antti and Valpola, Harri",
    title = "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
    booktitle = "Advances in neural information processing systems",
    pages = "1195--1204",
    year = "2017",
    abstract = "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="sound-separation-baseline">Sound separation baseline</h2>
<p>This baseline model consists of a TDCN++ masking network using STFT
analysis/synthesis and weighted mixture consistency, where the weights are
predicted by the network, with one scalar per source. The training loss is
thresholded negative signal-to-noise ratio (SNR).</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" target="_blank">
<span style="font-size:20px;">Task 4 - <strong>sound separation</strong> baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/>
The model architecture is able to handle variable number sources by using different loss functions for active and
inactive reference sources. For active reference sources (i.e. non-zero
reference source signals), the threshold for negative SNR is 30 dB, equivalent
to the error power being below the reference
power by 30 dB. For non-active reference sources (i.e. all-zero reference
source signals), the threshold is 20 dB measured relative to the mixture power,
which means gradients are clipped when the error power is 20 dB below the mixture power.
This model architecture achieves the following performance when trained and evaluated on the two variants of the FUSS dataset: reverberant and and dry (i.e. non-reverberant):</p>
<table class="table table-striped">
<thead>
<tr>
<th></th>
<th colspan="2">Validation</th>
<th colspan="2">Eval</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Single-source SI-SNR</td>
<td>Multi-source SI-SNRi</td>
<td>Single-source SI-SNR</td>
<td>Multi-source SI-SNRi</td>
</tr>
<tr>
<td>Reverberant FUSS</td>
<td>35.0 dB</td>
<td>13.0 dB</td>
<td>37.6 dB</td>
<td>12.5 dB</td>
</tr>
<tr>
<td>Dry FUSS</td>
<td>30.6 dB</td>
<td>10.5 dB</td>
<td>31.8 dB</td>
<td>10.2 dB</td>
</tr>
</tbody>
</table>
<h3>Download</h3>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" target="_blank">
<span style="font-size:20px;">Task 4 - <strong>sound separation</strong> baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h4>Reverberant FUSS baseline</h4>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_model_train.sh" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_model_train.sh" target="_blank">
<span style="font-size:20px;">Training script for reverberant FUSS baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_model_evaluate.sh" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_model_evaluate.sh" target="_blank">
<span style="font-size:20px;">Evaluation script for reverberant FUSS baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3743844/files/FUSS_baseline_model.tar.gz?download=1" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-table fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3743844/files/FUSS_baseline_model.tar.gz?download=1" target="_blank">
<span style="font-size:20px;">Reverberant FUSS baseline DNN pre-trained weights <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(102.3 MB)</span>
<br/>
</div>
</div>
<p><br/></p>
<h4>Dry FUSS baseline</h4>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_dry_model_train.sh" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_dry_model_train.sh" target="_blank">
<span style="font-size:20px;">Training script for dry FUSS baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_dry_model_evaluate.sh" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline/run_baseline_dry_model_evaluate.sh" target="_blank">
<span style="font-size:20px;">Evaluation script for dry FUSS baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3743844/files/FUSS_baseline_dry_model.tar.gz?download=1" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-table fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3743844/files/FUSS_baseline_dry_model.tar.gz?download=1" target="_blank">
<span style="font-size:20px;">Dry FUSS baseline DNN pre-trained weights <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(102.3 MB)</span>
<br/>
</div>
</div>
<p><br/></p>
<div class="btex-item" data-item="Kavalerov_WASPAA2019" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Kavalerov_WASPAA2019"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Ilya Kavalerov, Scott Wisdom, Hakan Erdogan, Brian Patton, Kevin Wilson, Jonathan <span class="bibtex-protected">Le Roux</span>, and John R. Hershey.
<em>Universal sound separation.</em>
In <span class="bibtex-protected">IEEE</span> Workshop on Applications of Signal Processing to Audio and Acoustics (<span class="bibtex-protected">WASPAA</span>), 175–179. October 2019.
URL: <a href="https://arxiv.org/abs/1905.03330">https://arxiv.org/abs/1905.03330</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1905.03330" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" class="panel-collapse collapse" id="collapseKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" role="tabpanel">
<h4>Universal Sound Separation</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1905.03330" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexKavalerov_WASPAA201976d356bc66a4469390953e2d82c908dalabel" class="modal fade" id="bibtexKavalerov_WASPAA201976d356bc66a4469390953e2d82c908da" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexKavalerov_WASPAA201976d356bc66a4469390953e2d82c908dalabel">Universal Sound Separation</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kavalerov_WASPAA2019,
    Author = "Kavalerov, Ilya and Wisdom, Scott and Erdogan, Hakan and Patton, Brian and Wilson, Kevin and {Le Roux}, Jonathan and Hershey, John R.",
    title = "Universal Sound Separation",
    year = "2019",
    booktitle = "{IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics ({WASPAA})",
    month = "October",
    pages = "175--179",
    url = "https://arxiv.org/abs/1905.03330"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Tzinis_ICASSP2020" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Tzinis_ICASSP2020"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Efthymios Tzinis, Scott Wisdom, John R. Hershey, Aren Jansen, and Daniel P. W. Ellis.
<em>Improving universal sound separation using sound classification.</em>
In <span class="bibtex-protected">IEEE</span> International Conference on Acoustics, Speech, and Signal Processing (<span class="bibtex-protected">ICASSP</span>). May 2020.
URL: <a href="https://arxiv.org/abs/1911.07951">https://arxiv.org/abs/1911.07951</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1911.07951" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" class="panel-collapse collapse" id="collapseTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" role="tabpanel">
<h4>Improving Universal Sound Separation Using Sound Classification</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1911.07951" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654label" class="modal fade" id="bibtexTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTzinis_ICASSP20206c2c06d2feaf4150ba66ef7b799ab654label">Improving Universal Sound Separation Using Sound Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tzinis_ICASSP2020,
    Author = "Tzinis, Efthymios and Wisdom, Scott and Hershey, John R. and Jansen, Aren and Ellis, Daniel P. W.",
    title = "Improving Universal Sound Separation Using Sound Classification",
    year = "2020",
    booktitle = "{IEEE} International Conference on Acoustics, Speech, and Signal Processing ({ICASSP})",
    month = "May",
    url = "https://arxiv.org/abs/1911.07951"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Wisdom_ICASSP2019" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Wisdom_ICASSP2019"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Scott Wisdom, John R Hershey, Kevin Wilson, Jeremy Thorpe, Michael Chinen, Brian Patton, and Rif A Saurous.
<em>Differentiable consistency constraints for improved deep speech enhancement.</em>
In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 900–904. IEEE, 2019.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1811.08521.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" class="panel-collapse collapse" id="collapseWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" role="tabpanel">
<h4>Differentiable consistency constraints for improved deep speech enhancement</h4>
<h5>Abstract</h5>
<p class="text-justify">In recent years, deep networks have led to dramatic improvements in speech enhancement by framing it as a data-driven pattern recognition problem. In many modern enhancement systems, large amounts of data are used to train a deep network to estimate masks for complex-valued short-time Fourier transforms (STFTs) to suppress noise and preserve speech. However, current masking approaches often neglect two important constraints: STFT consistency and mixture consistency. Without STFT consistency, the system's output is not necessarily the STFT of a time-domain signal, and without mixture consistency, the sum of the estimated sources does not necessarily equal the input mixture. Furthermore, the only previous approaches that apply mixture consistency use real-valued masks; mixture consistency has been ignored for complex-valued masks.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1811.08521.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexWisdom_ICASSP20190620864918eb4cac9e06144439f5d082label" class="modal fade" id="bibtexWisdom_ICASSP20190620864918eb4cac9e06144439f5d082" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexWisdom_ICASSP20190620864918eb4cac9e06144439f5d082label">Differentiable consistency constraints for improved deep speech enhancement</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wisdom_ICASSP2019,
    author = "Wisdom, Scott and Hershey, John R and Wilson, Kevin and Thorpe, Jeremy and Chinen, Michael and Patton, Brian and Saurous, Rif A",
    title = "Differentiable consistency constraints for improved deep speech enhancement",
    booktitle = "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "900--904",
    year = "2019",
    organization = "IEEE",
    abstract = "In recent years, deep networks have led to dramatic improvements in speech enhancement by framing it as a data-driven pattern recognition problem. In many modern enhancement systems, large amounts of data are used to train a deep network to estimate masks for complex-valued short-time Fourier transforms (STFTs) to suppress noise and preserve speech. However, current masking approaches often neglect two important constraints: STFT consistency and mixture consistency. Without STFT consistency, the system's output is not necessarily the STFT of a time-domain signal, and without mixture consistency, the sum of the estimated sources does not necessarily equal the input mixture. Furthermore, the only previous approaches that apply mixture consistency use real-valued masks; mixture consistency has been ignored for complex-valued masks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="sound-event-detection-and-separation-baseline">Sound event detection and separation baseline</h2>
<p>For the combined SED+SS baseline, we mix together audio from DESED and FUSS training and validation data, to create new mixtures with both in-domain (DESED) and open-domain (FUSS) sources.  The sound separation model is trained to separate these mixtures into three output signals: DESED background, mixture of DESED foreground sounds, and mixture of FUSS sounds. This model is trained in the same way as the baseline SS model, except without permutation invariance. On the DESED+FUSS validation set, this model achieves an average of <strong>18.6 dB</strong> SI-SNR improvement for the separated DESED foreground mixture, which is used as input to the SED model.</p>
<p>The baseline to combine SS and SED is then relying late integration of <a href="#sed_base">SED baseline</a> applied on separated sound sources.</p>
<p>The sound separation baseline has been trained using 3 sources, so it returns:</p>
<ul>
<li>DESED background</li>
<li>DESED foreground</li>
<li>FUSS mixture</li>
</ul>
<p>In our case, we use only the output of the second source (DESED foreground).</p>
<p>To get the predictions of the combination of SED and SS we do as follow:</p>
<ul>
<li>Get the output (not binarized with threshold) on the original mixtures using  the <a href="#sed_base">SED baseline</a></li>
<li>Get the output (not binarized with threshold) on the DESED foreground source from SS model <a href="#sed_base">SED baseline</a></li>
<li>Take the average of both outputs</li>
<li>Apply thresholds (different for F-scores and psds)</li>
<li>Apply median filtering (0.45s)</li>
</ul>
<div class="btex-item" data-item="Turpault2020b" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2020b"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault, Scott Wisdom Wisdom, Hakan Erdogan, John R. Herhey, Romain Serizel, Eduardo Fonseca, and Prem Seetharaman.
<em>Improving sound event detection in domestic environments using sound separation.</em>
working paper or preprint, 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" class="panel-collapse collapse" id="collapseTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" role="tabpanel">
<h4>Improving Sound Event Detection In Domestic Environments Using Sound Separation</h4>
<h5>Abstract</h5>
<p class="text-justify">Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2020b28146293b7314d2e9cd3b32c3f4f5587label" class="modal fade" id="bibtexTurpault2020b28146293b7314d2e9cd3b32c3f4f5587" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2020b28146293b7314d2e9cd3b32c3f4f5587label">Improving Sound Event Detection In Domestic Environments Using Sound Separation</h4>
</div>
<div class="modal-body">
<pre>@unpublished{Turpault2020b,
    AUTHOR = "Turpault, Nicolas and Wisdom, Scott Wisdom and Erdogan, Hakan and Herhey, John R. and Serizel, Romain and Fonseca, Eduardo and Seetharaman, Prem",
    title = "Improving Sound Event Detection In Domestic Environments Using Sound Separation",
    note = "working paper or preprint",
    institution = "Universit{\'e} de Lorraine, CNRS, Inria, Loria, France",
    year = "2020",
    abstract = "Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h3>Performance</h3>
<table class="table table-striped">
<thead>
<tr>
<td></td>
<td><strong>Macro F-score Event-based</strong></td>
<td><strong>PSDS macro F-score</strong></td>
<td>PSDS </td>
<td>PSDS cross-trigger</td>
<td>PSDS macro</td>
</tr>
</thead>
<tbody>
<tr>
<td>Validation</td>
<td><strong> 35.6 % </strong></td>
<td><strong> 60.5% </strong></td>
<td> 0.626 </td>
<td> 0.546 </td>
<td> 0.449 </td>
</tr>
</tbody>
</table>
<h3>Download</h3>
<h4>Sound separation</h4>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" target="_blank">
<span style="font-size:20px;">Task 4 - <strong>sound separation</strong> baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_desed_fuss_baseline/run_baseline_model_train.sh" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_desed_fuss_baseline/run_baseline_model_train.sh" target="_blank">
<span style="font-size:20px;">Training script for DESED/FUSS baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_desed_fuss_baseline/run_baseline_model_evaluate.sh" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_desed_fuss_baseline/run_baseline_model_evaluate.sh" target="_blank">
<span style="font-size:20px;">Evaluation script for DESED/FUSS baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3743844/files/FUSS_DESED_baseline_dry_2_model.tar.gz?download=1" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-table fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3743844/files/FUSS_DESED_baseline_dry_2_model.tar.gz?download=1" target="_blank">
<span style="font-size:20px;">DESED/FUSS baseline DNN pre-trained weights <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(101 MB)</span>
<br/>
</div>
</div>
<p><br/></p>
<h4>Sound event detection</h4>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" target="_blank">
<span style="font-size:20px;">Task 4 - <strong>sound event detection</strong> baseline <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/3745475" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-table fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/3745475" target="_blank">
<span style="font-size:20px;">SED baseline DNN pre-trained weights <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(18.3 MB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3726376">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3726376.svg"/>
</a>
<span class="text-muted">
                
                version 2
                
                
                </span>
</div>
</div>
<p><br/></p>
<h1 id="citation">Citation</h1>
<p>If you are using the <strong>dataset</strong> or <strong>baseline</strong> code, or want to refer <strong>challenge task</strong> please cite the following paper:</p>
<h2 id="task-and-datasets">Task and datasets</h2>
<div class="btex-item" data-item="Turpault2019_DCASE" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2019_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault, Romain Serizel, Ankit Parag Shah, and Justin Salamon.
<em>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis.</em>
In <span class="bibtex-protected">Workshop on Detection and Classification of Acoustic Scenes and Events</span>. New York City, United States, October 2019.
URL: <a href="https://hal.inria.fr/hal-02160855">https://hal.inria.fr/hal-02160855</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" class="panel-collapse collapse" id="collapseTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" role="tabpanel">
<h4>Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10% points in terms of F-measure.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02160855/file/Sound_event_detection_in_domestic_environments_with_weakly_labeled_data_and_soundscape_synthesis.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7label" class="modal fade" id="bibtexTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2019_DCASE2394d55ed5ac4b5eb9a7de503f7da8e7label">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Turpault2019_DCASE,
    Author = "Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in domestic environments with weakly labeled data and soundscape synthesis}",
    booktitle = "{Workshop on Detection and Classification of Acoustic Scenes and Events}",
    address = "New York City, United States",
    year = "2019",
    month = "October",
    keywords = "Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02160855",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02160855"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Wisdom_InPrep2020" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Wisdom_InPrep2020"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Scott Wisdom, Hakan Erdogan, Daniel P. W. Ellis, Romain Serizel, Nicolas Turpault, Eduardo Fonseca, Justin Salamon, Prem Seetharaman, and John R. Hershey.
<em>What's all the fuss about free universal sound separation data?</em>
In in preparation. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexWisdom_InPrep2020e2914f501b45458e94299e306011a025" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseWisdom_InPrep2020e2914f501b45458e94299e306011a025" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseWisdom_InPrep2020e2914f501b45458e94299e306011a025" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingWisdom_InPrep2020e2914f501b45458e94299e306011a025" class="panel-collapse collapse" id="collapseWisdom_InPrep2020e2914f501b45458e94299e306011a025" role="tabpanel">
<h4>What's All the FUSS About Free Universal Sound Separation Data?</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexWisdom_InPrep2020e2914f501b45458e94299e306011a025" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexWisdom_InPrep2020e2914f501b45458e94299e306011a025label" class="modal fade" id="bibtexWisdom_InPrep2020e2914f501b45458e94299e306011a025" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexWisdom_InPrep2020e2914f501b45458e94299e306011a025label">What's All the FUSS About Free Universal Sound Separation Data?</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wisdom_InPrep2020,
    Author = "Wisdom, Scott and Erdogan, Hakan and Ellis, Daniel P. W. and Serizel, Romain and Turpault, Nicolas and Fonseca, Eduardo and Salamon, Justin and Seetharaman, Prem and Hershey, John R.",
    title = "What's All the FUSS About Free Universal Sound Separation Data?",
    year = "2020",
    booktitle = "in preparation"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="baselines-1">Baselines</h2>
<h3>Sound event detection</h3>
<div class="btex-item" data-item="Turpault2020a" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2020a"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault and Romain Serizel.
<em>Training sound event detection on a heterogeneous dataset.</em>
working paper or preprint, 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" class="panel-collapse collapse" id="collapseTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" role="tabpanel">
<h4>Training Sound Event Detection On A Heterogeneous Dataset</h4>
<h5>Abstract</h5>
<p class="text-justify">Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2020a40ceda8b413c4379be6cf7bd198e0d46label" class="modal fade" id="bibtexTurpault2020a40ceda8b413c4379be6cf7bd198e0d46" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2020a40ceda8b413c4379be6cf7bd198e0d46label">Training Sound Event Detection On A Heterogeneous Dataset</h4>
</div>
<div class="modal-body">
<pre>@unpublished{Turpault2020a,
    AUTHOR = "Turpault, Nicolas and Serizel, Romain",
    title = "Training Sound Event Detection On A Heterogeneous Dataset",
    note = "working paper or preprint",
    institution = "Universit{\'e} de Lorraine, CNRS, Inria, Loria, France",
    year = "2020",
    abstract = "Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h3>Sound separation</h3>
<div class="btex-item" data-item="Kavalerov_WASPAA2019" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Kavalerov_WASPAA2019"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Ilya Kavalerov, Scott Wisdom, Hakan Erdogan, Brian Patton, Kevin Wilson, Jonathan <span class="bibtex-protected">Le Roux</span>, and John R. Hershey.
<em>Universal sound separation.</em>
In <span class="bibtex-protected">IEEE</span> Workshop on Applications of Signal Processing to Audio and Acoustics (<span class="bibtex-protected">WASPAA</span>), 175–179. October 2019.
URL: <a href="https://arxiv.org/abs/1905.03330">https://arxiv.org/abs/1905.03330</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1905.03330" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" class="panel-collapse collapse" id="collapseKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" role="tabpanel">
<h4>Universal Sound Separation</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1905.03330" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7label" class="modal fade" id="bibtexKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexKavalerov_WASPAA20198d2a47a9efd64b50bee6213cd8fd6cc7label">Universal Sound Separation</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kavalerov_WASPAA2019,
    Author = "Kavalerov, Ilya and Wisdom, Scott and Erdogan, Hakan and Patton, Brian and Wilson, Kevin and {Le Roux}, Jonathan and Hershey, John R.",
    title = "Universal Sound Separation",
    year = "2019",
    booktitle = "{IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics ({WASPAA})",
    month = "October",
    pages = "175--179",
    url = "https://arxiv.org/abs/1905.03330"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Tzinis_ICASSP2020" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Tzinis_ICASSP2020"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Efthymios Tzinis, Scott Wisdom, John R. Hershey, Aren Jansen, and Daniel P. W. Ellis.
<em>Improving universal sound separation using sound classification.</em>
In <span class="bibtex-protected">IEEE</span> International Conference on Acoustics, Speech, and Signal Processing (<span class="bibtex-protected">ICASSP</span>). May 2020.
URL: <a href="https://arxiv.org/abs/1911.07951">https://arxiv.org/abs/1911.07951</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTzinis_ICASSP20200653413582784cc2933952edda1c8324" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1911.07951" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseTzinis_ICASSP20200653413582784cc2933952edda1c8324" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTzinis_ICASSP20200653413582784cc2933952edda1c8324" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTzinis_ICASSP20200653413582784cc2933952edda1c8324" class="panel-collapse collapse" id="collapseTzinis_ICASSP20200653413582784cc2933952edda1c8324" role="tabpanel">
<h4>Improving Universal Sound Separation Using Sound Classification</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTzinis_ICASSP20200653413582784cc2933952edda1c8324" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1911.07951" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTzinis_ICASSP20200653413582784cc2933952edda1c8324label" class="modal fade" id="bibtexTzinis_ICASSP20200653413582784cc2933952edda1c8324" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTzinis_ICASSP20200653413582784cc2933952edda1c8324label">Improving Universal Sound Separation Using Sound Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tzinis_ICASSP2020,
    Author = "Tzinis, Efthymios and Wisdom, Scott and Hershey, John R. and Jansen, Aren and Ellis, Daniel P. W.",
    title = "Improving Universal Sound Separation Using Sound Classification",
    year = "2020",
    booktitle = "{IEEE} International Conference on Acoustics, Speech, and Signal Processing ({ICASSP})",
    month = "May",
    url = "https://arxiv.org/abs/1911.07951"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Wisdom_ICASSP2019" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Wisdom_ICASSP2019"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Scott Wisdom, John R Hershey, Kevin Wilson, Jeremy Thorpe, Michael Chinen, Brian Patton, and Rif A Saurous.
<em>Differentiable consistency constraints for improved deep speech enhancement.</em>
In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 900–904. IEEE, 2019.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1811.08521.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" class="panel-collapse collapse" id="collapseWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" role="tabpanel">
<h4>Differentiable consistency constraints for improved deep speech enhancement</h4>
<h5>Abstract</h5>
<p class="text-justify">In recent years, deep networks have led to dramatic improvements in speech enhancement by framing it as a data-driven pattern recognition problem. In many modern enhancement systems, large amounts of data are used to train a deep network to estimate masks for complex-valued short-time Fourier transforms (STFTs) to suppress noise and preserve speech. However, current masking approaches often neglect two important constraints: STFT consistency and mixture consistency. Without STFT consistency, the system's output is not necessarily the STFT of a time-domain signal, and without mixture consistency, the sum of the estimated sources does not necessarily equal the input mixture. Furthermore, the only previous approaches that apply mixture consistency use real-valued masks; mixture consistency has been ignored for complex-valued masks.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1811.08521.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9label" class="modal fade" id="bibtexWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexWisdom_ICASSP20192b1d4e35ead3413f98a0f2aecddc69e9label">Differentiable consistency constraints for improved deep speech enhancement</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wisdom_ICASSP2019,
    author = "Wisdom, Scott and Hershey, John R and Wilson, Kevin and Thorpe, Jeremy and Chinen, Michael and Patton, Brian and Saurous, Rif A",
    title = "Differentiable consistency constraints for improved deep speech enhancement",
    booktitle = "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    pages = "900--904",
    year = "2019",
    organization = "IEEE",
    abstract = "In recent years, deep networks have led to dramatic improvements in speech enhancement by framing it as a data-driven pattern recognition problem. In many modern enhancement systems, large amounts of data are used to train a deep network to estimate masks for complex-valued short-time Fourier transforms (STFTs) to suppress noise and preserve speech. However, current masking approaches often neglect two important constraints: STFT consistency and mixture consistency. Without STFT consistency, the system's output is not necessarily the STFT of a time-domain signal, and without mixture consistency, the sum of the estimated sources does not necessarily equal the input mixture. Furthermore, the only previous approaches that apply mixture consistency use real-valued masks; mixture consistency has been ignored for complex-valued masks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h3>Sound separation and sound event detection</h3>
<div class="btex-item" data-item="Turpault2020b" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Turpault2020b"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nicolas Turpault, Scott Wisdom Wisdom, Hakan Erdogan, John R. Herhey, Romain Serizel, Eduardo Fonseca, and Prem Seetharaman.
<em>Improving sound event detection in domestic environments using sound separation.</em>
working paper or preprint, 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" class="panel-collapse collapse" id="collapseTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" role="tabpanel">
<h4>Improving Sound Event Detection In Domestic Environments Using Sound Separation</h4>
<h5>Abstract</h5>
<p class="text-justify">Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9label" class="modal fade" id="bibtexTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexTurpault2020bd0bd49a0b3a04b339fec33c0a160e1d9label">Improving Sound Event Detection In Domestic Environments Using Sound Separation</h4>
</div>
<div class="modal-body">
<pre>@unpublished{Turpault2020b,
    AUTHOR = "Turpault, Nicolas and Wisdom, Scott Wisdom and Erdogan, Hakan and Herhey, John R. and Serizel, Romain and Fonseca, Eduardo and Seetharaman, Prem",
    title = "Improving Sound Event Detection In Domestic Environments Using Sound Separation",
    note = "working paper or preprint",
    institution = "Universit{\'e} de Lorraine, CNRS, Inria, Loria, France",
    year = "2020",
    abstract = "Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>