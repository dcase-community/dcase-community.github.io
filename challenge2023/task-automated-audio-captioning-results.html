<!DOCTYPE html><html lang="en">
<head>
    <title>Automated Audio Captioning - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2023/task-automated-audio-captioning-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description Automated audio captioning is the task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. Given the novelty of …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2023</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2023/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-sound-event-detection-with-weak-and-soft-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-weak-and-soft-labels"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>A: Sound Event Detection with Weak Labels and Synthetic Soundscapes</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-weak-labels-and-synthetic-soundscapes"><i class="fa fa-random fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-weak-labels-and-synthetic-soundscapes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>B: Sound Event Detection with Soft Labels</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-soft-labels"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-sound-event-detection-with-soft-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2023/task-automated-audio-captioning-and-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-automated-audio-captioning-and-language-based-audio-retrieval"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>A: Automated Audio-Captioning</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-automated-audio-captioning"><i class="fa dc-captioning fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2023/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>B: Language-Based Audio Retrieval</strong>
    </li>
            <li class="">
        <a href="/challenge2023/task-language-based-audio-retrieval"><i class="fa fa-file-text fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2023/task-foley-sound-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-task2"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2023/task-foley-sound-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2023/task-foley-sound-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2023/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2023/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wall-22.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-task1"></i><strong class="fa-stack-1x icon-text">A</strong><strong class="fa-stack-1x dcase-icon-top-text">Captioning</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 6</span></span><img src="../images/logos/dcase/dcase2023_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Automated Audio Captioning</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#systems-ranking-challenge-metrics">Systems ranking, challenge metrics</a></li>
<li><a href="#systems-ranking-additional-metrics">Systems ranking, additional metrics</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#overview-of-characteristics">Overview of characteristics</a></li>
<li><a href="#detailed-characteristics">Detailed characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>Automated audio captioning is the task of general audio content
description using free text. It is an intermodal translation task
(not speech-to-text), where a system accepts as an input an audio
signal and outputs the textual description (i.e. the caption) of
that signal. Given the novelty of the task of audio captioning,
current focus is on exploring and developing different methods
that can provide some kind of captions for a general audio recording.
To this aim, the Clotho dataset is used, which provides
good quality captions, without speech transcription, named entities,
and hapax legomena (i.e. words that appear once in a split).</p>
<p>Participants used the freely available splits of Clotho development
and evaluation, as well as any external data they deemed fit.
The developed systems are evaluated on their generated captions,
using the testing split of Clotho, which does not provide the corresponding
captions for the audio. More information about Task 6a: Automated
Audio Captioning can be found at the
<a class="btn btn-primary" href="/challenge2023/task-automated-audio-captioning" style="">task description page.</a></p>
<p>The ranking of the submitted systems is based on the achieved SPIDEr
metric penalized by fluency error detection (SPIDEr-FL). Though, in this page is provided a more thorough presentation,
grouping the metrics into those that are originated from machine translation
and to those that originated from captioning.</p>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Here are listed the best systems from all teams. The ranking is based on the
SPIDEr-FL. For more elaborated exploration of the performance of the
different systems, at the same table are listed the values achieved for
all the metrics employed in the task. The values for the metrics are for
the Clotho testing split and the Clotho evaluation split. The values for the
Clotho evaluation split are provided in order to allow further comparison
with systems and methods developed outside of this task, since captions for the Clotho
evaluation split are freely available.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="tes_spiderfl" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected<br/> metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="4">Submission Information</th>
<th class="sep-left-cell text-center" colspan="5">Clotho testing split</th>
<th class="sep-left-cell text-center" colspan="5">Clotho evaluation split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
              Best official <br/>system rank
            </th>
<th data-field="corresponding_author" data-sortable="false">
              Corresponding author
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="tes_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spiderfl" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eva_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="eva_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spiderfl" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wu_t6a_4</td>
<td>1</td>
<td>Shih-Lun Wu</td>
<td>wu2023_t6a</td>
<td>0.195</td>
<td>0.505</td>
<td>0.149</td>
<td>0.327</td>
<td>0.327</td>
<td>0.197</td>
<td>0.505</td>
<td>0.145</td>
<td>0.325</td>
<td>0.325</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_4</td>
<td>2</td>
<td>Joon-Hyuk Chang</td>
<td>chang2023_t6a</td>
<td>0.197</td>
<td>0.539</td>
<td>0.149</td>
<td>0.344</td>
<td>0.315</td>
<td>0.197</td>
<td>0.541</td>
<td>0.146</td>
<td>0.343</td>
<td>0.313</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_4</td>
<td>3</td>
<td>Etienne Labbe</td>
<td>labbe2023_t6a</td>
<td>0.193</td>
<td>0.486</td>
<td>0.142</td>
<td>0.314</td>
<td>0.314</td>
<td>0.193</td>
<td>0.500</td>
<td>0.140</td>
<td>0.320</td>
<td>0.320</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_4</td>
<td>4</td>
<td>Zhiyong Yan</td>
<td>yan2023_t6a</td>
<td>0.191</td>
<td>0.461</td>
<td>0.139</td>
<td>0.300</td>
<td>0.289</td>
<td>0.192</td>
<td>0.474</td>
<td>0.136</td>
<td>0.305</td>
<td>0.294</td>
</tr>
<tr>
<td></td>
<td>Schaumloeffel_t6a_1</td>
<td>5</td>
<td>Timothy Schaumloeffel</td>
<td>schaumloeffel2023_t6a</td>
<td>0.181</td>
<td>0.436</td>
<td>0.130</td>
<td>0.283</td>
<td>0.282</td>
<td>0.183</td>
<td>0.454</td>
<td>0.132</td>
<td>0.293</td>
<td>0.292</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_3</td>
<td>6</td>
<td>Jian Guan</td>
<td>guan2023_t6a</td>
<td>0.180</td>
<td>0.427</td>
<td>0.129</td>
<td>0.278</td>
<td>0.273</td>
<td>0.184</td>
<td>0.450</td>
<td>0.129</td>
<td>0.290</td>
<td>0.283</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_1</td>
<td>7</td>
<td>Marek Kadlčík</td>
<td>kadlčík2023_t6a</td>
<td>0.172</td>
<td>0.414</td>
<td>0.123</td>
<td>0.269</td>
<td>0.267</td>
<td>0.378</td>
<td>0.433</td>
<td>0.126</td>
<td>0.279</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_t6a_1</td>
<td>8</td>
<td>Kyogu Lee</td>
<td>lee2023_t6a</td>
<td>0.176</td>
<td>0.416</td>
<td>0.123</td>
<td>0.269</td>
<td>0.266</td>
<td>0.177</td>
<td>0.431</td>
<td>0.126</td>
<td>0.279</td>
<td>0.275</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>9</td>
<td>Felix Gontier</td>
<td>gontier2023_t6a</td>
<td>0.177</td>
<td>0.415</td>
<td>0.126</td>
<td>0.271</td>
<td>0.264</td>
<td>0.177</td>
<td>0.420</td>
<td>0.119</td>
<td>0.270</td>
<td>0.261</td>
</tr>
<tr>
<td></td>
<td>Greeshma_t6a_1</td>
<td>10</td>
<td>Karanth Greeshma</td>
<td>greeshma2023_t6a</td>
<td>0.178</td>
<td>0.406</td>
<td>0.125</td>
<td>0.265</td>
<td>0.261</td>
<td>0.178</td>
<td>0.419</td>
<td>0.121</td>
<td>0.270</td>
<td>0.264</td>
</tr>
<tr>
<td></td>
<td>Lim_t6a_1</td>
<td>11</td>
<td>Changwon Lim</td>
<td>lim2023_t6a</td>
<td>0.089</td>
<td>0.035</td>
<td>0.039</td>
<td>0.037</td>
<td>0.010</td>
<td>0.089</td>
<td>0.034</td>
<td>0.038</td>
<td>0.036</td>
<td>0.011</td>
</tr>
</tbody>
</table>
<h1 id="systems-ranking">Systems ranking</h1>
<p>Here are listed all submitted systems and their ranking according to the different
metrics and grouping of metrics. The first table shows all challenge metrics and
all systems, and the second table shows all systems but with contrastive metrics.</p>
<p>Detailed information for each system is provided in the next section.</p>
<h2 id="systems-ranking-challenge-metrics">Systems ranking, challenge metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="tes_spiderfl" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected<br/> metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="3">Submission Information</th>
<th class="sep-left-cell text-center" colspan="5">Clotho testing split</th>
<th class="sep-left-cell text-center" colspan="5">Clotho evaluation split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
              Best official <br/>system rank
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="tes_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spiderfl" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eva_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="eva_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spiderfl" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wu_t6a_4</td>
<td>1</td>
<td>wu2023_t6a</td>
<td>0.195</td>
<td>0.505</td>
<td>0.149</td>
<td>0.327</td>
<td>0.327</td>
<td>0.197</td>
<td>0.505</td>
<td>0.145</td>
<td>0.325</td>
<td>0.325</td>
</tr>
<tr>
<td></td>
<td>Wu_t6a_3</td>
<td>2</td>
<td>wu2023_t6a</td>
<td>0.196</td>
<td>0.504</td>
<td>0.149</td>
<td>0.326</td>
<td>0.326</td>
<td>0.197</td>
<td>0.525</td>
<td>0.147</td>
<td>0.336</td>
<td>0.336</td>
</tr>
<tr>
<td></td>
<td>Wu_t6a_2</td>
<td>3</td>
<td>wu2023_t6a</td>
<td>0.196</td>
<td>0.499</td>
<td>0.149</td>
<td>0.324</td>
<td>0.324</td>
<td>0.198</td>
<td>0.510</td>
<td>0.147</td>
<td>0.329</td>
<td>0.329</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_4</td>
<td>4</td>
<td>chang2023_t6a</td>
<td>0.197</td>
<td>0.539</td>
<td>0.149</td>
<td>0.344</td>
<td>0.315</td>
<td>0.197</td>
<td>0.541</td>
<td>0.146</td>
<td>0.343</td>
<td>0.313</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_4</td>
<td>5</td>
<td>labbe2023_t6a</td>
<td>0.193</td>
<td>0.486</td>
<td>0.142</td>
<td>0.314</td>
<td>0.314</td>
<td>0.193</td>
<td>0.500</td>
<td>0.140</td>
<td>0.320</td>
<td>0.320</td>
</tr>
<tr>
<td></td>
<td>Wu_t6a_1</td>
<td>6</td>
<td>wu2023_t6a</td>
<td>0.190</td>
<td>0.477</td>
<td>0.145</td>
<td>0.311</td>
<td>0.311</td>
<td>0.193</td>
<td>0.506</td>
<td>0.146</td>
<td>0.326</td>
<td>0.326</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_3</td>
<td>7</td>
<td>labbe2023_t6a</td>
<td>0.192</td>
<td>0.479</td>
<td>0.141</td>
<td>0.310</td>
<td>0.309</td>
<td>0.192</td>
<td>0.485</td>
<td>0.139</td>
<td>0.312</td>
<td>0.310</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_1</td>
<td>8</td>
<td>chang2023_t6a</td>
<td>0.188</td>
<td>0.486</td>
<td>0.138</td>
<td>0.312</td>
<td>0.308</td>
<td>0.188</td>
<td>0.483</td>
<td>0.137</td>
<td>0.309</td>
<td>0.307</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_2</td>
<td>9</td>
<td>labbe2023_t6a</td>
<td>0.189</td>
<td>0.470</td>
<td>0.139</td>
<td>0.304</td>
<td>0.304</td>
<td>0.190</td>
<td>0.474</td>
<td>0.136</td>
<td>0.305</td>
<td>0.303</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_4</td>
<td>10</td>
<td>yan2023_t6a</td>
<td>0.191</td>
<td>0.461</td>
<td>0.139</td>
<td>0.300</td>
<td>0.289</td>
<td>0.192</td>
<td>0.474</td>
<td>0.136</td>
<td>0.305</td>
<td>0.294</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_3</td>
<td>11</td>
<td>yan2023_t6a</td>
<td>0.190</td>
<td>0.457</td>
<td>0.139</td>
<td>0.298</td>
<td>0.288</td>
<td>0.190</td>
<td>0.468</td>
<td>0.135</td>
<td>0.302</td>
<td>0.292</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_1</td>
<td>12</td>
<td>yan2023_t6a</td>
<td>0.187</td>
<td>0.445</td>
<td>0.137</td>
<td>0.291</td>
<td>0.282</td>
<td>0.191</td>
<td>0.471</td>
<td>0.136</td>
<td>0.304</td>
<td>0.295</td>
</tr>
<tr>
<td></td>
<td>Schaumloeffel_t6a_1</td>
<td>13</td>
<td>schaumloeffel2023_t6a</td>
<td>0.181</td>
<td>0.436</td>
<td>0.130</td>
<td>0.283</td>
<td>0.282</td>
<td>0.183</td>
<td>0.454</td>
<td>0.132</td>
<td>0.293</td>
<td>0.292</td>
</tr>
<tr>
<td></td>
<td>Schaumloeffel_t6a_2</td>
<td>14</td>
<td>schaumloeffel2023_t6a</td>
<td>0.178</td>
<td>0.425</td>
<td>0.124</td>
<td>0.274</td>
<td>0.274</td>
<td>0.179</td>
<td>0.443</td>
<td>0.126</td>
<td>0.285</td>
<td>0.284</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_3</td>
<td>15</td>
<td>guan2023_t6a</td>
<td>0.180</td>
<td>0.427</td>
<td>0.129</td>
<td>0.278</td>
<td>0.273</td>
<td>0.184</td>
<td>0.450</td>
<td>0.129</td>
<td>0.290</td>
<td>0.283</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_4</td>
<td>16</td>
<td>guan2023_t6a</td>
<td>0.181</td>
<td>0.429</td>
<td>0.130</td>
<td>0.279</td>
<td>0.272</td>
<td>0.184</td>
<td>0.443</td>
<td>0.128</td>
<td>0.285</td>
<td>0.279</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_2</td>
<td>17</td>
<td>yan2023_t6a</td>
<td>0.185</td>
<td>0.424</td>
<td>0.132</td>
<td>0.278</td>
<td>0.270</td>
<td>0.189</td>
<td>0.460</td>
<td>0.136</td>
<td>0.298</td>
<td>0.286</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_1</td>
<td>18</td>
<td>guan2023_t6a</td>
<td>0.180</td>
<td>0.421</td>
<td>0.131</td>
<td>0.276</td>
<td>0.270</td>
<td>0.182</td>
<td>0.438</td>
<td>0.126</td>
<td>0.282</td>
<td>0.275</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_1</td>
<td>19</td>
<td>kadlčík2023_t6a</td>
<td>0.172</td>
<td>0.414</td>
<td>0.123</td>
<td>0.269</td>
<td>0.267</td>
<td>0.378</td>
<td>0.433</td>
<td>0.126</td>
<td>0.279</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_t6a_1</td>
<td>20</td>
<td>lee2023_t6a</td>
<td>0.176</td>
<td>0.416</td>
<td>0.123</td>
<td>0.269</td>
<td>0.266</td>
<td>0.177</td>
<td>0.431</td>
<td>0.126</td>
<td>0.279</td>
<td>0.275</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>21</td>
<td>gontier2023_t6a</td>
<td>0.177</td>
<td>0.415</td>
<td>0.126</td>
<td>0.271</td>
<td>0.264</td>
<td>0.177</td>
<td>0.420</td>
<td>0.119</td>
<td>0.270</td>
<td>0.261</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_2</td>
<td>22</td>
<td>guan2023_t6a</td>
<td>0.178</td>
<td>0.415</td>
<td>0.127</td>
<td>0.271</td>
<td>0.263</td>
<td>0.181</td>
<td>0.426</td>
<td>0.124</td>
<td>0.275</td>
<td>0.267</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_2</td>
<td>23</td>
<td>kadlčík2023_t6a</td>
<td>0.177</td>
<td>0.406</td>
<td>0.129</td>
<td>0.267</td>
<td>0.261</td>
<td>0.378</td>
<td>0.414</td>
<td>0.123</td>
<td>0.269</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Greeshma_t6a_1</td>
<td>24</td>
<td>greeshma2023_t6a</td>
<td>0.178</td>
<td>0.406</td>
<td>0.125</td>
<td>0.265</td>
<td>0.261</td>
<td>0.178</td>
<td>0.419</td>
<td>0.121</td>
<td>0.270</td>
<td>0.264</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_1</td>
<td>25</td>
<td>labbe2023_t6a</td>
<td>0.177</td>
<td>0.389</td>
<td>0.125</td>
<td>0.257</td>
<td>0.256</td>
<td>0.179</td>
<td>0.414</td>
<td>0.126</td>
<td>0.270</td>
<td>0.269</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_3</td>
<td>26</td>
<td>chang2023_t6a</td>
<td>0.194</td>
<td>0.527</td>
<td>0.142</td>
<td>0.335</td>
<td>0.231</td>
<td>0.195</td>
<td>0.539</td>
<td>0.143</td>
<td>0.341</td>
<td>0.233</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_2</td>
<td>27</td>
<td>chang2023_t6a</td>
<td>0.195</td>
<td>0.520</td>
<td>0.141</td>
<td>0.330</td>
<td>0.229</td>
<td>0.195</td>
<td>0.526</td>
<td>0.143</td>
<td>0.335</td>
<td>0.225</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_3</td>
<td>28</td>
<td>kadlčík2023_t6a</td>
<td>0.161</td>
<td>0.348</td>
<td>0.116</td>
<td>0.232</td>
<td>0.225</td>
<td>0.345</td>
<td>0.340</td>
<td>0.108</td>
<td>0.224</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_t6a_1</td>
<td>29</td>
<td>lim2023_t6a</td>
<td>0.089</td>
<td>0.035</td>
<td>0.039</td>
<td>0.037</td>
<td>0.010</td>
<td>0.089</td>
<td>0.034</td>
<td>0.038</td>
<td>0.036</td>
<td>0.011</td>
</tr>
</tbody>
</table>
<h2 id="systems-ranking-additional-metrics">Systems ranking, additional metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="tes_fense" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected<br/> metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="3">Submission Information</th>
<th class="sep-left-cell text-center" colspan="2">Clotho testing split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
              Best official <br/>system rank
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_sentence_bert" data-reversed="true" data-sortable="true" data-value-type="float3">
                Sentence-BERT
            </th>
<th class="text-center" data-chartable="true" data-field="tes_fense" data-reversed="true" data-sortable="true" data-value-type="float3">
                FENSE
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wu_t6a_4</td>
<td>1</td>
<td>wu2023_t6a</td>
<td>0.536</td>
<td>0.536</td>
</tr>
<tr>
<td></td>
<td>Wu_t6a_3</td>
<td>2</td>
<td>wu2023_t6a</td>
<td>0.536</td>
<td>0.536</td>
</tr>
<tr>
<td></td>
<td>Wu_t6a_2</td>
<td>3</td>
<td>wu2023_t6a</td>
<td>0.538</td>
<td>0.538</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_4</td>
<td>4</td>
<td>chang2023_t6a</td>
<td>0.530</td>
<td>0.488</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_4</td>
<td>5</td>
<td>labbe2023_t6a</td>
<td>0.523</td>
<td>0.522</td>
</tr>
<tr>
<td></td>
<td>Wu_t6a_1</td>
<td>6</td>
<td>wu2023_t6a</td>
<td>0.526</td>
<td>0.526</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_3</td>
<td>7</td>
<td>labbe2023_t6a</td>
<td>0.521</td>
<td>0.519</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_1</td>
<td>8</td>
<td>chang2023_t6a</td>
<td>0.527</td>
<td>0.521</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_2</td>
<td>9</td>
<td>labbe2023_t6a</td>
<td>0.523</td>
<td>0.522</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_4</td>
<td>10</td>
<td>yan2023_t6a</td>
<td>0.521</td>
<td>0.498</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_3</td>
<td>11</td>
<td>yan2023_t6a</td>
<td>0.523</td>
<td>0.501</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_1</td>
<td>12</td>
<td>yan2023_t6a</td>
<td>0.520</td>
<td>0.503</td>
</tr>
<tr>
<td></td>
<td>Schaumloeffel_t6a_1</td>
<td>13</td>
<td>schaumloeffel2023_t6a</td>
<td>0.501</td>
<td>0.498</td>
</tr>
<tr>
<td></td>
<td>Schaumloeffel_t6a_2</td>
<td>14</td>
<td>schaumloeffel2023_t6a</td>
<td>0.496</td>
<td>0.496</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_3</td>
<td>15</td>
<td>guan2023_t6a</td>
<td>0.496</td>
<td>0.487</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_4</td>
<td>16</td>
<td>guan2023_t6a</td>
<td>0.496</td>
<td>0.480</td>
</tr>
<tr>
<td></td>
<td>Yan_t6a_2</td>
<td>17</td>
<td>yan2023_t6a</td>
<td>0.509</td>
<td>0.490</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_1</td>
<td>18</td>
<td>guan2023_t6a</td>
<td>0.495</td>
<td>0.481</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_1</td>
<td>19</td>
<td>kadlčík2023_t6a</td>
<td>0.495</td>
<td>0.492</td>
</tr>
<tr>
<td></td>
<td>Lee_t6a_1</td>
<td>20</td>
<td>lee2023_t6a</td>
<td>0.500</td>
<td>0.495</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>21</td>
<td>gontier2023_t6a</td>
<td>0.482</td>
<td>0.472</td>
</tr>
<tr>
<td></td>
<td>Guan_t6a_2</td>
<td>22</td>
<td>guan2023_t6a</td>
<td>0.494</td>
<td>0.475</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_2</td>
<td>23</td>
<td>kadlčík2023_t6a</td>
<td>0.492</td>
<td>0.481</td>
</tr>
<tr>
<td></td>
<td>Greeshma_t6a_1</td>
<td>24</td>
<td>greeshma2023_t6a</td>
<td>0.486</td>
<td>0.477</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6a_1</td>
<td>25</td>
<td>labbe2023_t6a</td>
<td>0.481</td>
<td>0.480</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_3</td>
<td>26</td>
<td>chang2023_t6a</td>
<td>0.522</td>
<td>0.363</td>
</tr>
<tr>
<td></td>
<td>Chang_t6a_2</td>
<td>27</td>
<td>chang2023_t6a</td>
<td>0.522</td>
<td>0.362</td>
</tr>
<tr>
<td></td>
<td>Kadlčík_t6a_3</td>
<td>28</td>
<td>kadlčík2023_t6a</td>
<td>0.459</td>
<td>0.445</td>
</tr>
<tr>
<td></td>
<td>Lim_t6a_1</td>
<td>29</td>
<td>lim2023_t6a</td>
<td>0.121</td>
<td>0.033</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<p>In this section you can find the characteristics of the submitted systems. There are two tables
for easy reference, in the corresponding subsections. The first table has an overview of the systems
and the second has a detailed presentation of each system.</p>
<h2 id="overview-of-characteristics">Overview of characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="abbreviation" data-filter-control="true" data-filter-show-clear="true" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="total_parameters" data-scatter-y="tes_spiderfl" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_sys_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
            Rank
            </th>
<th class="sm-cell" data-field="abbreviation" data-sortable="true">
              Submission<br/>code
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spiderfl" data-reversed="false" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Method scheme/architecture
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="total_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Data<br/>augmentation
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Wu_t6a_4</td>
<td>0.327</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>2542000000</td>
<td>conformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Wu_t6a_3</td>
<td>0.326</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>2542000000</td>
<td>conformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Wu_t6a_2</td>
<td>0.324</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>887000000</td>
<td>conformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>Chang_t6a_4</td>
<td>0.315</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>1313200128</td>
<td>PANNs</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
</tr>
<tr>
<td>5</td>
<td>Labbe_t6a_4</td>
<td>0.314</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>98064347</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup, spec_augment, label_smoothing</td>
</tr>
<tr>
<td>6</td>
<td>Wu_t6a_1</td>
<td>0.311</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>127000000</td>
<td>conformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Labbe_t6a_3</td>
<td>0.309</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>42191083</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup, spec_augment, label_smoothing</td>
</tr>
<tr>
<td>8</td>
<td>Chang_t6a_1</td>
<td>0.308</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>218866688</td>
<td>PANNs</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
</tr>
<tr>
<td>9</td>
<td>Labbe_t6a_2</td>
<td>0.304</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>40133440</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup, spec_augment, label_smoothing</td>
</tr>
<tr>
<td>10</td>
<td>Yan_t6a_4</td>
<td>0.289</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Yan_t6a_3</td>
<td>0.288</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>Yan_t6a_1</td>
<td>0.282</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>Schaumloeffel_t6a_1</td>
<td>0.282</td>
<td>schaumloeffel2023_t6a</td>
<td>encoder-decoder</td>
<td>248325888</td>
<td>transformer</td>
<td>GPT2</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>14</td>
<td>Schaumloeffel_t6a_2</td>
<td>0.274</td>
<td>schaumloeffel2023_t6a</td>
<td>encoder-decoder</td>
<td>248325888</td>
<td>transformer</td>
<td>GPT2</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>15</td>
<td>Guan_t6a_3</td>
<td>0.273</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>35502652</td>
<td>PANNs (CNN10) + GAT, PANNs (CNN10)</td>
<td>transformer</td>
<td>SpecAugmentation</td>
</tr>
<tr>
<td>16</td>
<td>Guan_t6a_4</td>
<td>0.272</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>17768222</td>
<td>PANNs (CNN10) + GAT</td>
<td>transformer</td>
<td>SpecAugmentation</td>
</tr>
<tr>
<td>17</td>
<td>Yan_t6a_2</td>
<td>0.270</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>18</td>
<td>Guan_t6a_1</td>
<td>0.270</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>8884111</td>
<td>PANNs (CNN10) + GAT</td>
<td>transformer</td>
<td>SpecAugmentation</td>
</tr>
<tr>
<td>19</td>
<td>Kadlčík_t6a_1</td>
<td>0.267</td>
<td>kadlčík2023_t6a</td>
<td>encoder-decoder</td>
<td>1550000000</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>20</td>
<td>Lee_t6a_1</td>
<td>0.266</td>
<td>lee2023_t6a</td>
<td>encoder-decoder</td>
<td>178755308</td>
<td>cnn</td>
<td>transformer</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td>21</td>
<td>Baseline</td>
<td>0.264</td>
<td>gontier2023_t6a</td>
<td>encoder-decoder</td>
<td>98500000</td>
<td>PANNs</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>22</td>
<td>Guan_t6a_2</td>
<td>0.263</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>8884111</td>
<td>PANNs (CNN10) + GAT</td>
<td>transformer</td>
<td>SpecAugmentation</td>
</tr>
<tr>
<td>23</td>
<td>Kadlčík_t6a_2</td>
<td>0.261</td>
<td>kadlčík2023_t6a</td>
<td>encoder-decoder</td>
<td>244000000</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>24</td>
<td>Greeshma_t6a_1</td>
<td>0.261</td>
<td>greeshma2023_t6a</td>
<td>encoder-decoder</td>
<td>178755308</td>
<td>cnn</td>
<td>BART</td>
<td></td>
</tr>
<tr>
<td>25</td>
<td>Labbe_t6a_1</td>
<td>0.256</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>87715793</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup, spec_augment, label_smoothing</td>
</tr>
<tr>
<td>26</td>
<td>Chang_t6a_3</td>
<td>0.231</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>656600064</td>
<td>PANNs</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
</tr>
<tr>
<td>27</td>
<td>Chang_t6a_2</td>
<td>0.229</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>218866688</td>
<td>PANNs</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
</tr>
<tr>
<td>28</td>
<td>Kadlčík_t6a_3</td>
<td>0.225</td>
<td>kadlčík2023_t6a</td>
<td>encoder-decoder</td>
<td>39000000</td>
<td>transformer</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>29</td>
<td>Lim_t6a_1</td>
<td>0.010</td>
<td>lim2023_t6a</td>
<td>encoder-decoder</td>
<td>178755308</td>
<td>CNN14</td>
<td>transformer</td>
<td></td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="detailed-characteristics">Detailed characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="abbreviation" data-filter-control="true" data-filter-show-clear="true" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="total_parameters" data-scatter-y="tes_spiderfl" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_sys_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
            Rank
            </th>
<th class="sm-cell" data-field="abbreviation" data-sortable="true">
              Submission<br/>code
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spiderfl" data-reversed="false" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Method scheme/architecture
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="total_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="acoustic_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Acoustic<br/>features
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="word_embeddings" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word<br/>embeddings
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Data<br/>augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="input_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="learning_scheme" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning set-up
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="ensemble" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Ensemble method
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="loss_function" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Loss function
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="optimizer" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning set-up
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="learning_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="gradient_clipping" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Gradient clipping
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="gradient_norm" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Gradient norm for clipping
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="metric_monitored" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Metric monitored for training
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="dataset_audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used for audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="dataset_word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used for word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="dataset_audio_similarity" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used for audio similarity
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Wu_t6a_4</td>
<td>0.327</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>2542000000</td>
<td>conformer</td>
<td>BEATs</td>
<td>transformer</td>
<td>BART</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td></td>
<td>adamw</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>validation_acc</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Wu_t6a_3</td>
<td>0.326</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>2542000000</td>
<td>conformer</td>
<td>BEATs</td>
<td>transformer</td>
<td>BART</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td></td>
<td>adamw</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>validation_acc</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Wu_t6a_2</td>
<td>0.324</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>887000000</td>
<td>conformer</td>
<td>BEATs</td>
<td>transformer</td>
<td>BART</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td></td>
<td>adamw</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>validation_acc</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>Chang_t6a_4</td>
<td>0.315</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>1313200128</td>
<td>PANNs</td>
<td>PANNs</td>
<td>BART</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learninig</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-6</td>
<td></td>
<td></td>
<td>CIDEr</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>Labbe_t6a_4</td>
<td>0.314</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>98064347</td>
<td>cnn</td>
<td>ConvNeXt-tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, spec_augment, label_smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>5e-4</td>
<td></td>
<td>l2</td>
<td>validation_fense</td>
<td>Clotho, AudioCaps, MACS, WavCaps (without FreeSound)</td>
<td>Clotho, AudioCaps, MACS, WavCaps (without FreeSound)</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>Wu_t6a_1</td>
<td>0.311</td>
<td>wu2023_t6a</td>
<td>encoder-decoder</td>
<td>127000000</td>
<td>conformer</td>
<td>BEATs</td>
<td>transformer</td>
<td>BART</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td></td>
<td>adamw</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>validation_acc</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Labbe_t6a_3</td>
<td>0.309</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>42191083</td>
<td>cnn</td>
<td>ConvNeXt-tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, spec_augment, label_smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>5e-4</td>
<td></td>
<td>l2</td>
<td>validation_fense</td>
<td>Clotho, AudioCaps, MACS, WavCaps (without FreeSound)</td>
<td>Clotho, AudioCaps, MACS, WavCaps (without FreeSound)</td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>Chang_t6a_1</td>
<td>0.308</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>218866688</td>
<td>PANNs</td>
<td>PANNs</td>
<td>BART</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-6</td>
<td></td>
<td></td>
<td>validation loss</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>Labbe_t6a_2</td>
<td>0.304</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>40133440</td>
<td>cnn</td>
<td>ConvNeXt-tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, spec_augment, label_smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>5e-4</td>
<td></td>
<td>l2</td>
<td>validation_fense</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>Yan_t6a_4</td>
<td>0.289</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>audioset</td>
<td>transformer</td>
<td>BERT</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, FreeSound</td>
<td>Clotho, FreeSound</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Yan_t6a_3</td>
<td>0.288</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>audioset</td>
<td>transformer</td>
<td>BERT</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, FreeSound</td>
<td>Clotho, FreeSound</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>Yan_t6a_1</td>
<td>0.282</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>audioset</td>
<td>transformer</td>
<td>BERT</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, FreeSound</td>
<td>Clotho, FreeSound</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>Schaumloeffel_t6a_1</td>
<td>0.282</td>
<td>schaumloeffel2023_t6a</td>
<td>encoder-decoder</td>
<td>248325888</td>
<td>transformer</td>
<td>CLAP</td>
<td>GPT2</td>
<td></td>
<td>SpecAugment</td>
<td>48kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-5</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, MACS, WavText5k, SoundDescs</td>
<td>Clotho, AudioCaps, MACS, WavText5k, SoundDescs</td>
<td></td>
</tr>
<tr>
<td>14</td>
<td>Schaumloeffel_t6a_2</td>
<td>0.274</td>
<td>schaumloeffel2023_t6a</td>
<td>encoder-decoder</td>
<td>248325888</td>
<td>transformer</td>
<td>CLAP</td>
<td>GPT2</td>
<td></td>
<td>SpecAugment</td>
<td>48kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-5</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, MACS</td>
<td>Clotho, AudioCaps, MACS</td>
<td></td>
</tr>
<tr>
<td>15</td>
<td>Guan_t6a_3</td>
<td>0.273</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>35502652</td>
<td>PANNs (CNN10) + GAT, PANNs (CNN10)</td>
<td>log-mel energies</td>
<td>transformer</td>
<td>Word2Vec</td>
<td>SpecAugmentation</td>
<td>32.0kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy with label smoothing</td>
<td>adamw</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>SPIDEr metric</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>16</td>
<td>Guan_t6a_4</td>
<td>0.272</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>17768222</td>
<td>PANNs (CNN10) + GAT</td>
<td>log-mel energies</td>
<td>transformer</td>
<td>Word2Vec</td>
<td>SpecAugmentation</td>
<td>32.0kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy with label smoothing</td>
<td>adamw</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>SPIDEr metric</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>17</td>
<td>Yan_t6a_2</td>
<td>0.270</td>
<td>yan2023_t6a</td>
<td>encoder-decoder</td>
<td>90086352</td>
<td>transformer</td>
<td>audioset</td>
<td>transformer</td>
<td>BERT</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, FreeSound</td>
<td>Clotho, FreeSound</td>
<td></td>
</tr>
<tr>
<td>18</td>
<td>Guan_t6a_1</td>
<td>0.270</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>8884111</td>
<td>PANNs (CNN10) + GAT</td>
<td>log-mel energies</td>
<td>transformer</td>
<td>Word2Vec</td>
<td>SpecAugmentation</td>
<td>32.0kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy with label smoothing</td>
<td>adamw</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>SPIDEr metric</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>19</td>
<td>Kadlčík_t6a_1</td>
<td>0.267</td>
<td>kadlčík2023_t6a</td>
<td>encoder-decoder</td>
<td>1550000000</td>
<td>transformer</td>
<td>WhisperFeatureExtractor</td>
<td>transformer</td>
<td>Whisper</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>4e-6</td>
<td></td>
<td></td>
<td>SPIDEr</td>
<td>Clotho, AudioCaps, AudioSet</td>
<td>Clotho, AudioCaps, AudioSet</td>
<td></td>
</tr>
<tr>
<td>20</td>
<td>Lee_t6a_1</td>
<td>0.266</td>
<td>lee2023_t6a</td>
<td>encoder-decoder</td>
<td>178755308</td>
<td>cnn</td>
<td>PANNs</td>
<td>transformer</td>
<td>BART</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-5</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, WavText5K, SoundDescs</td>
<td>Clotho, AudioCaps, WavText5K, SoundDescs</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td>21</td>
<td>Baseline</td>
<td>0.264</td>
<td>gontier2023_t6a</td>
<td>encoder-decoder</td>
<td>98500000</td>
<td>PANNs</td>
<td>log-mel energies</td>
<td>transformer</td>
<td>BART</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-5</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>22</td>
<td>Guan_t6a_2</td>
<td>0.263</td>
<td>guan2023_t6a</td>
<td>encoder-decoder</td>
<td>8884111</td>
<td>PANNs (CNN10) + GAT</td>
<td>log-mel energies</td>
<td>transformer</td>
<td>Word2Vec</td>
<td>SpecAugmentation</td>
<td>32.0kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy with label smoothing</td>
<td>adamw</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>SPIDEr metric</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>23</td>
<td>Kadlčík_t6a_2</td>
<td>0.261</td>
<td>kadlčík2023_t6a</td>
<td>encoder-decoder</td>
<td>244000000</td>
<td>transformer</td>
<td>WhisperFeatureExtractor</td>
<td>transformer</td>
<td>Whisper</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>4e-6</td>
<td></td>
<td></td>
<td>SPIDEr</td>
<td>Clotho, AudioCaps, AudioSet</td>
<td>Clotho, AudioCaps, AudioSet</td>
<td></td>
</tr>
<tr>
<td>24</td>
<td>Greeshma_t6a_1</td>
<td>0.261</td>
<td>greeshma2023_t6a</td>
<td>encoder-decoder</td>
<td>178755308</td>
<td>cnn</td>
<td>log-mel energies</td>
<td>BART</td>
<td>BART</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-5</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>25</td>
<td>Labbe_t6a_1</td>
<td>0.256</td>
<td>labbe2023_t6a</td>
<td>encoder-decoder</td>
<td>87715793</td>
<td>cnn</td>
<td>PANNs-CNN14</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, spec_augment, label_smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>5e-4</td>
<td></td>
<td>l2</td>
<td>validation_fense</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>26</td>
<td>Chang_t6a_3</td>
<td>0.231</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>656600064</td>
<td>PANNs</td>
<td>PANNs</td>
<td>BART</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learninig</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-6</td>
<td></td>
<td></td>
<td>CIDEr</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td></td>
</tr>
<tr>
<td>27</td>
<td>Chang_t6a_2</td>
<td>0.229</td>
<td>chang2023_t6a</td>
<td>encoder-decoder</td>
<td>218866688</td>
<td>PANNs</td>
<td>PANNs</td>
<td>BART</td>
<td>BART</td>
<td>spec augmentation, AL-mixgen, synonyms substitution</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learninig</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-6</td>
<td></td>
<td></td>
<td>CIDEr</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td></td>
</tr>
<tr>
<td>28</td>
<td>Kadlčík_t6a_3</td>
<td>0.225</td>
<td>kadlčík2023_t6a</td>
<td>encoder-decoder</td>
<td>39000000</td>
<td>transformer</td>
<td>WhisperFeatureExtractor</td>
<td>transformer</td>
<td>Whisper</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>4e-6</td>
<td></td>
<td></td>
<td>SPIDEr</td>
<td>Clotho, AudioCaps, AudioSet</td>
<td>Clotho, AudioCaps, AudioSet</td>
<td></td>
</tr>
<tr>
<td>29</td>
<td>Lim_t6a_1</td>
<td>0.010</td>
<td>lim2023_t6a</td>
<td>encoder-decoder</td>
<td>178755308</td>
<td>CNN14</td>
<td>mel energies</td>
<td>transformer</td>
<td>PASST</td>
<td></td>
<td>44.1 kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adamw</td>
<td>1e-5</td>
<td></td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2023/technical_reports_task6a.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="chang2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-chang2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        HYU submission for the DCASE 2023 task 6a: automated audio captioning model using AL-MixGen and synonyms substitution
       </h4>
<p style="text-align:left">
        Jae-Heung Cho<sup>1</sup>, Yoon-Ah Park<sup>1</sup>, Jaewon Kim<sup>1</sup>, Joon-Hyuk Chang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">chang_t6a_1</span> <span class="label label-primary">chang_t6a_2</span> <span class="label label-primary">chang_t6a_3</span> <span class="label label-primary">chang_t6a_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-chang2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-chang2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-chang2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Chang_113_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-chang2023_t6a" class="panel-collapse collapse" id="collapse-chang2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       HYU submission for the DCASE 2023 task 6a: automated audio captioning model using AL-MixGen and synonyms substitution
      </h4>
<p style="text-align:left">
<small>
        Jae-Heung Cho<sup>1</sup>, Yoon-Ah Park<sup>1</sup>, Jaewon Kim<sup>1</sup>, Joon-Hyuk Chang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents the automated audio captioning model for participating in the detection and classification of acoustic scenes and events 2023 challenge task 6A. The model consists of two parts: an audio feature extractor and a language model. The audio feature extractor employed in our model is the pre-trained convolutional neural network 14 (CNN14), trained with AudioSet. CNN14 has demonstrated excellent performance in extracting audio features. For the language model, we utilized bidirectional autoregressive transformers model, which has achieved remarkable success in generating the text. We pre-trained the model with WavCaps, AudioCaps and Clotho dataset to manage the limitation of data availability, and then fine-tuned with Clotho dataset. Furthermore, AL-MixGen and synonyms substitution methods were also implemented for data augmentation. To improve the evaluation metric directly, we trained the model with reinforcement learning to optimize the CIDEr score. Finally, we achieved improved performance by adapting an ensemble of higher-performance models, leading to the accomplishment of 0.343 SPIDEr score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         AL-MixGen, SpecAugment, Synonym substitution
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-chang2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Chang_113_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-chang2023_t6alabel" class="modal fade" id="bibtex-chang2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexchang2023_t6alabel">
        HYU submission for the DCASE 2023 task 6a: automated audio captioning model using AL-MixGen and synonyms substitution
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{chang2023_t6a,
    Author = "Cho, Jae-Heung and Park, Yoon-Ah and Kim, Jaewon and Chang, Joon-Hyuk",
    title = "HYU submission for the DCASE 2023 task 6a: automated audio captioning model using AL-MixGen and synonyms substitution",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "This paper presents the automated audio captioning model for participating in the detection and classification of acoustic scenes and events 2023 challenge task 6A. The model consists of two parts: an audio feature extractor and a language model. The audio feature extractor employed in our model is the pre-trained convolutional neural network 14 (CNN14), trained with AudioSet. CNN14 has demonstrated excellent performance in extracting audio features. For the language model, we utilized bidirectional autoregressive transformers model, which has achieved remarkable success in generating the text. We pre-trained the model with WavCaps, AudioCaps and Clotho dataset to manage the limitation of data availability, and then fine-tuned with Clotho dataset. Furthermore, AL-MixGen and synonyms substitution methods were also implemented for data augmentation. To improve the evaluation metric directly, we trained the model with reinforcement learning to optimize the CIDEr score. Finally, we achieved improved performance by adapting an ensemble of higher-performance models, leading to the accomplishment of 0.343 SPIDEr score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="greeshma2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-greeshma2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2023 task 6 automated audio captioning and language-based retrieval
       </h4>
<p style="text-align:left">
        Karanth Greeshma<sup>1</sup>, Ninaad Rao<sup>1</sup>, Srikumar Subramanian<sup>1</sup>, Ankit Shah<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Carnegie Mellon University, Language Technologies Institute, Pittsburgh, PA, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">greeshma_t6a_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-greeshma2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-greeshma2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-greeshma2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Shah_25_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-greeshma2023_t6a').collapse('show');window.location.hash='#greeshma2023_t6a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-greeshma2023_t6a" class="panel-collapse collapse" id="collapse-greeshma2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2023 task 6 automated audio captioning and language-based retrieval
      </h4>
<p style="text-align:left">
<small>
        Karanth Greeshma<sup>1</sup>, Ninaad Rao<sup>1</sup>, Srikumar Subramanian<sup>1</sup>, Ankit Shah<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Carnegie Mellon University, Language Technologies Institute, Pittsburgh, PA, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The objective of this project is to examine audio signals utilizing natural language to capture their complex characteristics. This initiative is part of Task 6 in the DCASE 2023 Competition and consists of two subtasks. The first subtask is Automated Audio Captioning, which generates text descriptions of audio content. This task involves the intermodal processing of an audio signal as input and a text description as output. Our best-performing model for this uses the PANN architecture [1] with the CNN-14 feature extractor and BART [2] encoder and decoder. The second subtask is called Language-Based Audio Retrieval, where the system retrieves audio signals by searching for their sound content descriptions. The queries for this subtask are human-generated audio captions. In this task, our best-performing model uses CLAP [3] audio embeddings and Roberta text embeddings [4]. This document presents a summary of our work done for this challenge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-greeshma2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Shah_25_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/NinaadRao/audio-captioning-and-audio-retrieval" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-greeshma2023_t6alabel" class="modal fade" id="bibtex-greeshma2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexgreeshma2023_t6alabel">
        DCASE 2023 task 6 automated audio captioning and language-based retrieval
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{greeshma2023_t6a,
    Author = "Greeshma, Karanth and Rao, Ninaad and Subramanian, Srikumar and Shah, Ankit",
    title = "DCASE 2023 task 6 automated audio captioning and language-based retrieval",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "The objective of this project is to examine audio signals utilizing natural language to capture their complex characteristics. This initiative is part of Task 6 in the DCASE 2023 Competition and consists of two subtasks. The first subtask is Automated Audio Captioning, which generates text descriptions of audio content. This task involves the intermodal processing of an audio signal as input and a text description as output. Our best-performing model for this uses the PANN architecture [1] with the CNN-14 feature extractor and BART [2] encoder and decoder. The second subtask is called Language-Based Audio Retrieval, where the system retrieves audio signals by searching for their sound content descriptions. The queries for this subtask are human-generated audio captions. In this task, our best-performing model uses CLAP [3] audio embeddings and Roberta text embeddings [4]. This document presents a summary of our work done for this challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="guan2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-guan2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Ensemble systems with contrastive language-audio pretraining and attention-based audio features for audio captioning and retrieval
       </h4>
<p style="text-align:left">
        Feiyang Xiao<sup>1</sup>, Qiaoxi Zhu<sup>2</sup>, Haiyan Lan<sup>1</sup>, Wenwu Wang<sup>3</sup>, Jian Guan<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Group of Intelligent Signal Processing (GISP), College of Computer Science and Technology, Harbin Engineering University, Harbin, China, <sup>2</sup> Centre for Audio, Acoustic and Vibration (CAAV), University of Technology Sydney, Ultimo, Australia, <sup>3</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">guan_t6a_1</span> <span class="label label-primary">guan_t6a_2</span> <span class="label label-primary">guan_t6a_3</span> <span class="label label-primary">guan_t6a_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-guan2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-guan2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-guan2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Guan_83_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-guan2023_t6a" class="panel-collapse collapse" id="collapse-guan2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Ensemble systems with contrastive language-audio pretraining and attention-based audio features for audio captioning and retrieval
      </h4>
<p style="text-align:left">
<small>
        Feiyang Xiao<sup>1</sup>, Qiaoxi Zhu<sup>2</sup>, Haiyan Lan<sup>1</sup>, Wenwu Wang<sup>3</sup>, Jian Guan<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Group of Intelligent Signal Processing (GISP), College of Computer Science and Technology, Harbin Engineering University, Harbin, China, <sup>2</sup> Centre for Audio, Acoustic and Vibration (CAAV), University of Technology Sydney, Ultimo, Australia, <sup>3</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our submission on Task 6 (automated audio captioning and language-based audio retrieval) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge. The proposed systems in this submission are based on a contrastive language-audio pretraining strategy and the attention-based audio feature representation. Experiments show that our systems can achieve a SPIDEr-FL score of 28.32 on automated audio captioning and an mAP score of 31.18 on language-based audio retrieval.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-guan2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Guan_83_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-guan2023_t6alabel" class="modal fade" id="bibtex-guan2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexguan2023_t6alabel">
        Ensemble systems with contrastive language-audio pretraining and attention-based audio features for audio captioning and retrieval
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{guan2023_t6a,
    Author = "Xiao, Feiyang and Zhu, Qiaoxi and Lan, Haiyan and Wang, Wenwu and Guan, Jian",
    title = "Ensemble systems with contrastive language-audio pretraining and attention-based audio features for audio captioning and retrieval",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "This technical report describes our submission on Task 6 (automated audio captioning and language-based audio retrieval) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge. The proposed systems in this submission are based on a contrastive language-audio pretraining strategy and the attention-based audio feature representation. Experiments show that our systems can achieve a SPIDEr-FL score of 28.32 on automated audio captioning and an mAP score of 31.18 on language-based audio retrieval."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="kadl\u010D\xEDk2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-kadl\u010D\xEDk2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A whisper transformer for audio captioning trained with synthetic captions and transfer learning
       </h4>
<p style="text-align:left">
        Marek Kadlčı́k<sup>1,2</sup>, Adam Hájek<sup>1,2</sup>, Jürgen Kieslich<sup>2</sup>, Radosław Winiecki<sup>2,3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Student at Masaryk University, Brno, Czech Republic, <sup>2</sup>Student at Johannes Kepler University, Linz, Austria, <sup>3</sup>Student at Politechnika Poznańska, Poznan, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">kadlcik_t6a_1</span> <span class="label label-primary">kadlcik_t6a_2</span> <span class="label label-primary">kadlcik_t6a_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-kadl\u010D\xEDk2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-kadl\u010D\xEDk2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-kadl\u010D\xEDk2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-kadl\u010D\xEDk2023_t6a" class="panel-collapse collapse" id="collapse-kadl\u010D\xEDk2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A whisper transformer for audio captioning trained with synthetic captions and transfer learning
      </h4>
<p style="text-align:left">
<small>
        Marek Kadlčı́k<sup>1,2</sup>, Adam Hájek<sup>1,2</sup>, Jürgen Kieslich<sup>2</sup>, Radosław Winiecki<sup>2,3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Student at Masaryk University, Brno, Czech Republic, <sup>2</sup>Student at Johannes Kepler University, Linz, Austria, <sup>3</sup>Student at Politechnika Poznańska, Poznan, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The field of audio captioning has seen significant advancements in recent years, driven by the availability of large-scale audio datasets and advancements in deep learning techniques. In this technical report, we present our approach to audio captioning, focusing on the use of a pretrained speech-to-text Whisper model and pretraining on synthetic captions. We discuss our training procedures and present our experiments’ results, which include model size variations, dataset mixtures, and other hyperparameters. Our findings demonstrate the impact of different training strategies on the performance of the audio captioning model. Our code and trained models are publicly available on GitHub and Hugging Face Hub.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Gaussian noise, Time shifting, Gain
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-kadl\u010D\xEDk2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-kadl\u010D\xEDk2023_t6alabel" class="modal fade" id="bibtex-kadl\u010D\xEDk2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexkadl\u010D\xEDk2023_t6alabel">
        A whisper transformer for audio captioning trained with synthetic captions and transfer learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{kadl\u010D\xEDk2023_t6a,
    Author = "Kadlčı́k, Marek and Hájek, Adam and Kieslich, Jürgen and Winiecki, Radosław",
    title = "A whisper transformer for audio captioning trained with synthetic captions and transfer learning",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "The field of audio captioning has seen significant advancements in recent years, driven by the availability of large-scale audio datasets and advancements in deep learning techniques. In this technical report, we present our approach to audio captioning, focusing on the use of a pretrained speech-to-text Whisper model and pretraining on synthetic captions. We discuss our training procedures and present our experiments’ results, which include model size variations, dataset mixtures, and other hyperparameters. Our findings demonstrate the impact of different training strategies on the performance of the audio captioning model. Our code and trained models are publicly available on GitHub and Hugging Face Hub."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="labbe2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-labbe2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        IRIT-UPS DCASE 2023 audio captioning and retrieval system
       </h4>
<p style="text-align:left">
        Etienne Labbé<sup>1</sup>, Thomas Pellegrini<sup>1,2</sup>, Julien Pinquier<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>IRIT (UMR 5505), Université Paul Sabatier, CNRS, Toulouse, France, <sup>2</sup>Artificial and Natural Intelligence Toulouse Institute (ANITI)
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">labbe_t6a_1</span> <span class="label label-primary">labbe_t6a_2</span> <span class="label label-primary">labbe_t6a_3</span> <span class="label label-primary">labbe_t6a_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-labbe2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-labbe2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-labbe2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Labbe_59_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-labbe2023_t6a').collapse('show');window.location.hash='#labbe2023_t6a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-labbe2023_t6a" class="panel-collapse collapse" id="collapse-labbe2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       IRIT-UPS DCASE 2023 audio captioning and retrieval system
      </h4>
<p style="text-align:left">
<small>
        Etienne Labbé<sup>1</sup>, Thomas Pellegrini<sup>1,2</sup>, Julien Pinquier<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>IRIT (UMR 5505), Université Paul Sabatier, CNRS, Toulouse, France, <sup>2</sup>Artificial and Natural Intelligence Toulouse Institute (ANITI)
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report provides a concise overview of our systems submitted to the DCASE Challenge 2023 for tasks 6a, "Automated Audio Captioning" (AAC), and 6b, "Language-Based Audio Retrieval" (LBAR). In task 6a, we made four distinct submissions. The first submission employed a standard CNN14 encoder paired with a transformer decoder. In the second submission, we replaced this encoder with a ConvNeXt model to enhance audio representation. The third submission incorporated additional training data. We introduced a new task embedding approach to differentiate between different writing styles and audio types. Finally, in the fourth submission, we employed an ensemble method to combine five models trained on different seeds, aiming to improve the quality of the captions. For task 6b, we use the AAC models and we propose a novel approach to accomplish the LBAR task by leveraging the AAC system loss function without requiring any additional training. Our most successful AAC and LBAR systems achieved a SPIDEr-FL score of 0.320 and an mAP@10 score of 0.269. These results demonstrate relative improvements of 22.6\% and 21.2\% compared to the AAC and LBAR baselines, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         MixUp, SpecAugment, Label Smoothing
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-labbe2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Labbe_59_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Labbeti/dcase2023challenge-task6ab" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-labbe2023_t6alabel" class="modal fade" id="bibtex-labbe2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexlabbe2023_t6alabel">
        IRIT-UPS DCASE 2023 audio captioning and retrieval system
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{labbe2023_t6a,
    Author = "Labbé, Etienne and Pellegrini, Thomas and Pinquier, Julien",
    title = "IRIT-UPS DCASE 2023 audio captioning and retrieval system",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = {This technical report provides a concise overview of our systems submitted to the DCASE Challenge 2023 for tasks 6a, "Automated Audio Captioning" (AAC), and 6b, "Language-Based Audio Retrieval" (LBAR). In task 6a, we made four distinct submissions. The first submission employed a standard CNN14 encoder paired with a transformer decoder. In the second submission, we replaced this encoder with a ConvNeXt model to enhance audio representation. The third submission incorporated additional training data. We introduced a new task embedding approach to differentiate between different writing styles and audio types. Finally, in the fourth submission, we employed an ensemble method to combine five models trained on different seeds, aiming to improve the quality of the captions. For task 6b, we use the AAC models and we propose a novel approach to accomplish the LBAR task by leveraging the AAC system loss function without requiring any additional training. Our most successful AAC and LBAR systems achieved a SPIDEr-FL score of 0.320 and an mAP@10 score of 0.269. These results demonstrate relative improvements of 22.6\\% and 21.2\\% compared to the AAC and LBAR baselines, respectively.}
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="lee2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-lee2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Label-refined sequential training with noisy data for automated audio captioning
       </h4>
<p style="text-align:left">
        Jaeheon Sim<sup>1</sup>, Eungbeom Kim<sup>1</sup>, Kyogu Lee<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Interdisciplinary Program in Artifical Intelligence, Seoul National University, Seoul, Korea, <sup>2</sup>Department of Intelligence and Information, AIIS, Seoul National University, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">lee_t6a_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-lee2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-lee2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-lee2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Sim_99_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-lee2023_t6a" class="panel-collapse collapse" id="collapse-lee2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Label-refined sequential training with noisy data for automated audio captioning
      </h4>
<p style="text-align:left">
<small>
        Jaeheon Sim<sup>1</sup>, Eungbeom Kim<sup>1</sup>, Kyogu Lee<sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Interdisciplinary Program in Artifical Intelligence, Seoul National University, Seoul, Korea, <sup>2</sup>Department of Intelligence and Information, AIIS, Seoul National University, Seoul, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the submission to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 6A: Automated Audio Captioning. We utilize a label-refined sequential training method to leverage the large additional dataset which contains two types of noise including domain shift and label noise. We investigate the usefulness of the additional noisy dataset and observe that the models directly trained on the dataset naively including the additional and target dataset suffer from a poor performance. From this observation, we aim to fully leverage the additional dataset by addressing the two types of noise simultaneously. We sequentially train the model with the prior knowledge about the difference between the target dataset and each of the additional datasets, from the largest to the nearest. We finally train the model on the target dataset, thereby progressively minimizing the domain gap. After this training procedure, we apply a label refinement method which is based on pseudo-labelling from self-training method and repeat the sequential training procedure. The proposed method mitigates the noise in the dataset and achieves the improved performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-lee2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Sim_99_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-lee2023_t6alabel" class="modal fade" id="bibtex-lee2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexlee2023_t6alabel">
        Label-refined sequential training with noisy data for automated audio captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{lee2023_t6a,
    Author = "Sim, Jaeheon and Kim, Eungbeom and Lee, Kyogu",
    title = "Label-refined sequential training with noisy data for automated audio captioning",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "This technical report describes the submission to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 6A: Automated Audio Captioning. We utilize a label-refined sequential training method to leverage the large additional dataset which contains two types of noise including domain shift and label noise. We investigate the usefulness of the additional noisy dataset and observe that the models directly trained on the dataset naively including the additional and target dataset suffer from a poor performance. From this observation, we aim to fully leverage the additional dataset by addressing the two types of noise simultaneously. We sequentially train the model with the prior knowledge about the difference between the target dataset and each of the additional datasets, from the largest to the nearest. We finally train the model on the target dataset, thereby progressively minimizing the domain gap. After this training procedure, we apply a label refinement method which is based on pseudo-labelling from self-training method and repeat the sequential training procedure. The proposed method mitigates the noise in the dataset and achieves the improved performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="lim2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-lim2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CAU submission to DCASE 2023 task 6a: Audio captioning using wavegrams that contain frequency information
       </h4>
<p style="text-align:left">
        Seungmin Chou<sup>1</sup>, Jaeseung Yim<sup>1</sup>, Changwon Lim<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Chung-Ang University, Department of Applied Statistics, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">lim_t6a_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-lim2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-lim2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-lim2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Lim_121_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-lim2023_t6a" class="panel-collapse collapse" id="collapse-lim2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CAU submission to DCASE 2023 task 6a: Audio captioning using wavegrams that contain frequency information
      </h4>
<p style="text-align:left">
<small>
        Seungmin Chou<sup>1</sup>, Jaeseung Yim<sup>1</sup>, Changwon Lim<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Chung-Ang University, Department of Applied Statistics, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes an Automated Audio Captioning model for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6A. Utilizing wavegram and patchout as proposed in [1] and [2], respectively, we propose audio captioning using Wavegrams that contain frequency information. We use pre-trained models trained using AudioSet data, to make word embedding. Our proposed sequence-to-sequence model consists of CNN14 encoder and a Transformer decoder. Experiments show that the proposed model achieves a SPIDEr score of 0.011.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-lim2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Lim_121_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-lim2023_t6alabel" class="modal fade" id="bibtex-lim2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexlim2023_t6alabel">
        CAU submission to DCASE 2023 task 6a: Audio captioning using wavegrams that contain frequency information
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{lim2023_t6a,
    Author = "Chou, Seungmin and Yim, Jaeseung and Lim, Changwon",
    title = "CAU submission to DCASE 2023 task 6a: Audio captioning using wavegrams that contain frequency information",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "This technical report describes an Automated Audio Captioning model for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6A. Utilizing wavegram and patchout as proposed in [1] and [2], respectively, we propose audio captioning using Wavegrams that contain frequency information. We use pre-trained models trained using AudioSet data, to make word embedding. Our proposed sequence-to-sequence model consists of CNN14 encoder and a Transformer decoder. Experiments show that the proposed model achieves a SPIDEr score of 0.011."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="schaumloeffel2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-schaumloeffel2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        PEACS: Prefix encoding for auditory caption synthesis
       </h4>
<p style="text-align:left">
        Timothy Schaumlöffel<sup>1</sup>, Martina G. Vilas<sup>1,2</sup>, Gemma Roig<sup>1,3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Goethe University Frankfurt, Department of Computer Science, Robert-Mayer-Str. 11-15, 60323 Frankfurt, Germany, <sup>2</sup>Ernst Strüngmann Institute for Neuroscience, Deutschordenstraße 46, 60528 Frankfurt, Germany, <sup>3</sup>The Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">schaumloeffel_t6a_1</span> <span class="label label-primary">schaumloeffel_t6a_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-schaumloeffel2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-schaumloeffel2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-schaumloeffel2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-schaumloeffel2023_t6a" class="panel-collapse collapse" id="collapse-schaumloeffel2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       PEACS: Prefix encoding for auditory caption synthesis
      </h4>
<p style="text-align:left">
<small>
        Timothy Schaumlöffel<sup>1</sup>, Martina G. Vilas<sup>1,2</sup>, Gemma Roig<sup>1,3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Goethe University Frankfurt, Department of Computer Science, Robert-Mayer-Str. 11-15, 60323 Frankfurt, Germany, <sup>2</sup>Ernst Strüngmann Institute for Neuroscience, Deutschordenstraße 46, 60528 Frankfurt, Germany, <sup>3</sup>The Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder’s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-schaumloeffel2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-schaumloeffel2023_t6alabel" class="modal fade" id="bibtex-schaumloeffel2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexschaumloeffel2023_t6alabel">
        PEACS: Prefix encoding for auditory caption synthesis
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{schaumloeffel2023_t6a,
    Author = "Schaumlöffel, Timothy and Vilas, Martina G. and Roig, Gemma",
    title = "PEACS: Prefix encoding for auditory caption synthesis",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder’s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="wu2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-wu2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        BEATs-based audio captioning model with INSTRUCTOR embedding supervision and ChatGPT mix-up
       </h4>
<p style="text-align:left">
        Shih-Lun Wu<sup>1</sup>, Xuankai Chang<sup>1</sup>, Gordon Wichern<sup>2</sup>, Jee-weon Jung<sup>1</sup>, François Germain<sup>2</sup>, Jonathan Le Roux<sup>2</sup>, Shinji Watanabe<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA, <sup>2</sup>Speech &amp; Audio Team, Mitsubishi Electric Research Labs, Cambridge, MA, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">wu_t6a_1</span> <span class="label label-primary">wu_t6a_2</span> <span class="label label-primary">wu_t6a_3</span> <span class="label label-primary">wu_t6a_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-wu2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-wu2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-wu2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-wu2023_t6a" class="panel-collapse collapse" id="collapse-wu2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       BEATs-based audio captioning model with INSTRUCTOR embedding supervision and ChatGPT mix-up
      </h4>
<p style="text-align:left">
<small>
        Shih-Lun Wu<sup>1</sup>, Xuankai Chang<sup>1</sup>, Gordon Wichern<sup>2</sup>, Jee-weon Jung<sup>1</sup>, François Germain<sup>2</sup>, Jonathan Le Roux<sup>2</sup>, Shinji Watanabe<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA, <sup>2</sup>Speech &amp; Audio Team, Mitsubishi Electric Research Labs, Cambridge, MA, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       DCASE 2023 Task 6A, automated audio captioning (AAC), aims at generating informative descriptions for various sounds from nature and/or human activities. Our AAC system follows the sequence-to-sequence (seq2seq) architecture. The audio encoder stack is comprised of a frozen BEAT S Transformer followed by a 2-layer Conformer. The BEAT S module, which has been pretrained on both masked audio token prediction and audio event classification, extracts fine-grained (i.e., ≈ 50 Hz) audio features, while the Conformer downsamples and summarizes the audio features before they are cross-attended by the BART text decoder. Besides the autoregressive negative log-likelihood (NLL) loss computed on decoder outputs, we simultaneously apply an audio-text contrastive loss on our encoder output to infuse language modality knowledge into it. Specifically, we feed ground-truth captions into INSTRUCTOR Transformer, a state-of-the-art text embedding model, and teach our audio encoder to predict the INSTRUCTOR text embeddings through InfoNCE loss. In addition, we leverage ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increases not only the amount but also the complexity and diversity of our training data. During inference, we employ nucleus sampling and a hybrid reranking algorithm that considers both likelihood and audio-caption representation similarity. Combining our efforts, our best single model and ensemble system achieve 0.326 and 0.336 SPIDEr-FL scores, respectively, on the Clotho (V2) evaluation split.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment, MixUp
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-wu2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-wu2023_t6alabel" class="modal fade" id="bibtex-wu2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexwu2023_t6alabel">
        BEATs-based audio captioning model with INSTRUCTOR embedding supervision and ChatGPT mix-up
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{wu2023_t6a,
    Author = "Wu, Shih-Lun and Chang, Xuankai and Wichern, Gordon and Jung, Jee-weon and Germain, François and Roux, Jonathan Le and Watanabe, Shinji",
    title = "BEATs-based audio captioning model with INSTRUCTOR embedding supervision and ChatGPT mix-up",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "DCASE 2023 Task 6A, automated audio captioning (AAC), aims at generating informative descriptions for various sounds from nature and/or human activities. Our AAC system follows the sequence-to-sequence (seq2seq) architecture. The audio encoder stack is comprised of a frozen BEAT S Transformer followed by a 2-layer Conformer. The BEAT S module, which has been pretrained on both masked audio token prediction and audio event classification, extracts fine-grained (i.e., ≈ 50 Hz) audio features, while the Conformer downsamples and summarizes the audio features before they are cross-attended by the BART text decoder. Besides the autoregressive negative log-likelihood (NLL) loss computed on decoder outputs, we simultaneously apply an audio-text contrastive loss on our encoder output to infuse language modality knowledge into it. Specifically, we feed ground-truth captions into INSTRUCTOR Transformer, a state-of-the-art text embedding model, and teach our audio encoder to predict the INSTRUCTOR text embeddings through InfoNCE loss. In addition, we leverage ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increases not only the amount but also the complexity and diversity of our training data. During inference, we employ nucleus sampling and a hybrid reranking algorithm that considers both likelihood and audio-caption representation similarity. Combining our efforts, our best single model and ensemble system achieve 0.326 and 0.336 SPIDEr-FL scores, respectively, on the Clotho (V2) evaluation split."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="yan2023_t6a" style="box-shadow: none">
<div class="panel-heading" id="heading-yan2023_t6a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Leveraging multi-task training and image retrieval with CLAP for audio captioning
       </h4>
<p style="text-align:left">
        Haoran Sun<sup>1</sup>, Zhiyong Yan<sup>1</sup>, Yongqing Wang<sup>1</sup>, Heinrich Dinkel<sup>1</sup>, Junbo Zhang<sup>1</sup>, Yujun Wang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Xiaomi Corporation, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">yan_t6a_1</span> <span class="label label-primary">yan_t6a_2</span> <span class="label label-primary">yan_t6a_3</span> <span class="label label-primary">yan_t6a_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-yan2023_t6a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-yan2023_t6a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-yan2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Yan_127_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-yan2023_t6a" class="panel-collapse collapse" id="collapse-yan2023_t6a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Leveraging multi-task training and image retrieval with CLAP for audio captioning
      </h4>
<p style="text-align:left">
<small>
        Haoran Sun<sup>1</sup>, Zhiyong Yan<sup>1</sup>, Yongqing Wang<sup>1</sup>, Heinrich Dinkel<sup>1</sup>, Junbo Zhang<sup>1</sup>, Yujun Wang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Xiaomi Corporation, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report serves as our submission to Task 6 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 challenge. Our system, as described in this report, consists of two sub-systems designed for the respective sub-tasks: automated audio captioning (task A) and text-to-audio retrieval (task B). The text-to-audio retrieval system employs a tri-encoder architecture, where pre-trained audio and text encoders are trained to establish relationships. Additionally, an extra pre-trained image encoder is utilized to enhance the connections between these encoders. Through this retrieval process, the audio encoder can be considered a pre-trained encoder for task A. Furthermore, we employ multi-task training with audio tagging during the retrieval phase to strengthen the encoder for audio captioning. Pre-training is conducted using AudioCaps and a portion of WavCaps datasets, and both sub-systems are subsequently finetuned on Clotho dataset. Experimental results demonstrate that our model achieves a SPIDEr score of 0.305 and a SPIDEr-FL score of 0.294 for captioning, as well as an mAP (mean Average Precision) of 0.321 for text-to-audio retrieval.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-yan2023_t6a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2023/technical_reports/DCASE2023_Yan_127_t6a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-yan2023_t6alabel" class="modal fade" id="bibtex-yan2023_t6a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexyan2023_t6alabel">
        Leveraging multi-task training and image retrieval with CLAP for audio captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{yan2023_t6a,
    Author = "Sun, Haoran and Yan, Zhiyong and Wang, Yongqing and Dinkel, Heinrich and Zhang, Junbo and Wang, Yujun",
    title = "Leveraging multi-task training and image retrieval with CLAP for audio captioning",
    institution = "DCASE2023 Challenge",
    year = "2023",
    month = "May",
    abstract = "This technical report serves as our submission to Task 6 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 challenge. Our system, as described in this report, consists of two sub-systems designed for the respective sub-tasks: automated audio captioning (task A) and text-to-audio retrieval (task B). The text-to-audio retrieval system employs a tri-encoder architecture, where pre-trained audio and text encoders are trained to establish relationships. Additionally, an extra pre-trained image encoder is utilized to enhance the connections between these encoders. Through this retrieval process, the audio encoder can be considered a pre-trained encoder for task A. Furthermore, we employ multi-task training with audio tagging during the retrieval phase to strengthen the encoder for audio captioning. Pre-training is conducted using AudioCaps and a portion of WavCaps datasets, and both sub-systems are subsequently finetuned on Clotho dataset. Experimental results demonstrate that our model achieves a SPIDEr score of 0.305 and a SPIDEr-FL score of 0.294 for captioning, as well as an mAP (mean Average Precision) of 0.321 for text-to-audio retrieval."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>