<!DOCTYPE html><html lang="en">
<head>
    <title>Sound event detection and separation in domestic environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording (see also â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2020</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2020/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2020/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2020/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2020/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/roof-tiles-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-domestic fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Domestic</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2020_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound event detection and separation in domestic environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#supplementary-metrics">Supplementary metrics</a></li>
</ul>
</li>
<li><a href="#teams-ranking">Teams ranking</a>
<ul>
<li><a href="#supplementary-metrics-1">Supplementary metrics</a></li>
</ul>
</li>
<li><a href="#combined-ss-and-sed-ranking">Combined SS and SED ranking</a>
<ul>
<li><a href="#system-ranking">System ranking</a></li>
<li><a href="#team-ranking">Team ranking</a></li>
</ul>
</li>
<li><a href="#ss-ranking">SS ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
<li><a href="#complexity">Complexity</a></li>
<li><a href="#combined-ss-and-sed">Combined SS and SED</a></li>
<li><a href="#ss-systems">SS systems</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps).
The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong>
given that multiple events can be present in an audio recording (see also <a href="#fig1">Fig 1</a>).
The challenge of exploring the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong>
together with a small weakly annotated training set to improve system performance remains. Isolated sound events,
background sound files and scripst to design a <strong>training set with strongly annotated synthetic data</strong> are provided.
<strong>The labels in all the annotated subsets are verified and can be considered as reliable.</strong></p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_dev" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>DCASE2020 SED mean-teacher system</td>
<td>Liang2020</td>
<td></td>
<td>36.0 (35.3 - 36.8)</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.6 (21.0 - 22.4)</td>
<td>35.0</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.9 (21.3 - 22.7)</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>10.4 (9.7 - 11.1)</td>
<td>35.7</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.6 (20.8 - 22.4)</td>
<td>34.3</td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>DCASE2020 WASEDA IPS SED</td>
<td>HouB2020</td>
<td></td>
<td>34.9 (34.0 - 35.7)</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Conforemr SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>51.1 (50.1 - 52.3)</td>
<td>50.6</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Transforemr SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>46.4 (45.5 - 47.5)</td>
<td>47.3</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>transformer conformer Ensemble SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>50.7 (49.6 - 51.9)</td>
<td>49.8</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.3 (43.4 - 45.4)</td>
<td>45.8</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.6 (43.5 - 46.0)</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.1 (42.9 - 45.4)</td>
<td>46.9</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.3 (43.2 - 45.6)</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.7 (43.6 - 46.2)</td>
<td>47.2</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.3 (43.2 - 45.6)</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.4 (43.2 - 45.8)</td>
<td>47.2</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>guided_and_multi_branch_learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.5 (43.3 - 46.0)</td>
<td>47.3</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td></td>
<td>7.8 (7.3 - 8.2)</td>
<td>11.2</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td></td>
<td>7.5 (7.0 - 8.0)</td>
<td>8.1</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>43.7 (42.8 - 44.7)</td>
<td>44.6</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>43.9 (43.0 - 44.7)</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>44.4 (43.5 - 45.2)</td>
<td>44.0</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>44.2 (43.4 - 45.1)</td>
<td>44.5</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system copiaco</td>
<td>Copiaco2020b</td>
<td>Sound Separation</td>
<td>6.9 (6.7 - 7.2)</td>
<td>8.7</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>LJK PSH DCASE2020 Task4 SED 3</td>
<td>JiaKai2020</td>
<td></td>
<td>38.6 (37.7 - 39.7)</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>LJK PSH DCASE2020 Task4 SED 1</td>
<td>JiaKai2020</td>
<td></td>
<td>39.3 (38.4 - 40.4)</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>LJK PSH DCASE2020 Task4 SED 2</td>
<td>JiaKai2020</td>
<td></td>
<td>41.2 (40.1 - 42.4)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>LJK PSH DCASE2020 Task4 SED 4</td>
<td>JiaKai2020</td>
<td></td>
<td>40.6 (39.6 - 41.6)</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>47.0 (46.0 - 48.1)</td>
<td>46.4</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>46.3 (45.5 - 47.4)</td>
<td>47.7</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>44.9 (43.9 - 45.8)</td>
<td>48.2</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>47.8 (46.9 - 49.0)</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>DCASE2020 SED GCA system</td>
<td>HouZ2020</td>
<td></td>
<td>45.1 (44.2 - 45.8)</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher with thresholding</td>
<td>deBenito2020</td>
<td></td>
<td>38.2 (37.5 - 39.2)</td>
<td>43.4</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2020</td>
<td></td>
<td>37.9 (37.0 - 39.1)</td>
<td>40.9</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh_NTHU_3</td>
<td>Koh2020</td>
<td></td>
<td>46.6 (45.8 - 47.6)</td>
<td>48.0</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Koh_NTHU_2</td>
<td>Koh2020</td>
<td></td>
<td>45.2 (44.3 - 46.3)</td>
<td>48.4</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Koh_NTHU_1</td>
<td>Koh2020</td>
<td></td>
<td>45.2 (44.2 - 46.1)</td>
<td>46.4</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Koh_NTHU_4</td>
<td>Koh2020</td>
<td></td>
<td>46.3 (45.4 - 47.2)</td>
<td>49.6</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>UNIVPM-INRIA DAT HMM 1</td>
<td>Cornell2020</td>
<td></td>
<td>42.0 (40.9 - 43.1)</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN</td>
<td>Cornell2020</td>
<td></td>
<td>44.4 (43.3 - 45.5)</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN HMM 2</td>
<td>Cornell2020</td>
<td></td>
<td>43.2 (42.1 - 44.4)</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>UNIVPM-INRIA separation hmm</td>
<td>Cornell2020</td>
<td>Sound Separation</td>
<td>38.6 (37.5 - 39.6)</td>
<td>40.2</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>UNIVPM-INRIA ensemble MT+PCEN</td>
<td>Cornell2020</td>
<td></td>
<td>42.6 (41.6 - 43.5)</td>
<td>43.7</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao2020</td>
<td></td>
<td>44.1 (43.1 - 45.2)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td></td>
<td>46.4 (45.3 - 47.6)</td>
<td>49.5</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao2020</td>
<td></td>
<td>45.7 (44.7 - 47.0)</td>
<td>50.5</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao2020</td>
<td></td>
<td>46.2 (45.2 - 47.0)</td>
<td>49.6</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_valid</td>
<td>Liu2020</td>
<td></td>
<td>40.7 (39.7 - 41.7)</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_valid</td>
<td>Liu2020</td>
<td></td>
<td>41.8 (40.7 - 42.9)</td>
<td>49.5</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_pub</td>
<td>Liu2020</td>
<td></td>
<td>45.2 (44.2 - 46.5)</td>
<td>44.3</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>MT_PPDA_finall</td>
<td>Liu2020</td>
<td></td>
<td>43.1 (42.1 - 44.2)</td>
<td>46.8</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>PARK_fusion_P</td>
<td>Park2020</td>
<td></td>
<td>35.8 (35.0 - 36.6)</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>PARK_fusion_N</td>
<td>Park2020</td>
<td></td>
<td>26.5 (25.7 - 27.5)</td>
<td>41.1</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>PARK_fusion_M</td>
<td>Park2020</td>
<td></td>
<td>36.9 (36.1 - 37.7)</td>
<td>44.9</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>PARK_fusion_L</td>
<td>Park2020</td>
<td></td>
<td>34.7 (34.1 - 35.6)</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>Sound Separation</td>
<td>34.5 (33.5 - 35.3)</td>
<td>36.7</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>CTK_NU NMF-CNN-2</td>
<td>Chan2020</td>
<td></td>
<td>44.4 (43.5 - 45.5)</td>
<td>45.7</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>CTK_NU NMF-CNN-4</td>
<td>Chan2020</td>
<td></td>
<td>46.3 (45.3 - 47.4)</td>
<td>48.6</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>CTK_NU NMF-CNN-3</td>
<td>Chan2020</td>
<td></td>
<td>45.8 (45.0 - 47.0)</td>
<td>48.0</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>CTK_NU NMF-CNN-1</td>
<td>Chan2020</td>
<td></td>
<td>43.5 (42.6 - 44.7)</td>
<td>45.2</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>42.7 (41.6 - 43.6)</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>42.6 (41.8 - 43.7)</td>
<td>45.7</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>41.6 (40.6 - 42.7)</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>43.6 (42.4 - 44.6)</td>
<td>45.6</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Basic ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>43.1 (42.3 - 44.1)</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Multi-scale ResNet block with weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>44.1 (43.4 - 44.8)</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Basic ResNet block with weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>42.4 (41.4 - 43.4)</td>
<td>48.4</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Multi-scale ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>44.1 (43.3 - 45.0)</td>
<td>48.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>DCASE2020 SED baseline system</td>
<td>turpault2020a</td>
<td></td>
<td>34.9 (34.0 - 35.7)</td>
<td>34.8</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td>turpault2020b</td>
<td>Sound Separation</td>
<td>36.5 (35.6 - 37.2)</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>DCASE2020 UPB SED system 1</td>
<td>Ebbers2020</td>
<td></td>
<td>47.2 (46.5 - 48.1)</td>
<td>48.3</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="PSDS_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="PSDS Cross-trigger (Evaluation dataset)" data-chartable="true" data-field="PSDS_all" data-sortable="true" data-value-type="float3">
<br/>PSDS Cross-trigger<br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Public evaluation)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Public evaluation)
            </th>
<th class="text-center" data-axis-label="PSDS Cross-trigger (Public evaluation)" data-chartable="true" data-field="PSDS_youtube" data-sortable="true" data-value-type="float3">
<br/>PSDS Cross-trigger<br/>(Public evaluation)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Vimeo dataset)
            </th>
<th class="text-center" data-axis-label="PSDS Cross-trigger (Vimeo dataset)" data-chartable="true" data-field="PSDS_vimeo" data-sortable="true" data-value-type="float3">
<br/>PSDS Cross-trigger<br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>DCASE2020 SED mean-teacher system</td>
<td>Liang2020</td>
<td></td>
<td>36.0 (35.3 - 36.8)</td>
<td></td>
<td>40.7</td>
<td></td>
<td>25.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.6 (21.0 - 22.4)</td>
<td></td>
<td>23.7</td>
<td></td>
<td>15.4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.9 (21.3 - 22.7)</td>
<td></td>
<td>24.0</td>
<td></td>
<td>15.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>10.4 (9.7 - 11.1)</td>
<td></td>
<td>11.9</td>
<td></td>
<td>6.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.6 (20.8 - 22.4)</td>
<td></td>
<td>23.5</td>
<td></td>
<td>15.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>DCASE2020 WASEDA IPS SED</td>
<td>HouB2020</td>
<td></td>
<td>34.9 (34.0 - 35.7)</td>
<td></td>
<td>38.1</td>
<td></td>
<td>27.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Conforemr SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>51.1 (50.1 - 52.3)</td>
<td></td>
<td>55.7</td>
<td></td>
<td>39.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Transforemr SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>46.4 (45.5 - 47.5)</td>
<td></td>
<td>51.1</td>
<td></td>
<td>34.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>transformer conformer Ensemble SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>50.7 (49.6 - 51.9)</td>
<td></td>
<td>55.2</td>
<td></td>
<td>39.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.3 (43.4 - 45.4)</td>
<td></td>
<td>48.7</td>
<td></td>
<td>32.2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.6 (43.5 - 46.0)</td>
<td></td>
<td>49.7</td>
<td></td>
<td>31.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.1 (42.9 - 45.4)</td>
<td></td>
<td>48.6</td>
<td></td>
<td>32.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.3 (43.2 - 45.6)</td>
<td></td>
<td>49.0</td>
<td></td>
<td>32.2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.7 (43.6 - 46.2)</td>
<td></td>
<td>49.5</td>
<td></td>
<td>32.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td></td>
<td>44.3 (43.2 - 45.6)</td>
<td></td>
<td>49.0</td>
<td></td>
<td>32.2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.4 (43.2 - 45.8)</td>
<td></td>
<td>49.3</td>
<td></td>
<td>32.2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>guided_and_multi_branch_learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.5 (43.3 - 46.0)</td>
<td></td>
<td>49.3</td>
<td></td>
<td>32.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td></td>
<td>7.8 (7.3 - 8.2)</td>
<td></td>
<td>8.5</td>
<td></td>
<td>5.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td></td>
<td>7.5 (7.0 - 8.0)</td>
<td></td>
<td>7.9</td>
<td></td>
<td>5.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>43.7 (42.8 - 44.7)</td>
<td>0.645</td>
<td>48.0</td>
<td>0.701</td>
<td>33.0</td>
<td>0.529</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>43.9 (43.0 - 44.7)</td>
<td>0.646</td>
<td>48.1</td>
<td>0.704</td>
<td>33.5</td>
<td>0.521</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>44.4 (43.5 - 45.2)</td>
<td>0.641</td>
<td>48.0</td>
<td>0.698</td>
<td>35.5</td>
<td>0.522</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>44.2 (43.4 - 45.1)</td>
<td>0.650</td>
<td>47.9</td>
<td>0.705</td>
<td>35.2</td>
<td>0.531</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system copiaco</td>
<td>Copiaco2020b</td>
<td>Sound Separation</td>
<td>6.9 (6.7 - 7.2)</td>
<td></td>
<td>7.3</td>
<td></td>
<td>5.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>LJK PSH DCASE2020 Task4 SED 3</td>
<td>JiaKai2020</td>
<td></td>
<td>38.6 (37.7 - 39.7)</td>
<td>0.582</td>
<td>42.6</td>
<td>0.639</td>
<td>28.8</td>
<td>0.468</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>LJK PSH DCASE2020 Task4 SED 1</td>
<td>JiaKai2020</td>
<td></td>
<td>39.3 (38.4 - 40.4)</td>
<td>0.595</td>
<td>43.9</td>
<td>0.644</td>
<td>28.1</td>
<td>0.509</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>LJK PSH DCASE2020 Task4 SED 2</td>
<td>JiaKai2020</td>
<td></td>
<td>41.2 (40.1 - 42.4)</td>
<td>0.602</td>
<td>45.8</td>
<td>0.653</td>
<td>29.7</td>
<td>0.513</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>LJK PSH DCASE2020 Task4 SED 4</td>
<td>JiaKai2020</td>
<td></td>
<td>40.6 (39.6 - 41.6)</td>
<td>0.598</td>
<td>44.1</td>
<td>0.647</td>
<td>31.9</td>
<td>0.494</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>47.0 (46.0 - 48.1)</td>
<td></td>
<td>50.6</td>
<td></td>
<td>37.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>46.3 (45.5 - 47.4)</td>
<td></td>
<td>50.3</td>
<td></td>
<td>36.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>44.9 (43.9 - 45.8)</td>
<td></td>
<td>49.4</td>
<td></td>
<td>33.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>47.8 (46.9 - 49.0)</td>
<td></td>
<td>52.3</td>
<td></td>
<td>35.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>DCASE2020 SED GCA system</td>
<td>HouZ2020</td>
<td></td>
<td>45.1 (44.2 - 45.8)</td>
<td>0.600</td>
<td>49.0</td>
<td>0.654</td>
<td>35.2</td>
<td>0.474</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher with thresholding</td>
<td>deBenito2020</td>
<td></td>
<td>38.2 (37.5 - 39.2)</td>
<td>0.575</td>
<td>42.0</td>
<td>0.630</td>
<td>29.1</td>
<td>0.460</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2020</td>
<td></td>
<td>37.9 (37.0 - 39.1)</td>
<td>0.575</td>
<td>41.5</td>
<td>0.630</td>
<td>29.4</td>
<td>0.460</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh_NTHU_3</td>
<td>Koh2020</td>
<td></td>
<td>46.6 (45.8 - 47.6)</td>
<td>0.584</td>
<td>51.5</td>
<td>0.636</td>
<td>34.5</td>
<td>0.476</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Koh_NTHU_2</td>
<td>Koh2020</td>
<td></td>
<td>45.2 (44.3 - 46.3)</td>
<td>0.645</td>
<td>48.7</td>
<td>0.688</td>
<td>36.4</td>
<td>0.549</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Koh_NTHU_1</td>
<td>Koh2020</td>
<td></td>
<td>45.2 (44.2 - 46.1)</td>
<td>0.624</td>
<td>49.1</td>
<td>0.669</td>
<td>35.4</td>
<td>0.523</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Koh_NTHU_4</td>
<td>Koh2020</td>
<td></td>
<td>46.3 (45.4 - 47.2)</td>
<td>0.586</td>
<td>50.3</td>
<td>0.639</td>
<td>36.1</td>
<td>0.478</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>UNIVPM-INRIA DAT HMM 1</td>
<td>Cornell2020</td>
<td></td>
<td>42.0 (40.9 - 43.1)</td>
<td></td>
<td>45.6</td>
<td></td>
<td>32.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN</td>
<td>Cornell2020</td>
<td></td>
<td>44.4 (43.3 - 45.5)</td>
<td></td>
<td>48.6</td>
<td></td>
<td>33.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN HMM 2</td>
<td>Cornell2020</td>
<td></td>
<td>43.2 (42.1 - 44.4)</td>
<td></td>
<td>47.9</td>
<td></td>
<td>31.4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>UNIVPM-INRIA separation hmm</td>
<td>Cornell2020</td>
<td>Sound Separation</td>
<td>38.6 (37.5 - 39.6)</td>
<td></td>
<td>42.3</td>
<td></td>
<td>29.4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>UNIVPM-INRIA ensemble MT+PCEN</td>
<td>Cornell2020</td>
<td></td>
<td>42.6 (41.6 - 43.5)</td>
<td></td>
<td>47.7</td>
<td></td>
<td>29.4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao2020</td>
<td></td>
<td>44.1 (43.1 - 45.2)</td>
<td></td>
<td>47.6</td>
<td></td>
<td>35.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td></td>
<td>46.4 (45.3 - 47.6)</td>
<td></td>
<td>50.5</td>
<td></td>
<td>36.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao2020</td>
<td></td>
<td>45.7 (44.7 - 47.0)</td>
<td></td>
<td>49.6</td>
<td></td>
<td>35.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao2020</td>
<td></td>
<td>46.2 (45.2 - 47.0)</td>
<td></td>
<td>49.9</td>
<td></td>
<td>37.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_valid</td>
<td>Liu2020</td>
<td></td>
<td>40.7 (39.7 - 41.7)</td>
<td></td>
<td>45.4</td>
<td></td>
<td>27.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_valid</td>
<td>Liu2020</td>
<td></td>
<td>41.8 (40.7 - 42.9)</td>
<td></td>
<td>46.0</td>
<td></td>
<td>31.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_pub</td>
<td>Liu2020</td>
<td></td>
<td>45.2 (44.2 - 46.5)</td>
<td></td>
<td>51.2</td>
<td></td>
<td>30.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>MT_PPDA_finall</td>
<td>Liu2020</td>
<td></td>
<td>43.1 (42.1 - 44.2)</td>
<td></td>
<td>47.2</td>
<td></td>
<td>32.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>PARK_fusion_P</td>
<td>Park2020</td>
<td></td>
<td>35.8 (35.0 - 36.6)</td>
<td></td>
<td>38.9</td>
<td></td>
<td>28.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>PARK_fusion_N</td>
<td>Park2020</td>
<td></td>
<td>26.5 (25.7 - 27.5)</td>
<td></td>
<td>28.1</td>
<td></td>
<td>22.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>PARK_fusion_M</td>
<td>Park2020</td>
<td></td>
<td>36.9 (36.1 - 37.7)</td>
<td></td>
<td>40.2</td>
<td></td>
<td>28.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>PARK_fusion_L</td>
<td>Park2020</td>
<td></td>
<td>34.7 (34.1 - 35.6)</td>
<td></td>
<td>38.1</td>
<td></td>
<td>26.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>Sound Separation</td>
<td>34.5 (33.5 - 35.3)</td>
<td></td>
<td>37.8</td>
<td></td>
<td>26.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>CTK_NU NMF-CNN-2</td>
<td>Chan2020</td>
<td></td>
<td>44.4 (43.5 - 45.5)</td>
<td>0.522</td>
<td>47.8</td>
<td>0.551</td>
<td>35.1</td>
<td>0.452</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>CTK_NU NMF-CNN-4</td>
<td>Chan2020</td>
<td></td>
<td>46.3 (45.3 - 47.4)</td>
<td>0.534</td>
<td>50.5</td>
<td>0.567</td>
<td>35.3</td>
<td>0.460</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>CTK_NU NMF-CNN-3</td>
<td>Chan2020</td>
<td></td>
<td>45.8 (45.0 - 47.0)</td>
<td>0.543</td>
<td>49.8</td>
<td>0.575</td>
<td>35.4</td>
<td>0.468</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>CTK_NU NMF-CNN-1</td>
<td>Chan2020</td>
<td></td>
<td>43.5 (42.6 - 44.7)</td>
<td>0.503</td>
<td>47.5</td>
<td>0.535</td>
<td>33.2</td>
<td>0.436</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>42.7 (41.6 - 43.6)</td>
<td></td>
<td>46.9</td>
<td></td>
<td>31.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>42.6 (41.8 - 43.7)</td>
<td></td>
<td>47.5</td>
<td></td>
<td>29.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>41.6 (40.6 - 42.7)</td>
<td></td>
<td>45.5</td>
<td></td>
<td>30.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>43.6 (42.4 - 44.6)</td>
<td></td>
<td>48.5</td>
<td></td>
<td>30.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Basic ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>43.1 (42.3 - 44.1)</td>
<td>0.495</td>
<td>46.6</td>
<td>0.538</td>
<td>33.7</td>
<td>0.400</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Multi-scale ResNet block with weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>44.1 (43.4 - 44.8)</td>
<td>0.510</td>
<td>47.5</td>
<td>0.556</td>
<td>35.3</td>
<td>0.411</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Basic ResNet block with weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>42.4 (41.4 - 43.4)</td>
<td>0.506</td>
<td>46.3</td>
<td>0.552</td>
<td>32.1</td>
<td>0.400</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Multi-scale ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>44.1 (43.3 - 45.0)</td>
<td>0.503</td>
<td>48.8</td>
<td>0.559</td>
<td>32.5</td>
<td>0.379</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>DCASE2020 SED baseline system</td>
<td>turpault2020a</td>
<td></td>
<td>34.9 (34.0 - 35.7)</td>
<td>0.496</td>
<td>38.1</td>
<td>0.552</td>
<td>27.8</td>
<td>0.378</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td>turpault2020b</td>
<td>Sound Separation</td>
<td>36.5 (35.6 - 37.2)</td>
<td>0.497</td>
<td>39.8</td>
<td>0.549</td>
<td>28.8</td>
<td>0.383</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>DCASE2020 UPB SED system 1</td>
<td>Ebbers2020</td>
<td></td>
<td>47.2 (46.5 - 48.1)</td>
<td></td>
<td>50.9</td>
<td></td>
<td>38.7</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation<br/>
</th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_dev" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>DCASE2020 SED mean-teacher system</td>
<td>Liang2020</td>
<td></td>
<td>36.0 (35.3 - 36.8)</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.9 (21.3 - 22.7)</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>DCASE2020 WASEDA IPS SED</td>
<td>HouB2020</td>
<td></td>
<td>34.9 (34.0 - 35.7)</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Conforemr SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>51.1 (50.1 - 52.3)</td>
<td>50.6</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.7 (43.6 - 46.2)</td>
<td>47.2</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td></td>
<td>7.8 (7.3 - 8.2)</td>
<td>11.2</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>44.4 (43.5 - 45.2)</td>
<td>44.0</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>LJK PSH DCASE2020 Task4 SED 2</td>
<td>JiaKai2020</td>
<td></td>
<td>41.2 (40.1 - 42.4)</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>47.8 (46.9 - 49.0)</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>DCASE2020 SED GCA system</td>
<td>HouZ2020</td>
<td></td>
<td>45.1 (44.2 - 45.8)</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher with Thresholds</td>
<td>deBenito2020</td>
<td></td>
<td>38.2 (37.5 - 39.2)</td>
<td>43.4</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh_NTHU_3</td>
<td>Koh2020</td>
<td></td>
<td>46.6 (45.8 - 47.6)</td>
<td>48.0</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN</td>
<td>Cornell2020</td>
<td></td>
<td>44.4 (43.3 - 45.5)</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td></td>
<td>46.4 (45.3 - 47.6)</td>
<td>49.5</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_pub</td>
<td>Liu2020</td>
<td></td>
<td>45.2 (44.2 - 46.5)</td>
<td>44.3</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>PARK_fusion_M</td>
<td>Park2020</td>
<td></td>
<td>36.9 (36.1 - 37.7)</td>
<td>44.9</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>Sound Separation</td>
<td>34.5 (33.5 - 35.3)</td>
<td>36.7</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>CTK_NU NMF-CNN-4</td>
<td>Chan2020</td>
<td></td>
<td>46.3 (45.3 - 47.4)</td>
<td>48.6</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>43.6 (42.4 - 44.6)</td>
<td>45.6</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Multi-scale ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>44.1 (43.3 - 45.0)</td>
<td>48.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td>turpault2020b</td>
<td>Sound Separation</td>
<td>36.5 (35.6 - 37.2)</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>DCASE2020 UPB SED system 1</td>
<td>Ebbers2020</td>
<td></td>
<td>47.2 (46.5 - 48.1)</td>
<td>48.3</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics-1">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="PSDS_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="sound_separation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound<br/>Separation
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="PSDS Cross-trigger (Evaluation dataset)" data-chartable="true" data-field="PSDS_all" data-sortable="true" data-value-type="float3">
<br/>PSDS Cross-trigger<br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Public evaluation)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Public evaluation)
            </th>
<th class="text-center" data-axis-label="PSDS Cross-trigger (Public evaluation)" data-chartable="true" data-field="PSDS_youtube" data-sortable="true" data-value-type="float3">
<br/>PSDS Cross-trigger<br/>(Public evaluation)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Vimeo dataset)
            </th>
<th class="text-center" data-axis-label="PSDS Cross-trigger (Vimeo dataset)" data-chartable="true" data-field="PSDS_vimeo" data-sortable="true" data-value-type="float3">
<br/>PSDS Cross-trigger<br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>DCASE2020 SED mean-teacher system</td>
<td>Liang2020</td>
<td></td>
<td>36.0 (35.3 - 36.8)</td>
<td></td>
<td>40.7</td>
<td></td>
<td>25.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td></td>
<td>21.9 (21.3 - 22.7)</td>
<td></td>
<td>24.0</td>
<td></td>
<td>15.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>DCASE2020 WASEDA IPS SED</td>
<td>HouB2020</td>
<td></td>
<td>34.9 (34.0 - 35.7)</td>
<td></td>
<td>38.1</td>
<td></td>
<td>27.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Conforemr SED</td>
<td>Miyazaki2020</td>
<td></td>
<td>51.1 (50.1 - 52.3)</td>
<td></td>
<td>55.7</td>
<td></td>
<td>39.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Guided multi branch learning</td>
<td>Huang2020</td>
<td>Sound Separation</td>
<td>44.7 (43.6 - 46.2)</td>
<td></td>
<td>49.5</td>
<td></td>
<td>32.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td></td>
<td>7.8 (7.3 - 8.2)</td>
<td></td>
<td>8.5</td>
<td></td>
<td>5.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td></td>
<td>44.4 (43.5 - 45.2)</td>
<td>0.641</td>
<td>48.0</td>
<td>0.698</td>
<td>35.5</td>
<td>0.522</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>LJK PSH DCASE2020 Task4 SED 2</td>
<td>JiaKai2020</td>
<td></td>
<td>41.2 (40.1 - 42.4)</td>
<td>0.602</td>
<td>45.8</td>
<td>0.653</td>
<td>29.7</td>
<td>0.513</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td></td>
<td>47.8 (46.9 - 49.0)</td>
<td></td>
<td>52.3</td>
<td></td>
<td>35.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>DCASE2020 SED GCA system</td>
<td>HouZ2020</td>
<td></td>
<td>45.1 (44.2 - 45.8)</td>
<td>0.600</td>
<td>49.0</td>
<td>0.654</td>
<td>35.2</td>
<td>0.474</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher with Thresholds</td>
<td>deBenito2020</td>
<td></td>
<td>38.2 (37.5 - 39.2)</td>
<td>0.575</td>
<td>42.0</td>
<td>0.630</td>
<td>29.1</td>
<td>0.460</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh_NTHU_3</td>
<td>Koh2020</td>
<td></td>
<td>46.6 (45.8 - 47.6)</td>
<td>0.584</td>
<td>51.5</td>
<td>0.636</td>
<td>34.5</td>
<td>0.476</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN</td>
<td>Cornell2020</td>
<td></td>
<td>44.4 (43.3 - 45.5)</td>
<td></td>
<td>48.6</td>
<td></td>
<td>33.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td></td>
<td>46.4 (45.3 - 47.6)</td>
<td></td>
<td>50.5</td>
<td></td>
<td>36.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_pub</td>
<td>Liu2020</td>
<td></td>
<td>45.2 (44.2 - 46.5)</td>
<td></td>
<td>51.2</td>
<td></td>
<td>30.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>PARK_fusion_M</td>
<td>Park2020</td>
<td></td>
<td>36.9 (36.1 - 37.7)</td>
<td></td>
<td>40.2</td>
<td></td>
<td>28.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>Sound Separation</td>
<td>34.5 (33.5 - 35.3)</td>
<td></td>
<td>37.8</td>
<td></td>
<td>26.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>CTK_NU NMF-CNN-4</td>
<td>Chan2020</td>
<td></td>
<td>46.3 (45.3 - 47.4)</td>
<td>0.534</td>
<td>50.5</td>
<td>0.567</td>
<td>35.3</td>
<td>0.460</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td></td>
<td>43.6 (42.4 - 44.6)</td>
<td></td>
<td>48.5</td>
<td></td>
<td>30.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Multi-scale ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td></td>
<td>44.1 (43.3 - 45.0)</td>
<td>0.503</td>
<td>48.8</td>
<td>0.559</td>
<td>32.5</td>
<td>0.379</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td>turpault2020b</td>
<td>Sound Separation</td>
<td>36.5 (35.6 - 37.2)</td>
<td>0.497</td>
<td>39.8</td>
<td>0.549</td>
<td>28.8</td>
<td>0.383</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>DCASE2020 UPB SED system 1</td>
<td>Ebbers2020</td>
<td></td>
<td>47.2 (46.5 - 48.1)</td>
<td></td>
<td>50.9</td>
<td></td>
<td>38.7</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="combined-ss-and-sed-ranking">Combined SS and SED ranking</h1>
<h2 id="system-ranking">System ranking</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Public evaluation)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Public evaluation)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system copiaco</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>7.3</td>
<td>5.7</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>37.8</td>
<td>26.9</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>UNIVPM-INRIA separation hmm</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>42.3</td>
<td>29.4</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>Guided multi branch learning</td>
<td>Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>48.6</td>
<td>32.8</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>Guided multi branch learning</td>
<td>Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>49.3</td>
<td>32.2</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>Guided_and_multi_branch_learning</td>
<td>Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>49.3</td>
<td>32.6</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Guided multi branch learning</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>49.5</td>
<td>32.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td></td>
<td>36.5 (35.6 - 37.2)</td>
<td>39.8</td>
<td>28.8</td>
</tr>
</tbody>
</table>
<h2 id="team-ranking">Team ranking</h2>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Public evaluation)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Public evaluation)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system copiaco</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>7.3</td>
<td>5.7</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>37.8</td>
<td>26.9</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>UNIVPM-INRIA separation hmm</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>42.3</td>
<td>29.4</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Guided multi branch learning</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>49.5</td>
<td>32.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td></td>
<td>36.5 (35.6 - 37.2)</td>
<td>39.8</td>
<td>28.8</td>
</tr>
</tbody>
</table>
<h1 id="ss-ranking">SS ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="Single_source_SI_SNR" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Single-source SI-SNR (Evaluation dataset)" data-chartable="true" data-field="Single_source_SI_SNR" data-sortable="true" data-value-type="float1-percentage">
                Single-source<br/>SI-SNR<br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Multi-source SI-SNRi (Evaluation dataset)" data-chartable="true" data-field="Multi_source_SI_SNRi" data-sortable="true" data-value-type="float1-percentage">
                Multi-source<br/>SI-SNRi<br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SS_1</td>
<td>DCASE2020 SS system (STFT)</td>
<td>Liang2020</td>
<td>33.8</td>
<td>7.0</td>
</tr>
<tr>
<td></td>
<td>Xiaomi_task4_SS_2</td>
<td>DCASE2020 SS system (STFT + MFCC)</td>
<td>Liang2020</td>
<td>31.8</td>
<td>8.4</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_baseline_system</td>
<td>DCASE2020 SS baseline system</td>
<td>kavalerov2019universal</td>
<td>37.6</td>
<td>12.5</td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SS_1</td>
<td>DCASE2020 WASEDA IPS SS</td>
<td>HouB2020</td>
<td>37.6</td>
<td>12.5</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="Baseline_dcase2019" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Lin_ICT_task4_3" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["Class_f_score_Alarm_bell_ringing", "Class_f_score_Blender", "Class_f_score_Cat", "Class_f_score_Dishes", "Class_f_score_Dog", "Class_f_score_Electric_shaver_toothbrush", "Class_f_score_Frying", "Class_f_score_Running_water", "Class_f_score_Speech", "Class_f_score_Vacuum_cleaner"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission<br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="Class_f_score_Alarm_bell_ringing" data-sortable="true" data-value-type="float1-percentage">
                Alarm<br/>Bell<br/>Ringing
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Blender" data-sortable="true" data-value-type="float1-percentage">
                Blender
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Cat" data-sortable="true" data-value-type="float1-percentage">
                Cat
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dishes" data-sortable="true" data-value-type="float1-percentage">
                Dishes
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dog" data-sortable="true" data-value-type="float1-percentage">
                Dog
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Electric_shaver_toothbrush" data-sortable="true" data-value-type="float1-percentage">
                Electric<br/>shave<br/>toothbrush
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Frying" data-sortable="true" data-value-type="float1-percentage">
                Frying
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Running_water" data-sortable="true" data-value-type="float1-percentage">
                Running<br/>water
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Speech" data-sortable="true" data-value-type="float1-percentage">
                Speech
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Vacuum_cleaner" data-sortable="true" data-value-type="float1-percentage">
                Vacuum<br/>cleaner
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>DCASE2020 SED mean-teacher system</td>
<td>Liang2020</td>
<td>36.0 (35.3 - 36.8)</td>
<td>29.8</td>
<td>39.3</td>
<td>64.4</td>
<td>26.6</td>
<td>35.4</td>
<td>29.8</td>
<td>32.8</td>
<td>24.0</td>
<td>55.1</td>
<td>23.7</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td>21.6 (21.0 - 22.4)</td>
<td>36.5</td>
<td>5.0</td>
<td>50.7</td>
<td>22.5</td>
<td>37.1</td>
<td>5.8</td>
<td>9.3</td>
<td>1.6</td>
<td>43.1</td>
<td>3.8</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td>21.9 (21.3 - 22.7)</td>
<td>37.6</td>
<td>5.1</td>
<td>51.7</td>
<td>23.0</td>
<td>37.0</td>
<td>6.4</td>
<td>9.1</td>
<td>1.6</td>
<td>43.0</td>
<td>3.8</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td>10.4 (9.7 - 11.1)</td>
<td>10.2</td>
<td>10.3</td>
<td>0.5</td>
<td>0.7</td>
<td>0.6</td>
<td>20.0</td>
<td>33.0</td>
<td>8.4</td>
<td>1.2</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>DCASE2020 SED CNNR</td>
<td>Rykaczewski2020</td>
<td>21.6 (20.8 - 22.4)</td>
<td>36.7</td>
<td>4.3</td>
<td>51.7</td>
<td>22.3</td>
<td>36.6</td>
<td>5.7</td>
<td>9.3</td>
<td>1.2</td>
<td>43.1</td>
<td>3.8</td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>DCASE2020 WASEDA IPS SED</td>
<td>HouB2020</td>
<td>34.9 (34.0 - 35.7)</td>
<td>35.9</td>
<td>37.0</td>
<td>62.3</td>
<td>26.0</td>
<td>27.1</td>
<td>25.9</td>
<td>24.7</td>
<td>24.3</td>
<td>48.2</td>
<td>39.0</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Conforemr SED</td>
<td>Miyazaki2020</td>
<td>51.1 (50.1 - 52.3)</td>
<td>40.9</td>
<td>51.5</td>
<td>67.3</td>
<td>38.9</td>
<td>51.4</td>
<td>46.7</td>
<td>53.1</td>
<td>35.3</td>
<td>64.4</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Transforemr SED</td>
<td>Miyazaki2020</td>
<td>46.4 (45.5 - 47.5)</td>
<td>33.5</td>
<td>51.4</td>
<td>56.2</td>
<td>39.6</td>
<td>49.2</td>
<td>42.2</td>
<td>47.0</td>
<td>27.7</td>
<td>63.0</td>
<td>55.4</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>transformer conformer Ensemble SED</td>
<td>Miyazaki2020</td>
<td>50.7 (49.6 - 51.9)</td>
<td>43.7</td>
<td>53.8</td>
<td>64.1</td>
<td>39.5</td>
<td>52.0</td>
<td>48.2</td>
<td>47.5</td>
<td>29.0</td>
<td>66.6</td>
<td>62.7</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td>44.3 (43.4 - 45.4)</td>
<td>41.0</td>
<td>49.0</td>
<td>55.3</td>
<td>26.1</td>
<td>44.8</td>
<td>40.3</td>
<td>43.1</td>
<td>27.0</td>
<td>58.6</td>
<td>56.2</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td>44.6 (43.5 - 46.0)</td>
<td>43.4</td>
<td>42.0</td>
<td>55.4</td>
<td>25.9</td>
<td>38.8</td>
<td>41.5</td>
<td>46.2</td>
<td>30.2</td>
<td>63.1</td>
<td>60.3</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>41.4</td>
<td>42.3</td>
<td>56.8</td>
<td>28.4</td>
<td>37.4</td>
<td>39.0</td>
<td>47.1</td>
<td>30.0</td>
<td>59.7</td>
<td>59.1</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>41.2</td>
<td>40.4</td>
<td>54.7</td>
<td>24.8</td>
<td>40.0</td>
<td>42.6</td>
<td>47.7</td>
<td>30.4</td>
<td>62.3</td>
<td>59.1</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>42.2</td>
<td>43.4</td>
<td>57.1</td>
<td>27.7</td>
<td>38.1</td>
<td>40.2</td>
<td>48.8</td>
<td>30.8</td>
<td>60.4</td>
<td>59.1</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>guided multi-branch learning</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>41.0</td>
<td>40.4</td>
<td>55.0</td>
<td>24.3</td>
<td>40.1</td>
<td>42.6</td>
<td>47.7</td>
<td>30.4</td>
<td>62.6</td>
<td>59.1</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>guided multi branch learning</td>
<td>Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>42.5</td>
<td>42.7</td>
<td>56.9</td>
<td>27.5</td>
<td>38.5</td>
<td>39.8</td>
<td>46.6</td>
<td>30.0</td>
<td>61.0</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>guided_and_multi_branch_learning</td>
<td>Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>41.9</td>
<td>41.3</td>
<td>56.5</td>
<td>27.8</td>
<td>37.6</td>
<td>42.0</td>
<td>47.6</td>
<td>30.0</td>
<td>61.7</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td>7.8 (7.3 - 8.2)</td>
<td>3.8</td>
<td>9.7</td>
<td>13.1</td>
<td>3.2</td>
<td>5.1</td>
<td>4.1</td>
<td>1.0</td>
<td>9.3</td>
<td>10.4</td>
<td>18.0</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>DCASE2020 SED system copiaco</td>
<td>Copiaco2020a</td>
<td>7.5 (7.0 - 8.0)</td>
<td>4.3</td>
<td>11.7</td>
<td>13.6</td>
<td>1.6</td>
<td>6.2</td>
<td>1.3</td>
<td>1.2</td>
<td>5.3</td>
<td>12.1</td>
<td>17.0</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td>43.7 (42.8 - 44.7)</td>
<td>26.8</td>
<td>55.1</td>
<td>67.9</td>
<td>27.9</td>
<td>32.8</td>
<td>27.1</td>
<td>44.4</td>
<td>29.4</td>
<td>65.7</td>
<td>60.8</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td>43.9 (43.0 - 44.7)</td>
<td>26.5</td>
<td>53.6</td>
<td>68.6</td>
<td>28.1</td>
<td>33.9</td>
<td>29.7</td>
<td>43.6</td>
<td>31.6</td>
<td>64.9</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td>44.4 (43.5 - 45.2)</td>
<td>28.2</td>
<td>56.8</td>
<td>68.3</td>
<td>29.8</td>
<td>34.1</td>
<td>28.9</td>
<td>42.7</td>
<td>30.0</td>
<td>65.4</td>
<td>60.4</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>DCASE 2020 task4 SED system with semi-supervised loss function</td>
<td>Kim2020</td>
<td>44.2 (43.4 - 45.1)</td>
<td>27.2</td>
<td>58.2</td>
<td>67.2</td>
<td>28.7</td>
<td>34.3</td>
<td>26.7</td>
<td>44.3</td>
<td>31.9</td>
<td>66.3</td>
<td>58.1</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system copiaco</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>3.9</td>
<td>5.8</td>
<td>15.0</td>
<td>1.0</td>
<td>6.3</td>
<td>1.3</td>
<td>2.2</td>
<td>3.4</td>
<td>15.7</td>
<td>14.3</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>LJK PSH DCASE2020 Task4 SED 3</td>
<td>JiaKai2020</td>
<td>38.6 (37.7 - 39.7)</td>
<td>34.5</td>
<td>41.8</td>
<td>57.1</td>
<td>28.7</td>
<td>31.7</td>
<td>39.7</td>
<td>38.5</td>
<td>23.4</td>
<td>54.9</td>
<td>36.4</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>LJK PSH DCASE2020 Task4 SED 1</td>
<td>JiaKai2020</td>
<td>39.3 (38.4 - 40.4)</td>
<td>33.8</td>
<td>37.3</td>
<td>59.6</td>
<td>28.9</td>
<td>33.8</td>
<td>38.9</td>
<td>36.8</td>
<td>28.6</td>
<td>55.0</td>
<td>41.7</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>LJK PSH DCASE2020 Task4 SED 2</td>
<td>JiaKai2020</td>
<td>41.2 (40.1 - 42.4)</td>
<td>34.2</td>
<td>36.6</td>
<td>58.9</td>
<td>31.0</td>
<td>42.7</td>
<td>40.3</td>
<td>42.3</td>
<td>28.3</td>
<td>58.8</td>
<td>39.2</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>LJK PSH DCASE2020 Task4 SED 4</td>
<td>JiaKai2020</td>
<td>40.6 (39.6 - 41.6)</td>
<td>29.7</td>
<td>42.6</td>
<td>58.5</td>
<td>28.5</td>
<td>33.4</td>
<td>42.2</td>
<td>44.2</td>
<td>29.7</td>
<td>51.2</td>
<td>46.0</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td>47.0 (46.0 - 48.1)</td>
<td>40.2</td>
<td>55.4</td>
<td>60.5</td>
<td>34.3</td>
<td>36.2</td>
<td>50.4</td>
<td>45.0</td>
<td>36.5</td>
<td>56.7</td>
<td>54.9</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td>46.3 (45.5 - 47.4)</td>
<td>45.5</td>
<td>48.6</td>
<td>59.1</td>
<td>30.5</td>
<td>33.5</td>
<td>51.6</td>
<td>47.3</td>
<td>33.2</td>
<td>57.0</td>
<td>56.8</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td>44.9 (43.9 - 45.8)</td>
<td>39.5</td>
<td>47.7</td>
<td>59.4</td>
<td>31.8</td>
<td>35.6</td>
<td>48.0</td>
<td>45.2</td>
<td>29.1</td>
<td>59.0</td>
<td>55.2</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>DCASE2020 cross-domain sound event detection</td>
<td>Hao2020</td>
<td>47.8 (46.9 - 49.0)</td>
<td>42.8</td>
<td>56.9</td>
<td>63.2</td>
<td>31.7</td>
<td>36.4</td>
<td>49.8</td>
<td>50.2</td>
<td>30.1</td>
<td>61.9</td>
<td>55.0</td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>DCASE2020 SED GCA system</td>
<td>HouZ2020</td>
<td>45.1 (44.2 - 45.8)</td>
<td>35.5</td>
<td>50.9</td>
<td>63.7</td>
<td>30.9</td>
<td>41.4</td>
<td>42.4</td>
<td>40.9</td>
<td>26.7</td>
<td>63.1</td>
<td>56.2</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher with thresholding</td>
<td>deBenito2020</td>
<td>38.2 (37.5 - 39.2)</td>
<td>38.5</td>
<td>42.2</td>
<td>63.1</td>
<td>22.3</td>
<td>21.5</td>
<td>36.8</td>
<td>30.8</td>
<td>23.5</td>
<td>54.0</td>
<td>51.5</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>5-Resolution Mean Teacher</td>
<td>de Benito-Gorron2020</td>
<td>37.9 (37.0 - 39.1)</td>
<td>40.3</td>
<td>42.4</td>
<td>61.5</td>
<td>20.8</td>
<td>14.5</td>
<td>40.9</td>
<td>28.5</td>
<td>24.3</td>
<td>48.4</td>
<td>60.4</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh_NTHU_3</td>
<td>Koh2020</td>
<td>46.6 (45.8 - 47.6)</td>
<td>38.4</td>
<td>50.7</td>
<td>66.6</td>
<td>29.5</td>
<td>42.0</td>
<td>49.1</td>
<td>44.7</td>
<td>24.3</td>
<td>66.0</td>
<td>56.0</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Koh_NTHU_2</td>
<td>Koh2020</td>
<td>45.2 (44.3 - 46.3)</td>
<td>37.6</td>
<td>53.4</td>
<td>67.1</td>
<td>31.0</td>
<td>43.1</td>
<td>43.7</td>
<td>44.2</td>
<td>25.9</td>
<td>65.1</td>
<td>42.2</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Koh_NTHU_1</td>
<td>Koh2020</td>
<td>45.2 (44.2 - 46.1)</td>
<td>37.6</td>
<td>53.4</td>
<td>67.1</td>
<td>31.0</td>
<td>43.1</td>
<td>45.8</td>
<td>36.8</td>
<td>29.1</td>
<td>65.1</td>
<td>44.0</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Koh_NTHU_4</td>
<td>Koh2020</td>
<td>46.3 (45.4 - 47.2)</td>
<td>38.4</td>
<td>50.7</td>
<td>66.6</td>
<td>29.5</td>
<td>42.0</td>
<td>44.1</td>
<td>50.6</td>
<td>23.1</td>
<td>66.0</td>
<td>53.0</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>UNIVPM-INRIA DAT HMM 1</td>
<td>Cornell2020</td>
<td>42.0 (40.9 - 43.1)</td>
<td>41.3</td>
<td>44.4</td>
<td>68.2</td>
<td>29.3</td>
<td>35.6</td>
<td>38.8</td>
<td>35.1</td>
<td>27.4</td>
<td>50.2</td>
<td>50.8</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN</td>
<td>Cornell2020</td>
<td>44.4 (43.3 - 45.5)</td>
<td>45.2</td>
<td>49.4</td>
<td>69.8</td>
<td>25.2</td>
<td>33.0</td>
<td>45.2</td>
<td>35.3</td>
<td>32.4</td>
<td>55.3</td>
<td>55.0</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>UNIVPM-INRIA ensemble DAT+PCEN HMM 2</td>
<td>Cornell2020</td>
<td>43.2 (42.1 - 44.4)</td>
<td>40.0</td>
<td>50.4</td>
<td>63.3</td>
<td>26.4</td>
<td>33.5</td>
<td>46.1</td>
<td>39.1</td>
<td>25.8</td>
<td>52.4</td>
<td>57.0</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>UNIVPM-INRIA separation hmm</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>30.3</td>
<td>43.4</td>
<td>65.6</td>
<td>28.3</td>
<td>25.2</td>
<td>41.9</td>
<td>32.4</td>
<td>23.8</td>
<td>49.1</td>
<td>47.6</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>UNIVPM-INRIA ensemble MT+PCEN</td>
<td>Cornell2020</td>
<td>42.6 (41.6 - 43.5)</td>
<td>45.4</td>
<td>39.7</td>
<td>60.2</td>
<td>29.5</td>
<td>36.9</td>
<td>47.3</td>
<td>36.2</td>
<td>23.6</td>
<td>56.6</td>
<td>51.2</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao2020</td>
<td>44.1 (43.1 - 45.2)</td>
<td>39.5</td>
<td>51.4</td>
<td>47.8</td>
<td>29.7</td>
<td>35.1</td>
<td>43.3</td>
<td>50.0</td>
<td>31.8</td>
<td>52.5</td>
<td>60.9</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td>46.4 (45.3 - 47.6)</td>
<td>39.6</td>
<td>48.1</td>
<td>50.3</td>
<td>32.9</td>
<td>36.7</td>
<td>54.3</td>
<td>54.1</td>
<td>36.4</td>
<td>49.2</td>
<td>63.7</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao2020</td>
<td>45.7 (44.7 - 47.0)</td>
<td>39.6</td>
<td>52.3</td>
<td>48.7</td>
<td>31.5</td>
<td>37.5</td>
<td>49.8</td>
<td>50.8</td>
<td>34.3</td>
<td>49.7</td>
<td>64.0</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao2020</td>
<td>46.2 (45.2 - 47.0)</td>
<td>40.1</td>
<td>54.9</td>
<td>53.5</td>
<td>33.8</td>
<td>22.0</td>
<td>53.0</td>
<td>56.7</td>
<td>36.1</td>
<td>49.9</td>
<td>62.9</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_valid</td>
<td>Liu2020</td>
<td>40.7 (39.7 - 41.7)</td>
<td>39.8</td>
<td>36.0</td>
<td>64.6</td>
<td>23.7</td>
<td>33.1</td>
<td>27.2</td>
<td>52.1</td>
<td>24.3</td>
<td>59.8</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_valid</td>
<td>Liu2020</td>
<td>41.8 (40.7 - 42.9)</td>
<td>39.8</td>
<td>36.0</td>
<td>62.4</td>
<td>23.7</td>
<td>32.8</td>
<td>39.1</td>
<td>52.1</td>
<td>24.3</td>
<td>59.6</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>MT_PPDA_cg_glu_pub</td>
<td>Liu2020</td>
<td>45.2 (44.2 - 46.5)</td>
<td>50.4</td>
<td>38.7</td>
<td>66.7</td>
<td>24.9</td>
<td>36.0</td>
<td>40.6</td>
<td>52.9</td>
<td>30.7</td>
<td>59.4</td>
<td>52.7</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>MT_PPDA_finall</td>
<td>Liu2020</td>
<td>43.1 (42.1 - 44.2)</td>
<td>43.6</td>
<td>36.3</td>
<td>65.5</td>
<td>24.2</td>
<td>32.6</td>
<td>40.2</td>
<td>52.2</td>
<td>27.2</td>
<td>59.6</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>PARK_fusion_P</td>
<td>Park2020</td>
<td>35.8 (35.0 - 36.6)</td>
<td>10.1</td>
<td>38.5</td>
<td>53.4</td>
<td>13.5</td>
<td>37.7</td>
<td>42.2</td>
<td>33.9</td>
<td>24.6</td>
<td>56.1</td>
<td>48.5</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>PARK_fusion_N</td>
<td>Park2020</td>
<td>26.5 (25.7 - 27.5)</td>
<td>10.1</td>
<td>38.5</td>
<td>33.3</td>
<td>13.5</td>
<td>37.7</td>
<td>18.2</td>
<td>32.0</td>
<td>15.3</td>
<td>25.9</td>
<td>40.9</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>PARK_fusion_M</td>
<td>Park2020</td>
<td>36.9 (36.1 - 37.7)</td>
<td>21.0</td>
<td>38.5</td>
<td>53.4</td>
<td>13.5</td>
<td>37.7</td>
<td>42.2</td>
<td>33.9</td>
<td>24.6</td>
<td>56.1</td>
<td>48.5</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>PARK_fusion_L</td>
<td>Park2020</td>
<td>34.7 (34.1 - 35.6)</td>
<td>19.4</td>
<td>38.5</td>
<td>52.2</td>
<td>13.0</td>
<td>37.4</td>
<td>37.3</td>
<td>27.7</td>
<td>23.2</td>
<td>53.5</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>DCASE2020 SS+SED system</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>37.9</td>
<td>36.3</td>
<td>58.1</td>
<td>23.7</td>
<td>26.3</td>
<td>23.1</td>
<td>26.5</td>
<td>24.5</td>
<td>48.8</td>
<td>40.5</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>CTK_NU NMF-CNN-2</td>
<td>Chan2020</td>
<td>44.4 (43.5 - 45.5)</td>
<td>40.3</td>
<td>54.0</td>
<td>49.4</td>
<td>22.0</td>
<td>34.6</td>
<td>43.9</td>
<td>47.6</td>
<td>31.5</td>
<td>58.6</td>
<td>61.7</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>CTK_NU NMF-CNN-4</td>
<td>Chan2020</td>
<td>46.3 (45.3 - 47.4)</td>
<td>41.6</td>
<td>55.1</td>
<td>55.3</td>
<td>20.1</td>
<td>46.4</td>
<td>42.2</td>
<td>50.2</td>
<td>34.3</td>
<td>58.7</td>
<td>59.9</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>CTK_NU NMF-CNN-3</td>
<td>Chan2020</td>
<td>45.8 (45.0 - 47.0)</td>
<td>44.8</td>
<td>52.9</td>
<td>54.4</td>
<td>20.1</td>
<td>41.6</td>
<td>42.2</td>
<td>49.8</td>
<td>33.3</td>
<td>59.8</td>
<td>59.9</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>CTK_NU NMF-CNN-1</td>
<td>Chan2020</td>
<td>43.5 (42.6 - 44.7)</td>
<td>42.5</td>
<td>49.1</td>
<td>55.0</td>
<td>17.7</td>
<td>46.0</td>
<td>36.0</td>
<td>42.7</td>
<td>33.1</td>
<td>55.2</td>
<td>57.6</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td>42.7 (41.6 - 43.6)</td>
<td>35.3</td>
<td>41.2</td>
<td>54.5</td>
<td>31.1</td>
<td>41.5</td>
<td>40.7</td>
<td>39.3</td>
<td>27.3</td>
<td>59.7</td>
<td>55.4</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td>42.6 (41.8 - 43.7)</td>
<td>42.5</td>
<td>37.7</td>
<td>56.1</td>
<td>29.8</td>
<td>41.2</td>
<td>38.6</td>
<td>44.1</td>
<td>29.6</td>
<td>56.5</td>
<td>49.6</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td>41.6 (40.6 - 42.7)</td>
<td>39.4</td>
<td>45.5</td>
<td>49.8</td>
<td>29.8</td>
<td>40.5</td>
<td>38.3</td>
<td>38.5</td>
<td>23.1</td>
<td>59.5</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>DCASE2020 SED</td>
<td>Yen2020</td>
<td>43.6 (42.4 - 44.6)</td>
<td>44.6</td>
<td>38.5</td>
<td>55.9</td>
<td>30.8</td>
<td>39.3</td>
<td>43.4</td>
<td>41.2</td>
<td>30.0</td>
<td>55.7</td>
<td>56.4</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Basic ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td>43.1 (42.3 - 44.1)</td>
<td>34.9</td>
<td>46.3</td>
<td>63.0</td>
<td>27.7</td>
<td>42.5</td>
<td>33.8</td>
<td>37.8</td>
<td>27.1</td>
<td>67.8</td>
<td>49.6</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Multi-scale ResNet block with weakly labeled data augmentation</td>
<td>Tang2020</td>
<td>44.1 (43.4 - 44.8)</td>
<td>34.4</td>
<td>48.6</td>
<td>62.7</td>
<td>23.8</td>
<td>41.0</td>
<td>35.9</td>
<td>42.8</td>
<td>32.3</td>
<td>66.7</td>
<td>52.8</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Basic ResNet block with weakly labeled data augmentation</td>
<td>Tang2020</td>
<td>42.4 (41.4 - 43.4)</td>
<td>40.3</td>
<td>41.4</td>
<td>62.7</td>
<td>27.3</td>
<td>37.0</td>
<td>33.3</td>
<td>48.3</td>
<td>20.0</td>
<td>66.6</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Multi-scale ResNet block without weakly labeled data augmentation</td>
<td>Tang2020</td>
<td>44.1 (43.3 - 45.0)</td>
<td>30.4</td>
<td>45.1</td>
<td>64.5</td>
<td>29.6</td>
<td>40.6</td>
<td>35.3</td>
<td>48.1</td>
<td>27.6</td>
<td>63.6</td>
<td>57.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>DCASE2020 SED baseline system</td>
<td>turpault2020a</td>
<td>34.9 (34.0 - 35.7)</td>
<td>35.9</td>
<td>37.0</td>
<td>62.6</td>
<td>26.0</td>
<td>27.1</td>
<td>25.9</td>
<td>24.7</td>
<td>24.3</td>
<td>48.2</td>
<td>39.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>DCASE2020 SS+SED baseline system</td>
<td>turpault2020b</td>
<td>36.5 (35.6 - 37.2)</td>
<td>38.7</td>
<td>37.5</td>
<td>62.8</td>
<td>24.5</td>
<td>29.6</td>
<td>28.0</td>
<td>28.0</td>
<td>21.5</td>
<td>51.6</td>
<td>43.7</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>DCASE2020 UPB SED system 1</td>
<td>Ebbers2020</td>
<td>47.2 (46.5 - 48.1)</td>
<td>28.5</td>
<td>56.4</td>
<td>64.0</td>
<td>24.4</td>
<td>37.4</td>
<td>45.8</td>
<td>51.3</td>
<td>37.6</td>
<td>60.6</td>
<td>67.2</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>Liang2020</td>
<td>36.0 (35.3 - 36.8)</td>
<td>16kHz</td>
<td>time stretching,pitch shifting,reverberation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>Rykaczewski2020</td>
<td>21.6 (21.0 - 22.4)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>Rykaczewski2020</td>
<td>21.9 (21.3 - 22.7)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>Rykaczewski2020</td>
<td>10.4 (9.7 - 11.1)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>Rykaczewski2020</td>
<td>21.6 (20.8 - 22.4)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>HouB2020</td>
<td>34.9 (34.0 - 35.7)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Miyazaki2020</td>
<td>51.1 (50.1 - 52.3)</td>
<td>16kHz</td>
<td>time shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Miyazaki2020</td>
<td>46.4 (45.5 - 47.5)</td>
<td>16kHz</td>
<td>time shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>Miyazaki2020</td>
<td>50.7 (49.6 - 51.9)</td>
<td>16kHz</td>
<td>time shifting, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>Huang2020</td>
<td>44.3 (43.4 - 45.4)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>Huang2020</td>
<td>44.6 (43.5 - 46.0)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>44.1kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>Copiaco2020a</td>
<td>7.8 (7.3 - 8.2)</td>
<td>44.1kHz</td>
<td></td>
<td>scalogram, signal energy, spectral centroid</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>Copiaco2020a</td>
<td>7.5 (7.0 - 8.0)</td>
<td>16kHz</td>
<td></td>
<td>scalogram, signal energy, spectral centroid</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Kim2020</td>
<td>43.7 (42.8 - 44.7)</td>
<td>16kHz</td>
<td>mixup, specaugment, Gaussian noise</td>
<td>mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Kim2020</td>
<td>43.9 (43.0 - 44.7)</td>
<td>16kHz</td>
<td>mixup, specaugment, Gaussian noise</td>
<td>mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Kim2020</td>
<td>44.4 (43.5 - 45.2)</td>
<td>16kHz</td>
<td>mixup, specaugment, Gaussian noise</td>
<td>mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Kim2020</td>
<td>44.2 (43.4 - 45.1)</td>
<td>16kHz</td>
<td>mixup, specaugment, Gaussian noise</td>
<td>mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>16kHz</td>
<td></td>
<td>scalogram, spectral centroid, signal energy</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>JiaKai2020</td>
<td>38.6 (37.7 - 39.7)</td>
<td>16kHz</td>
<td>pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>JiaKai2020</td>
<td>39.3 (38.4 - 40.4)</td>
<td>16kHz</td>
<td>pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>JiaKai2020</td>
<td>41.2 (40.1 - 42.4)</td>
<td>16kHz</td>
<td>pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>JiaKai2020</td>
<td>40.6 (39.6 - 41.6)</td>
<td>16kHz</td>
<td>pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>Hao2020</td>
<td>47.0 (46.0 - 48.1)</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>Hao2020</td>
<td>46.3 (45.5 - 47.4)</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>Hao2020</td>
<td>44.9 (43.9 - 45.8)</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>Hao2020</td>
<td>47.8 (46.9 - 49.0)</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>HouZ2020</td>
<td>45.1 (44.2 - 45.8)</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>deBenito2020</td>
<td>38.2 (37.5 - 39.2)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>de Benito-Gorron2020</td>
<td>37.9 (37.0 - 39.1)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh2020</td>
<td>46.6 (45.8 - 47.6)</td>
<td>16kHz</td>
<td>Gaussian noise, mixup, time shifting, pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Koh2020</td>
<td>45.2 (44.3 - 46.3)</td>
<td>16kHz</td>
<td>Gaussian noise, mixup, time shifting, pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Koh2020</td>
<td>45.2 (44.2 - 46.1)</td>
<td>16kHz</td>
<td>Gaussian noise, mixup, time shifting, pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Koh2020</td>
<td>46.3 (45.4 - 47.2)</td>
<td>16kHz</td>
<td>Gaussian noise, mixup, time shifting, pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>Cornell2020</td>
<td>42.0 (40.9 - 43.1)</td>
<td>16kHz</td>
<td>contrast, overdrive, pitch shifting, highshelf, lowshelf, noise burst, sine burst, roll</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>Cornell2020</td>
<td>44.4 (43.3 - 45.5)</td>
<td>16kHz</td>
<td>contrast, overdrive, pitch shifting, highshelf, lowshelf, noise burst, sine burst, roll</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>Cornell2020</td>
<td>43.2 (42.1 - 44.4)</td>
<td>16kHz</td>
<td>contrast, overdrive, pitch shifting, highshelf, lowshelf, noise burst, sine burst, roll</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>16kHz</td>
<td>contrast, overdrive, pitch shifting, highshelf, lowshelf</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>Cornell2020</td>
<td>42.6 (41.6 - 43.5)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao2020</td>
<td>44.1 (43.1 - 45.2)</td>
<td>16kHz</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td>46.4 (45.3 - 47.6)</td>
<td>16kHz</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao2020</td>
<td>45.7 (44.7 - 47.0)</td>
<td>16kHz</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao2020</td>
<td>46.2 (45.2 - 47.0)</td>
<td>16kHz</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>40.7 (39.7 - 41.7)</td>
<td>16kHz</td>
<td>noise addition, pitch shifting, time rolling, dynamic range compression</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>41.8 (40.7 - 42.9)</td>
<td>16kHz</td>
<td>noise addition, pitch shifting, time rolling, dynamic range compression</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>45.2 (44.2 - 46.5)</td>
<td>16kHz</td>
<td>noise addition, pitch shifting, time rolling, dynamic range compression</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>Liu2020</td>
<td>43.1 (42.1 - 44.2)</td>
<td>16kHz</td>
<td>noise addition, pitch shifting, time rolling, dynamic range compression</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Park2020</td>
<td>35.8 (35.0 - 36.6)</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Park2020</td>
<td>26.5 (25.7 - 27.5)</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>Park2020</td>
<td>36.9 (36.1 - 37.7)</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>Park2020</td>
<td>34.7 (34.1 - 35.6)</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>16kHz</td>
<td>sound event separation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>Chan2020</td>
<td>44.4 (43.5 - 45.5)</td>
<td>22.05kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>Chan2020</td>
<td>46.3 (45.3 - 47.4)</td>
<td>22.05kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>Chan2020</td>
<td>45.8 (45.0 - 47.0)</td>
<td>22.05kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>Chan2020</td>
<td>43.5 (42.6 - 44.7)</td>
<td>22.05kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>Yen2020</td>
<td>42.7 (41.6 - 43.6)</td>
<td>16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>Yen2020</td>
<td>42.6 (41.8 - 43.7)</td>
<td>16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>Yen2020</td>
<td>41.6 (40.6 - 42.7)</td>
<td>16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>Yen2020</td>
<td>43.6 (42.4 - 44.6)</td>
<td>16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Tang2020</td>
<td>43.1 (42.3 - 44.1)</td>
<td>22.05kHz</td>
<td>specaugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Tang2020</td>
<td>44.1 (43.4 - 44.8)</td>
<td>22.05kHz</td>
<td>specaugment, time stretching, pitch shifting, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Tang2020</td>
<td>42.4 (41.4 - 43.4)</td>
<td>22.05kHz</td>
<td>specaugment, time stretching, pitch shifting, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Tang2020</td>
<td>44.1 (43.3 - 45.0)</td>
<td>22.05kHz</td>
<td>specaugment</td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>turpault2020a</td>
<td>34.9 (34.0 - 35.7)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>turpault2020b</td>
<td>36.5 (35.6 - 37.2)</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Ebbers2020</td>
<td>47.2 (46.5 - 48.1)</td>
<td>16kHz</td>
<td>scaling, mixup, frequency warping, blurring, frequency masking, time masking, Gaussian noise</td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-field="machine_learning_semi_supervised" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Semi-supervised approach
            </th>
<th class="text-center narrow-col" data-field="post-processing" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Post-processing
            </th>
<th class="text-center narrow-col" data-field="segmentation_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Segmentation<br/>method
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>Liang2020</td>
<td>36.0 (35.3 - 36.8)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>Rykaczewski2020</td>
<td>21.6 (21.0 - 22.4)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>Rykaczewski2020</td>
<td>21.9 (21.3 - 22.7)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>Rykaczewski2020</td>
<td>10.4 (9.7 - 11.1)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>Rykaczewski2020</td>
<td>21.6 (20.8 - 22.4)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>HouB2020</td>
<td>34.9 (34.0 - 35.7)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Miyazaki2020</td>
<td>51.1 (50.1 - 52.3)</td>
<td>conformer</td>
<td>mean-teacher student</td>
<td>thresholding, median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Miyazaki2020</td>
<td>46.4 (45.5 - 47.5)</td>
<td>transformer</td>
<td>mean-teacher student</td>
<td>thresholding, median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>Miyazaki2020</td>
<td>50.7 (49.6 - 51.9)</td>
<td>transformer, conformer</td>
<td>mean-teacher student</td>
<td>thresholding, median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>Huang2020</td>
<td>44.3 (43.4 - 45.4)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>Huang2020</td>
<td>44.6 (43.5 - 46.0)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>CNN</td>
<td>guided multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>CNN</td>
<td>guided and multi branch learning</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layers, global_max_pooling, global average pooling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>Copiaco2020a</td>
<td>7.8 (7.3 - 8.2)</td>
<td>AlexNet, transfer learning CNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>P-norm</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>Copiaco2020a</td>
<td>7.5 (7.0 - 8.0)</td>
<td>AlexNet, transfer learning CNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>P-norm</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Kim2020</td>
<td>43.7 (42.8 - 44.7)</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td>median filtering (79ms)</td>
<td></td>
<td>thresholding</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Kim2020</td>
<td>43.9 (43.0 - 44.7)</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td>median filtering (79ms)</td>
<td></td>
<td>thresholding</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Kim2020</td>
<td>44.4 (43.5 - 45.2)</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td>median filtering (79ms)</td>
<td></td>
<td>thresholding</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Kim2020</td>
<td>44.2 (43.4 - 45.1)</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td>median filtering (79ms)</td>
<td></td>
<td>thresholding</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>AlexNet, transfer learning CNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>P-norm</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>JiaKai2020</td>
<td>38.6 (37.7 - 39.7)</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>macro F1 vote</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>JiaKai2020</td>
<td>39.3 (38.4 - 40.4)</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>macro F1 vote</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>JiaKai2020</td>
<td>41.2 (40.1 - 42.4)</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>macro F1 vote</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>JiaKai2020</td>
<td>40.6 (39.6 - 41.6)</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>macro F1 vote</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>Hao2020</td>
<td>47.0 (46.0 - 48.1)</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>Hao2020</td>
<td>46.3 (45.5 - 47.4)</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>Hao2020</td>
<td>44.9 (43.9 - 45.8)</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>Hao2020</td>
<td>47.8 (46.9 - 49.0)</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>HouZ2020</td>
<td>45.1 (44.2 - 45.8)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>deBenito2020</td>
<td>38.2 (37.5 - 39.2)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (45ms)</td>
<td></td>
<td>mean, class-specific thresholding</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>de Benito-Gorron2020</td>
<td>37.9 (37.0 - 39.1)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (45ms)</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh2020</td>
<td>46.6 (45.8 - 47.6)</td>
<td>FP-CRNN</td>
<td>mean-teacher student, interpolation consistency training, shift consistency training, weakly pseudo-labeling</td>
<td>median filtering (0.45s)</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Koh2020</td>
<td>45.2 (44.3 - 46.3)</td>
<td>CRNN</td>
<td>mean-teacher student, interpolation consistency training, shift consistency training, weakly pseudo-labeling</td>
<td>median filtering (adaptive window size)</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Koh2020</td>
<td>45.2 (44.2 - 46.1)</td>
<td>CRNN</td>
<td>mean-teacher student, interpolation consistency training, shift consistency training, weakly pseudo-labeling</td>
<td>median filtering (0.45s)</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Koh2020</td>
<td>46.3 (45.4 - 47.2)</td>
<td>FP-CRNN</td>
<td>mean-teacher student, interpolation consistency training, shift consistency training, weakly pseudo-labeling</td>
<td>median filtering (adaptive window size)</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>Cornell2020</td>
<td>42.0 (40.9 - 43.1)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>Cornell2020</td>
<td>44.4 (43.3 - 45.5)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>Cornell2020</td>
<td>43.2 (42.1 - 44.4)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>Cornell2020</td>
<td>42.6 (41.6 - 43.5)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>HMM smoothing</td>
<td></td>
<td>HMM smoothing</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao2020</td>
<td>44.1 (43.1 - 45.2)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>CRNN</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td>46.4 (45.3 - 47.6)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>CRNN</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao2020</td>
<td>45.7 (44.7 - 47.0)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>CRNN</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao2020</td>
<td>46.2 (45.2 - 47.0)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td>attention layers</td>
<td>CRNN</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>40.7 (39.7 - 41.7)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>41.8 (40.7 - 42.9)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>45.2 (44.2 - 46.5)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>Liu2020</td>
<td>43.1 (42.1 - 44.2)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Park2020</td>
<td>35.8 (35.0 - 36.6)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Park2020</td>
<td>26.5 (25.7 - 27.5)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>Park2020</td>
<td>36.9 (36.1 - 37.7)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>Park2020</td>
<td>34.7 (34.1 - 35.6)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td>weighted mean</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>P-norm</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>Chan2020</td>
<td>44.4 (43.5 - 45.5)</td>
<td>NMF, CNN</td>
<td>consistency enforcement</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>Chan2020</td>
<td>46.3 (45.3 - 47.4)</td>
<td>NMF, CNN</td>
<td>consistency enforcement</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>Chan2020</td>
<td>45.8 (45.0 - 47.0)</td>
<td>NMF, CNN</td>
<td>consistency enforcement</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>Chan2020</td>
<td>43.5 (42.6 - 44.7)</td>
<td>NMF, CNN</td>
<td>consistency enforcement</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>Yen2020</td>
<td>42.7 (41.6 - 43.6)</td>
<td>CRNN</td>
<td>guided learning pseudo-labelling, mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>Yen2020</td>
<td>42.6 (41.8 - 43.7)</td>
<td>CRNN</td>
<td>guided learning pseudo-labelling, mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>Yen2020</td>
<td>41.6 (40.6 - 42.7)</td>
<td>CRNN</td>
<td>guided learning pseudo-labelling, mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>Yen2020</td>
<td>43.6 (42.4 - 44.6)</td>
<td>CRNN</td>
<td>guided learning pseudo-labelling, mean-teacher student</td>
<td>median filtering (with adaptive window size)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Tang2020</td>
<td>43.1 (42.3 - 44.1)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Tang2020</td>
<td>44.1 (43.4 - 44.8)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Tang2020</td>
<td>42.4 (41.4 - 43.4)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Tang2020</td>
<td>44.1 (43.3 - 45.0)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>turpault2020a</td>
<td>34.9 (34.0 - 35.7)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>turpault2020b</td>
<td>36.5 (35.6 - 37.2)</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>P-norm</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Ebbers2020</td>
<td>47.2 (46.5 - 48.1)</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td>median filtering (200-400ms)</td>
<td>small segment tagging + pseudo-labelled training</td>
<td>mean</td>
</tr>
</tbody>
</table>
<h2 id="complexity">Complexity</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="system_complexity" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_complexity_time" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Training time
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SED_1</td>
<td>Liang2020</td>
<td>36.0 (35.3 - 36.8)</td>
<td>1112420</td>
<td></td>
<td>3h (1 v100)</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_3</td>
<td>Rykaczewski2020</td>
<td>21.6 (21.0 - 22.4)</td>
<td>165460</td>
<td></td>
<td>8h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_2</td>
<td>Rykaczewski2020</td>
<td>21.9 (21.3 - 22.7)</td>
<td>165460</td>
<td></td>
<td>8h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_4</td>
<td>Rykaczewski2020</td>
<td>10.4 (9.7 - 11.1)</td>
<td>165460</td>
<td></td>
<td>8h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Rykaczewski_Samsung_taks4_SED_1</td>
<td>Rykaczewski2020</td>
<td>21.6 (20.8 - 22.4)</td>
<td>165460</td>
<td></td>
<td>8h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SED_1</td>
<td>HouB2020</td>
<td>34.9 (34.0 - 35.7)</td>
<td>1112420</td>
<td></td>
<td>12h (1  k80)</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_1</td>
<td>Miyazaki2020</td>
<td>51.1 (50.1 - 52.3)</td>
<td>17097712</td>
<td>8</td>
<td>12h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_2</td>
<td>Miyazaki2020</td>
<td>46.4 (45.5 - 47.5)</td>
<td>72107714</td>
<td>7</td>
<td>12h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Miyazaki_NU_task4_SED_3</td>
<td>Miyazaki2020</td>
<td>50.7 (49.6 - 51.9)</td>
<td>89205426</td>
<td>15</td>
<td>12h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_3</td>
<td>Huang2020</td>
<td>44.3 (43.4 - 45.4)</td>
<td>1145928</td>
<td></td>
<td>9h (GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_1</td>
<td>Huang2020</td>
<td>44.6 (43.5 - 46.0)</td>
<td>6878788</td>
<td>6</td>
<td>5h for each model(GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>10316572</td>
<td>9</td>
<td>5h for each model (GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_4</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>6878788</td>
<td>6</td>
<td>5h for each model(GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>10316572</td>
<td>9</td>
<td>5h for each model (GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SED_2</td>
<td>Huang2020</td>
<td>44.3 (43.2 - 45.6)</td>
<td>6878788</td>
<td>6</td>
<td>5h for each model(GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>6878788</td>
<td>9</td>
<td>5h for each model (GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>10316572</td>
<td>9</td>
<td>5h for each model (GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_2</td>
<td>Copiaco2020a</td>
<td>7.8 (7.3 - 8.2)</td>
<td>60000000</td>
<td></td>
<td>60h ((R)(TM) i7-5600U CPU @ 2.60GHz)</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SED_1</td>
<td>Copiaco2020a</td>
<td>7.5 (7.0 - 8.0)</td>
<td>60000000</td>
<td></td>
<td>60h ((R)(TM) i7-5600U CPU @ 2.60GHz)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_1</td>
<td>Kim2020</td>
<td>43.7 (42.8 - 44.7)</td>
<td>13049904</td>
<td>5</td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_2</td>
<td>Kim2020</td>
<td>43.9 (43.0 - 44.7)</td>
<td>13049904</td>
<td>5</td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_4</td>
<td>Kim2020</td>
<td>44.4 (43.5 - 45.2)</td>
<td>13049904</td>
<td>5</td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_AiTeR_GIST_SED_3</td>
<td>Kim2020</td>
<td>44.2 (43.4 - 45.1)</td>
<td>13049904</td>
<td>5</td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>6000000</td>
<td></td>
<td>60h ((R)(TM) i7-5600U CPU @ 2.60GHz)</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_3</td>
<td>JiaKai2020</td>
<td>38.6 (37.7 - 39.7)</td>
<td>1042132</td>
<td>5</td>
<td>2.5h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_1</td>
<td>JiaKai2020</td>
<td>39.3 (38.4 - 40.4)</td>
<td>1042132</td>
<td>7</td>
<td>2.5h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_2</td>
<td>JiaKai2020</td>
<td>41.2 (40.1 - 42.4)</td>
<td>1042132</td>
<td>10</td>
<td>2.5h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_SED_4</td>
<td>JiaKai2020</td>
<td>40.6 (39.6 - 41.6)</td>
<td>1042132</td>
<td>5</td>
<td>2.5h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_2</td>
<td>Hao2020</td>
<td>47.0 (46.0 - 48.1)</td>
<td>1914974</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_3</td>
<td>Hao2020</td>
<td>46.3 (45.5 - 47.4)</td>
<td>2178270</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_1</td>
<td>Hao2020</td>
<td>44.9 (43.9 - 45.8)</td>
<td>1916264</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hao_CQU_task4_SED_4</td>
<td>Hao2020</td>
<td>47.8 (46.9 - 49.0)</td>
<td>2134091</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhenwei_Hou_task4_SED_1</td>
<td>HouZ2020</td>
<td>45.1 (44.2 - 45.8)</td>
<td>1112420</td>
<td>6</td>
<td>9h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>deBenito2020</td>
<td>38.2 (37.5 - 39.2)</td>
<td>5562100</td>
<td>5</td>
<td>10h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_SED_1</td>
<td>de Benito-Gorron2020</td>
<td>37.9 (37.0 - 39.1)</td>
<td>5562100</td>
<td>5</td>
<td>10h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_3</td>
<td>Koh2020</td>
<td>46.6 (45.8 - 47.6)</td>
<td>12807540</td>
<td>5</td>
<td>19h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_2</td>
<td>Koh2020</td>
<td>45.2 (44.3 - 46.3)</td>
<td>3337260</td>
<td>3</td>
<td>14h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_1</td>
<td>Koh2020</td>
<td>45.2 (44.2 - 46.1)</td>
<td>3337260</td>
<td>3</td>
<td>14h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Koh_NTHU_task4_SED_4</td>
<td>Koh2020</td>
<td>46.3 (45.4 - 47.2)</td>
<td>12807540</td>
<td>5</td>
<td>19h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_2</td>
<td>Cornell2020</td>
<td>42.0 (40.9 - 43.1)</td>
<td>1112420</td>
<td></td>
<td>48h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_1</td>
<td>Cornell2020</td>
<td>44.4 (43.3 - 45.5)</td>
<td>3337260</td>
<td>3</td>
<td>48h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_4</td>
<td>Cornell2020</td>
<td>43.2 (42.1 - 44.4)</td>
<td>3337260</td>
<td>3</td>
<td>48h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>1112420</td>
<td></td>
<td>3h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SED_3</td>
<td>Cornell2020</td>
<td>42.6 (41.6 - 43.5)</td>
<td>1113082</td>
<td></td>
<td>6h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_1</td>
<td>Yao2020</td>
<td>44.1 (43.1 - 45.2)</td>
<td>2051582</td>
<td>4</td>
<td>17h (1 GTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_3</td>
<td>Yao2020</td>
<td>46.4 (45.3 - 47.6)</td>
<td>6726008</td>
<td>6</td>
<td>21h (1 GTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_2</td>
<td>Yao2020</td>
<td>45.7 (44.7 - 47.0)</td>
<td>6726008</td>
<td>6</td>
<td>21h (1 GTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Yao_UESTC_task4_SED_4</td>
<td>Yao2020</td>
<td>46.2 (45.2 - 47.0)</td>
<td>12710274</td>
<td>4</td>
<td>24h (1 GTX 2070)</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>40.7 (39.7 - 41.7)</td>
<td>1112421</td>
<td>1</td>
<td>48h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>41.8 (40.7 - 42.9)</td>
<td>1112421</td>
<td>6</td>
<td>48h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_1</td>
<td>Liu2020</td>
<td>45.2 (44.2 - 46.5)</td>
<td>1112421</td>
<td>6</td>
<td>48h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_thinkit_task4_SED_4</td>
<td>Liu2020</td>
<td>43.1 (42.1 - 44.2)</td>
<td>1112421</td>
<td>6</td>
<td>48h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Park2020</td>
<td>35.8 (35.0 - 36.6)</td>
<td>1112420</td>
<td>5</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_1</td>
<td>Park2020</td>
<td>26.5 (25.7 - 27.5)</td>
<td>1112420</td>
<td>5</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_2</td>
<td>Park2020</td>
<td>36.9 (36.1 - 37.7)</td>
<td>1112420</td>
<td>5</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>PARK_JHU_task4_SED_3</td>
<td>Park2020</td>
<td>34.7 (34.1 - 35.6)</td>
<td>1112420</td>
<td>5</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>1112420</td>
<td>3</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_2</td>
<td>Chan2020</td>
<td>44.4 (43.5 - 45.5)</td>
<td>5038932</td>
<td></td>
<td>11h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_4</td>
<td>Chan2020</td>
<td>46.3 (45.3 - 47.4)</td>
<td>10077864</td>
<td>2</td>
<td>17h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_3</td>
<td>Chan2020</td>
<td>45.8 (45.0 - 47.0)</td>
<td>10077864</td>
<td>2</td>
<td>17h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_SED_1</td>
<td>Chan2020</td>
<td>43.5 (42.6 - 44.7)</td>
<td>5038932</td>
<td></td>
<td>6h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_4</td>
<td>Yen2020</td>
<td>42.7 (41.6 - 43.6)</td>
<td>1403370</td>
<td></td>
<td>5h (1 GTX TITAN)</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_2</td>
<td>Yen2020</td>
<td>42.6 (41.8 - 43.7)</td>
<td>1403370</td>
<td></td>
<td>5h (1 GTX TITAN)</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_3</td>
<td>Yen2020</td>
<td>41.6 (40.6 - 42.7)</td>
<td>1403370</td>
<td></td>
<td>5h (1 GTX TITAN)</td>
</tr>
<tr>
<td></td>
<td>YenKu_NTU_task4_SED_1</td>
<td>Yen2020</td>
<td>43.6 (42.4 - 44.6)</td>
<td>1403370</td>
<td></td>
<td>5h (1 GTX TITAN)</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_1</td>
<td>Tang2020</td>
<td>43.1 (42.3 - 44.1)</td>
<td>1687466</td>
<td></td>
<td>5h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_4</td>
<td>Tang2020</td>
<td>44.1 (43.4 - 44.8)</td>
<td>3186890</td>
<td></td>
<td>17h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_2</td>
<td>Tang2020</td>
<td>42.4 (41.4 - 43.4)</td>
<td>1687466</td>
<td></td>
<td>6h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Tang_SCU_task4_SED_3</td>
<td>Tang2020</td>
<td>44.1 (43.3 - 45.0)</td>
<td>3186890</td>
<td></td>
<td>15h (1 TITAN Xp)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SED_baseline_system</td>
<td>turpault2020a</td>
<td>34.9 (34.0 - 35.7)</td>
<td>1112420</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td>turpault2020b</td>
<td>36.5 (35.6 - 37.2)</td>
<td>1112420</td>
<td>3</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_SED_1</td>
<td>Ebbers2020</td>
<td>47.2 (46.5 - 48.1)</td>
<td>20319432</td>
<td>8</td>
<td>24h (4 RTX 2080)</td>
</tr>
</tbody>
</table>
<h2 id="combined-ss-and-sed">Combined SS and SED</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="ss_system_total_parameters" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="text-center narrow-col" data-field="ss_system_network_architecture" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound separation<br/>method
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="ss_system_total_parameters" data-sortable="true" data-value-type="numeric-unit">
                SS Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-field="ss_system_trainining_time" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                SS Training time
            </th>
<th class="text-center narrow-col" data-field="sed_ss_separated_sources_used" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sources<br/>used for SED
            </th>
<th class="text-center narrow-col" data-field="sed_ss_integration_type" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                SS/SED Integration<br/>type
            </th>
<th class="text-center narrow-col" data-field="sed_ss_integration_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                SS/SED Integration<br/>method
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Copiaco_UOW_task4_SS_SED_1</td>
<td>Copiaco2020b</td>
<td>6.9 (6.7 - 7.2)</td>
<td>DNN</td>
<td>500000</td>
<td>3h ((R)(TM) i7-5600U CPU @ 2.60GHz)</td>
<td>DESED foreground</td>
<td>early</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Chen_NTHU_task4_SS_SED_1</td>
<td>Chen2020</td>
<td>34.5 (33.5 - 35.3)</td>
<td>ConvTasNet</td>
<td>1112420</td>
<td>3h (1 GTX 1080 Ti)</td>
<td>DESED foreground, FUSS</td>
<td>both</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Cornell_UPM-INRIA_task4_SS_SED_1</td>
<td>Cornell2020</td>
<td>38.6 (37.5 - 39.6)</td>
<td>ConvTasNet (X= 5, R= 3)</td>
<td>3203999</td>
<td>12h (1 GTX 1080 Ti)</td>
<td>all sources</td>
<td>late</td>
<td>concat</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_4</td>
<td>Huang2020</td>
<td>44.1 (42.9 - 45.4)</td>
<td>TDCN++</td>
<td>1112420</td>
<td>3h (1 GTX 1080 Ti)</td>
<td>all sources</td>
<td>late</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_3</td>
<td>Huang2020</td>
<td>44.4 (43.2 - 45.8)</td>
<td>TDCN++</td>
<td>1112420</td>
<td>3h (1 GTX 1080 Ti)</td>
<td>all sources</td>
<td>late</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_2</td>
<td>Huang2020</td>
<td>44.5 (43.3 - 46.0)</td>
<td>TDCN++</td>
<td>1112420</td>
<td>3h (1 GTX 1080 Ti)</td>
<td>all sources</td>
<td>late</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Huang_ICT-TOSHIBA_task4_SS_SED_1</td>
<td>Huang2020</td>
<td>44.7 (43.6 - 46.2)</td>
<td>TDCN++</td>
<td>1112420</td>
<td>3h (1 GTX 1080 Ti)</td>
<td>all sources</td>
<td>late</td>
<td>average</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_SED_baseline_system</td>
<td></td>
<td>36.5 (35.6 - 37.2)</td>
<td>TDCN++</td>
<td>9179401</td>
<td></td>
<td>DESED sources</td>
<td>late</td>
<td>P-norm</td>
</tr>
</tbody>
</table>
<h2 id="ss-systems">SS systems</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="false" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="Single_source_SI_SNR" data-scatter-y="total_parameters" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="Single_source_SI_SNR" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="Single_source_SI_SNR" data-sortable="true" data-value-type="float1">
                Single source<br/>SI-SNR<br/>(Eval)
            </th>
<th class="text-center narrow-col" data-field="network_architecture" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sound separation<br/>method
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="total_parameters" data-sortable="true" data-value-type="numeric-unit">
                SS Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-field="basis" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Input<br/>features
            </th>
<th class="text-center narrow-col" data-field="data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data<br/>augmentation
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiaomi_task4_SS_1</td>
<td>Liang2020</td>
<td>33.8</td>
<td>TDCN++Â </td>
<td>27538206</td>
<td>stft</td>
<td>time shifting, pitch shifting</td>
</tr>
<tr>
<td></td>
<td>Xiaomi_task4_SS_2</td>
<td>Liang2020</td>
<td>31.8</td>
<td>TDCN++Â  Â </td>
<td>27636510</td>
<td>stft, mfcc</td>
<td>time shifting, pitch shifting</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020_SS_baseline_system</td>
<td>kavalerov2019universal</td>
<td>37.6</td>
<td>TDCN++</td>
<td>9179401</td>
<td>stft (32ms / 8ms)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_IPS_task4_SS_1</td>
<td>HouB2020</td>
<td>37.6</td>
<td>TDCN++</td>
<td></td>
<td>stft (32ms / 8ms)</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2020/technical_reports_task4.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Chan2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Chan2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised NMF-CNN For Sound Event Detection
       </h4>
<p style="text-align:left">
        Chan, Teck Kai<sup>1,2</sup> and Chin, Cheng Siong<sup>1</sup> and Li, Ye<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Newcastle University Singapore, Faculty of Science, Agriculture, and Engineering, Singapore, <sup>2</sup>Xylem Water Solution Singapore Pte Ltd, 3A International Business Park,Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">CTK_NU_task4_SED_1</span> <span class="label label-primary">CTK_NU_task4_SED_2</span> <span class="label label-primary">CTK_NU_task4_SED_3</span> <span class="label label-primary">CTK_NU_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chan2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chan2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chan2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Chan_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chan2020" class="panel-collapse collapse" id="collapse-Chan2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised NMF-CNN For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Chan, Teck Kai<sup>1,2</sup> and Chin, Cheng Siong<sup>1</sup> and Li, Ye<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Newcastle University Singapore, Faculty of Science, Agriculture, and Engineering, Singapore, <sup>2</sup>Xylem Water Solution Singapore Pte Ltd, 3A International Business Park,Singapore
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       For the DCASE 2020 Challenge Task 4, this paper proposed a combinative approach using Nonnegative Matrix Factorization (NMF) and Convolutional Neural Network (CNN). The main idea begins with utilizing NMF to approximate strong labels for the weakly labeled data. Sub-sequently, based on the approximated strongly labeled data, two different CNNs are trained using a semi-supervised framework where one CNN is used for clip-level prediction and the other for frame-level prediction. Using this idea, the best model trained can achieve an event-based F1-score of 45.7% on the validation dataset. Using an ensemble of models, the event-based F1-score can be increased to 48.6%. By comparing with the baseline model, the proposed model outperforms the baseline model by a margin of over 8%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         NMF, CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chan2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Chan_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chan2020label" class="modal fade" id="bibtex-Chan2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChan2020label">
        Semi-Supervised NMF-CNN For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chan2020,
    Author = "Chan, Teck Kai and Chin, Cheng Siong and Li, Ye",
    title = "Semi-Supervised NMF-CNN For Sound Event Detection",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "For the DCASE 2020 Challenge Task 4, this paper proposed a combinative approach using Nonnegative Matrix Factorization (NMF) and Convolutional Neural Network (CNN). The main idea begins with utilizing NMF to approximate strong labels for the weakly labeled data. Sub-sequently, based on the approximated strongly labeled data, two different CNNs are trained using a semi-supervised framework where one CNN is used for clip-level prediction and the other for frame-level prediction. Using this idea, the best model trained can achieve an event-based F1-score of 45.7\% on the validation dataset. Using an ensemble of models, the event-based F1-score can be increased to 48.6\%. By comparing with the baseline model, the proposed model outperforms the baseline model by a margin of over 8\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chen2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Chen2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        Chen, You-Siang<sup>1</sup> and Lin, Zi Jie<sup>1</sup> and Li, Shang-En<sup>2</sup> and Koh, Chih-Yuan<sup>1</sup> and Bai, Mingsian R.<sup>1</sup> and Chien, Jen-Tzung<sup>2</sup> and liu, Yi-Wen<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>National Tsing Hua University, Hsinchu, <sup>2</sup>National Chiao Tung University, Hsinchu, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chen_NTHU_task4_SS_SED_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chen2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chen2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Chen_28b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chen2020" class="panel-collapse collapse" id="collapse-Chen2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        Chen, You-Siang<sup>1</sup> and Lin, Zi Jie<sup>1</sup> and Li, Shang-En<sup>2</sup> and Koh, Chih-Yuan<sup>1</sup> and Bai, Mingsian R.<sup>1</sup> and Chien, Jen-Tzung<sup>2</sup> and liu, Yi-Wen<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>National Tsing Hua University, Hsinchu, <sup>2</sup>National Chiao Tung University, Hsinchu, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we propose a hybrid neural network (NN) to handle the tasks of sound event separation (SES) and sound event detection (SED) in Task 4 of DCASE 2020 challenge. The convolutional time-domain audio separation network (Conv-TasNet) is employed to extract the foreground sound events defined in DCASE challenge. By comparing the baseline SED network with various training strategies, we demonstrate that the SES network is capable of enhancing the SED performance effectively in terms of several event-based performance metrics including macro F1 and poly-phonic sound detection score (PSDS).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Sound event separation
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, ConvTasNet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         P-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Chen_28b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chen2020label" class="modal fade" id="bibtex-Chen2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChen2020label">
        Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chen2020,
    Author = "Chen, You-Siang and Lin, Zi Jie and Li, Shang-En and Koh, Chih-Yuan and Bai, Mingsian R. and Chien, Jen-Tzung and liu, Yi-Wen",
    title = "Combined Sound Event Detection And Sound Event Separation Networks For DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this paper, we propose a hybrid neural network (NN) to handle the tasks of sound event separation (SES) and sound event detection (SED) in Task 4 of DCASE 2020 challenge. The convolutional time-domain audio separation network (Conv-TasNet) is employed to extract the foreground sound events defined in DCASE challenge. By comparing the baseline SED network with various training strategies, we demonstrate that the SES network is capable of enhancing the SED performance effectively in terms of several event-based performance metrics including macro F1 and poly-phonic sound detection score (PSDS)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Copiaco2020a" style="box-shadow: none">
<div class="panel-heading" id="heading-Copiaco2020a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection And Classification Using CWT Scalograms And Deep Learning
       </h4>
<p style="text-align:left">
        Copiaco, Abigail<sup>1</sup> and Ritz, Christian<sup>2</sup> and Fasciani, Stefano<sup>3</sup> and Abdulaziz, Nidhal<sup>4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Engineering and Information Sciences Dept., University of Wollongong, Northfields Wollongong, Australia, <sup>2</sup>School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Northfields Wollongong, Australia, <sup>3</sup>, Musicology Dept., University of Oslo, Norway, <sup>4</sup>University of Wollongong in Dubai, Dubai
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Copiaco_UOW_task4_SED_1</span> <span class="label label-primary">Copiaco_UOW_task4_SED_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Copiaco2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Copiaco2020a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Copiaco2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Copacio_68a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Copiaco2020a" class="panel-collapse collapse" id="collapse-Copiaco2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection And Classification Using CWT Scalograms And Deep Learning
      </h4>
<p style="text-align:left">
<small>
        Copiaco, Abigail<sup>1</sup> and Ritz, Christian<sup>2</sup> and Fasciani, Stefano<sup>3</sup> and Abdulaziz, Nidhal<sup>4</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Engineering and Information Sciences Dept., University of Wollongong, Northfields Wollongong, Australia, <sup>2</sup>School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Northfields Wollongong, Australia, <sup>3</sup>, Musicology Dept., University of Oslo, Norway, <sup>4</sup>University of Wollongong in Dubai, Dubai
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes our proposed system for the DCASE 2020 Task 4 challenge. In this work, we examine the combination of signal energy and spectral centroid features with 0.05 s of time windowing for the detection of sound events. Along with this, spectro-temporal features extracted from Fast Fourier Transform (FFT) based wavelet coefficients of the audio files were used for classification. These coefficients are mapped into images called scalograms, which are fed into the layers of AlexNet, a pre-trained Deep Convolutional Neural Network (DCNN), for transfer learning. Through the validation set, this method gathered an average F1-score of 74% amongst the 10 classes of the DESED database for weak labelling. However, this technique is not deemed to be suitable for classification with strong time stamps labelling, gathering an F1-score of a mere 11.21%
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         scalogram, signal energy, spectral centroid
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         AlexNet, transfer learning CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         P-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Copiaco2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Copacio_68a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Copiaco2020alabel" class="modal fade" id="bibtex-Copiaco2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCopiaco2020alabel">
        Sound Event Detection And Classification Using CWT Scalograms And Deep Learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Copiaco2020a,
    Author = "Copiaco, Abigail and Ritz, Christian and Fasciani, Stefano and Abdulaziz, Nidhal",
    title = "Sound Event Detection And Classification Using CWT Scalograms And Deep Learning",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "This report describes our proposed system for the DCASE 2020 Task 4 challenge. In this work, we examine the combination of signal energy and spectral centroid features with 0.05 s of time windowing for the detection of sound events. Along with this, spectro-temporal features extracted from Fast Fourier Transform (FFT) based wavelet coefficients of the audio files were used for classification. These coefficients are mapped into images called scalograms, which are fed into the layers of AlexNet, a pre-trained Deep Convolutional Neural Network (DCNN), for transfer learning. Through the validation set, this method gathered an average F1-score of 74\% amongst the 10 classes of the DESED database for weak labelling. However, this technique is not deemed to be suitable for classification with strong time stamps labelling, gathering an F1-score of a mere 11.21\%"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Copiaco2020b" style="box-shadow: none">
<div class="panel-heading" id="heading-Copiaco2020b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Detecting And Classifying Separated Sound Events Using Wavelet_Based Scalograms And Deep Learning
       </h4>
<p style="text-align:left">
        Copiaco, Abigail<sup>1</sup> and Ritz, Christian<sup>2</sup> and Fasciani, Stefano<sup>3</sup> and Abdulaziz, Nidhal<sup>4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Engineering and Information Sciences Dept., University of Wollongong, Northfields Wollongong, Australia, <sup>2</sup>School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Northfields Wollongong, Australia, <sup>3</sup>, Musicology Dept., University of Oslo, Norway, <sup>4</sup>University of Wollongong in Dubai, Dubai
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Copiaco_UOW_task4_SS_SED_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Copiaco2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Copiaco2020b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Copiaco2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Copacio_68b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Copiaco2020b" class="panel-collapse collapse" id="collapse-Copiaco2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Detecting And Classifying Separated Sound Events Using Wavelet_Based Scalograms And Deep Learning
      </h4>
<p style="text-align:left">
<small>
        Copiaco, Abigail<sup>1</sup> and Ritz, Christian<sup>2</sup> and Fasciani, Stefano<sup>3</sup> and Abdulaziz, Nidhal<sup>4</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Engineering and Information Sciences Dept., University of Wollongong, Northfields Wollongong, Australia, <sup>2</sup>School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Northfields Wollongong, Australia, <sup>3</sup>, Musicology Dept., University of Oslo, Norway, <sup>4</sup>University of Wollongong in Dubai, Dubai
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes our proposed system submitted to the DCASE 2020 Task 4 challenge that includes sound separation and detection. In this work, we examine the use of a Deep Neural Network (DNN) based sound separation system as a preprocessing technique to the sound event classification technique used. For the detection of sound events, a combination of signal energy and spectral centroid features with 0.05-s of time windowing was utilized. Along with this, spectro-temporal features extracted from a Fast Fourier Transform (FFT) based wavelet coefficients of the audio files were used for classification. These coefficients are mapped into images called scalograms, which are fed into the layers of AlexNet, a pre-trained Deep Convolutional Neural Network (DCNN), for transfer learning. Through the validation set, this method gathered an average F1-score of 70% amongst the 10 classes of the DESED database for weak labelling However, this technique is not deemed to be suitable for classification with strong time stamps labelling, gathering an F1-score of a mere 8.73%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         scalogram, signal energy, spectral centroid
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         AlexNet, transfer learning CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         P-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Copiaco2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Copacio_68b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Copiaco2020blabel" class="modal fade" id="bibtex-Copiaco2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCopiaco2020blabel">
        Detecting And Classifying Separated Sound Events Using Wavelet_Based Scalograms And Deep Learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Copiaco2020b,
    Author = "Copiaco, Abigail and Ritz, Christian and Fasciani, Stefano and Abdulaziz, Nidhal",
    title = "Detecting And Classifying Separated Sound Events Using Wavelet\_Based Scalograms And Deep Learning",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "This report describes our proposed system submitted to the DCASE 2020 Task 4 challenge that includes sound separation and detection. In this work, we examine the use of a Deep Neural Network (DNN) based sound separation system as a preprocessing technique to the sound event classification technique used. For the detection of sound events, a combination of signal energy and spectral centroid features with 0.05-s of time windowing was utilized. Along with this, spectro-temporal features extracted from a Fast Fourier Transform (FFT) based wavelet coefficients of the audio files were used for classification. These coefficients are mapped into images called scalograms, which are fed into the layers of AlexNet, a pre-trained Deep Convolutional Neural Network (DCNN), for transfer learning. Through the validation set, this method gathered an average F1-score of 70\% amongst the 10 classes of the DESED database for weak labelling However, this technique is not deemed to be suitable for classification with strong time stamps labelling, gathering an F1-score of a mere 8.73\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cornell2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Cornell2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The UNIVPM-INRIA Systems For The DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        Cornell, Samuele Cornell<sup>1</sup> and Pepe, Giovanni<sup>1</sup> and Principi, Emanuele<sup>1</sup> and Pariente, Manuel<sup>2</sup> and Olvera, Michel<sup>2</sup> and Gabrielli, Leonardo<sup>1</sup> and Squartini, Stefano<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Dept. Information Engineering, UniversitÃ  Politecnica delle Marche, Ancona, Italy, <sup>2</sup>Dept. Information and Communication Sciences and Technologies, INRIA Nancy Grand-Est, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cornell_UPM-INRIA_task4_SED_1</span> <span class="label label-primary">Cornell_UPM-INRIA_task4_SED_2</span> <span class="label label-primary">Cornell_UPM-INRIA_task4_SED_3</span> <span class="label label-primary">Cornell_UPM-INRIA_task4_SED_4</span> <span class="label label-primary">Cornell_UPM-INRIA_task4_SS_SED_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cornell2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cornell2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cornell2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Cornell_130.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Cornell2020').collapse('show');window.location.hash='#Cornell2020';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cornell2020" class="panel-collapse collapse" id="collapse-Cornell2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The UNIVPM-INRIA Systems For The DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        Cornell, Samuele Cornell<sup>1</sup> and Pepe, Giovanni<sup>1</sup> and Principi, Emanuele<sup>1</sup> and Pariente, Manuel<sup>2</sup> and Olvera, Michel<sup>2</sup> and Gabrielli, Leonardo<sup>1</sup> and Squartini, Stefano<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Dept. Information Engineering, UniversitÃ  Politecnica delle Marche, Ancona, Italy, <sup>2</sup>Dept. Information and Communication Sciences and Technologies, INRIA Nancy Grand-Est, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we propose different Sound Event Detection (SED) systems for the 2020 DCASE Task 4 challenge. Given the mismatch between synthetic labelled data and target domain data, we exploit a domain adversarial training to improve the network invariance to different types of background noise. Furthermore, we use dynamic mixing and augmentation of synthetic examples at training time as well as prediction smoothing by using Hidden Markov Models. In one system, we also show that using a learnable dynamic compression, Per-Channel Energy Normalization (PCEN) front-end improves robustness to background noise by making it Gaussian. Finally, an ensemble of models proves beneficial to improve the prediction score. Concerning joint separation and sound event detection we propose a permutation-invariant training scheme to optimize directly the Sound-Event-Detection objective.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         contrast, overdrive, pitch shifting, highshelf, lowshelf
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, mel-energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, DPRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         HMM smoothing
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cornell2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Cornell_130.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/popcornell/UNIVPM-INRIA-DCASE2020" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cornell2020label" class="modal fade" id="bibtex-Cornell2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCornell2020label">
        The UNIVPM-INRIA Systems For The DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cornell2020,
    Author = "Cornell, Samuele Cornell and Pepe, Giovanni and Principi, Emanuele and Pariente, Manuel and Olvera, Michel and Gabrielli, Leonardo and Squartini, Stefano",
    title = "The UNIVPM-INRIA Systems For The DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we propose different Sound Event Detection (SED) systems for the 2020 DCASE Task 4 challenge. Given the mismatch between synthetic labelled data and target domain data, we exploit a domain adversarial training to improve the network invariance to different types of background noise. Furthermore, we use dynamic mixing and augmentation of synthetic examples at training time as well as prediction smoothing by using Hidden Markov Models. In one system, we also show that using a learnable dynamic compression, Per-Channel Energy Normalization (PCEN) front-end improves robustness to background noise by making it Gaussian. Finally, an ensemble of models proves beneficial to improve the prediction score. Concerning joint separation and sound event detection we propose a permutation-invariant training scheme to optimize directly the Sound-Event-Detection objective."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="deBenito2020" style="box-shadow: none">
<div class="panel-heading" id="heading-deBenito2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Resolution Mean Teacher For DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and Toledano, Doroteo T.
       </p>
<p style="text-align:left">
<em>
         Universidad AutÃ³noma de Madrid, Madrid, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">deBenito_AUDIAS_task4_SED_1</span> <span class="label label-primary">deBenito_AUDIAS_task4_SED_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-deBenito2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-deBenito2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-deBenito2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_deBenito_93.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-deBenito2020" class="panel-collapse collapse" id="collapse-deBenito2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Resolution Mean Teacher For DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and Toledano, Doroteo T.
       </small>
<br/>
<small>
<em>
         Universidad AutÃ³noma de Madrid, Madrid, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our participation in DCASE 2020 Task 4: Sound event detection and separation in domestic environments. A multi-resolution feature extraction approach is proposed, aiming to take advantage of the different lengths and spectral characteristics of each target category. The combination of up to five different time-frequency resolutions via model fusion is able to outperform the baseline results. In addition, we propose class-specific thresholds for the F 1 -score metric, further improving the results over the Validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-deBenito2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_deBenito_93.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-deBenito2020label" class="modal fade" id="bibtex-deBenito2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexdeBenito2020label">
        Multi-Resolution Mean Teacher For DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{deBenito2020,
    Author = "de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and Toledano, Doroteo T.",
    title = "Multi-Resolution Mean Teacher For DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we describe our participation in DCASE 2020 Task 4: Sound event detection and separation in domestic environments. A multi-resolution feature extraction approach is proposed, aiming to take advantage of the different lengths and spectral characteristics of each target category. The combination of up to five different time-frequency resolutions via model fusion is able to outperform the baseline results. In addition, we propose class-specific thresholds for the F 1 -score metric, further improving the results over the Validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ebbers2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Ebbers2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolutional Recurrent Neural Networks For Weakly Labeled Semi-Supervised Sound Event Detection In Domestic Environments
       </h4>
<p style="text-align:left">
        Ebbers, Janek and Haeb-Umbach, Reinhold
       </p>
<p style="text-align:left">
<em>
         Paderborn University, Paderborn, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ebbers_UPB_task4_SED_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ebbers2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ebbers2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ebbers2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Ebbers_150.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Ebbers2020').collapse('show');window.location.hash='#Ebbers2020';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ebbers2020" class="panel-collapse collapse" id="collapse-Ebbers2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolutional Recurrent Neural Networks For Weakly Labeled Semi-Supervised Sound Event Detection In Domestic Environments
      </h4>
<p style="text-align:left">
<small>
        Ebbers, Janek and Haeb-Umbach, Reinhold
       </small>
<br/>
<small>
<em>
         Paderborn University, Paderborn, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report we present our system for the Detection and Classification of acoustic scenes and events (DCASE) 2020 Challenge Task 4: Sound event detection in domestic environments. We present a convolutional recurrent neural network (CRNN) with two recurrent neural network (RNN) classifiers sharing the same preprocessing convolutional neural network (CNN). Both recurrent networks perform audio tagging. One is processing the input audio signal in forward direction and the other in backward direction. The networks are trained jointly using weakly labeled data, such that at each time step an active event is tagged by at least one of the networks given that the event is either in the past captured by the forward RNN or in the future captured by the backward RNN. This way the models are encouraged to output tags as soon as possible. After training, the networks can be used for sound event detection by applying them to smaller audio segments, e.g. 200 ms. Further we propose a tag conditioned CNN as a second stage which is supposed to refine sound event detection. Given its receptive field and file tags as input it performs strong label prediction trained using pseudo strong labels provided by the CRNN system. By ensembling four CRNNs and four CNNs we obtain an event-based F-score of 48.3% on the validation set, which is 13.5% higher than the baseline. We are going to release the source code at https://github.com/fgnt/pb_sed.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         scaling, mixup, frequency warping, blurring, frequency masking, time masking, gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ebbers2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Ebbers_150.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/fgnt/pb_sed" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ebbers2020label" class="modal fade" id="bibtex-Ebbers2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEbbers2020label">
        Convolutional Recurrent Neural Networks For Weakly Labeled Semi-Supervised Sound Event Detection In Domestic Environments
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ebbers2020,
    Author = "Ebbers, Janek and Haeb-Umbach, Reinhold",
    title = "Convolutional Recurrent Neural Networks For Weakly Labeled Semi-Supervised Sound Event Detection In Domestic Environments",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this report we present our system for the Detection and Classification of acoustic scenes and events (DCASE) 2020 Challenge Task 4: Sound event detection in domestic environments. We present a convolutional recurrent neural network (CRNN) with two recurrent neural network (RNN) classifiers sharing the same preprocessing convolutional neural network (CNN). Both recurrent networks perform audio tagging. One is processing the input audio signal in forward direction and the other in backward direction. The networks are trained jointly using weakly labeled data, such that at each time step an active event is tagged by at least one of the networks given that the event is either in the past captured by the forward RNN or in the future captured by the backward RNN. This way the models are encouraged to output tags as soon as possible. After training, the networks can be used for sound event detection by applying them to smaller audio segments, e.g. 200 ms. Further we propose a tag conditioned CNN as a second stage which is supposed to refine sound event detection. Given its receptive field and file tags as input it performs strong label prediction trained using pseudo strong labels provided by the CRNN system. By ensembling four CRNNs and four CNNs we obtain an event-based F-score of 48.3\% on the validation set, which is 13.5\% higher than the baseline. We are going to release the source code at https://github.com/fgnt/pb\_sed."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hao2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Hao2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Cross-domain sound event detection: from synthesized audio to real audio
       </h4>
<p style="text-align:left">
        Hao, Junyong and Hou, Zhenwei and Peng, Wang
       </p>
<p style="text-align:left">
<em>
         Chongqing University, Chongqing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hao_CQU_task4_SED_1</span> <span class="label label-primary">Hao_CQU_task4_SED_2</span> <span class="label label-primary">Hao_CQU_task4_SED_3</span> <span class="label label-primary">Hao_CQU_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hao2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hao2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hao2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Hao_26.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hao2020" class="panel-collapse collapse" id="collapse-Hao2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Cross-domain sound event detection: from synthesized audio to real audio
      </h4>
<p style="text-align:left">
<small>
        Hao, Junyong and Hou, Zhenwei and Peng, Wang
       </small>
<br/>
<small>
<em>
         Chongqing University, Chongqing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes some of the system information submitted to DCASE2020 task4 - Sound Event Detection in Domestic Environments. We use the dataset of DCASE2019 task4 to train our model, contains strongly labeled synthetic data, large unlabeled data, and weakly labeled data. There is a very large domain gaps in the statistical distribution between the synthesized audio and the real audio, and the performance of the SED model obtained on the synthesized audio applied to the real audio is greatly reduced. To perform this task, we propose a DA-CRNN network for joint learning of sound event detection (SED) and domain adaptation (DA).We consider the impact of the distribution within a single sound on the generalization perfor- mance of the model by mitigating the impact of complex background noise on event detection and the self-correlation consistency regularization of clip-level sound event classification, these make the intra-domain of a single sound smoother ; for cross-domain adaptation, adversarial learning through feature extraction network with frame-level domain discriminator and clip-level domain discriminator, forcing the feature extraction network to learn the invariant features of the domain, and further improve the generalization performance of the model. We did not use sound source separation, achieved an F1 score of 48.25% on the validation dataset and an F1 score of 49.43% on the public evaluation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hao2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Hao_26.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hao2020label" class="modal fade" id="bibtex-Hao2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHao2020label">
        Cross-domain sound event detection: from synthesized audio to real audio
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hao2020,
    Author = "Hao, Junyong and Hou, Zhenwei and Peng, Wang",
    title = "Cross-domain sound event detection: from synthesized audio to real audio",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "This technical report describes some of the system information submitted to DCASE2020 task4 - Sound Event Detection in Domestic Environments. We use the dataset of DCASE2019 task4 to train our model, contains strongly labeled synthetic data, large unlabeled data, and weakly labeled data. There is a very large domain gaps in the statistical distribution between the synthesized audio and the real audio, and the performance of the SED model obtained on the synthesized audio applied to the real audio is greatly reduced. To perform this task, we propose a DA-CRNN network for joint learning of sound event detection (SED) and domain adaptation (DA).We consider the impact of the distribution within a single sound on the generalization perfor- mance of the model by mitigating the impact of complex background noise on event detection and the self-correlation consistency regularization of clip-level sound event classification, these make the intra-domain of a single sound smoother ; for cross-domain adaptation, adversarial learning through feature extraction network with frame-level domain discriminator and clip-level domain discriminator, forcing the feature extraction network to learn the invariant features of the domain, and further improve the generalization performance of the model. We did not use sound source separation, achieved an F1 score of 48.25\% on the validation dataset and an F1 score of 49.43\% on the public evaluation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="HouB2020" style="box-shadow: none">
<div class="panel-heading" id="heading-HouB2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Fine-Tuning Using Grid Search &amp; Gradient Visualization
       </h4>
<p style="text-align:left">
        Hou, Bowei and Radzikwoski, Kacper and Farid, Ahmed
       </p>
<p style="text-align:left">
<em>
         Waseda University, Fukuoka, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hou_IPS_task4_SED_1</span> <span class="label label-primary">Hou_IPS_task4_SS_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-HouB2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-HouB2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-HouB2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_HouB_95.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-HouB2020" class="panel-collapse collapse" id="collapse-HouB2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Fine-Tuning Using Grid Search &amp; Gradient Visualization
      </h4>
<p style="text-align:left">
<small>
        Hou, Bowei and Radzikwoski, Kacper and Farid, Ahmed
       </small>
<br/>
<small>
<em>
         Waseda University, Fukuoka, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we briefly describe the models used in the task 4 challenge of DCASE2020. We utilized previously available models and fine-tuned them using the grid search algorithm and gradient visualization. This is the first attempt by our team to enter a competition on sound source manipulation.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-HouB2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_HouB_95.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-HouB2020label" class="modal fade" id="bibtex-HouB2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHouB2020label">
        Fine-Tuning Using Grid Search &amp; Gradient Visualization
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{HouB2020,
    Author = "Hou, Bowei and Radzikwoski, Kacper and Farid, Ahmed",
    title = "Fine-Tuning Using Grid Search \&amp; Gradient Visualization",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we briefly describe the models used in the task 4 challenge of DCASE2020. We utilized previously available models and fine-tuned them using the grid search algorithm and gradient visualization. This is the first attempt by our team to enter a competition on sound source manipulation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="HouZ2020" style="box-shadow: none">
<div class="panel-heading" id="heading-HouZ2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUTHOR GUIDELINES FOR DCASE 2020 CHALLENGE TECHNICAL REPORT
       </h4>
<p style="text-align:left">
        Hou, Zhenwei and Hao, Junyong and Peng, Wang
       </p>
<p style="text-align:left">
<em>
         University of Chongqing, Chongqing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhenwei_Hou_task4_SED_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-HouZ2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-HouZ2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-HouZ2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_HouZ_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-HouZ2020" class="panel-collapse collapse" id="collapse-HouZ2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUTHOR GUIDELINES FOR DCASE 2020 CHALLENGE TECHNICAL REPORT
      </h4>
<p style="text-align:left">
<small>
        Hou, Zhenwei and Hao, Junyong and Peng, Wang
       </small>
<br/>
<small>
<em>
         University of Chongqing, Chongqing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present the sound event detection system of task 4 (Sound event detection and separation in do- mestic environments) of the DCASE2020 challenge. We propose an improved CRNN that Context Gating and channel attention mechanism are co-embedded into backbone network. It aims to construct a general and efficient attention structure for extracting features of sound events, and give full play to the advantages of attention mechanism in event feature extraction. In the case of replacing the CRNN in the baseline model with the structure we designed and keeping the other parts unchanged, the macro F-score of our model on the validation set is 4 percentage points higher than the baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-HouZ2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_HouZ_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-HouZ2020label" class="modal fade" id="bibtex-HouZ2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHouZ2020label">
        AUTHOR GUIDELINES FOR DCASE 2020 CHALLENGE TECHNICAL REPORT
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{HouZ2020,
    Author = "Hou, Zhenwei and Hao, Junyong and Peng, Wang",
    title = "AUTHOR GUIDELINES FOR DCASE 2020 CHALLENGE TECHNICAL REPORT",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we present the sound event detection system of task 4 (Sound event detection and separation in do- mestic environments) of the DCASE2020 challenge. We propose an improved CRNN that Context Gating and channel attention mechanism are co-embedded into backbone network. It aims to construct a general and efficient attention structure for extracting features of sound events, and give full play to the advantages of attention mechanism in event feature extraction. In the case of replacing the CRNN in the baseline model with the structure we designed and keeping the other parts unchanged, the macro F-score of our model on the validation set is 4 percentage points higher than the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Guided Multi-Branch Learning Systems For DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        Huang, Yuxin<sup>1,2</sup> and Lin, Liwei<sup>1,2</sup> and Ma, Shuo<sup>1</sup> and Wang, Xiangdong<sup>1</sup> and Liu, Hong<sup>1</sup> and Qian, Yueliang<sup>1</sup> and Liu, Min<sup>3</sup> and Ouch, Kazushige<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Bejing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Huang_ICT-TOSHIBA_task4_SED_1</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SED_2</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SED_3</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SED_4</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SS_SED_1</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SS_SED_2</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SS_SED_3</span> <span class="label label-primary">Huang_ICT-TOSHIBA_task4_SS_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Huang_145.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2020" class="panel-collapse collapse" id="collapse-Huang2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Guided Multi-Branch Learning Systems For DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        Huang, Yuxin<sup>1,2</sup> and Lin, Liwei<sup>1,2</sup> and Ma, Shuo<sup>1</sup> and Wang, Xiangdong<sup>1</sup> and Liu, Hong<sup>1</sup> and Qian, Yueliang<sup>1</sup> and Liu, Min<sup>3</sup> and Ouch, Kazushige<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Bejing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we describe in detail our systems for DCASE2020 task 4. The systems are based on the first-place system of DCASE 2019 task 4, which adopts the multiple instance learning (MIL) framework with embedding-level attention pooling and a semi-supervised learning approach called guided learning. The multi-branch learning method is then incorporated into the system to further improve the performance. Multiple branches with different pooling strategies (embedding-level or instance-level) and different pooling modules (attention pooling, global max pooling or global average pooling) are used and shares the same feature encoder. To better exploit the synthetic data with strong labels, inspired by multi-task learning, a sound event detection (SED) branch is also added. Therefore, multiple branches pursuing different purposes and focusing on different characteristics of the data can help the feature encoder model the feature space better and avoid over-fitting. To combine sound separation with sound event detection, we train models using the output of the baseline system of sound separation and fuse the event detection results of models with of without sound separation.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time shifting, frequency shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Huang_145.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2020label" class="modal fade" id="bibtex-Huang2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2020label">
        Guided Multi-Branch Learning Systems For DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang2020,
    Author = "Huang, Yuxin and Lin, Liwei and Ma, Shuo and Wang, Xiangdong and Liu, Hong and Qian, Yueliang and Liu, Min and Ouch, Kazushige",
    title = "Guided Multi-Branch Learning Systems For DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this paper, we describe in detail our systems for DCASE2020 task 4. The systems are based on the first-place system of DCASE 2019 task 4, which adopts the multiple instance learning (MIL) framework with embedding-level attention pooling and a semi-supervised learning approach called guided learning. The multi-branch learning method is then incorporated into the system to further improve the performance. Multiple branches with different pooling strategies (embedding-level or instance-level) and different pooling modules (attention pooling, global max pooling or global average pooling) are used and shares the same feature encoder. To better exploit the synthetic data with strong labels, inspired by multi-task learning, a sound event detection (SED) branch is also added. Therefore, multiple branches pursuing different purposes and focusing on different characteristics of the data can help the feature encoder model the feature space better and avoid over-fitting. To combine sound separation with sound event detection, we train models using the output of the baseline system of sound separation and fuse the event detection results of models with of without sound separation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="JiaKai2020" style="box-shadow: none">
<div class="panel-heading" id="heading-JiaKai2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        JiaKai, Lu
       </p>
<p style="text-align:left">
<em>
         PFU Shanghai Co., LTD, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">LJK_PSH_task4_SED_1</span> <span class="label label-primary">LJK_PSH_task4_SED_2</span> <span class="label label-primary">LJK_PSH_task4_SED_3</span> <span class="label label-primary">LJK_PSH_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-JiaKai2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-JiaKai2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-JiaKai2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_JiaKai_105.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-JiaKai2020" class="panel-collapse collapse" id="collapse-JiaKai2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        JiaKai, Lu
       </small>
<br/>
<small>
<em>
         PFU Shanghai Co., LTD, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present our neural network for the DCASE 2020 challengeâ€™s Task 4 (Sound event detection and separation in domestic environments). This task evaluates systems for the largescale detection of sound events using weakly labeled data, and explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance to doing audio tagging and sound event detection. We propose a mean-teacher model with convolutional neural network (CNN) and recurrent neural network (RNN) to maximize the use of unlabeled in-domain dataset. The architecture is based on our 2018 competition model.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         macro F1 vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-JiaKai2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_JiaKai_105.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-JiaKai2020label" class="modal fade" id="bibtex-JiaKai2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJiaKai2020label">
        Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{JiaKai2020,
    Author = "JiaKai, Lu",
    title = "Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this paper, we present our neural network for the DCASE 2020 challengeâ€™s Task 4 (Sound event detection and separation in domestic environments). This task evaluates systems for the largescale detection of sound events using weakly labeled data, and explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance to doing audio tagging and sound event detection. We propose a mean-teacher model with convolutional neural network (CNN) and recurrent neural network (RNN) to maximize the use of unlabeled in-domain dataset. The architecture is based on our 2018 competition model."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="kavalerov2019universal" style="box-shadow: none">
<div class="panel-heading" id="heading-kavalerov2019universal" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Universal sound separation
       </h4>
<p style="text-align:left">
        Kavalerov, Ilya<sup>1,2</sup> and Wisdom, Scott<sup>1</sup> and Erdogan, Hakan<sup>1</sup> and Patton, Brian<sup>1</sup> and Wilson, Kevin<sup>1</sup> and Le Roux, Jonathan<sup>3</sup> and Hershey, John R<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Google Research, Cambridge MA, <sup>2</sup>Department of Electrical and Computer Engineering, UMD, <sup>3</sup>Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2020_SS_baseline_system</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-kavalerov2019universal" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-kavalerov2019universal" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-kavalerov2019universal" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://arxiv.org/pdf/1905.03330" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-kavalerov2019universal').collapse('show');window.location.hash='#kavalerov2019universal';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-kavalerov2019universal" class="panel-collapse collapse" id="collapse-kavalerov2019universal" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Universal sound separation
      </h4>
<p style="text-align:left">
<small>
        Kavalerov, Ilya<sup>1,2</sup> and Wisdom, Scott<sup>1</sup> and Erdogan, Hakan<sup>1</sup> and Patton, Brian<sup>1</sup> and Wilson, Kevin<sup>1</sup> and Le Roux, Jonathan<sup>3</sup> and Hershey, John R<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Google Research, Cambridge MA, <sup>2</sup>Department of Electrical and Computer Engineering, UMD, <sup>3</sup>Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-kavalerov2019universal" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://arxiv.org/pdf/1905.03330" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-kavalerov2019universallabel" class="modal fade" id="bibtex-kavalerov2019universal" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexkavalerov2019universallabel">
        Universal sound separation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{kavalerov2019universal,
    Author = "Kavalerov, Ilya and Wisdom, Scott and Erdogan, Hakan and Patton, Brian and Wilson, Kevin and Le Roux, Jonathan and Hershey, John R",
    title = "Universal sound separation",
    institution = "DCASE2020 Challenge",
    year = "2019",
    month = "October",
    booktitle = "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    pages = "175--179",
    abstract = "Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Polyphonic Sound Event Detection Based On Convolutional Recurrent Neural Networks With Semi-Supervised Loss Function For DCASE Challenge 2020 Task 4
       </h4>
<p style="text-align:left">
        Kim, Nam Kyun<sup>1</sup> and Kim, Hong Kook<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Electrical Engineering and Computer Science, Gwangju, South Korea, <sup>2</sup>AI Graduate School (GIST), Gwangju Institute of Science and Technology, Gwangju, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_AiTeR_GIST_SED_1</span> <span class="label label-primary">Kim_AiTeR_GIST_SED_2</span> <span class="label label-primary">Kim_AiTeR_GIST_SED_3</span> <span class="label label-primary">Kim_AiTeR_GIST_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Kim_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2020" class="panel-collapse collapse" id="collapse-Kim2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Polyphonic Sound Event Detection Based On Convolutional Recurrent Neural Networks With Semi-Supervised Loss Function For DCASE Challenge 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        Kim, Nam Kyun<sup>1</sup> and Kim, Hong Kook<sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>School of Electrical Engineering and Computer Science, Gwangju, South Korea, <sup>2</sup>AI Graduate School (GIST), Gwangju Institute of Science and Technology, Gwangju, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report proposes a polyphonic sound event detection (SED) method for the DCASE 2020 Challenge Task 4. The proposed SED method is based on semi-supervised learning to deal with the different combination of training datasets such as weakly labeled dataset, unlabeled dataset, and strongly labeled synthetic dataset. Especially, the target label of each audio clip from weakly labeled or unlabeled dataset is first predicted by using the mean teacher model that is the DCASE 2020 baseline. The data with predicted labels are used for training the proposed SED model, which consists of CNNs with skip connections and self-attention mechanism, followed by RNNs. In order to compensate for the erroneous prediction of weakly labeled and unlabeled data, a semi-supervised loss function is employed for the proposed SED model. In this work, several versions of the proposed SED model are implemented and evaluated on the validation set according to the different parameter setting for the semi-supervised loss function, and then an ensemble model that combines five-fold validation models is finally selected as our final model.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, SpecAugment, Gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel-spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         thresholding
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Kim_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2020label" class="modal fade" id="bibtex-Kim2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2020label">
        Polyphonic Sound Event Detection Based On Convolutional Recurrent Neural Networks With Semi-Supervised Loss Function For DCASE Challenge 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2020,
    Author = "Kim, Nam Kyun and Kim, Hong Kook",
    title = "Polyphonic Sound Event Detection Based On Convolutional Recurrent Neural Networks With Semi-Supervised Loss Function For DCASE Challenge 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "This report proposes a polyphonic sound event detection (SED) method for the DCASE 2020 Challenge Task 4. The proposed SED method is based on semi-supervised learning to deal with the different combination of training datasets such as weakly labeled dataset, unlabeled dataset, and strongly labeled synthetic dataset. Especially, the target label of each audio clip from weakly labeled or unlabeled dataset is first predicted by using the mean teacher model that is the DCASE 2020 baseline. The data with predicted labels are used for training the proposed SED model, which consists of CNNs with skip connections and self-attention mechanism, followed by RNNs. In order to compensate for the erroneous prediction of weakly labeled and unlabeled data, a semi-supervised loss function is employed for the proposed SED model. In this work, several versions of the proposed SED model are implemented and evaluated on the validation set according to the different parameter setting for the semi-supervised loss function, and then an ensemble model that combines five-fold validation models is finally selected as our final model."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koh2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Koh2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection By Consistency Training And Pseudo-Labeling With Feature-Pyramid Convolutional Recurrent Neural Networks
       </h4>
<p style="text-align:left">
        Koh, Chih-Yuan<sup>1</sup> and Chen, You-Siang<sup>1</sup> and Li, Shang-En<sup>2</sup> and Liu, Yi-Wen<sup>1</sup> and Chien, Jen-Tzung<sup>2</sup> and Bai, Mingsian R.<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>National Tsing Hua University, Hsinchu, <sup>2</sup>National Chiao Tung University, Hsinchu, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Koh_NTHU_task4_SED_1</span> <span class="label label-primary">Koh_NTHU_task4_SED_2</span> <span class="label label-primary">Koh_NTHU_task4_SED_3</span> <span class="label label-primary">Koh_NTHU_task4_SED_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Koh2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koh2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koh2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Koh_28a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koh2020" class="panel-collapse collapse" id="collapse-Koh2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection By Consistency Training And Pseudo-Labeling With Feature-Pyramid Convolutional Recurrent Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Koh, Chih-Yuan<sup>1</sup> and Chen, You-Siang<sup>1</sup> and Li, Shang-En<sup>2</sup> and Liu, Yi-Wen<sup>1</sup> and Chien, Jen-Tzung<sup>2</sup> and Bai, Mingsian R.<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>National Tsing Hua University, Hsinchu, <sup>2</sup>National Chiao Tung University, Hsinchu, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       A event detection system for DCASE 2020 task 4 is presented. To efficiently utilize large amount of unlabeled in-domain data, three semi-supervised learning strategies are applied: 1) interpolation consistency training (ICT), 2) shift consistency training (SCT), 3) weakly pseudo-labeling. In addition, we propose FP-CRNN, a convolutional recurrent neural network which contains feature-pyramid components and is based on the provided baseline. In terms of event-based F-measure, these approaches outperform the baseline, at 34.8%, by a large margin, with an F-measure of 48.4% for the baseline network which is trained with the combination of all three strategies and 49.6% for FP-CRNN with the same training strategies.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Gaussian noise, Mixup, time shifting, pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probabilities, thresholding
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koh2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Koh_28a.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koh2020label" class="modal fade" id="bibtex-Koh2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoh2020label">
        Sound Event Detection By Consistency Training And Pseudo-Labeling With Feature-Pyramid Convolutional Recurrent Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Koh2020,
    Author = "Koh, Chih-Yuan and Chen, You-Siang and Li, Shang-En and Liu, Yi-Wen and Chien, Jen-Tzung and Bai, Mingsian R.",
    title = "Sound Event Detection By Consistency Training And Pseudo-Labeling With Feature-Pyramid Convolutional Recurrent Neural Networks",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "A event detection system for DCASE 2020 task 4 is presented. To efficiently utilize large amount of unlabeled in-domain data, three semi-supervised learning strategies are applied: 1) interpolation consistency training (ICT), 2) shift consistency training (SCT), 3) weakly pseudo-labeling. In addition, we propose FP-CRNN, a convolutional recurrent neural network which contains feature-pyramid components and is based on the provided baseline. In terms of event-based F-measure, these approaches outperform the baseline, at 34.8\%, by a large margin, with an F-measure of 48.4\% for the baseline network which is trained with the combination of all three strategies and 49.6\% for FP-CRNN with the same training strategies."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liang2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Liang2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Mean Teacher With Sound Source Separation And Data Augmentation For DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        Liang, Chuming and Ying, Haorong and Chen, Yueyi and Wang, Zhao
       </p>
<p style="text-align:left">
<em>
         Xiaomi AI Lab, Wuhan, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xiaomi_task4_SED_1</span> <span class="label label-primary">HaorongYing_task4_SS_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liang2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liang2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Liang_30.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Liang2020').collapse('show');window.location.hash='#Liang2020';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liang2020" class="panel-collapse collapse" id="collapse-Liang2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Mean Teacher With Sound Source Separation And Data Augmentation For DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        Liang, Chuming and Ying, Haorong and Chen, Yueyi and Wang, Zhao
       </small>
<br/>
<small>
<em>
         Xiaomi AI Lab, Wuhan, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present our system for the DCASE 2020 challenge Task4(Sound event detection and separation in domestic environments). The target of this task is to provide time boundaries of multiple events in an audio recording using a system trained with unlabeled, weakly-labeled and synthetic data. Also, sound source separation is encouraged to use in the system. We propose a mean-teacher model with convolutional and recurrent neural network(CRNN) structure and adopt data augmentation and sound source separation technique to improve the performance of sound event detection.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching,pitch shift,reverberation
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Liang_30.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/JustKowalski/sound_separation" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liang2020label" class="modal fade" id="bibtex-Liang2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiang2020label">
        Mean Teacher With Sound Source Separation And Data Augmentation For DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liang2020,
    Author = "Liang, Chuming and Ying, Haorong and Chen, Yueyi and Wang, Zhao",
    title = "Mean Teacher With Sound Source Separation And Data Augmentation For DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this paper, we present our system for the DCASE 2020 challenge Task4(Sound event detection and separation in domestic environments). The target of this task is to provide time boundaries of multiple events in an audio recording using a system trained with unlabeled, weakly-labeled and synthetic data. Also, sound source separation is encouraged to use in the system. We propose a mean-teacher model with convolutional and recurrent neural network(CRNN) structure and adopt data augmentation and sound source separation technique to improve the performance of sound event detection."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised Sound Event Detection Based On Mean Teacher With Power Pooling And Data Augmentation
       </h4>
<p style="text-align:left">
        Liu, Yuzhuo<sup>1,2</sup> and Chen, Chengxin<sup>1,2</sup> and Kuang, Jianzhong<sup>1,2</sup> and Zhang, Pengyuan<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute of Acoustics, Key Laboratory of Speech Acoustics &amp; Content Understanding, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_thinkit_task4_SED_1</span> <span class="label label-primary">Liu_thinkit_task4_SED_2</span> <span class="label label-primary">Liu_thinkit_task4_SED_3</span> <span class="label label-primary">Liu_thinkit_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Liu_124.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2020" class="panel-collapse collapse" id="collapse-Liu2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised Sound Event Detection Based On Mean Teacher With Power Pooling And Data Augmentation
      </h4>
<p style="text-align:left">
<small>
        Liu, Yuzhuo<sup>1,2</sup> and Chen, Chengxin<sup>1,2</sup> and Kuang, Jianzhong<sup>1,2</sup> and Zhang, Pengyuan<sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Institute of Acoustics, Key Laboratory of Speech Acoustics &amp; Content Understanding, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe the details of the system submitted to DCASE2020 task4: sound event detection (SED) and separation in domestic environments. We mainly focus on the scenario that recognizes sound events without source separation. The training set includes synthetic strongly labeled data, weakly labeled data and unlabeled data. For training SED models with weak labeling, a power pooling function is introduced to generate clip-level predictions from frame-level ones. Additionally, three traditional data augmentation approaches are applied on all data. We also ensemble models with different strategies. Our best system finally achieves an F1 of 49.55% on the validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         noise addition, pitch shifting, time rolling, dynamic range compression
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted averaging
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Liu_124.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2020label" class="modal fade" id="bibtex-Liu2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2020label">
        Semi-Supervised Sound Event Detection Based On Mean Teacher With Power Pooling And Data Augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2020,
    Author = "Liu, Yuzhuo and Chen, Chengxin and Kuang, Jianzhong and Zhang, Pengyuan",
    title = "Semi-Supervised Sound Event Detection Based On Mean Teacher With Power Pooling And Data Augmentation",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we describe the details of the system submitted to DCASE2020 task4: sound event detection (SED) and separation in domestic environments. We mainly focus on the scenario that recognizes sound events without source separation. The training set includes synthetic strongly labeled data, weakly labeled data and unlabeled data. For training SED models with weak labeling, a power pooling function is introduced to generate clip-level predictions from frame-level ones. Additionally, three traditional data augmentation approaches are applied on all data. We also ensemble models with different strategies. Our best system finally achieves an F1 of 49.55\% on the validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Miyazaki2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Miyazaki2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolution-Augmented Transformer For Semi-Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Miyazaki, Koichi<sup>1<sup> and Komatsu, Tatsuya<sup>2<sup> and Hayashi, Tomoki<sup>1,3<sup> and Watanabe, Shinji<sup>4<sup> and Toda, Tomoki<sup>1<sup> and Takeda, Kazuya<sup>1<sup>
</sup></sup></sup></sup></sup></sup></sup></sup></sup></sup></sup></sup></p>
<p style="text-align:left">
<em>
<sup>1<sup>Nagoya University, Japan, <sup>2<sup>LINE Corporation, Japan, <sup>3<sup>Human Dataware Lab. Co., Japan, <sup>4<sup> Johns Hopkins University, USA
        </sup></sup></sup></sup></sup></sup></sup></sup></em>
</p>
<p style="text-align:left">
<span class="label label-primary">Miyazaki_NU_task4_SED_1</span> <span class="label label-primary">Miyazaki_NU_task4_SED_2</span> <span class="label label-primary">Miyazaki_NU_task4_SED_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Miyazaki2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Miyazaki2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Miyazaki2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Miyazaki_108.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Miyazaki2020" class="panel-collapse collapse" id="collapse-Miyazaki2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolution-Augmented Transformer For Semi-Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Miyazaki, Koichi<sup>1<sup> and Komatsu, Tatsuya<sup>2<sup> and Hayashi, Tomoki<sup>1,3<sup> and Watanabe, Shinji<sup>4<sup> and Toda, Tomoki<sup>1<sup> and Takeda, Kazuya<sup>1<sup>
</sup></sup></sup></sup></sup></sup></sup></sup></sup></sup></sup></sup></small>
<br/>
<small>
<em>
<sup>1<sup>Nagoya University, Japan, <sup>2<sup>LINE Corporation, Japan, <sup>3<sup>Human Dataware Lab. Co., Japan, <sup>4<sup> Johns Hopkins University, USA
        </sup></sup></sup></sup></sup></sup></sup></sup></em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE2020 Task4: sound event detection and separation in domestic environments. Our model employs conformer blocks, which combine the self-attention and depth-wise convolution networks, to efficiently capture the global and local context information of an audio feature sequence. In addition to this novel architecture, we further improve the performance by utilizing a mean teacher semi-supervised learning technique, data augmentation, and post-processing optimized for each sound event class. We demonstrate that the proposed method achieves the event-based macro F1 score of 50.7% on the validation set, significantly outperforming that of the baseline score (34.8%).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time shifting, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Conformer, Transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Miyazaki2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Miyazaki_108.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Miyazaki2020label" class="modal fade" id="bibtex-Miyazaki2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMiyazaki2020label">
        Convolution-Augmented Transformer For Semi-Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Miyazaki2020,
    Author = "Miyazaki, Koichi and Komatsu, Tatsuya and Hayashi, Tomoki and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya",
    title = "Convolution-Augmented Transformer For Semi-Supervised Sound Event Detection",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE2020 Task4: sound event detection and separation in domestic environments. Our model employs conformer blocks, which combine the self-attention and depth-wise convolution networks, to efficiently capture the global and local context information of an audio feature sequence. In addition to this novel architecture, we further improve the performance by utilizing a mean teacher semi-supervised learning technique, data augmentation, and post-processing optimized for each sound event class. We demonstrate that the proposed method achieves the event-based macro F1 score of 50.7\% on the validation set, significantly outperforming that of the baseline score (34.8\%)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Park2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Joint Acoustic And Supervised Inference For Sound Event Detection
       </h4>
<p style="text-align:left">
        Park, Sangwook and Bellur, Ashwin and Kothinti, Sandeep and Kapurchali, Masoumeh Heidari and Elhilali, Mounya
       </p>
<p style="text-align:left">
<em>
         Johns Hopkins University, Baltimore, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">PARK_JHU_task4_SED_1</span> <span class="label label-primary">PARK_JHU_task4_SED_2</span> <span class="label label-primary">PARK_JHU_task4_SED_3</span> <span class="label label-primary">PARK_JHU_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Park2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Park_118.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2020" class="panel-collapse collapse" id="collapse-Park2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Joint Acoustic And Supervised Inference For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Park, Sangwook and Bellur, Ashwin and Kothinti, Sandeep and Kapurchali, Masoumeh Heidari and Elhilali, Mounya
       </small>
<br/>
<small>
<em>
         Johns Hopkins University, Baltimore, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This is a technical report about a sound event detection system for the task 4 of DCASE2020. The purpose of a sound event detection is to find event class label as well as its time boundaries. To achieve this purpose, we considered several methods such signal enhancement and event boundary detection, and built five systems by integrating these methods with supervised system trained by using Mean Teacher model. In particular, we estimate event boundaries of weakly labeled data by performing a event boundary detection. Then, we used the estimated strong label in training the supervised system. In addition, we adopt a fusion method by calculating weighted averaging posterior over the five outputs from each individual system. In experiments with validation set, we found that a final result of our system shows an improvement about 11 % in class averaging f-score compared to a baseline performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted averaging
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Park_118.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2020label" class="modal fade" id="bibtex-Park2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2020label">
        Joint Acoustic And Supervised Inference For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Park2020,
    Author = "Park, Sangwook and Bellur, Ashwin and Kothinti, Sandeep and Kapurchali, Masoumeh Heidari and Elhilali, Mounya",
    title = "Joint Acoustic And Supervised Inference For Sound Event Detection",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "This is a technical report about a sound event detection system for the task 4 of DCASE2020. The purpose of a sound event detection is to find event class label as well as its time boundaries. To achieve this purpose, we considered several methods such signal enhancement and event boundary detection, and built five systems by integrating these methods with supervised system trained by using Mean Teacher model. In particular, we estimate event boundaries of weakly labeled data by performing a event boundary detection. Then, we used the estimated strong label in training the supervised system. In addition, we adopt a fusion method by calculating weighted averaging posterior over the five outputs from each individual system. In experiments with validation set, we found that a final result of our system shows an improvement about 11 \% in class averaging f-score compared to a baseline performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Rykaczewski2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Rykaczewski2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Task Learning Paradigm For Sound Event Detection
       </h4>
<p style="text-align:left">
        Rykaczewski, Krzysztof
       </p>
<p style="text-align:left">
<em>
         Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Rykaczewski_Samsung_taks4_SED_1</span> <span class="label label-primary">Rykaczewski_Samsung_taks4_SED_2</span> <span class="label label-primary">Rykaczewski_Samsung_taks4_SED_3</span> <span class="label label-primary">Rykaczewski_Samsung_taks4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Rykaczewski2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Rykaczewski2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Rykaczewski2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Rykaczewski_129.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Rykaczewski2020" class="panel-collapse collapse" id="collapse-Rykaczewski2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Task Learning Paradigm For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Rykaczewski, Krzysztof
       </small>
<br/>
<small>
<em>
         Samsung R&amp;D Institute Poland, Warsaw, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our system submitted to DCASE2020 task 4. This task evaluates systems for the detection of sound events in domestic environments using large-scale weakly labeled data. To perform this task, we propose resdual convolutional recurrent neural networks (CRNN) as our system and trained by datasets including strong and weak labels. We also use mean-teacher model based on confidence thresholding and smooth embedding method. In addition, we also apply specaugment for the labeled data shortage problem. Finally, we achieve better performance than DCASE2020 baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Rykaczewski2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Rykaczewski_129.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Rykaczewski2020label" class="modal fade" id="bibtex-Rykaczewski2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRykaczewski2020label">
        Multi-Task Learning Paradigm For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Rykaczewski2020,
    Author = "Rykaczewski, Krzysztof",
    title = "Multi-Task Learning Paradigm For Sound Event Detection",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we describe our system submitted to DCASE2020 task 4. This task evaluates systems for the detection of sound events in domestic environments using large-scale weakly labeled data. To perform this task, we propose resdual convolutional recurrent neural networks (CRNN) as our system and trained by datasets including strong and weak labels. We also use mean-teacher model based on confidence thresholding and smooth embedding method. In addition, we also apply specaugment for the labeled data shortage problem. Finally, we achieve better performance than DCASE2020 baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tang2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Tang2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4
       </h4>
<p style="text-align:left">
        Tang, Maolin and Guo, Longyin and Zhang, Yanqiu and Yan, Weiran and Zhao, Qijun
       </p>
<p style="text-align:left">
<em>
         Sichuan University, ChengDu, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Tang_SCU_task4_SED_1</span> <span class="label label-primary">Tang_SCU_task4_SED_2</span> <span class="label label-primary">Tang_SCU_task4_SED_3</span> <span class="label label-primary">Tang_SCU_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Tang2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tang2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Tang_100.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tang2020" class="panel-collapse collapse" id="collapse-Tang2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4
      </h4>
<p style="text-align:left">
<small>
        Tang, Maolin and Guo, Longyin and Zhang, Yanqiu and Yan, Weiran and Zhao, Qijun
       </small>
<br/>
<small>
<em>
         Sichuan University, ChengDu, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our method for task 4 of DCASE 2020 challenge (Sound event detection and separation in domestic environments). The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled (with timestamps). We find that models perform well on synthetic data, but may not perform well on real data. We thus improve the baseline [1] by using a variety of data augmentation methods and synthesizing more complex synthetic data for training. Moreover, we present multi-scale residual convolutional recurrent neural network (CRNN) to solve the problem of multi-scale detection. The promising results on the validation set prove the effectiveness of our method.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tang2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Tang_100.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tang2020label" class="modal fade" id="bibtex-Tang2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTang2020label">
        Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Tang2020,
    Author = "Tang, Maolin and Guo, Longyin and Zhang, Yanqiu and Yan, Weiran and Zhao, Qijun",
    title = "Multi-Scale Residual CRNN With Data Augmentation For DCASE 2020 Task 4",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this technical report, we present our method for task 4 of DCASE 2020 challenge (Sound event detection and separation in domestic environments). The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled (with timestamps). We find that models perform well on synthetic data, but may not perform well on real data. We thus improve the baseline [1] by using a variety of data augmentation methods and synthesizing more complex synthetic data for training. Moreover, we present multi-scale residual convolutional recurrent neural network (CRNN) to solve the problem of multi-scale detection. The promising results on the validation set prove the effectiveness of our method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="turpault2020a" style="box-shadow: none">
<div class="panel-heading" id="heading-turpault2020a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Training Sound Event Detection On A Heterogeneous Dataset
       </h4>
<p style="text-align:left">
        Turpault, Nicolas and Serizel, Romain
       </p>
<p style="text-align:left">
<em>
         UniversitÃ© de Lorraine, CNRS, Inria, Loria, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2020_SED_baseline_system</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-turpault2020a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-turpault2020a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-turpault2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891665/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-turpault2020a').collapse('show');window.location.hash='#turpault2020a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-turpault2020a" class="panel-collapse collapse" id="collapse-turpault2020a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Training Sound Event Detection On A Heterogeneous Dataset
      </h4>
<p style="text-align:left">
<small>
        Turpault, Nicolas and Serizel, Romain
       </small>
<br/>
<small>
<em>
         UniversitÃ© de Lorraine, CNRS, Inria, Loria, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         p-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-turpault2020a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891665/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-turpault2020alabel" class="modal fade" id="bibtex-turpault2020a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexturpault2020alabel">
        Training Sound Event Detection On A Heterogeneous Dataset
       </h4>
</div>
<div class="modal-body">
<pre>@unpublished{turpault2020a,
    AUTHOR = "Turpault, Nicolas and Serizel, Romain",
    title = "Training Sound Event Detection On A Heterogeneous Dataset",
    note = "working paper or preprint",
    institution = "DCASE2020 Challenge",
    year = "2020",
    abstract = "Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default to replicate other approaches are shown to be sub-optimal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="turpault2020b" style="box-shadow: none">
<div class="panel-heading" id="heading-turpault2020b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Improving Sound Event Detection In Domestic Environments Using Sound Separation
       </h4>
<p style="text-align:left">
        Turpault, Nicolas<sup>1</sup> and Wisdom, Scott<sup>2</sup> and Erdogan, Hakan<sup>2</sup> and Herhey, John R.<sup>2</sup> and Serizel, Romain<sup>1</sup> and Fonseca, Eduardo<sup>3</sup> and Seetharaman, Prem<sup>4</sup> and Salomon, Justin<sup>5</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Universite de Lorraine, CNRS, Inria, Loria, Nancy, France, <sup>2</sup>Google Research, AI Perception, Cambridge, United States, <sup>3</sup>Music Technology Group, Universitat Pompeu Fabra, Barcelona, <sup>4</sup>Interactive Audio Lab, Northwestern University, Evanston, United States<sup>5</sup>Adobe Research, San Francisco, United States
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2020_SS_SED_baseline_system</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-turpault2020b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-turpault2020b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-turpault2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891700/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-turpault2020b').collapse('show');window.location.hash='#turpault2020b';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-turpault2020b" class="panel-collapse collapse" id="collapse-turpault2020b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Improving Sound Event Detection In Domestic Environments Using Sound Separation
      </h4>
<p style="text-align:left">
<small>
        Turpault, Nicolas<sup>1</sup> and Wisdom, Scott<sup>2</sup> and Erdogan, Hakan<sup>2</sup> and Herhey, John R.<sup>2</sup> and Serizel, Romain<sup>1</sup> and Fonseca, Eduardo<sup>3</sup> and Seetharaman, Prem<sup>4</sup> and Salomon, Justin<sup>5</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Universite de Lorraine, CNRS, Inria, Loria, Nancy, France, <sup>2</sup>Google Research, AI Perception, Cambridge, United States, <sup>3</sup>Music Technology Group, Universitat Pompeu Fabra, Barcelona, <sup>4</sup>Interactive Audio Lab, Northwestern University, Evanston, United States<sup>5</sup>Adobe Research, San Francisco, United States
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         p-norm
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-turpault2020b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02891700/document" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/turpaultn/dcase20_task4/tree/public_branch/baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-turpault2020blabel" class="modal fade" id="bibtex-turpault2020b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexturpault2020blabel">
        Improving Sound Event Detection In Domestic Environments Using Sound Separation
       </h4>
</div>
<div class="modal-body">
<pre>@unpublished{turpault2020b,
    AUTHOR = "Turpault, Nicolas and Wisdom, Scott and Erdogan, Hakan and Herhey, John R. and Serizel, Romain and Fonseca, Eduardo and Seetharaman, Prem and Salomon, Justin",
    title = "Improving Sound Event Detection In Domestic Environments Using Sound Separation",
    note = "working paper or preprint",
    institution = "DCASE2020 Challenge",
    year = "2020",
    abstract = "Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing stage for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods of combining separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the universal sound separation model to the sound event detection data in terms of both separation and sound event detection performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yao2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Yao2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection In Domestic Environments Using Dense Recurrent Neural Network
       </h4>
<p style="text-align:left">
        Yao, Tianchu and Shi, Chuang and Li, Huiyong
       </p>
<p style="text-align:left">
<em>
         University of Electronic Science and Technology of China, Chengdu, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yao_UESTC_task4_SED_1</span> <span class="label label-primary">Yao_UESTC_task4_SED_2</span> <span class="label label-primary">Yao_UESTC_task4_SED_3</span> <span class="label label-primary">Yao_UESTC_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yao2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yao2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yao2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Yao_66.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yao2020" class="panel-collapse collapse" id="collapse-Yao2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection In Domestic Environments Using Dense Recurrent Neural Network
      </h4>
<p style="text-align:left">
<small>
        Yao, Tianchu and Shi, Chuang and Li, Huiyong
       </small>
<br/>
<small>
<em>
         University of Electronic Science and Technology of China, Chengdu, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we introduce our sound events detection system using a mean-teacher model with convolutional recurrent neural network (CRNN) for DCASE 2020 Task4, which include residual convolutional block and dense recurrent neural network (DRNN) block. To improve the performance of system, we propose to use various methods such as multi-scale input layer, data augmentation, median window filters and model fusion. By combining those method, our system achieves 15% improvement on macro-averaged F-score on the development set, as compared to the baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, time-shift
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric-mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yao2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Yao_66.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yao2020label" class="modal fade" id="bibtex-Yao2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYao2020label">
        Sound Event Detection In Domestic Environments Using Dense Recurrent Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yao2020,
    Author = "Yao, Tianchu and Shi, Chuang and Li, Huiyong",
    title = "Sound Event Detection In Domestic Environments Using Dense Recurrent Neural Network",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this paper, we introduce our sound events detection system using a mean-teacher model with convolutional recurrent neural network (CRNN) for DCASE 2020 Task4, which include residual convolutional block and dense recurrent neural network (DRNN) block. To improve the performance of system, we propose to use various methods such as multi-scale input layer, data augmentation, median window filters and model fusion. By combining those method, our system achieves 15\% improvement on macro-averaged F-score on the development set, as compared to the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yen2020" style="box-shadow: none">
<div class="panel-heading" id="heading-Yen2020" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The Academia Sinica System Of Sound Event Detection And Separation For DCASE 2020
       </h4>
<p style="text-align:left">
        Yen, Hao and Ku<sup>1,2</sup>, Pin-Jui and Yen<sup>1,2</sup>, Ming-Chi and Lee<sup>1</sup>, Hung-Shin and Wang<sup>1,2</sup>, Hsin-Min Wang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute of Information Science, Academia Sinica, Taiwan, <sup>2</sup>Dept. Electrical Engineering, National Taiwan University, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">YenKu_NTU_task4_SED_1</span> <span class="label label-primary">YenKu_NTU_task4_SED_2</span> <span class="label label-primary">YenKu_NTU_task4_SED_3</span> <span class="label label-primary">YenKu_NTU_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yen2020" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yen2020" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Yen_81.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Yen2020').collapse('show');window.location.hash='#Yen2020';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yen2020" class="panel-collapse collapse" id="collapse-Yen2020" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The Academia Sinica System Of Sound Event Detection And Separation For DCASE 2020
      </h4>
<p style="text-align:left">
<small>
        Yen, Hao and Ku<sup>1,2</sup>, Pin-Jui and Yen<sup>1,2</sup>, Ming-Chi and Lee<sup>1</sup>, Hung-Shin and Wang<sup>1,2</sup>, Hsin-Min Wang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Institute of Information Science, Academia Sinica, Taiwan, <sup>2</sup>Dept. Electrical Engineering, National Taiwan University, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present the system of sound event detection and separation in domestic environments for DCASE 2020. The goal of the task aims to determine which sound events appear in a clip and detailed temporal ranges they occupy. The system is trained by using real data, which are either weakly-labeled or unlabeled, and synthesized data with a strongly annotated label. Our proposed model structure starts with a feature-level front-end based on convolution neural networks (CNN) followed by both embedding-level and instance-level back-end attention modules. To take full advantage of a large amount of unlabeled data, we jointly adopt guided learning mechanism and Mean Teacher, which averages model weights instead of label predictions, to carry out weakly-supervised and semi-supervised learning. A group of adaptive median windows for each sound event is also utilized in post-processing for smoothing frame-level predictions. In the public evaluation set of DCASE 2019, our best system achieves 48.50% event-based F-score, much better than the official baseline performance (38.14%) with a relative improvement of 27.16%. Moreover, in the development set of DCASE 2020, our system is also superior to the baseline while using the student model as the back-end classifier. The F1-score is relatively improved by 32.91%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16 kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yen2020" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2020/technical_reports/DCASE2020_Yen_81.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/dodohow1011/Academia_System_for_DCASE2020" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yen2020label" class="modal fade" id="bibtex-Yen2020" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYen2020label">
        The Academia Sinica System Of Sound Event Detection And Separation For DCASE 2020
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yen2020,
    Author = "Yen, Hao and Ku, Pin-Jui and Yen, Ming-Chi and Lee, Hung-Shin and Wang, Hsin-Min Wang",
    title = "The Academia Sinica System Of Sound Event Detection And Separation For DCASE 2020",
    institution = "DCASE2020 Challenge",
    year = "2020",
    month = "June",
    abstract = "In this report, we present the system of sound event detection and separation in domestic environments for DCASE 2020. The goal of the task aims to determine which sound events appear in a clip and detailed temporal ranges they occupy. The system is trained by using real data, which are either weakly-labeled or unlabeled, and synthesized data with a strongly annotated label. Our proposed model structure starts with a feature-level front-end based on convolution neural networks (CNN) followed by both embedding-level and instance-level back-end attention modules. To take full advantage of a large amount of unlabeled data, we jointly adopt guided learning mechanism and Mean Teacher, which averages model weights instead of label predictions, to carry out weakly-supervised and semi-supervised learning. A group of adaptive median windows for each sound event is also utilized in post-processing for smoothing frame-level predictions. In the public evaluation set of DCASE 2019, our best system achieves 48.50\% event-based F-score, much better than the official baseline performance (38.14\%) with a relative improvement of 27.16\%. Moreover, in the development set of DCASE 2020, our system is also superior to the baseline while using the student model as the back-end classifier. The F1-score is relatively improved by 32.91\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>