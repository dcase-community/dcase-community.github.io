<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Localization and Detection - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2019/task-sound-event-localization-and-detection-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The sound event localization and detection (SELD) task includes recognizing the temporal onset and offset of sound events when active, classifying the sound events into known set of classes, and further localizing the events in space when active. The focus of the current SELD task is to build â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2019</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2019/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2019/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2019/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-urban-sound-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2019/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2019/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2019/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wall-12.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-warning"></i><i class="fa dc-localization fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Localization</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 3</span></span><img src="../images/logos/dcase/dcase2019_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Localization and Detection</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#acoustic-environment-wise-performance">Acoustic environment-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a></li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The sound event localization and detection (SELD) task includes recognizing the temporal onset and offset of sound events when active, classifying the sound events into known set of classes, and further localizing the events in space when active. </p>
<p>The focus of the current SELD task is to build systems that are robust to reverberations in different acoustic environments (rooms). The task provides two datasets, development and evaluation, recorded in five different acoustics environments. 
Among the two datasets, only the development dataset provides the reference labels. The participants are expected to build systems using the provided four cross-validation splits of the development dataset, and finally test their system on the unseen evaluation dataset. </p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2019/task-sound-event-localization-and-detection" style="">task description page.</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<p>Performance of all the submitted systems on the evaluation and the development datasets</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="anchor" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_er" data-scatter-y="eval_doa" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="2">Submission Information</th>
<th class="sep-left-cell" colspan="5">Evaluation dataset</th>
<th class="sep-left-cell" colspan="4">Development dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
                Submission name
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="anchor_rank" data-sortable="true" data-value-type="int">
                Official <br/>rank
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error <br/>Rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error <br/>Rate
            </th>
<th class="text-center" data-chartable="true" data-field="dev_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="dev_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_2</td>
<td>Kapka2019</td>
<td>1</td>
<td>0.08</td>
<td>94.7</td>
<td>3.7</td>
<td>96.8</td>
<td>0.14</td>
<td>89.3</td>
<td>5.7</td>
<td>95.6</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_4</td>
<td>Kapka2019</td>
<td>2</td>
<td>0.08</td>
<td>94.7</td>
<td>3.7</td>
<td>96.8</td>
<td>0.15</td>
<td>88.7</td>
<td>5.7</td>
<td>95.6</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_3</td>
<td>Kapka2019</td>
<td>3</td>
<td>0.10</td>
<td>93.5</td>
<td>4.6</td>
<td>96.0</td>
<td>0.14</td>
<td>89.3</td>
<td>5.7</td>
<td>95.6</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_4</td>
<td>Cao2019</td>
<td>4</td>
<td>0.08</td>
<td>95.5</td>
<td>5.5</td>
<td>92.2</td>
<td>0.13</td>
<td>92.8</td>
<td>6.7</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_1</td>
<td>Xue2019</td>
<td>5</td>
<td>0.06</td>
<td>96.3</td>
<td>9.7</td>
<td>92.3</td>
<td>0.11</td>
<td>93.4</td>
<td>9.0</td>
<td>90.6</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_2</td>
<td>He2019</td>
<td>6</td>
<td>0.06</td>
<td>96.7</td>
<td>22.4</td>
<td>94.1</td>
<td>0.14</td>
<td>91.6</td>
<td>24.8</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_1</td>
<td>He2019</td>
<td>7</td>
<td>0.06</td>
<td>96.6</td>
<td>23.8</td>
<td>94.4</td>
<td>0.14</td>
<td>91.6</td>
<td>24.8</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_1</td>
<td>Cao2019</td>
<td>8</td>
<td>0.09</td>
<td>95.1</td>
<td>5.5</td>
<td>91.0</td>
<td>0.13</td>
<td>93.0</td>
<td>6.6</td>
<td>89.4</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_4</td>
<td>Xue2019</td>
<td>9</td>
<td>0.07</td>
<td>95.9</td>
<td>10.0</td>
<td>92.6</td>
<td>0.11</td>
<td>93.4</td>
<td>9.2</td>
<td>90.6</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_1</td>
<td>Jee2019</td>
<td>10</td>
<td>0.12</td>
<td>93.7</td>
<td>4.2</td>
<td>91.8</td>
<td>0.15</td>
<td>91.9</td>
<td>4.6</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_3</td>
<td>Xue2019</td>
<td>11</td>
<td>0.08</td>
<td>95.6</td>
<td>10.1</td>
<td>92.2</td>
<td>0.11</td>
<td>93.4</td>
<td>9.3</td>
<td>90.6</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_4</td>
<td>He2019</td>
<td>12</td>
<td>0.06</td>
<td>96.3</td>
<td>26.1</td>
<td>93.4</td>
<td>0.15</td>
<td>91.3</td>
<td>25.8</td>
<td>89.5</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_3</td>
<td>Cao2019</td>
<td>13</td>
<td>0.10</td>
<td>94.9</td>
<td>5.8</td>
<td>90.4</td>
<td>0.13</td>
<td>93.0</td>
<td>6.8</td>
<td>89.4</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_2</td>
<td>Xue2019</td>
<td>14</td>
<td>0.09</td>
<td>95.2</td>
<td>9.2</td>
<td>91.5</td>
<td>0.14</td>
<td>92.2</td>
<td>9.6</td>
<td>90.2</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_3</td>
<td>He2019</td>
<td>15</td>
<td>0.08</td>
<td>95.6</td>
<td>24.4</td>
<td>92.9</td>
<td>0.15</td>
<td>91.3</td>
<td>25.8</td>
<td>89.5</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_2</td>
<td>Cao2019</td>
<td>16</td>
<td>0.12</td>
<td>93.8</td>
<td>5.5</td>
<td>89.0</td>
<td>0.13</td>
<td>93.0</td>
<td>7.2</td>
<td>89.4</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Nguyen2019</td>
<td>17</td>
<td>0.11</td>
<td>93.4</td>
<td>5.4</td>
<td>88.8</td>
<td>0.17</td>
<td>89.3</td>
<td>5.1</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_3</td>
<td>MazzonYasuda2019</td>
<td>18</td>
<td>0.10</td>
<td>94.2</td>
<td>6.4</td>
<td>88.8</td>
<td>0.03</td>
<td>98.2</td>
<td>0.6</td>
<td>93.2</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_3</td>
<td>Chang2019</td>
<td>19</td>
<td>0.14</td>
<td>91.9</td>
<td>2.7</td>
<td>90.8</td>
<td>0.24</td>
<td>85.9</td>
<td>3.6</td>
<td>88.7</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_4</td>
<td>Nguyen2019</td>
<td>20</td>
<td>0.12</td>
<td>93.2</td>
<td>5.5</td>
<td>88.7</td>
<td>0.17</td>
<td>89.3</td>
<td>5.1</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_4</td>
<td>Chang2019</td>
<td>21</td>
<td>0.17</td>
<td>90.5</td>
<td>3.1</td>
<td>94.1</td>
<td>0.28</td>
<td>84.2</td>
<td>4.1</td>
<td>91.8</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_2</td>
<td>MazzonYasuda2019</td>
<td>22</td>
<td>0.13</td>
<td>93.0</td>
<td>5.0</td>
<td>88.2</td>
<td>0.04</td>
<td>98.2</td>
<td>2.2</td>
<td>92.9</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_2</td>
<td>Chang2019</td>
<td>23</td>
<td>0.14</td>
<td>92.3</td>
<td>9.7</td>
<td>95.3</td>
<td>0.25</td>
<td>85.0</td>
<td>10.6</td>
<td>92.8</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_1</td>
<td>Chang2019</td>
<td>24</td>
<td>0.13</td>
<td>92.8</td>
<td>8.4</td>
<td>91.4</td>
<td>0.26</td>
<td>85.2</td>
<td>9.6</td>
<td>88.4</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_1</td>
<td>MazzonYasuda2019</td>
<td>25</td>
<td>0.12</td>
<td>93.3</td>
<td>7.1</td>
<td>88.1</td>
<td>0.03</td>
<td>98.2</td>
<td>2.1</td>
<td>93.2</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_3</td>
<td>Ranjan2019</td>
<td>26</td>
<td>0.16</td>
<td>90.9</td>
<td>5.7</td>
<td>91.8</td>
<td>0.27</td>
<td>83.4</td>
<td>13.5</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_4</td>
<td>Ranjan2019</td>
<td>27</td>
<td>0.16</td>
<td>90.7</td>
<td>6.4</td>
<td>92.0</td>
<td>0.27</td>
<td>83.4</td>
<td>13.5</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_1</td>
<td>Park2019</td>
<td>28</td>
<td>0.15</td>
<td>91.9</td>
<td>5.1</td>
<td>87.4</td>
<td>0.17</td>
<td>90.6</td>
<td>6.4</td>
<td>85.7</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_1</td>
<td>Nguyen2019</td>
<td>29</td>
<td>0.15</td>
<td>91.1</td>
<td>5.6</td>
<td>89.8</td>
<td>0.21</td>
<td>86.9</td>
<td>5.1</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_2</td>
<td>Leung2019</td>
<td>30</td>
<td>0.12</td>
<td>93.3</td>
<td>25.9</td>
<td>91.1</td>
<td>0.20</td>
<td>88.4</td>
<td>25.4</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_2</td>
<td>Park2019</td>
<td>31</td>
<td>0.15</td>
<td>91.8</td>
<td>5.0</td>
<td>87.2</td>
<td>0.17</td>
<td>90.5</td>
<td>6.4</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Grondin_MIT_task3_1</td>
<td>Grondin2019</td>
<td>32</td>
<td>0.14</td>
<td>92.2</td>
<td>7.4</td>
<td>87.5</td>
<td>0.21</td>
<td>87.2</td>
<td>6.8</td>
<td>84.7</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_1</td>
<td>Leung2019</td>
<td>33</td>
<td>0.12</td>
<td>93.4</td>
<td>27.2</td>
<td>90.7</td>
<td>0.20</td>
<td>88.1</td>
<td>26.9</td>
<td>89.0</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_3</td>
<td>Park2019</td>
<td>34</td>
<td>0.15</td>
<td>91.9</td>
<td>7.0</td>
<td>87.4</td>
<td>0.18</td>
<td>90.1</td>
<td>8.3</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_4</td>
<td>MazzonYasuda2019</td>
<td>35</td>
<td>0.14</td>
<td>92.0</td>
<td>7.3</td>
<td>87.1</td>
<td>0.04</td>
<td>98.1</td>
<td>5.7</td>
<td>93.0</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_4</td>
<td>Park2019</td>
<td>36</td>
<td>0.15</td>
<td>91.8</td>
<td>7.0</td>
<td>87.2</td>
<td>0.18</td>
<td>89.9</td>
<td>8.3</td>
<td>85.2</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_1</td>
<td>Ranjan2019</td>
<td>37</td>
<td>0.18</td>
<td>89.9</td>
<td>8.6</td>
<td>90.1</td>
<td>0.27</td>
<td>83.4</td>
<td>13.5</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_2</td>
<td>Ranjan2019</td>
<td>38</td>
<td>0.22</td>
<td>86.8</td>
<td>7.8</td>
<td>90.0</td>
<td>0.27</td>
<td>83.4</td>
<td>13.5</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>ZhaoLu_UESTC_task3_1</td>
<td>ZhaoLu2019</td>
<td>39</td>
<td>0.18</td>
<td>89.3</td>
<td>6.8</td>
<td>84.3</td>
<td>0.20</td>
<td>87.5</td>
<td>8.0</td>
<td>83.4</td>
</tr>
<tr>
<td></td>
<td>Rough_EMED_task3_2</td>
<td>Rough2019</td>
<td>40</td>
<td>0.18</td>
<td>89.7</td>
<td>9.4</td>
<td>85.5</td>
<td>0.10</td>
<td>86.0</td>
<td>10.0</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_2</td>
<td>Nguyen2019</td>
<td>41</td>
<td>0.17</td>
<td>89.7</td>
<td>8.0</td>
<td>77.3</td>
<td>0.21</td>
<td>86.9</td>
<td>5.1</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_2</td>
<td>Jee2019</td>
<td>42</td>
<td>0.19</td>
<td>89.1</td>
<td>8.1</td>
<td>85.0</td>
<td>0.18</td>
<td>90.3</td>
<td>7.2</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Tan_NTU_task3_1</td>
<td>Tan2019</td>
<td>43</td>
<td>0.17</td>
<td>89.8</td>
<td>15.4</td>
<td>84.4</td>
<td>0.20</td>
<td>87.7</td>
<td>17.2</td>
<td>83.9</td>
</tr>
<tr>
<td></td>
<td>Lewandowski_SRPOL_task3_1</td>
<td>Kapka2019</td>
<td>44</td>
<td>0.19</td>
<td>89.4</td>
<td>36.2</td>
<td>87.7</td>
<td>0.26</td>
<td>84.1</td>
<td>35.3</td>
<td>87.9</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_2</td>
<td>Cordourier2019</td>
<td>45</td>
<td>0.22</td>
<td>86.5</td>
<td>20.8</td>
<td>85.7</td>
<td>0.12</td>
<td>92.4</td>
<td>15.9</td>
<td>89.9</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_1</td>
<td>Cordourier2019</td>
<td>46</td>
<td>0.22</td>
<td>86.3</td>
<td>19.9</td>
<td>85.6</td>
<td>0.11</td>
<td>93.3</td>
<td>15.0</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_4</td>
<td>Krause2019</td>
<td>47</td>
<td>0.22</td>
<td>87.4</td>
<td>31.0</td>
<td>87.0</td>
<td>0.19</td>
<td>88.5</td>
<td>46.9</td>
<td>88.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_FOA_baseline</td>
<td>Adavanne2019</td>
<td>48</td>
<td>0.28</td>
<td>85.4</td>
<td>24.6</td>
<td>85.7</td>
<td>0.34</td>
<td>79.9</td>
<td>28.5</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>Perezlopez_UPF_task3_1</td>
<td>Perezlopez2019</td>
<td>49</td>
<td>0.29</td>
<td>82.1</td>
<td>9.3</td>
<td>75.8</td>
<td>0.32</td>
<td>79.7</td>
<td>9.1</td>
<td>76.4</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_1</td>
<td>Chytas2019</td>
<td>50</td>
<td>0.29</td>
<td>82.4</td>
<td>18.6</td>
<td>75.6</td>
<td>0.31</td>
<td>81.2</td>
<td>19.8</td>
<td>75.3</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_3</td>
<td>Anemueller2019</td>
<td>51</td>
<td>0.28</td>
<td>83.8</td>
<td>29.2</td>
<td>84.1</td>
<td>0.30</td>
<td>82.1</td>
<td>28.9</td>
<td>85.8</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_2</td>
<td>Chytas2019</td>
<td>52</td>
<td>0.29</td>
<td>82.3</td>
<td>18.7</td>
<td>75.7</td>
<td>0.31</td>
<td>81.2</td>
<td>19.8</td>
<td>75.3</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_2</td>
<td>Krause2019</td>
<td>53</td>
<td>0.32</td>
<td>82.9</td>
<td>31.7</td>
<td>85.7</td>
<td>0.35</td>
<td>80.7</td>
<td>62.2</td>
<td>84.1</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_1</td>
<td>Krause2019</td>
<td>54</td>
<td>0.30</td>
<td>83.0</td>
<td>32.5</td>
<td>85.3</td>
<td>0.33</td>
<td>80.9</td>
<td>32.5</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_1</td>
<td>Anemueller2019</td>
<td>55</td>
<td>0.33</td>
<td>81.3</td>
<td>28.2</td>
<td>84.5</td>
<td>0.33</td>
<td>80.8</td>
<td>26.4</td>
<td>86.4</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task3_1</td>
<td>Kong2019</td>
<td>56</td>
<td>0.29</td>
<td>83.4</td>
<td>37.6</td>
<td>81.3</td>
<td>0.31</td>
<td>81.2</td>
<td>43.9</td>
<td>78.4</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_2</td>
<td>Anemueller2019</td>
<td>57</td>
<td>0.36</td>
<td>79.8</td>
<td>25.0</td>
<td>84.1</td>
<td>0.33</td>
<td>80.8</td>
<td>26.4</td>
<td>86.4</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_MIC_baseline</td>
<td>Adavanne2019</td>
<td>58</td>
<td>0.30</td>
<td>83.2</td>
<td>38.1</td>
<td>83.4</td>
<td>0.35</td>
<td>80.1</td>
<td>30.8</td>
<td>84.0</td>
</tr>
<tr>
<td></td>
<td>Lin_YYZN_task3_1</td>
<td>Lin2019</td>
<td>59</td>
<td>1.03</td>
<td>2.6</td>
<td>21.9</td>
<td>31.6</td>
<td>1.02</td>
<td>2.5</td>
<td>15.4</td>
<td>30.7</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_3</td>
<td>Krause2019</td>
<td>60</td>
<td>0.35</td>
<td>80.3</td>
<td>52.6</td>
<td>83.6</td>
<td>0.48</td>
<td>72.0</td>
<td>70.7</td>
<td>79.3</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="anchor" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_er" data-scatter-y="eval_doa" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="anchor_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="2">Submission Information</th>
<th class="sep-left-cell" colspan="5">Evaluation dataset</th>
<th class="sep-left-cell" colspan="4">Development dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
                Submission name
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="anchor_rank" data-sortable="true" data-value-type="int">
                Best official <br/>system rank
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error <br/>Rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error <br/>Rate
            </th>
<th class="text-center" data-chartable="true" data-field="dev_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="dev_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Nguyen2019</td>
<td>17</td>
<td>0.11</td>
<td>93.4</td>
<td>5.4</td>
<td>88.8</td>
<td>0.17</td>
<td>89.3</td>
<td>5.1</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_1</td>
<td>Jee2019</td>
<td>10</td>
<td>0.12</td>
<td>93.7</td>
<td>4.2</td>
<td>91.8</td>
<td>0.15</td>
<td>91.9</td>
<td>4.6</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_1</td>
<td>Xue2019</td>
<td>5</td>
<td>0.06</td>
<td>96.3</td>
<td>9.7</td>
<td>92.3</td>
<td>0.11</td>
<td>93.4</td>
<td>9.0</td>
<td>90.6</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_2</td>
<td>He2019</td>
<td>6</td>
<td>0.06</td>
<td>96.7</td>
<td>22.4</td>
<td>94.1</td>
<td>0.14</td>
<td>91.6</td>
<td>24.8</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Tan_NTU_task3_1</td>
<td>Tan2019</td>
<td>43</td>
<td>0.17</td>
<td>89.8</td>
<td>15.4</td>
<td>84.4</td>
<td>0.20</td>
<td>87.7</td>
<td>17.2</td>
<td>83.9</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_2</td>
<td>Cordourier2019</td>
<td>45</td>
<td>0.22</td>
<td>86.5</td>
<td>20.8</td>
<td>85.7</td>
<td>0.12</td>
<td>92.4</td>
<td>15.9</td>
<td>89.9</td>
</tr>
<tr>
<td></td>
<td>ZhaoLu_UESTC_task3_1</td>
<td>ZhaoLu2019</td>
<td>39</td>
<td>0.18</td>
<td>89.3</td>
<td>6.8</td>
<td>84.3</td>
<td>0.20</td>
<td>87.5</td>
<td>8.0</td>
<td>83.4</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_1</td>
<td>Park2019</td>
<td>28</td>
<td>0.15</td>
<td>91.9</td>
<td>5.1</td>
<td>87.4</td>
<td>0.17</td>
<td>90.6</td>
<td>6.4</td>
<td>85.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_FOA_baseline</td>
<td>Adavanne2019</td>
<td>48</td>
<td>0.28</td>
<td>85.4</td>
<td>24.6</td>
<td>85.7</td>
<td>0.34</td>
<td>79.9</td>
<td>28.5</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>Perezlopez_UPF_task3_1</td>
<td>Perezlopez2019</td>
<td>49</td>
<td>0.29</td>
<td>82.1</td>
<td>9.3</td>
<td>75.8</td>
<td>0.32</td>
<td>79.7</td>
<td>9.1</td>
<td>76.4</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_3</td>
<td>Chang2019</td>
<td>19</td>
<td>0.14</td>
<td>91.9</td>
<td>2.7</td>
<td>90.8</td>
<td>0.24</td>
<td>85.9</td>
<td>3.6</td>
<td>88.7</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_4</td>
<td>Cao2019</td>
<td>4</td>
<td>0.08</td>
<td>95.5</td>
<td>5.5</td>
<td>92.2</td>
<td>0.13</td>
<td>92.8</td>
<td>6.7</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Rough_EMED_task3_2</td>
<td>Rough2019</td>
<td>40</td>
<td>0.18</td>
<td>89.7</td>
<td>9.4</td>
<td>85.5</td>
<td>0.10</td>
<td>86.0</td>
<td>10.0</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_2</td>
<td>Kapka2019</td>
<td>1</td>
<td>0.08</td>
<td>94.7</td>
<td>3.7</td>
<td>96.8</td>
<td>0.14</td>
<td>89.3</td>
<td>5.7</td>
<td>95.6</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_3</td>
<td>Anemueller2019</td>
<td>51</td>
<td>0.28</td>
<td>83.8</td>
<td>29.2</td>
<td>84.1</td>
<td>0.30</td>
<td>82.1</td>
<td>28.9</td>
<td>85.8</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_2</td>
<td>Leung2019</td>
<td>30</td>
<td>0.12</td>
<td>93.3</td>
<td>25.9</td>
<td>91.1</td>
<td>0.20</td>
<td>88.4</td>
<td>25.4</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_4</td>
<td>Krause2019</td>
<td>47</td>
<td>0.22</td>
<td>87.4</td>
<td>31.0</td>
<td>87.0</td>
<td>0.19</td>
<td>88.5</td>
<td>46.9</td>
<td>88.6</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task3_1</td>
<td>Kong2019</td>
<td>56</td>
<td>0.29</td>
<td>83.4</td>
<td>37.6</td>
<td>81.3</td>
<td>0.31</td>
<td>81.2</td>
<td>43.9</td>
<td>78.4</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_1</td>
<td>Chytas2019</td>
<td>50</td>
<td>0.29</td>
<td>82.4</td>
<td>18.6</td>
<td>75.6</td>
<td>0.31</td>
<td>81.2</td>
<td>19.8</td>
<td>75.3</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_3</td>
<td>Ranjan2019</td>
<td>26</td>
<td>0.16</td>
<td>90.9</td>
<td>5.7</td>
<td>91.8</td>
<td>0.27</td>
<td>83.4</td>
<td>13.5</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>Grondin_MIT_task3_1</td>
<td>Grondin2019</td>
<td>32</td>
<td>0.14</td>
<td>92.2</td>
<td>7.4</td>
<td>87.5</td>
<td>0.21</td>
<td>87.2</td>
<td>6.8</td>
<td>84.7</td>
</tr>
<tr>
<td></td>
<td>Lin_YYZN_task3_1</td>
<td>Lin2019</td>
<td>59</td>
<td>1.03</td>
<td>2.6</td>
<td>21.9</td>
<td>31.6</td>
<td>1.02</td>
<td>2.5</td>
<td>15.4</td>
<td>30.7</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_3</td>
<td>MazzonYasuda2019</td>
<td>18</td>
<td>0.10</td>
<td>94.2</td>
<td>6.4</td>
<td>88.8</td>
<td>0.03</td>
<td>98.2</td>
<td>0.6</td>
<td>93.2</td>
</tr>
</tbody>
</table>
<h1 id="acoustic-environment-wise-performance">Acoustic environment-wise performance</h1>
<p>Performance of submitted systems on different acoustic environments of the evaluation dataset.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="false" data-id-field="anchor" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_ir1_fr" data-scatter-y="eval_ir1_doa" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="anchor_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="3">Submission Information</th>
<th class="sep-left-cell" colspan="4">Location 1</th>
<th class="sep-left-cell" colspan="4">Location 2</th>
<th class="sep-left-cell" colspan="4">Location 3</th>
<th class="sep-left-cell" colspan="4">Location 4</th>
<th class="sep-left-cell" colspan="4">Location 5</th>
</tr>
<tr>
<th class="sm-cell" data-field="anchor" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="text-center" data-field="anchor_rank" data-sortable="true" data-value-type="int">
                Official rank
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir1_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error<br/> rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir1_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir1_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir1_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir2_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error<br/> rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir2_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir2_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir2_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir3_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error<br/> rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir3_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir3_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir3_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir4_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error<br/> rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir4_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir4_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir4_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir5_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error<br/> rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir5_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_ir5_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_ir5_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_2</td>
<td>Kapka2019</td>
<td>1</td>
<td>0.05</td>
<td>96.4</td>
<td>2.5</td>
<td>98.0</td>
<td>0.09</td>
<td>94.6</td>
<td>3.3</td>
<td>96.4</td>
<td>0.11</td>
<td>92.5</td>
<td>4.6</td>
<td>95.2</td>
<td>0.09</td>
<td>94.4</td>
<td>4.0</td>
<td>97.3</td>
<td>0.07</td>
<td>95.8</td>
<td>3.9</td>
<td>97.3</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_4</td>
<td>Kapka2019</td>
<td>2</td>
<td>0.05</td>
<td>96.6</td>
<td>2.5</td>
<td>98.0</td>
<td>0.09</td>
<td>94.3</td>
<td>3.3</td>
<td>96.4</td>
<td>0.12</td>
<td>91.8</td>
<td>4.6</td>
<td>95.2</td>
<td>0.08</td>
<td>95.0</td>
<td>4.0</td>
<td>97.3</td>
<td>0.07</td>
<td>95.8</td>
<td>4.0</td>
<td>97.3</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_3</td>
<td>Kapka2019</td>
<td>3</td>
<td>0.06</td>
<td>95.7</td>
<td>4.1</td>
<td>97.3</td>
<td>0.13</td>
<td>91.5</td>
<td>4.2</td>
<td>94.5</td>
<td>0.14</td>
<td>90.8</td>
<td>5.9</td>
<td>94.5</td>
<td>0.08</td>
<td>94.9</td>
<td>4.6</td>
<td>96.6</td>
<td>0.08</td>
<td>94.5</td>
<td>4.4</td>
<td>96.9</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_4</td>
<td>Cao2019</td>
<td>4</td>
<td>0.06</td>
<td>96.5</td>
<td>5.4</td>
<td>92.1</td>
<td>0.09</td>
<td>94.8</td>
<td>5.9</td>
<td>92.2</td>
<td>0.08</td>
<td>95.6</td>
<td>4.8</td>
<td>92.7</td>
<td>0.10</td>
<td>94.5</td>
<td>5.4</td>
<td>91.7</td>
<td>0.07</td>
<td>96.0</td>
<td>6.1</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_1</td>
<td>Xue2019</td>
<td>5</td>
<td>0.05</td>
<td>97.4</td>
<td>10.1</td>
<td>92.1</td>
<td>0.06</td>
<td>96.3</td>
<td>10.3</td>
<td>92.9</td>
<td>0.08</td>
<td>95.6</td>
<td>10.0</td>
<td>91.8</td>
<td>0.07</td>
<td>96.1</td>
<td>9.6</td>
<td>92.6</td>
<td>0.07</td>
<td>96.2</td>
<td>8.3</td>
<td>92.1</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_2</td>
<td>He2019</td>
<td>6</td>
<td>0.04</td>
<td>97.6</td>
<td>20.8</td>
<td>94.5</td>
<td>0.09</td>
<td>95.1</td>
<td>25.2</td>
<td>93.4</td>
<td>0.06</td>
<td>96.9</td>
<td>22.8</td>
<td>94.1</td>
<td>0.07</td>
<td>96.5</td>
<td>21.8</td>
<td>94.1</td>
<td>0.05</td>
<td>97.3</td>
<td>21.3</td>
<td>94.4</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_1</td>
<td>He2019</td>
<td>7</td>
<td>0.04</td>
<td>97.8</td>
<td>22.7</td>
<td>94.5</td>
<td>0.09</td>
<td>95.0</td>
<td>27.0</td>
<td>94.0</td>
<td>0.06</td>
<td>96.4</td>
<td>23.1</td>
<td>94.6</td>
<td>0.06</td>
<td>96.7</td>
<td>23.6</td>
<td>94.5</td>
<td>0.06</td>
<td>96.8</td>
<td>22.7</td>
<td>94.3</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_1</td>
<td>Cao2019</td>
<td>8</td>
<td>0.08</td>
<td>96.0</td>
<td>5.4</td>
<td>91.0</td>
<td>0.10</td>
<td>94.6</td>
<td>5.8</td>
<td>90.8</td>
<td>0.09</td>
<td>95.3</td>
<td>5.1</td>
<td>91.7</td>
<td>0.10</td>
<td>94.8</td>
<td>5.3</td>
<td>90.9</td>
<td>0.10</td>
<td>95.0</td>
<td>6.0</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_4</td>
<td>Xue2019</td>
<td>9</td>
<td>0.05</td>
<td>97.2</td>
<td>10.2</td>
<td>92.5</td>
<td>0.08</td>
<td>95.1</td>
<td>11.1</td>
<td>92.8</td>
<td>0.08</td>
<td>95.5</td>
<td>10.2</td>
<td>92.3</td>
<td>0.08</td>
<td>95.3</td>
<td>9.9</td>
<td>92.7</td>
<td>0.07</td>
<td>96.2</td>
<td>8.8</td>
<td>92.8</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_1</td>
<td>Jee2019</td>
<td>10</td>
<td>0.10</td>
<td>94.6</td>
<td>4.1</td>
<td>91.6</td>
<td>0.13</td>
<td>92.9</td>
<td>4.3</td>
<td>91.8</td>
<td>0.12</td>
<td>93.5</td>
<td>4.5</td>
<td>91.9</td>
<td>0.13</td>
<td>93.0</td>
<td>3.8</td>
<td>91.6</td>
<td>0.11</td>
<td>94.4</td>
<td>4.3</td>
<td>91.9</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_3</td>
<td>Xue2019</td>
<td>11</td>
<td>0.06</td>
<td>97.1</td>
<td>10.1</td>
<td>92.0</td>
<td>0.10</td>
<td>94.3</td>
<td>11.3</td>
<td>92.0</td>
<td>0.08</td>
<td>95.3</td>
<td>10.2</td>
<td>92.0</td>
<td>0.08</td>
<td>95.1</td>
<td>10.0</td>
<td>92.6</td>
<td>0.07</td>
<td>96.1</td>
<td>8.9</td>
<td>92.6</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_4</td>
<td>He2019</td>
<td>12</td>
<td>0.05</td>
<td>97.5</td>
<td>24.0</td>
<td>93.5</td>
<td>0.08</td>
<td>95.6</td>
<td>29.2</td>
<td>93.3</td>
<td>0.06</td>
<td>96.3</td>
<td>26.4</td>
<td>93.8</td>
<td>0.07</td>
<td>96.0</td>
<td>25.9</td>
<td>93.3</td>
<td>0.07</td>
<td>96.3</td>
<td>25.4</td>
<td>93.1</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_3</td>
<td>Cao2019</td>
<td>13</td>
<td>0.08</td>
<td>95.7</td>
<td>5.5</td>
<td>89.9</td>
<td>0.11</td>
<td>94.1</td>
<td>6.4</td>
<td>90.4</td>
<td>0.09</td>
<td>95.3</td>
<td>5.1</td>
<td>90.9</td>
<td>0.10</td>
<td>94.5</td>
<td>5.8</td>
<td>90.1</td>
<td>0.10</td>
<td>94.7</td>
<td>6.3</td>
<td>90.5</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_2</td>
<td>Xue2019</td>
<td>14</td>
<td>0.07</td>
<td>96.3</td>
<td>9.0</td>
<td>91.6</td>
<td>0.10</td>
<td>94.4</td>
<td>10.7</td>
<td>91.4</td>
<td>0.09</td>
<td>95.0</td>
<td>9.4</td>
<td>91.7</td>
<td>0.10</td>
<td>94.4</td>
<td>8.9</td>
<td>91.3</td>
<td>0.08</td>
<td>95.6</td>
<td>8.1</td>
<td>91.5</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_3</td>
<td>He2019</td>
<td>15</td>
<td>0.07</td>
<td>96.4</td>
<td>24.4</td>
<td>92.9</td>
<td>0.09</td>
<td>95.0</td>
<td>26.5</td>
<td>93.4</td>
<td>0.09</td>
<td>95.0</td>
<td>24.2</td>
<td>92.5</td>
<td>0.09</td>
<td>95.3</td>
<td>23.4</td>
<td>92.6</td>
<td>0.07</td>
<td>96.2</td>
<td>23.9</td>
<td>93.0</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_2</td>
<td>Cao2019</td>
<td>16</td>
<td>0.08</td>
<td>95.4</td>
<td>5.3</td>
<td>89.5</td>
<td>0.17</td>
<td>91.1</td>
<td>5.7</td>
<td>86.9</td>
<td>0.10</td>
<td>94.6</td>
<td>5.1</td>
<td>90.2</td>
<td>0.12</td>
<td>93.5</td>
<td>5.5</td>
<td>88.9</td>
<td>0.11</td>
<td>94.3</td>
<td>5.9</td>
<td>89.6</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Nguyen2019</td>
<td>17</td>
<td>0.09</td>
<td>94.9</td>
<td>4.0</td>
<td>88.9</td>
<td>0.12</td>
<td>92.7</td>
<td>4.7</td>
<td>88.9</td>
<td>0.12</td>
<td>92.8</td>
<td>7.0</td>
<td>88.7</td>
<td>0.11</td>
<td>93.6</td>
<td>4.8</td>
<td>89.4</td>
<td>0.13</td>
<td>92.8</td>
<td>6.5</td>
<td>87.9</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_3</td>
<td>MazzonYasuda2019</td>
<td>18</td>
<td>0.09</td>
<td>95.1</td>
<td>6.2</td>
<td>88.6</td>
<td>0.10</td>
<td>94.2</td>
<td>6.4</td>
<td>89.5</td>
<td>0.11</td>
<td>93.8</td>
<td>6.5</td>
<td>89.1</td>
<td>0.12</td>
<td>93.2</td>
<td>5.9</td>
<td>88.2</td>
<td>0.10</td>
<td>94.5</td>
<td>6.9</td>
<td>88.7</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_3</td>
<td>Chang2019</td>
<td>19</td>
<td>0.12</td>
<td>93.2</td>
<td>2.5</td>
<td>90.3</td>
<td>0.15</td>
<td>91.9</td>
<td>2.5</td>
<td>91.3</td>
<td>0.14</td>
<td>91.2</td>
<td>2.9</td>
<td>91.4</td>
<td>0.15</td>
<td>91.4</td>
<td>2.7</td>
<td>90.5</td>
<td>0.15</td>
<td>91.6</td>
<td>2.7</td>
<td>90.4</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_4</td>
<td>Nguyen2019</td>
<td>20</td>
<td>0.10</td>
<td>94.9</td>
<td>4.0</td>
<td>89.0</td>
<td>0.12</td>
<td>92.3</td>
<td>4.7</td>
<td>88.9</td>
<td>0.12</td>
<td>93.0</td>
<td>7.1</td>
<td>88.5</td>
<td>0.11</td>
<td>93.5</td>
<td>4.8</td>
<td>89.4</td>
<td>0.13</td>
<td>92.4</td>
<td>6.5</td>
<td>87.8</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_4</td>
<td>Chang2019</td>
<td>21</td>
<td>0.14</td>
<td>92.0</td>
<td>2.9</td>
<td>94.1</td>
<td>0.21</td>
<td>89.0</td>
<td>2.8</td>
<td>93.6</td>
<td>0.16</td>
<td>90.3</td>
<td>3.1</td>
<td>94.1</td>
<td>0.17</td>
<td>90.4</td>
<td>3.3</td>
<td>94.5</td>
<td>0.17</td>
<td>90.6</td>
<td>3.3</td>
<td>94.3</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_2</td>
<td>MazzonYasuda2019</td>
<td>22</td>
<td>0.12</td>
<td>93.9</td>
<td>4.9</td>
<td>87.8</td>
<td>0.13</td>
<td>93.0</td>
<td>5.1</td>
<td>89.0</td>
<td>0.13</td>
<td>92.7</td>
<td>5.0</td>
<td>88.6</td>
<td>0.14</td>
<td>92.2</td>
<td>4.7</td>
<td>87.8</td>
<td>0.13</td>
<td>93.2</td>
<td>5.5</td>
<td>87.6</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_2</td>
<td>Chang2019</td>
<td>23</td>
<td>0.12</td>
<td>93.4</td>
<td>9.5</td>
<td>95.1</td>
<td>0.17</td>
<td>91.2</td>
<td>9.6</td>
<td>95.0</td>
<td>0.14</td>
<td>92.1</td>
<td>10.0</td>
<td>94.9</td>
<td>0.13</td>
<td>92.5</td>
<td>9.7</td>
<td>96.0</td>
<td>0.14</td>
<td>92.3</td>
<td>9.9</td>
<td>95.3</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_1</td>
<td>Chang2019</td>
<td>24</td>
<td>0.13</td>
<td>93.0</td>
<td>8.2</td>
<td>90.9</td>
<td>0.14</td>
<td>92.3</td>
<td>8.4</td>
<td>91.7</td>
<td>0.13</td>
<td>92.5</td>
<td>9.1</td>
<td>91.2</td>
<td>0.12</td>
<td>93.0</td>
<td>7.8</td>
<td>91.7</td>
<td>0.12</td>
<td>93.4</td>
<td>8.5</td>
<td>91.5</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_1</td>
<td>MazzonYasuda2019</td>
<td>25</td>
<td>0.10</td>
<td>94.7</td>
<td>6.8</td>
<td>87.9</td>
<td>0.13</td>
<td>93.2</td>
<td>7.2</td>
<td>88.7</td>
<td>0.13</td>
<td>92.7</td>
<td>7.2</td>
<td>88.7</td>
<td>0.14</td>
<td>92.5</td>
<td>6.6</td>
<td>87.6</td>
<td>0.12</td>
<td>93.6</td>
<td>7.5</td>
<td>87.8</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_3</td>
<td>Ranjan2019</td>
<td>26</td>
<td>0.13</td>
<td>93.0</td>
<td>6.3</td>
<td>92.4</td>
<td>0.21</td>
<td>88.7</td>
<td>5.2</td>
<td>91.0</td>
<td>0.16</td>
<td>90.9</td>
<td>5.7</td>
<td>92.1</td>
<td>0.17</td>
<td>90.2</td>
<td>5.3</td>
<td>91.4</td>
<td>0.15</td>
<td>91.7</td>
<td>5.9</td>
<td>92.1</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_4</td>
<td>Ranjan2019</td>
<td>27</td>
<td>0.12</td>
<td>93.2</td>
<td>6.8</td>
<td>93.0</td>
<td>0.21</td>
<td>88.5</td>
<td>5.7</td>
<td>90.8</td>
<td>0.15</td>
<td>90.8</td>
<td>6.7</td>
<td>92.4</td>
<td>0.17</td>
<td>89.7</td>
<td>6.5</td>
<td>91.9</td>
<td>0.15</td>
<td>91.0</td>
<td>6.6</td>
<td>92.1</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_1</td>
<td>Park2019</td>
<td>28</td>
<td>0.13</td>
<td>93.4</td>
<td>4.5</td>
<td>87.1</td>
<td>0.15</td>
<td>91.7</td>
<td>5.1</td>
<td>88.3</td>
<td>0.14</td>
<td>92.1</td>
<td>5.3</td>
<td>87.7</td>
<td>0.17</td>
<td>90.4</td>
<td>5.0</td>
<td>86.9</td>
<td>0.15</td>
<td>91.8</td>
<td>5.4</td>
<td>86.8</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_1</td>
<td>Nguyen2019</td>
<td>29</td>
<td>0.12</td>
<td>92.9</td>
<td>4.0</td>
<td>90.7</td>
<td>0.17</td>
<td>89.5</td>
<td>4.9</td>
<td>88.0</td>
<td>0.15</td>
<td>90.7</td>
<td>7.2</td>
<td>90.1</td>
<td>0.15</td>
<td>90.9</td>
<td>5.1</td>
<td>90.3</td>
<td>0.14</td>
<td>91.6</td>
<td>6.7</td>
<td>89.7</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_2</td>
<td>Leung2019</td>
<td>30</td>
<td>0.10</td>
<td>94.9</td>
<td>25.2</td>
<td>91.1</td>
<td>0.14</td>
<td>92.1</td>
<td>27.7</td>
<td>91.1</td>
<td>0.13</td>
<td>92.3</td>
<td>26.9</td>
<td>90.7</td>
<td>0.12</td>
<td>93.3</td>
<td>24.7</td>
<td>91.0</td>
<td>0.11</td>
<td>94.1</td>
<td>25.2</td>
<td>91.6</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_2</td>
<td>Park2019</td>
<td>31</td>
<td>0.13</td>
<td>93.2</td>
<td>4.5</td>
<td>86.9</td>
<td>0.15</td>
<td>91.7</td>
<td>5.0</td>
<td>88.3</td>
<td>0.15</td>
<td>91.8</td>
<td>5.1</td>
<td>87.3</td>
<td>0.16</td>
<td>90.7</td>
<td>5.1</td>
<td>87.2</td>
<td>0.15</td>
<td>91.7</td>
<td>5.2</td>
<td>86.5</td>
</tr>
<tr>
<td></td>
<td>Grondin_MIT_task3_1</td>
<td>Grondin2019</td>
<td>32</td>
<td>0.12</td>
<td>93.7</td>
<td>6.8</td>
<td>88.6</td>
<td>0.17</td>
<td>90.3</td>
<td>7.0</td>
<td>86.0</td>
<td>0.14</td>
<td>92.2</td>
<td>7.7</td>
<td>87.7</td>
<td>0.15</td>
<td>91.5</td>
<td>7.8</td>
<td>87.1</td>
<td>0.12</td>
<td>93.2</td>
<td>7.7</td>
<td>88.2</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_1</td>
<td>Leung2019</td>
<td>33</td>
<td>0.10</td>
<td>94.9</td>
<td>27.3</td>
<td>90.6</td>
<td>0.13</td>
<td>92.6</td>
<td>28.6</td>
<td>91.2</td>
<td>0.13</td>
<td>92.1</td>
<td>28.3</td>
<td>90.2</td>
<td>0.13</td>
<td>93.0</td>
<td>25.5</td>
<td>90.3</td>
<td>0.10</td>
<td>94.2</td>
<td>26.4</td>
<td>91.2</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_3</td>
<td>Park2019</td>
<td>34</td>
<td>0.13</td>
<td>93.4</td>
<td>7.1</td>
<td>87.1</td>
<td>0.15</td>
<td>91.7</td>
<td>7.7</td>
<td>88.3</td>
<td>0.14</td>
<td>92.1</td>
<td>6.7</td>
<td>87.7</td>
<td>0.17</td>
<td>90.4</td>
<td>6.8</td>
<td>86.9</td>
<td>0.15</td>
<td>91.8</td>
<td>7.1</td>
<td>86.8</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_4</td>
<td>MazzonYasuda2019</td>
<td>35</td>
<td>0.12</td>
<td>93.7</td>
<td>7.6</td>
<td>86.9</td>
<td>0.16</td>
<td>91.1</td>
<td>6.7</td>
<td>87.2</td>
<td>0.15</td>
<td>91.7</td>
<td>7.5</td>
<td>87.1</td>
<td>0.16</td>
<td>91.1</td>
<td>7.0</td>
<td>86.5</td>
<td>0.14</td>
<td>92.6</td>
<td>7.3</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_4</td>
<td>Park2019</td>
<td>36</td>
<td>0.13</td>
<td>93.2</td>
<td>7.0</td>
<td>86.9</td>
<td>0.15</td>
<td>91.7</td>
<td>7.8</td>
<td>88.3</td>
<td>0.15</td>
<td>91.8</td>
<td>6.7</td>
<td>87.3</td>
<td>0.16</td>
<td>90.7</td>
<td>6.9</td>
<td>87.2</td>
<td>0.15</td>
<td>91.7</td>
<td>7.0</td>
<td>86.5</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_1</td>
<td>Ranjan2019</td>
<td>37</td>
<td>0.13</td>
<td>92.5</td>
<td>8.9</td>
<td>91.0</td>
<td>0.24</td>
<td>87.4</td>
<td>8.2</td>
<td>87.7</td>
<td>0.15</td>
<td>91.3</td>
<td>8.6</td>
<td>91.2</td>
<td>0.19</td>
<td>88.6</td>
<td>8.3</td>
<td>90.2</td>
<td>0.19</td>
<td>89.5</td>
<td>9.0</td>
<td>90.5</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_2</td>
<td>Ranjan2019</td>
<td>38</td>
<td>0.18</td>
<td>89.4</td>
<td>8.0</td>
<td>90.8</td>
<td>0.29</td>
<td>84.3</td>
<td>7.0</td>
<td>88.5</td>
<td>0.19</td>
<td>88.2</td>
<td>8.0</td>
<td>90.5</td>
<td>0.24</td>
<td>85.0</td>
<td>8.1</td>
<td>89.7</td>
<td>0.23</td>
<td>87.0</td>
<td>7.7</td>
<td>90.4</td>
</tr>
<tr>
<td></td>
<td>ZhaoLu_UESTC_task3_1</td>
<td>ZhaoLu2019</td>
<td>39</td>
<td>0.16</td>
<td>90.8</td>
<td>6.8</td>
<td>84.7</td>
<td>0.18</td>
<td>89.6</td>
<td>7.1</td>
<td>84.8</td>
<td>0.18</td>
<td>88.8</td>
<td>7.0</td>
<td>84.8</td>
<td>0.20</td>
<td>88.0</td>
<td>6.4</td>
<td>83.4</td>
<td>0.18</td>
<td>89.2</td>
<td>6.6</td>
<td>83.6</td>
</tr>
<tr>
<td></td>
<td>Rough_EMED_task3_2</td>
<td>Rough2019</td>
<td>40</td>
<td>0.16</td>
<td>90.7</td>
<td>10.2</td>
<td>85.1</td>
<td>0.18</td>
<td>90.2</td>
<td>9.5</td>
<td>86.7</td>
<td>0.18</td>
<td>89.2</td>
<td>9.2</td>
<td>85.5</td>
<td>0.21</td>
<td>87.8</td>
<td>8.9</td>
<td>84.7</td>
<td>0.17</td>
<td>90.5</td>
<td>9.1</td>
<td>85.3</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_2</td>
<td>Nguyen2019</td>
<td>41</td>
<td>0.14</td>
<td>91.7</td>
<td>6.3</td>
<td>78.0</td>
<td>0.19</td>
<td>88.2</td>
<td>5.6</td>
<td>77.4</td>
<td>0.18</td>
<td>89.4</td>
<td>10.2</td>
<td>77.3</td>
<td>0.18</td>
<td>89.1</td>
<td>8.0</td>
<td>76.5</td>
<td>0.17</td>
<td>90.1</td>
<td>9.7</td>
<td>77.5</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_2</td>
<td>Jee2019</td>
<td>42</td>
<td>0.16</td>
<td>90.5</td>
<td>8.0</td>
<td>84.8</td>
<td>0.18</td>
<td>89.7</td>
<td>7.9</td>
<td>86.9</td>
<td>0.18</td>
<td>89.4</td>
<td>8.7</td>
<td>83.9</td>
<td>0.22</td>
<td>87.0</td>
<td>7.5</td>
<td>84.6</td>
<td>0.18</td>
<td>89.2</td>
<td>8.2</td>
<td>84.7</td>
</tr>
<tr>
<td></td>
<td>Tan_NTU_task3_1</td>
<td>Tan2019</td>
<td>43</td>
<td>0.14</td>
<td>92.1</td>
<td>14.5</td>
<td>84.5</td>
<td>0.22</td>
<td>87.8</td>
<td>15.1</td>
<td>84.2</td>
<td>0.17</td>
<td>89.3</td>
<td>15.1</td>
<td>83.9</td>
<td>0.18</td>
<td>89.4</td>
<td>16.1</td>
<td>84.0</td>
<td>0.16</td>
<td>90.3</td>
<td>16.3</td>
<td>85.1</td>
</tr>
<tr>
<td></td>
<td>Lewandowski_SRPOL_task3_1</td>
<td>Kapka2019</td>
<td>44</td>
<td>0.14</td>
<td>92.4</td>
<td>36.1</td>
<td>89.5</td>
<td>0.32</td>
<td>83.5</td>
<td>32.9</td>
<td>79.4</td>
<td>0.18</td>
<td>89.5</td>
<td>37.5</td>
<td>88.7</td>
<td>0.17</td>
<td>90.1</td>
<td>36.8</td>
<td>89.5</td>
<td>0.15</td>
<td>91.2</td>
<td>37.8</td>
<td>91.2</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_2</td>
<td>Cordourier2019</td>
<td>45</td>
<td>0.21</td>
<td>86.9</td>
<td>20.9</td>
<td>86.3</td>
<td>0.22</td>
<td>86.6</td>
<td>20.0</td>
<td>85.9</td>
<td>0.21</td>
<td>87.0</td>
<td>21.0</td>
<td>84.9</td>
<td>0.24</td>
<td>84.6</td>
<td>20.3</td>
<td>84.7</td>
<td>0.20</td>
<td>87.6</td>
<td>21.5</td>
<td>86.5</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_1</td>
<td>Cordourier2019</td>
<td>46</td>
<td>0.19</td>
<td>88.6</td>
<td>19.5</td>
<td>86.5</td>
<td>0.26</td>
<td>85.0</td>
<td>19.7</td>
<td>85.6</td>
<td>0.22</td>
<td>86.2</td>
<td>20.3</td>
<td>85.6</td>
<td>0.24</td>
<td>85.3</td>
<td>19.8</td>
<td>84.4</td>
<td>0.22</td>
<td>86.5</td>
<td>20.1</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_4</td>
<td>Krause2019</td>
<td>47</td>
<td>0.19</td>
<td>89.6</td>
<td>30.1</td>
<td>88.4</td>
<td>0.31</td>
<td>83.4</td>
<td>31.8</td>
<td>82.8</td>
<td>0.22</td>
<td>87.2</td>
<td>31.8</td>
<td>87.2</td>
<td>0.23</td>
<td>87.0</td>
<td>30.3</td>
<td>87.1</td>
<td>0.18</td>
<td>89.7</td>
<td>31.2</td>
<td>89.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_FOA_baseline</td>
<td>Adavanne2019</td>
<td>48</td>
<td>0.24</td>
<td>87.5</td>
<td>24.2</td>
<td>87.6</td>
<td>0.41</td>
<td>80.3</td>
<td>23.3</td>
<td>79.2</td>
<td>0.26</td>
<td>85.3</td>
<td>25.4</td>
<td>85.8</td>
<td>0.27</td>
<td>86.1</td>
<td>24.0</td>
<td>87.1</td>
<td>0.24</td>
<td>87.4</td>
<td>26.0</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>Perezlopez_UPF_task3_1</td>
<td>Perezlopez2019</td>
<td>49</td>
<td>0.20</td>
<td>87.9</td>
<td>7.4</td>
<td>80.2</td>
<td>0.51</td>
<td>71.0</td>
<td>9.8</td>
<td>61.2</td>
<td>0.25</td>
<td>84.6</td>
<td>10.5</td>
<td>79.9</td>
<td>0.27</td>
<td>83.5</td>
<td>9.6</td>
<td>79.1</td>
<td>0.26</td>
<td>84.1</td>
<td>9.2</td>
<td>78.4</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_1</td>
<td>Chytas2019</td>
<td>50</td>
<td>0.29</td>
<td>82.3</td>
<td>17.8</td>
<td>74.3</td>
<td>0.29</td>
<td>83.2</td>
<td>18.7</td>
<td>78.2</td>
<td>0.28</td>
<td>82.4</td>
<td>18.0</td>
<td>74.5</td>
<td>0.29</td>
<td>82.3</td>
<td>18.7</td>
<td>76.1</td>
<td>0.30</td>
<td>81.9</td>
<td>19.6</td>
<td>75.1</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_3</td>
<td>Anemueller2019</td>
<td>51</td>
<td>0.23</td>
<td>85.7</td>
<td>30.3</td>
<td>86.3</td>
<td>0.41</td>
<td>79.2</td>
<td>27.4</td>
<td>76.9</td>
<td>0.28</td>
<td>82.4</td>
<td>28.8</td>
<td>83.5</td>
<td>0.26</td>
<td>85.2</td>
<td>28.4</td>
<td>85.8</td>
<td>0.22</td>
<td>86.5</td>
<td>31.1</td>
<td>87.9</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_2</td>
<td>Chytas2019</td>
<td>52</td>
<td>0.29</td>
<td>82.3</td>
<td>18.5</td>
<td>74.1</td>
<td>0.28</td>
<td>83.5</td>
<td>18.6</td>
<td>78.4</td>
<td>0.29</td>
<td>81.9</td>
<td>18.2</td>
<td>74.7</td>
<td>0.29</td>
<td>82.2</td>
<td>18.4</td>
<td>76.1</td>
<td>0.30</td>
<td>81.9</td>
<td>19.9</td>
<td>75.0</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_2</td>
<td>Krause2019</td>
<td>53</td>
<td>0.26</td>
<td>85.9</td>
<td>31.0</td>
<td>87.3</td>
<td>0.43</td>
<td>77.9</td>
<td>32.5</td>
<td>80.7</td>
<td>0.29</td>
<td>83.7</td>
<td>31.9</td>
<td>86.4</td>
<td>0.33</td>
<td>82.0</td>
<td>31.0</td>
<td>85.8</td>
<td>0.29</td>
<td>85.2</td>
<td>32.3</td>
<td>88.1</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_1</td>
<td>Krause2019</td>
<td>54</td>
<td>0.27</td>
<td>85.0</td>
<td>31.3</td>
<td>86.4</td>
<td>0.43</td>
<td>78.3</td>
<td>32.3</td>
<td>81.3</td>
<td>0.28</td>
<td>83.4</td>
<td>33.4</td>
<td>85.7</td>
<td>0.31</td>
<td>81.7</td>
<td>32.4</td>
<td>85.4</td>
<td>0.24</td>
<td>86.2</td>
<td>33.0</td>
<td>87.9</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_1</td>
<td>Anemueller2019</td>
<td>55</td>
<td>0.26</td>
<td>84.8</td>
<td>27.3</td>
<td>86.0</td>
<td>0.45</td>
<td>76.5</td>
<td>28.1</td>
<td>79.4</td>
<td>0.32</td>
<td>81.5</td>
<td>28.9</td>
<td>85.8</td>
<td>0.37</td>
<td>78.8</td>
<td>28.0</td>
<td>84.5</td>
<td>0.28</td>
<td>85.0</td>
<td>28.8</td>
<td>87.0</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task3_1</td>
<td>Kong2019</td>
<td>56</td>
<td>0.26</td>
<td>85.2</td>
<td>30.5</td>
<td>81.0</td>
<td>0.31</td>
<td>83.0</td>
<td>43.4</td>
<td>82.4</td>
<td>0.29</td>
<td>83.0</td>
<td>34.3</td>
<td>80.8</td>
<td>0.30</td>
<td>83.0</td>
<td>39.3</td>
<td>81.5</td>
<td>0.29</td>
<td>82.6</td>
<td>40.0</td>
<td>80.9</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_2</td>
<td>Anemueller2019</td>
<td>57</td>
<td>0.32</td>
<td>82.1</td>
<td>24.5</td>
<td>84.6</td>
<td>0.43</td>
<td>76.5</td>
<td>25.2</td>
<td>81.7</td>
<td>0.33</td>
<td>80.4</td>
<td>24.9</td>
<td>85.0</td>
<td>0.37</td>
<td>79.1</td>
<td>24.7</td>
<td>83.9</td>
<td>0.34</td>
<td>80.6</td>
<td>25.5</td>
<td>85.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_MIC_baseline</td>
<td>Adavanne2019</td>
<td>58</td>
<td>0.27</td>
<td>84.3</td>
<td>37.5</td>
<td>81.5</td>
<td>0.33</td>
<td>82.0</td>
<td>39.0</td>
<td>85.1</td>
<td>0.29</td>
<td>83.5</td>
<td>37.4</td>
<td>82.7</td>
<td>0.31</td>
<td>82.2</td>
<td>36.4</td>
<td>83.3</td>
<td>0.28</td>
<td>84.1</td>
<td>40.1</td>
<td>84.3</td>
</tr>
<tr>
<td></td>
<td>Lin_YYZN_task3_1</td>
<td>Lin2019</td>
<td>59</td>
<td>1.04</td>
<td>4.2</td>
<td>4.9</td>
<td>27.9</td>
<td>1.06</td>
<td>3.5</td>
<td>39.2</td>
<td>37.8</td>
<td>1.00</td>
<td>0.0</td>
<td>35.2</td>
<td>31.5</td>
<td>1.03</td>
<td>1.7</td>
<td>57.0</td>
<td>32.4</td>
<td>1.03</td>
<td>3.4</td>
<td>9.3</td>
<td>28.7</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_3</td>
<td>Krause2019</td>
<td>60</td>
<td>0.27</td>
<td>84.7</td>
<td>54.2</td>
<td>85.7</td>
<td>0.50</td>
<td>74.1</td>
<td>49.1</td>
<td>79.3</td>
<td>0.32</td>
<td>80.8</td>
<td>54.6</td>
<td>83.2</td>
<td>0.37</td>
<td>79.2</td>
<td>50.5</td>
<td>84.2</td>
<td>0.32</td>
<td>82.7</td>
<td>54.7</td>
<td>85.8</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<p>Summary of the submitted systems characteristics.</p>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="anchor" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th data-field="anchor_rank" data-sortable="true" data-value-type="int">
                Rank
            </th>
<th class="sm-cell" data-field="anchor" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_architecture" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-field="system_params" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-value-type="numeric-unit">
                Classifier<br/>params
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_input_format" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Audio<br/>format
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_acoustic_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Acoustic<br/>feature
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_aug" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_fs" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Kapka_SRPOL_task3_2</td>
<td>Kapka2019</td>
<td>CRNN</td>
<td>2651634</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>2</td>
<td>Kapka_SRPOL_task3_4</td>
<td>Kapka2019</td>
<td>CRNN</td>
<td>2651634</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>3</td>
<td>Kapka_SRPOL_task3_3</td>
<td>Kapka2019</td>
<td>CRNN</td>
<td>2651634</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>4</td>
<td>Cao_Surrey_task3_4</td>
<td>Cao2019</td>
<td>CRNN9, ensemble</td>
<td>6976673</td>
<td>both</td>
<td>log mel and intensity vector</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>5</td>
<td>Xue_JDAI_task3_1</td>
<td>Xue2019</td>
<td>CRNN, ensemble</td>
<td>2641263</td>
<td>Microphone Array</td>
<td>Log-Mel, Constant Q-transform, raw phase spectrogram, smoothed cross-power angular spectra</td>
<td>speed perturbtion</td>
<td>48kHz</td>
</tr>
<tr>
<td>6</td>
<td>He_THU_task3_2</td>
<td>He2019</td>
<td>CRNN</td>
<td>474273</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram, Mel-spectrogram</td>
<td>time and frequency masking (SpecAugment)</td>
<td>48kHz</td>
</tr>
<tr>
<td>7</td>
<td>He_THU_task3_1</td>
<td>He2019</td>
<td>CRNN</td>
<td>474273</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram, Mel-spectrogram</td>
<td>time and frequency masking (SpecAugment)</td>
<td>48kHz</td>
</tr>
<tr>
<td>8</td>
<td>Cao_Surrey_task3_1</td>
<td>Cao2019</td>
<td>CRNN9, ensemble</td>
<td>6965153</td>
<td>Ambisonic</td>
<td>log mel and intensity vector</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>9</td>
<td>Xue_JDAI_task3_4</td>
<td>Xue2019</td>
<td>CRNN, ensemble</td>
<td>2641263</td>
<td>Microphone Array</td>
<td>Log-Mel, Constant Q-transform, raw phase spectrogram, smoothed cross-power angular spectra</td>
<td>speed perturbtion</td>
<td>48kHz</td>
</tr>
<tr>
<td>10</td>
<td>Jee_NTU_task3_1</td>
<td>Jee2019</td>
<td>CRNN</td>
<td>10209057</td>
<td>Microphone Array</td>
<td>log-mel spectrogram and GCC-PHAT</td>
<td>mixup</td>
<td>34 kHz</td>
</tr>
<tr>
<td>11</td>
<td>Xue_JDAI_task3_3</td>
<td>Xue2019</td>
<td>CRNN, ensemble</td>
<td>2641263</td>
<td>Microphone Array</td>
<td>Log-Mel, Constant Q-transform, raw phase spectrogram, smoothed cross-power angular spectra</td>
<td>speed perturbtion</td>
<td>48kHz</td>
</tr>
<tr>
<td>12</td>
<td>He_THU_task3_4</td>
<td>He2019</td>
<td>CRNN</td>
<td>474273</td>
<td>Microphone Array</td>
<td>phase and magnitude spectrogram, Mel-spectrogram</td>
<td>time and frequency masking (SpecAugment)</td>
<td>48kHz</td>
</tr>
<tr>
<td>13</td>
<td>Cao_Surrey_task3_3</td>
<td>Cao2019</td>
<td>CRNN11, ensemble</td>
<td>7071969</td>
<td>both</td>
<td>log mel and intensity vector</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>14</td>
<td>Xue_JDAI_task3_2</td>
<td>Xue2019</td>
<td>CRNN, ensemble</td>
<td>2641263</td>
<td>Microphone Array</td>
<td>Log-Mel, Constant Q-transform, raw phase spectrogram, smoothed cross-power angular spectra</td>
<td>speed perturbtion</td>
<td>48kHz</td>
</tr>
<tr>
<td>15</td>
<td>He_THU_task3_3</td>
<td>He2019</td>
<td>CRNN</td>
<td>474273</td>
<td>Microphone Array</td>
<td>phase and magnitude spectrogram, Mel-spectrogram</td>
<td>time and frequency masking (SpecAugment)</td>
<td>48kHz</td>
</tr>
<tr>
<td>16</td>
<td>Cao_Surrey_task3_2</td>
<td>Cao2019</td>
<td>CRNN11, ensemble</td>
<td>7071969</td>
<td>both</td>
<td>log mel and intensity vector</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>17</td>
<td>Nguyen_NTU_task3_3</td>
<td>Nguyen2019</td>
<td>CRNN, ensemble</td>
<td>5620844</td>
<td>Ambisonic</td>
<td>log-mel energies, magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>18</td>
<td>MazzonYasuda_NTT_task3_3</td>
<td>MazzonYasuda2019</td>
<td>CRNN, ResNet, ensemble</td>
<td>134896144</td>
<td>both</td>
<td>log-mel energies, GCC</td>
<td>FOA domain spatial augmentation</td>
<td>32kHz</td>
</tr>
<tr>
<td>19</td>
<td>Chang_HYU_task3_3</td>
<td>Chang2019</td>
<td>CRNN (for SAD), CRNN (for SED), CNN (for SEL)</td>
<td>280876424</td>
<td>Microphone Array</td>
<td>Multi-resolution Cochleagram (for SAD), log-mel spectrogram (for SED), generalized cross-correlation phase transform (for SEL)</td>
<td>pitch shifting, audio file mixing</td>
<td>48kHz</td>
</tr>
<tr>
<td>20</td>
<td>Nguyen_NTU_task3_4</td>
<td>Nguyen2019</td>
<td>CRNN, ensemble</td>
<td>4382721</td>
<td>Ambisonic</td>
<td>log-mel energies, magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>21</td>
<td>Chang_HYU_task3_4</td>
<td>Chang2019</td>
<td>CRNN (for SAD), CRNN (for SED), CNN (for SEL)</td>
<td>280941952</td>
<td>Microphone Array</td>
<td>Multi-resolution Cochleagram (for SAD), log-mel spectrogram (for SED), generalized cross-correlation phase transform (for SEL)</td>
<td>pitch shifting, audio file mixing</td>
<td>48kHz</td>
</tr>
<tr>
<td>22</td>
<td>MazzonYasuda_NTT_task3_2</td>
<td>MazzonYasuda2019</td>
<td>CRNN, ResNet, ensemble</td>
<td>539584576</td>
<td>both</td>
<td>log-mel energies, GCC</td>
<td>FOA domain spatial augmentation</td>
<td>32kHz</td>
</tr>
<tr>
<td>23</td>
<td>Chang_HYU_task3_2</td>
<td>Chang2019</td>
<td>CRNN (for SAD), CRNN (for SED), CNN (for SEL)</td>
<td>280941952</td>
<td>Ambisonic</td>
<td>Multi-resolution Cochleagram (for SAD), log-mel spectrogram (for SED), generalized cross-correlation phase transform (for SEL)</td>
<td>pitch shifting, audio file mixing</td>
<td>48kHz</td>
</tr>
<tr>
<td>24</td>
<td>Chang_HYU_task3_1</td>
<td>Chang2019</td>
<td>CRNN (for SAD), CRNN (for SED), CNN (for SEL)</td>
<td>280876424</td>
<td>Ambisonic</td>
<td>Multi-resolution Cochleagram (for SAD), log-mel spectrogram (for SED), generalized cross-correlation phase transform (for SEL)</td>
<td>pitch shifting, audio file mixing</td>
<td>48kHz</td>
</tr>
<tr>
<td>25</td>
<td>MazzonYasuda_NTT_task3_1</td>
<td>MazzonYasuda2019</td>
<td>CRNN, ResNet, ensemble</td>
<td>539584576</td>
<td>both</td>
<td>log-mel energies, GCC</td>
<td>FOA domain spatial augmentation</td>
<td>32kHz</td>
</tr>
<tr>
<td>26</td>
<td>Ranjan_NTU_task3_3</td>
<td>Ranjan2019</td>
<td>Ensemble  of 4 ResNet RNN models</td>
<td>12315976</td>
<td>Microphone Array</td>
<td>phase and log magnitude melspectrogram</td>
<td>time shifting</td>
<td>48kHz</td>
</tr>
<tr>
<td>27</td>
<td>Ranjan_NTU_task3_4</td>
<td>Ranjan2019</td>
<td>ResNet RNN trained using all 4 splits</td>
<td>3078994</td>
<td>Microphone Array</td>
<td>phase and log magnitude melspectrogram</td>
<td>time shifting</td>
<td>48kHz</td>
</tr>
<tr>
<td>28</td>
<td>Park_ETRI_task3_1</td>
<td>Park2019</td>
<td>CRNN, TrellisNet</td>
<td>23116833</td>
<td>both</td>
<td>log mel-band energy and mel-band intensity vector</td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td>29</td>
<td>Nguyen_NTU_task3_1</td>
<td>Nguyen2019</td>
<td>CRNN</td>
<td>1008235</td>
<td>Ambisonic</td>
<td>log-mel energies, magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>30</td>
<td>Leung_DBS_task3_2</td>
<td>Leung2019</td>
<td>CRNN, ensemble</td>
<td>3193745</td>
<td>Ambisonic</td>
<td>complex spectrograms, phase and log magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>31</td>
<td>Park_ETRI_task3_2</td>
<td>Park2019</td>
<td>CRNN, TrellisNet</td>
<td>23116833</td>
<td>both</td>
<td>log mel-band energy and mel-band intensity vector</td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td>32</td>
<td>Grondin_MIT_task3_1</td>
<td>Grondin2019</td>
<td>CRNN, ensemble</td>
<td>18191014</td>
<td>Microphone Array</td>
<td>phase and magnitude spectrogram, GCC, TDOA</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>33</td>
<td>Leung_DBS_task3_1</td>
<td>Leung2019</td>
<td>CRNN, ensemble</td>
<td>1891635</td>
<td>Ambisonic</td>
<td>complex spectrograms, phase and log magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>34</td>
<td>Park_ETRI_task3_3</td>
<td>Park2019</td>
<td>CRNN</td>
<td>21147681</td>
<td>both</td>
<td>log mel-band energy and mel-band intensity vector</td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td>35</td>
<td>MazzonYasuda_NTT_task3_4</td>
<td>MazzonYasuda2019</td>
<td>CRNN, ResNet, ensemble</td>
<td>16862018</td>
<td>both</td>
<td>log-mel energies, GCC</td>
<td>FOA domain spatial augmentation</td>
<td>32kHz</td>
</tr>
<tr>
<td>36</td>
<td>Park_ETRI_task3_4</td>
<td>Park2019</td>
<td>CRNN</td>
<td>21147681</td>
<td>both</td>
<td>log mel-band energy and mel-band intensity vector</td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td>37</td>
<td>Ranjan_NTU_task3_1</td>
<td>Ranjan2019</td>
<td>ResNet_RNN</td>
<td>3076690</td>
<td>Microphone Array</td>
<td>phase and log magnitude melspectrogram</td>
<td>time shifting</td>
<td>48kHz</td>
</tr>
<tr>
<td>38</td>
<td>Ranjan_NTU_task3_2</td>
<td>Ranjan2019</td>
<td>ResNet_RNN</td>
<td>3078994</td>
<td>Microphone Array</td>
<td>phase and log magnitude melspectrogram</td>
<td>time shifting</td>
<td>48kHz</td>
</tr>
<tr>
<td>39</td>
<td>ZhaoLu_UESTC_task3_1</td>
<td>ZhaoLu2019</td>
<td>CNN+LSTM</td>
<td>7862177</td>
<td>Microphone Array</td>
<td>LogMel spectrogram and GCC</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>40</td>
<td>Rough_EMED_task3_2</td>
<td>Rough2019</td>
<td>CRNN</td>
<td>116118</td>
<td>Microphone Array</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>41</td>
<td>Nguyen_NTU_task3_2</td>
<td>Nguyen2019</td>
<td>CRNN</td>
<td>1008235</td>
<td>Ambisonic</td>
<td>log-mel energies, magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>42</td>
<td>Jee_NTU_task3_2</td>
<td>Jee2019</td>
<td>CRNN</td>
<td>10209057</td>
<td>Microphone Array</td>
<td>log-mel spectrogram and GCC-PHAT</td>
<td>mixup</td>
<td>34 kHz</td>
</tr>
<tr>
<td>43</td>
<td>Tan_NTU_task3_1</td>
<td>Tan2019</td>
<td>ResNet RNN + TDOA</td>
<td>116118</td>
<td>Microphone Array</td>
<td>log mel spectrogram,  TDOA</td>
<td>frame shifting</td>
<td>48kHz</td>
</tr>
<tr>
<td>44</td>
<td>Lewandowski_SRPOL_task3_1</td>
<td>Kapka2019</td>
<td>CRNN</td>
<td>508574</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td>time / frequency masking, time warping (specAugment)</td>
<td>48kHz</td>
</tr>
<tr>
<td>45</td>
<td>Cordourier_IL_task3_2</td>
<td>Cordourier2019</td>
<td>Four CRNNs in ensamble</td>
<td>731280</td>
<td>Microphone Array</td>
<td>phase, magnitude spectrogram and GCC</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>46</td>
<td>Cordourier_IL_task3_1</td>
<td>Cordourier2019</td>
<td>CRNN</td>
<td>182820</td>
<td>Microphone Array</td>
<td>phase, magnitude spectrogram and GCC</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>47</td>
<td>Krause_AGH_task3_4</td>
<td>Krause2019</td>
<td>CRNN, ensemble</td>
<td>615626</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr data-hline="true">
<td>48</td>
<td>DCASE2019_FOA_baseline</td>
<td>Adavanne2019</td>
<td>CRNN</td>
<td></td>
<td>Ambisonic</td>
<td>Phase and Magnitude Spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>49</td>
<td>Perezlopez_UPF_task3_1</td>
<td>Perezlopez2019</td>
<td>CRNN</td>
<td>150000</td>
<td>Ambisonic</td>
<td>DOA, diffuseness, DOA variance, energy density</td>
<td>mixup</td>
<td>48kHz</td>
</tr>
<tr>
<td>50</td>
<td>Chytas_UTH_task3_1</td>
<td>Chytas2019</td>
<td>CNN, ensemble</td>
<td>100420239</td>
<td>Microphone Array</td>
<td>raw audio data, spectrogram</td>
<td>channel permutations, segment addition</td>
<td>16kHz</td>
</tr>
<tr>
<td>51</td>
<td>Anemueller_UOL_task3_3</td>
<td>Anemueller2019</td>
<td>CRNN</td>
<td>750137</td>
<td>Ambisonic</td>
<td>group-delay and magnitude spectrogram, each in Mel-bands and FFT-bands</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>52</td>
<td>Chytas_UTH_task3_2</td>
<td>Chytas2019</td>
<td>CNN, ensemble</td>
<td>100420239</td>
<td>Microphone Array</td>
<td>raw audio data, spectrogram</td>
<td>channel permutations, segment addition</td>
<td>16kHz</td>
</tr>
<tr>
<td>53</td>
<td>Krause_AGH_task3_2</td>
<td>Krause2019</td>
<td>CRNN</td>
<td>282089</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>54</td>
<td>Krause_AGH_task3_1</td>
<td>Krause2019</td>
<td>CRNN</td>
<td>333537</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>55</td>
<td>Anemueller_UOL_task3_1</td>
<td>Anemueller2019</td>
<td>CRNN</td>
<td>552761</td>
<td>Ambisonic</td>
<td>group-delay and magnitude spectrogram, each in Mel-bands and FFT-bands</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>56</td>
<td>Kong_SURREY_task3_1</td>
<td>Kong2019</td>
<td>CNN</td>
<td>4686144</td>
<td>Ambisonic</td>
<td>magnitude spectrogram</td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td>57</td>
<td>Anemueller_UOL_task3_2</td>
<td>Anemueller2019</td>
<td>CRNN</td>
<td>552761</td>
<td>Ambisonic</td>
<td>group-delay and magnitude spectrogram, each in Mel-bands and FFT-bands</td>
<td></td>
<td>48kHz</td>
</tr>
<tr data-hline="true">
<td>58</td>
<td>DCASE2019_MIC_baseline</td>
<td>Adavanne2019</td>
<td>CRNN</td>
<td></td>
<td>Microphone Array</td>
<td>Phase and Magnitude Spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
<tr>
<td>59</td>
<td>Lin_YYZN_task3_1</td>
<td>Lin2019</td>
<td>CRNN</td>
<td>613537</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td>60</td>
<td>Krause_AGH_task3_3</td>
<td>Krause2019</td>
<td>CRNN</td>
<td>429857</td>
<td>Ambisonic</td>
<td>phase and magnitude spectrogram</td>
<td></td>
<td>48kHz</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2019/technical_reports_task3.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Adavanne2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Adavanne2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A MULTI-ROOM REVERBERANT DATASET FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Sharath Adavanne, Archontis Politis, Tuomas Virtanen
       </p>
<p style="text-align:left">
<em>
         Tampere University, Finland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2019_FOA_baseline</span> <span class="label label-primary">DCASE2019_MIC_baseline</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Adavanne2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Adavanne2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Adavanne2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Adavanne.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Adavanne2019').collapse('show');window.location.hash='#Adavanne2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Adavanne2019" class="panel-collapse collapse" id="collapse-Adavanne2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A MULTI-ROOM REVERBERANT DATASET FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Sharath Adavanne, Archontis Politis, Tuomas Virtanen
       </small>
<br/>
<small>
<em>
         Tampere University, Finland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset with each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate bench mark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Adavanne2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Adavanne.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/sharathadavanne/seld-dcase2019" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Adavanne2019label" class="modal fade" id="bibtex-Adavanne2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAdavanne2019label">
        A MULTI-ROOM REVERBERANT DATASET FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Adavanne2019,
    Author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    title = "A MULTI-ROOM REVERBERANT DATASET FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset with each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate bench mark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Anemueller2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Anemueller2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GROUP DELAY FEATURES FOR SOUND EVENT DETECTION AND LOCALIZATION (TASK 3) OF THE DCASE 2019 CHALLENGE
       </h4>
<p style="text-align:left">
        Eike Nustede, Jorn Anemuller
       </p>
<p style="text-align:left">
<em>
         University of Oldenburg
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Anemueller_UOL_task3_1</span> <span class="label label-primary">Anemueller_UOL_task3_2</span> <span class="label label-primary">Anemueller_UOL_task3_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Anemueller2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Anemueller2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Anemueller2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Anemueller_96.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Anemueller2019" class="panel-collapse collapse" id="collapse-Anemueller2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GROUP DELAY FEATURES FOR SOUND EVENT DETECTION AND LOCALIZATION (TASK 3) OF THE DCASE 2019 CHALLENGE
      </h4>
<p style="text-align:left">
<small>
        Eike Nustede, Jorn Anemuller
       </small>
<br/>
<small>
<em>
         University of Oldenburg
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event localization algorithms utilize features that encode a sourceâ€™s time-difference of arrival across an array of microphones. Direct encoding of a signalâ€™s phase in sub-bands is a common representation that is not without its shortcomings, since phase as a circular variable is prone to 2Ï€-wrapping and systematic phase-advancement across frequencies. Group delay encoding may constitute a more robust feature for data-driven algorithms as it represents time-delays of the signalâ€™s spectral-band envelopes. Computed through derivation of phase across frequency, it is in practice characterized by a lower degree of variability, resulting in reduced wrapping and to some extent permitting the computation of average group delay across (e.g., Mel-scaled) bands. The present contribution incorporates group delay features into the baseline system of DCASE 2019 task3, supplementing them with amplitude features. System setup is based on the provided baseline systemâ€™s convolutional recurrent neural network architecure with some variation of its topology.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Anemueller2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Anemueller_96.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Anemueller2019label" class="modal fade" id="bibtex-Anemueller2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAnemueller2019label">
        GROUP DELAY FEATURES FOR SOUND EVENT DETECTION AND LOCALIZATION (TASK 3) OF THE DCASE 2019 CHALLENGE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Anemueller2019,
    Author = "Nustede, Eike and Anemuller, Jorn",
    title = "GROUP DELAY FEATURES FOR SOUND EVENT DETECTION AND LOCALIZATION (TASK 3) OF THE DCASE 2019 CHALLENGE",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Sound event localization algorithms utilize features that encode a sourceâ€™s time-difference of arrival across an array of microphones. Direct encoding of a signalâ€™s phase in sub-bands is a common representation that is not without its shortcomings, since phase as a circular variable is prone to 2Ï€-wrapping and systematic phase-advancement across frequencies. Group delay encoding may constitute a more robust feature for data-driven algorithms as it represents time-delays of the signalâ€™s spectral-band envelopes. Computed through derivation of phase across frequency, it is in practice characterized by a lower degree of variability, resulting in reduced wrapping and to some extent permitting the computation of average group delay across (e.g., Mel-scaled) bands. The present contribution incorporates group delay features into the baseline system of DCASE 2019 task3, supplementing them with amplitude features. System setup is based on the provided baseline systemâ€™s convolutional recurrent neural network architecure with some variation of its topology."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cao2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Cao2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        TWO-STAGE SOUND EVENT LOCALIZATION AND DETECTION USING INTENSITY VECTOR AND GENERALIZED CROSS-CORRELATION
       </h4>
<p style="text-align:left">
        Yin Cao, Turab Iqbal, Qiuqiang Kong, Miguel Galindo, Wenwu Wang, Mark Plumbley
       </p>
<p style="text-align:left">
<em>
         University of Surrey
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cao_Surrey_task3_1</span> <span class="label label-primary">Cao_Surrey_task3_2</span> <span class="label label-primary">Cao_Surrey_task3_3</span> <span class="label label-primary">Cao_Surrey_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cao2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cao2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cao2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Cao_74.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Cao2019').collapse('show');window.location.hash='#Cao2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cao2019" class="panel-collapse collapse" id="collapse-Cao2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       TWO-STAGE SOUND EVENT LOCALIZATION AND DETECTION USING INTENSITY VECTOR AND GENERALIZED CROSS-CORRELATION
      </h4>
<p style="text-align:left">
<small>
        Yin Cao, Turab Iqbal, Qiuqiang Kong, Miguel Galindo, Wenwu Wang, Mark Plumbley
       </small>
<br/>
<small>
<em>
         University of Surrey
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event localization and detection (SELD) refers to the spatial and temporal localization of sound events in addition to classification. The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Task 3 introduces a strongly labelled dataset to address this problem. In this report, a two-stage polyphonic sound event detection and localization method. The method utilizes log mel features for event detection, and uses intensity vector and GCC features for localization. Intensity vector and GCC features use the supplied Ambisonic and microphone array signals, respectively. This method trains SED first, after which the learned feature layers are transferred for direction of arrival (DOA) estimation. It then uses the SED ground truth as a mask to train DOA estimation. Experimental results show that the proposed method is able to localize and detect overlapping sound events in different environments. It is also able to improve the performance of both SED and DOA estimation, and performs significantly better than the baseline method.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cao2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Cao_74.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/yinkalario/DCASE2019-TASK3" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cao2019label" class="modal fade" id="bibtex-Cao2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCao2019label">
        TWO-STAGE SOUND EVENT LOCALIZATION AND DETECTION USING INTENSITY VECTOR AND GENERALIZED CROSS-CORRELATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cao2019,
    Author = "Cao, Yin and Iqbal, Turab and Kong, Qiuqiang and Galindo, Miguel and Wang, Wenwu and Plumbley, Mark",
    title = "TWO-STAGE SOUND EVENT LOCALIZATION AND DETECTION USING INTENSITY VECTOR AND GENERALIZED CROSS-CORRELATION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Sound event localization and detection (SELD) refers to the spatial and temporal localization of sound events in addition to classification. The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Task 3 introduces a strongly labelled dataset to address this problem. In this report, a two-stage polyphonic sound event detection and localization method. The method utilizes log mel features for event detection, and uses intensity vector and GCC features for localization. Intensity vector and GCC features use the supplied Ambisonic and microphone array signals, respectively. This method trains SED first, after which the learned feature layers are transferred for direction of arrival (DOA) estimation. It then uses the SED ground truth as a mask to train DOA estimation. Experimental results show that the proposed method is able to localize and detect overlapping sound events in different environments. It is also able to improve the performance of both SED and DOA estimation, and performs significantly better than the baseline method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Chang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        THREE-STAGE APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Kyoungjin Noh, Choi Jeong-Hwan, Jeon Dongyeop, Chang Joon-Hyuk
       </p>
<p style="text-align:left">
<em>
         Hanyang University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chang_HYU_task3_1</span> <span class="label label-primary">Chang_HYU_task3_2</span> <span class="label label-primary">Chang_HYU_task3_3</span> <span class="label label-primary">Chang_HYU_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chang_81.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chang2019" class="panel-collapse collapse" id="collapse-Chang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       THREE-STAGE APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Kyoungjin Noh, Choi Jeong-Hwan, Jeon Dongyeop, Chang Joon-Hyuk
       </small>
<br/>
<small>
<em>
         Hanyang University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes our three-stage approach system for sound event localization and detection (SELD) task. The system consists of three parts: sound activity detection (SAD), sound event detection (SED), and sound event localization (SEL). Firstly, we employ the multi-resolution cochleagram (MRCG) from 4-channel audio and convolutional recurrent neural network (CRNN) model to detect sound activity. Secondly, we extract log mel-spectrogram from 4-channel audio, harmonic percussive source separation (HPSS) audio, mono audio, and train another CRNN model. Lastly, we exploit the generalized cross-correlation phase transform (GCC-PHAT) of each microphone pairs as an input feature of the convolutional neural network (CNN) model for the SEL. Then we combine SAD, SED, and SEL results to obtain the final prediction for the SELD task. To augment overlapped frames that degrade overall performance, we randomly select two non-overlapped audio files and mix them. We also average the predictions of several models to improve the result. Experimental results on the four cross-validation splits for the TAU Spatial Sound Events 2019-Microphone Array dataset are error rate: 0.23, F score: 85.91\%, DOA error: 3.62-degree , and frame recall: 88.66\%, respectively.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chang_81.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chang2019label" class="modal fade" id="bibtex-Chang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChang2019label">
        THREE-STAGE APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chang2019,
    Author = "Noh, Kyoungjin and Jeong-Hwan, Choi and Dongyeop, Jeon and Joon-Hyuk, Chang",
    title = "THREE-STAGE APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper describes our three-stage approach system for sound event localization and detection (SELD) task. The system consists of three parts: sound activity detection (SAD), sound event detection (SED), and sound event localization (SEL). Firstly, we employ the multi-resolution cochleagram (MRCG) from 4-channel audio and convolutional recurrent neural network (CRNN) model to detect sound activity. Secondly, we extract log mel-spectrogram from 4-channel audio, harmonic percussive source separation (HPSS) audio, mono audio, and train another CRNN model. Lastly, we exploit the generalized cross-correlation phase transform (GCC-PHAT) of each microphone pairs as an input feature of the convolutional neural network (CNN) model for the SEL. Then we combine SAD, SED, and SEL results to obtain the final prediction for the SELD task. To augment overlapped frames that degrade overall performance, we randomly select two non-overlapped audio files and mix them. We also average the predictions of several models to improve the result. Experimental results on the four cross-validation splits for the TAU Spatial Sound Events 2019-Microphone Array dataset are error rate: 0.23, F score: 85.91\\%, DOA error: 3.62-degree , and frame recall: 88.66\\%, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chytas2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Chytas2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        HIERARCHICAL DETECTION OF SOUND EVENTS AND THEIR LOCALIZATION USING CONVOLUTIONAL NEURAL NETWORKS WITH ADAPTIVE THRESHOLDS
       </h4>
<p style="text-align:left">
        Sotirios Panagiotis Chytas, Gerasimos Potamianos
       </p>
<p style="text-align:left">
<em>
         University of Thessaly
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chytas_UTH_task3_1</span> <span class="label label-primary">Chytas_UTH_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chytas2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chytas2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chytas2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chytas_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chytas2019" class="panel-collapse collapse" id="collapse-Chytas2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       HIERARCHICAL DETECTION OF SOUND EVENTS AND THEIR LOCALIZATION USING CONVOLUTIONAL NEURAL NETWORKS WITH ADAPTIVE THRESHOLDS
      </h4>
<p style="text-align:left">
<small>
        Sotirios Panagiotis Chytas, Gerasimos Potamianos
       </small>
<br/>
<small>
<em>
         University of Thessaly
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The paper details our approach to address Task 3 of the DCASE 2019 Challenge, namely that of sound event localization and detection (SELD). Our developed system is based on multi-channel convolutional neural networks (CNNs), combined with data augmentation and ensembling. In particular, it follows a hierarchical approach that first determines adaptive thresholds for the multi-label sound event detection (SED) problem, based on a CNN operating on spectrograms over long-duration windows. It subsequently exploits the derived thresholds in an ensemble of CNNs operating on raw waveforms over shorter-duration sliding windows to provide the desired event segmentation and labeling. Finally, it employs a separate event localization CNN to yield direction-of-arrival (DOA) source estimates of the detected sound events. The system is evaluated on the microphone-array development dataset of the SELD Task. Compared to the corresponding baseline provided by the Challenge organizers, it achieves relative improvements of 12\% in SED error, 2\% in F-score, 36\% in DOA error, and 3\% in the combined SELD metric, but trails significantly in frame-recall.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chytas2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chytas_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chytas2019label" class="modal fade" id="bibtex-Chytas2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChytas2019label">
        HIERARCHICAL DETECTION OF SOUND EVENTS AND THEIR LOCALIZATION USING CONVOLUTIONAL NEURAL NETWORKS WITH ADAPTIVE THRESHOLDS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chytas2019,
    Author = "Chytas, Sotirios Panagiotis and Potamianos, Gerasimos",
    title = "HIERARCHICAL DETECTION OF SOUND EVENTS AND THEIR LOCALIZATION USING CONVOLUTIONAL NEURAL NETWORKS WITH ADAPTIVE THRESHOLDS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The paper details our approach to address Task 3 of the DCASE 2019 Challenge, namely that of sound event localization and detection (SELD). Our developed system is based on multi-channel convolutional neural networks (CNNs), combined with data augmentation and ensembling. In particular, it follows a hierarchical approach that first determines adaptive thresholds for the multi-label sound event detection (SED) problem, based on a CNN operating on spectrograms over long-duration windows. It subsequently exploits the derived thresholds in an ensemble of CNNs operating on raw waveforms over shorter-duration sliding windows to provide the desired event segmentation and labeling. Finally, it employs a separate event localization CNN to yield direction-of-arrival (DOA) source estimates of the detected sound events. The system is evaluated on the microphone-array development dataset of the SELD Task. Compared to the corresponding baseline provided by the Challenge organizers, it achieves relative improvements of 12\\% in SED error, 2\\% in F-score, 36\\% in DOA error, and 3\\% in the combined SELD metric, but trails significantly in frame-recall."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cordourier2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Cordourier2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GCC-PHAT CROSS-CORRELATION AUDIO FEATURES FOR SIMULTANEOUS SOUND EVENT LOCALIZATION AND DETECTION (SELD) ON MULTIPLE ROOMS
       </h4>
<p style="text-align:left">
        Hector Cordourier Maruri, Paulo Lopez Meyer, Jonathan Huang, Juan Antonio Del Hoyo Ontiveros, Hong Lu
       </p>
<p style="text-align:left">
<em>
         Intel Corporation
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cordourier_IL_task3_1</span> <span class="label label-primary">Cordourier_IL_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cordourier2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cordourier2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cordourier2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Cordourier_89.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cordourier2019" class="panel-collapse collapse" id="collapse-Cordourier2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GCC-PHAT CROSS-CORRELATION AUDIO FEATURES FOR SIMULTANEOUS SOUND EVENT LOCALIZATION AND DETECTION (SELD) ON MULTIPLE ROOMS
      </h4>
<p style="text-align:left">
<small>
        Hector Cordourier Maruri, Paulo Lopez Meyer, Jonathan Huang, Juan Antonio Del Hoyo Ontiveros, Hong Lu
       </small>
<br/>
<small>
<em>
         Intel Corporation
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this work, we show a simultaneous sound event localization and detection (SELD) system, with enhanced acoustic features, in which we propose using the well-known Generalized Cross Correlation (GCC) PATH algorithm, to augment the magnitude and phase regular Fourier spectra features at each frame. GCC has already been used for some time to calculate the Time Difference of Arrival (TDOA) in simultaneous audio signals, in moderately reverberent environments, using classic signal processing techniques, and can assist audio source localization in current deep learning machines. The neural net architecture we used is a Convolutional Recurrent Neural Network (CRNN), and is tested using the sound database prepared for the Task 3 of the 2019 DCASE Challenge. Our proposed system is able to achieve 20.4 of direction of arrival error, 86.4\% frame recall, 87.1\% F-score and 0.20 error rate detection in testing samples.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cordourier2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Cordourier_89.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cordourier2019label" class="modal fade" id="bibtex-Cordourier2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCordourier2019label">
        GCC-PHAT CROSS-CORRELATION AUDIO FEATURES FOR SIMULTANEOUS SOUND EVENT LOCALIZATION AND DETECTION (SELD) ON MULTIPLE ROOMS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cordourier2019,
    Author = "Maruri, Hector Cordourier and Meyer, Paulo Lopez and Huang, Jonathan and Ontiveros, Juan Antonio Del Hoyo and Lu, Hong",
    title = "GCC-PHAT CROSS-CORRELATION AUDIO FEATURES FOR SIMULTANEOUS SOUND EVENT LOCALIZATION AND DETECTION (SELD) ON MULTIPLE ROOMS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this work, we show a simultaneous sound event localization and detection (SELD) system, with enhanced acoustic features, in which we propose using the well-known Generalized Cross Correlation (GCC) PATH algorithm, to augment the magnitude and phase regular Fourier spectra features at each frame. GCC has already been used for some time to calculate the Time Difference of Arrival (TDOA) in simultaneous audio signals, in moderately reverberent environments, using classic signal processing techniques, and can assist audio source localization in current deep learning machines. The neural net architecture we used is a Convolutional Recurrent Neural Network (CRNN), and is tested using the sound database prepared for the Task 3 of the 2019 DCASE Challenge. Our proposed system is able to achieve 20.4 of direction of arrival error, 86.4\\% frame recall, 87.1\\% F-score and 0.20 error rate detection in testing samples."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Grondin2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Grondin2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT LOCALIZATION AND DETECTION USING CRNN ON PAIRS OF MICROPHONES
       </h4>
<p style="text-align:left">
        Francois Grondin, James Glass, Iwona Sobieraj, Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         University of Surrey, Massachusetts Institute of Technology
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Grondin_MIT_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Grondin2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Grondin2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Grondin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Grondin_71.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Grondin2019" class="panel-collapse collapse" id="collapse-Grondin2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT LOCALIZATION AND DETECTION USING CRNN ON PAIRS OF MICROPHONES
      </h4>
<p style="text-align:left">
<small>
        Francois Grondin, James Glass, Iwona Sobieraj, Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         University of Surrey, Massachusetts Institute of Technology
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper proposes a sound event localization and detection from multichannel recording method. The proposed system is based on two Convolutional Recurrent Neural Networks (CRNN) to perform sound event detection (SED) and time difference of arrival (TDOA) estimation on each pair of microphones of a microphone array. In this paper, the system is evaluated with a four-microphone array, and thus combines the results from six pairs of microphones to provide a final classification and a 3-D direction of arrival (DOA) estimate. Results demonstrate that the proposed approach outperforms the DCASE 2019 baseline system.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Grondin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Grondin_71.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Grondin2019label" class="modal fade" id="bibtex-Grondin2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGrondin2019label">
        SOUND EVENT LOCALIZATION AND DETECTION USING CRNN ON PAIRS OF MICROPHONES
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Grondin2019,
    Author = "Grondin, Francois and Glass, James and Sobieraj, Iwona and Plumbley, Mark D.",
    title = "SOUND EVENT LOCALIZATION AND DETECTION USING CRNN ON PAIRS OF MICROPHONES",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper proposes a sound event localization and detection from multichannel recording method. The proposed system is based on two Convolutional Recurrent Neural Networks (CRNN) to perform sound event detection (SED) and time difference of arrival (TDOA) estimation on each pair of microphones of a microphone array. In this paper, the system is evaluated with a four-microphone array, and thus combines the results from six pairs of microphones to provide a final classification and a 3-D direction of arrival (DOA) estimate. Results demonstrate that the proposed approach outperforms the DCASE 2019 baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="He2019" style="box-shadow: none">
<div class="panel-heading" id="heading-He2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DATA AUGMENTATION AND PRIOR KNOWLEDGE-BASED REGULARIZATION FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Jingyang Zhang, Wenhao Ding, Liang He
       </p>
<p style="text-align:left">
<em>
         Tsinghua University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">He_THU_task3_1</span> <span class="label label-primary">He_THU_task3_2</span> <span class="label label-primary">He_THU_task3_3</span> <span class="label label-primary">He_THU_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-He2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-He2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-He2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_He_97.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-He2019" class="panel-collapse collapse" id="collapse-He2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DATA AUGMENTATION AND PRIOR KNOWLEDGE-BASED REGULARIZATION FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Jingyang Zhang, Wenhao Ding, Liang He
       </small>
<br/>
<small>
<em>
         Tsinghua University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The goal of sound event localization and detection (SELD) is detecting the presence of polyphonic sound events and identifying the sources of those events at the same time. In this paper, we propose an entire pipeline, which contains data augmentation, network prediction and post-processing stage, to deal with the SELD task. In data augmentation part, we expand the official dataset with SpecAugment [1]. In network prediction part, we train the event detection network and the localization network separately, and utilize the prediction of events to output localization prediction for active frames. In post-processing part, we propose a prior knowledge based regularization (PKR), which calculates the average value of the localization prediction of each event segment and replace the prediction of this event with this average value. We theoretically prove that this technique could reduce mean square error (MSE). After evaluating our system on DCASE 2019 Challenge Task 3 Development Dataset, we approximately achieve a 59\% reduction in SED error rate (ER) and a 13\% reduction in directions-of-arrival(DOA) error over the baseline system (on Ambisonic dataset).
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-He2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_He_97.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-He2019label" class="modal fade" id="bibtex-He2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHe2019label">
        DATA AUGMENTATION AND PRIOR KNOWLEDGE-BASED REGULARIZATION FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{He2019,
    Author = "Zhang, Jingyang and Ding, Wenhao and He, Liang",
    title = "DATA AUGMENTATION AND PRIOR KNOWLEDGE-BASED REGULARIZATION FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The goal of sound event localization and detection (SELD) is detecting the presence of polyphonic sound events and identifying the sources of those events at the same time. In this paper, we propose an entire pipeline, which contains data augmentation, network prediction and post-processing stage, to deal with the SELD task. In data augmentation part, we expand the official dataset with SpecAugment [1]. In network prediction part, we train the event detection network and the localization network separately, and utilize the prediction of events to output localization prediction for active frames. In post-processing part, we propose a prior knowledge based regularization (PKR), which calculates the average value of the localization prediction of each event segment and replace the prediction of this event with this average value. We theoretically prove that this technique could reduce mean square error (MSE). After evaluating our system on DCASE 2019 Challenge Task 3 Development Dataset, we approximately achieve a 59\\% reduction in SED error rate (ER) and a 13\\% reduction in directions-of-arrival(DOA) error over the baseline system (on Ambisonic dataset)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jee2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Jee2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT LOCALIZATION AND DETECTION USING CONVOLUTIONAL RECURRENT NEURAL NETWORK
       </h4>
<p style="text-align:left">
        Wen Jie Jee, Rohith Mars, Pranay Pratik, Srikanth Nagisetty, Chong Soon Lim
       </p>
<p style="text-align:left">
<em>
         Nanyang Technological University, Panasonic R&amp;D Center Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jee_NTU_task3_1</span> <span class="label label-primary">Jee_NTU_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jee2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jee2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jee2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Jee_104.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jee2019" class="panel-collapse collapse" id="collapse-Jee2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT LOCALIZATION AND DETECTION USING CONVOLUTIONAL RECURRENT NEURAL NETWORK
      </h4>
<p style="text-align:left">
<small>
        Wen Jie Jee, Rohith Mars, Pranay Pratik, Srikanth Nagisetty, Chong Soon Lim
       </small>
<br/>
<small>
<em>
         Nanyang Technological University, Panasonic R&amp;D Center Singapore
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report details the methods used in the development set of DCASE 2019 Task 3, and the results of the investigations. Data augmentation mixup was used in an attempt to train the model for greater generalization. The kernel size of the pooling layers were also modified to a more intuitive size. In addition, different kernel sizes of the convolutional layers were also investigated and results are reported. Our best model achieved an F-score of 91.9\% and a DOA error of 4.588-degree on the development set, which showed an improvement of 10\% and about 25-degree, respectively compared to the baseline system
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jee2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Jee_104.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jee2019label" class="modal fade" id="bibtex-Jee2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJee2019label">
        SOUND EVENT LOCALIZATION AND DETECTION USING CONVOLUTIONAL RECURRENT NEURAL NETWORK
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jee2019,
    Author = "Jee, Wen Jie and Mars, Rohith and Pratik, Pranay and Nagisetty, Srikanth and Lim, Chong Soon",
    title = "SOUND EVENT LOCALIZATION AND DETECTION USING CONVOLUTIONAL RECURRENT NEURAL NETWORK",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report details the methods used in the development set of DCASE 2019 Task 3, and the results of the investigations. Data augmentation mixup was used in an attempt to train the model for greater generalization. The kernel size of the pooling layers were also modified to a more intuitive size. In addition, different kernel sizes of the convolutional layers were also investigated and results are reported. Our best model achieved an F-score of 91.9\\% and a DOA error of 4.588-degree on the development set, which showed an improvement of 10\\% and about 25-degree, respectively compared to the baseline system"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kapka2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kapka2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND SOURCE DETECTION, LOCALIZATION AND CLASSIFICATION USING CONSECUTIVE ENSEMBLE OF CRNN MODELS
       </h4>
<p style="text-align:left">
        Slawomir Kapka, Mateusz Lewandowski
       </p>
<p style="text-align:left">
<em>
         Samsung R&amp;D Institute Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lewandowski_SRPOL_task3_1</span> <span class="label label-primary">Kapka_SRPOL_task3_2</span> <span class="label label-primary">Kapka_SRPOL_task3_3</span> <span class="label label-primary">Kapka_SRPOL_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kapka2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kapka2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kapka2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kapka_26.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kapka2019" class="panel-collapse collapse" id="collapse-Kapka2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND SOURCE DETECTION, LOCALIZATION AND CLASSIFICATION USING CONSECUTIVE ENSEMBLE OF CRNN MODELS
      </h4>
<p style="text-align:left">
<small>
        Slawomir Kapka, Mateusz Lewandowski
       </small>
<br/>
<small>
<em>
         Samsung R&amp;D Institute Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our method for DCASE2019 task 3: Sound Event Localization and Detection. We use four CRNN SELDnet-like single output models which run in a consecutive manner to recover all possible information of occurring events. We decompose the SELD task into estimating number of active sources, estimating direction of arrival of a single source, estimating direction of arrival of the second source where the direction of the first one is known and a multi-label classification task. We use custom consecutive ensemble to predict eventsâ€™ onset, offset, direction of arrival and class. The proposed approach is evaluated on the development set of TAU Spatial Sound Events 2019 - Ambisonic.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kapka2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kapka_26.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kapka2019label" class="modal fade" id="bibtex-Kapka2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKapka2019label">
        SOUND SOURCE DETECTION, LOCALIZATION AND CLASSIFICATION USING CONSECUTIVE ENSEMBLE OF CRNN MODELS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kapka2019,
    Author = "Kapka, Slawomir and Lewandowski, Mateusz",
    title = "SOUND SOURCE DETECTION, LOCALIZATION AND CLASSIFICATION USING CONSECUTIVE ENSEMBLE OF CRNN MODELS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we describe our method for DCASE2019 task 3: Sound Event Localization and Detection. We use four CRNN SELDnet-like single output models which run in a consecutive manner to recover all possible information of occurring events. We decompose the SELD task into estimating number of active sources, estimating direction of arrival of a single source, estimating direction of arrival of the second source where the direction of the first one is known and a multi-label classification task. We use custom consecutive ensemble to predict eventsâ€™ onset, offset, direction of arrival and class. The proposed approach is evaluated on the development set of TAU Spatial Sound Events 2019 - Ambisonic."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Yin Cao, Turab Iqbal, Wenwu Wang, Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         University of Surrey
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kong_SURREY_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2019" class="panel-collapse collapse" id="collapse-Kong2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
      </h4>
<p style="text-align:left">
<small>
        Qiuqiang Kong, Yin Cao, Turab Iqbal, Wenwu Wang, Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         University of Surrey
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2019label" class="modal fade" id="bibtex-Kong2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2019label">
        CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2019,
    Author = "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.",
    title = "CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Krause2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Krause2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        ARBORESCENT NEURAL NETWORK ARCHITECTURES FOR SOUND EVENT DETECTION AND LOCALIZATION
       </h4>
<p style="text-align:left">
        Daniel Krause, Konrad Kowalczyk
       </p>
<p style="text-align:left">
<em>
         AGH University of Science and Technology
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Krause_AGH_task3_1</span> <span class="label label-primary">Krause_AGH_task3_2</span> <span class="label label-primary">Krause_AGH_task3_3</span> <span class="label label-primary">Krause_AGH_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Krause2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Krause2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Krause2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Krause_95.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Krause2019').collapse('show');window.location.hash='#Krause2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Krause2019" class="panel-collapse collapse" id="collapse-Krause2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       ARBORESCENT NEURAL NETWORK ARCHITECTURES FOR SOUND EVENT DETECTION AND LOCALIZATION
      </h4>
<p style="text-align:left">
<small>
        Daniel Krause, Konrad Kowalczyk
       </small>
<br/>
<small>
<em>
         AGH University of Science and Technology
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes our contribution to the task of sound event localization and detection (SELD) using first-order ambisonic signals at the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. Our approach is based on arborescent convolutional recurrent neural networks with the aim to achieve joint localization and detection of overlapping acoustic events. Three submitted systems can be briefly summarized as follows. System 1 splits the neural network into two branches associated with localization and detection tasks. This splitting is performed directly after the first convolutional layer. System 2 utilizes depthwise separable convolutions in order to exploit interchannel dependencies whilst substantially reducing the model complexity. System 3 exhibits a tree-like architecture in which relations between the channels for phase and magnitude are exploited independently in two branches, and they are concatenated before the recurrent layers. Finally, System 4 is based on score fusion of the first two systems.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Krause2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Krause_95.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/danielkrause/DCASE2019_SELD" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Krause2019label" class="modal fade" id="bibtex-Krause2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKrause2019label">
        ARBORESCENT NEURAL NETWORK ARCHITECTURES FOR SOUND EVENT DETECTION AND LOCALIZATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Krause2019,
    Author = "Krause, Daniel and Kowalczyk, Konrad",
    title = "ARBORESCENT NEURAL NETWORK ARCHITECTURES FOR SOUND EVENT DETECTION AND LOCALIZATION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper describes our contribution to the task of sound event localization and detection (SELD) using first-order ambisonic signals at the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. Our approach is based on arborescent convolutional recurrent neural networks with the aim to achieve joint localization and detection of overlapping acoustic events. Three submitted systems can be briefly summarized as follows. System 1 splits the neural network into two branches associated with localization and detection tasks. This splitting is performed directly after the first convolutional layer. System 2 utilizes depthwise separable convolutions in order to exploit interchannel dependencies whilst substantially reducing the model complexity. System 3 exhibits a tree-like architecture in which relations between the channels for phase and magnitude are exploited independently in two branches, and they are concatenated before the recurrent layers. Finally, System 4 is based on score fusion of the first two systems."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Leung2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Leung2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SPECTRUM COMBINATION AND CONVOLUTIONAL RECURRENT NEURAL NETWORKS FOR JOINT LOCALIZATION AND DETECTION OF SOUND EVENTS
       </h4>
<p style="text-align:left">
        Shuangran Leung, Yi Ren
       </p>
<p style="text-align:left">
<em>
         DBSonics
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Leung_DBS_task3_1</span> <span class="label label-primary">Leung_DBS_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Leung2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Leung2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Leung2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Leung_90.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Leung2019" class="panel-collapse collapse" id="collapse-Leung2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SPECTRUM COMBINATION AND CONVOLUTIONAL RECURRENT NEURAL NETWORKS FOR JOINT LOCALIZATION AND DETECTION OF SOUND EVENTS
      </h4>
<p style="text-align:left">
<small>
        Shuangran Leung, Yi Ren
       </small>
<br/>
<small>
<em>
         DBSonics
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this work, we combine existing Short Time Fourier Transforms (STFT) of 4-channel array audio signals to create new features, and show that this augmented input improves the performance of DCASE2019 task 3 baseline system [1] in both sound event detection (SED) and direction-of-arrival (DOA) estimation. Techniques like ensembling and finetuning with masked DOA output are also applied and shown to further improve both SED and DOA accuracy.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Leung2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Leung_90.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Leung2019label" class="modal fade" id="bibtex-Leung2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLeung2019label">
        SPECTRUM COMBINATION AND CONVOLUTIONAL RECURRENT NEURAL NETWORKS FOR JOINT LOCALIZATION AND DETECTION OF SOUND EVENTS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Leung2019,
    Author = "Leung, Shuangran and Ren, Yi",
    title = "SPECTRUM COMBINATION AND CONVOLUTIONAL RECURRENT NEURAL NETWORKS FOR JOINT LOCALIZATION AND DETECTION OF SOUND EVENTS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this work, we combine existing Short Time Fourier Transforms (STFT) of 4-channel array audio signals to create new features, and show that this augmented input improves the performance of DCASE2019 task 3 baseline system [1] in both sound event detection (SED) and direction-of-arrival (DOA) estimation. Techniques like ensembling and finetuning with masked DOA output are also applied and shown to further improve both SED and DOA accuracy."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lin2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Lin2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A REPORT ON SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Yifeng Lin, Zhisheng Wang
       </p>
<p style="text-align:left">
<em>
         Esound corporation
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lin_YYZN_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lin2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lin2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lin_110.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lin2019" class="panel-collapse collapse" id="collapse-Lin2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A REPORT ON SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Yifeng Lin, Zhisheng Wang
       </small>
<br/>
<small>
<em>
         Esound corporation
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we make a little change to the baseline of Sound Event Localization and Detection. We add a Gaussian-noise on the input data to find if noise would help us improve the neutral network. Sound event detection is performed stacked convolutional and recurrent neural network and the evaluation is reported using standard metrics of error rate and F-score. The studied neutral network with noise on input data are seen to consistently perform equal to the origin baseline with respect to error rate metric.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lin_110.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lin2019label" class="modal fade" id="bibtex-Lin2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLin2019label">
        A REPORT ON SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lin2019,
    Author = "Lin, Yifeng and Wang, Zhisheng",
    title = "A REPORT ON SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this paper, we make a little change to the baseline of Sound Event Localization and Detection. We add a Gaussian-noise on the input data to find if noise would help us improve the neutral network. Sound event detection is performed stacked convolutional and recurrent neural network and the evaluation is reported using standard metrics of error rate and F-score. The studied neutral network with noise on input data are seen to consistently perform equal to the origin baseline with respect to error rate metric."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="MazzonYasuda2019" style="box-shadow: none">
<div class="panel-heading" id="heading-MazzonYasuda2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT LOCALIZATION AND DETECTION USING FOA DOMAIN SPATIAL AUGMENTATION
       </h4>
<p style="text-align:left">
        Luca Mazzon, Masahiro Yasuda, Yuma Koizumi, Noboru Harada
       </p>
<p style="text-align:left">
<em>
         NTT Media Intelligence Laboratories
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">MazzonYasuda_NTT_task3_1</span> <span class="label label-primary">MazzonYasuda_NTT_task3_2</span> <span class="label label-primary">MazzonYasuda_NTT_task3_3</span> <span class="label label-primary">MazzonYasuda_NTT_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-MazzonYasuda2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-MazzonYasuda2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-MazzonYasuda2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_MazzonYasuda_93.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-MazzonYasuda2019" class="panel-collapse collapse" id="collapse-MazzonYasuda2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT LOCALIZATION AND DETECTION USING FOA DOMAIN SPATIAL AUGMENTATION
      </h4>
<p style="text-align:left">
<small>
        Luca Mazzon, Masahiro Yasuda, Yuma Koizumi, Noboru Harada
       </small>
<br/>
<small>
<em>
         NTT Media Intelligence Laboratories
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the system participating to the DCASE 2019, Task 3: Sound Event Localization and Detection challenge. The system consists of a convolutional recurrent neural network (CRNN) reinforced by a ResNet structure. A two-stage training strategy with label masking is adopted. The main advancement of the proposed method is a data augmentation method based on rotation in the first order Ambisonics (FOA) domain. The proposed spatial augmentation enables us to augment direction of arrival (DOA) labels without losing physical relationships between steering vectors and observations. Evaluation results on development dataset show that, even though the proposed method did not use any ensemble method in this experiment, (i) the proposed method outperformed a state-of-the-art system published before the submission deadline and (ii) the DOA error has significantly decreased: 2.73-degree better than the state-of-the-art system.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-MazzonYasuda2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_MazzonYasuda_93.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-MazzonYasuda2019label" class="modal fade" id="bibtex-MazzonYasuda2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMazzonYasuda2019label">
        SOUND EVENT LOCALIZATION AND DETECTION USING FOA DOMAIN SPATIAL AUGMENTATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{MazzonYasuda2019,
    Author = "Mazzon, Luca and Yasuda, Masahiro and Koizumi, Yuma and Harada, Noboru",
    title = "SOUND EVENT LOCALIZATION AND DETECTION USING FOA DOMAIN SPATIAL AUGMENTATION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes the system participating to the DCASE 2019, Task 3: Sound Event Localization and Detection challenge. The system consists of a convolutional recurrent neural network (CRNN) reinforced by a ResNet structure. A two-stage training strategy with label masking is adopted. The main advancement of the proposed method is a data augmentation method based on rotation in the first order Ambisonics (FOA) domain. The proposed spatial augmentation enables us to augment direction of arrival (DOA) labels without losing physical relationships between steering vectors and observations. Evaluation results on development dataset show that, even though the proposed method did not use any ensemble method in this experiment, (i) the proposed method outperformed a state-of-the-art system published before the submission deadline and (ii) the DOA error has significantly decreased: 2.73-degree better than the state-of-the-art system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Nguyen2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019 TASK 3: A TWO-STEP SYSTEM FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Thi Ngoc Tho Nguyen, Douglas L. Jones, Rishabh Ranjan, Sathish Jayabalan, Woon Seng Gan
       </p>
<p style="text-align:left">
<em>
         Nanyang Technological University, University of Illinois Urbana-Champaign
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nguyen_NTU_task3_1</span> <span class="label label-primary">Nguyen_NTU_task3_2</span> <span class="label label-primary">Nguyen_NTU_task3_3</span> <span class="label label-primary">Nguyen_NTU_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nguyen2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Nguyen_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2019" class="panel-collapse collapse" id="collapse-Nguyen2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019 TASK 3: A TWO-STEP SYSTEM FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Thi Ngoc Tho Nguyen, Douglas L. Jones, Rishabh Ranjan, Sathish Jayabalan, Woon Seng Gan
       </small>
<br/>
<small>
<em>
         Nanyang Technological University, University of Illinois Urbana-Champaign
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event detection and sound event localization requires different features from audio input signals. While sound event detection mainly relies on time-frequency patterns to distinguish different event classes, sound event localization uses magnitude or phase differences between microphones to estimate source directions. Therefore, we propose a two-step system to do sound event localization and detection. In the first step, we detect the sound events and estimate the directions-of-arrival separately. In the second step, we combine the results of the event detector and direction-of-arrival estimator together. The obtained results show a significant improvement over the baseline solution for sound event localization and detection in DCASE 2019 task 3 challenge. Using the development dataset on 4 fold cross-validation, the proposed system achieves an F1 score of 86.9\% for sound event detection and an error of 5.15 degrees for direction-of-arrival estimation while the baseline F1 score and error are 79.9\% and 28.5 degrees respectively.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Nguyen_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2019label" class="modal fade" id="bibtex-Nguyen2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2019label">
        DCASE 2019 TASK 3: A TWO-STEP SYSTEM FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nguyen2019,
    Author = "Nguyen, Thi Ngoc Tho and Jones, Douglas L. and Ranjan, Rishabh and Jayabalan, Sathish and Gan, Woon Seng",
    title = "DCASE 2019 TASK 3: A TWO-STEP SYSTEM FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Sound event detection and sound event localization requires different features from audio input signals. While sound event detection mainly relies on time-frequency patterns to distinguish different event classes, sound event localization uses magnitude or phase differences between microphones to estimate source directions. Therefore, we propose a two-step system to do sound event localization and detection. In the first step, we detect the sound events and estimate the directions-of-arrival separately. In the second step, we combine the results of the event detector and direction-of-arrival estimator together. The obtained results show a significant improvement over the baseline solution for sound event localization and detection in DCASE 2019 task 3 challenge. Using the development dataset on 4 fold cross-validation, the proposed system achieves an F1 score of 86.9\\% for sound event detection and an error of 5.15 degrees for direction-of-arrival estimation while the baseline F1 score and error are 79.9\\% and 28.5 degrees respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Park2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        REASSEMBLY LEARNING FOR SOUND EVENT LOCALIZATION AND DETECTION USING CRNN AND TRELLISNET
       </h4>
<p style="text-align:left">
        Sooyoung Park, Wootaek Lim, Sangwon Suh, Youngho Jeong
       </p>
<p style="text-align:left">
<em>
         Electronics and Telecommunications Research Institute
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Park_ETRI_task3_1</span> <span class="label label-primary">Park_ETRI_task3_2</span> <span class="label label-primary">Park_ETRI_task3_3</span> <span class="label label-primary">Park_ETRI_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Park2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Park_33.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2019" class="panel-collapse collapse" id="collapse-Park2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       REASSEMBLY LEARNING FOR SOUND EVENT LOCALIZATION AND DETECTION USING CRNN AND TRELLISNET
      </h4>
<p style="text-align:left">
<small>
        Sooyoung Park, Wootaek Lim, Sangwon Suh, Youngho Jeong
       </small>
<br/>
<small>
<em>
         Electronics and Telecommunications Research Institute
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report proposes a deep learning based approach, reassembly learning, for polyphonic sound event localization and detection. Sound event localization and detection is a joint task of two dependent sub-tasks: sound event detection and direction of arrival estimation. Joint learning has performance degradation compared to learning each sub-task separately. For this reason, we propose a reassembly learning to design a single network that deals with dependent sub-tasks together. Reassembly learning is a method to divide multi-task into individual sub-tasks, to train each sub-task, and then to reassemble and fine-tune into a single network. Experimental results show that the reassembly learning has good performance in the sound event localization and detection. Besides, the convolutional recurrent neural networks have been known as a state of art in both sound classification and detection applications. In DCASE 2019 challenge task 3, we suggest new architecture, trellis network based on temporal convolution networks, which can replace the convolutional recurrent neural networks. Trellis network shows a strong point in the direction of arrival estimation and has the possibility of being applied to a variety of sound classification and detection applications.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Park_33.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2019label" class="modal fade" id="bibtex-Park2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2019label">
        REASSEMBLY LEARNING FOR SOUND EVENT LOCALIZATION AND DETECTION USING CRNN AND TRELLISNET
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Park2019,
    Author = "Park, Sooyoung and Lim, Wootaek and Suh, Sangwon and Jeong, Youngho",
    title = "REASSEMBLY LEARNING FOR SOUND EVENT LOCALIZATION AND DETECTION USING CRNN AND TRELLISNET",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report proposes a deep learning based approach, reassembly learning, for polyphonic sound event localization and detection. Sound event localization and detection is a joint task of two dependent sub-tasks: sound event detection and direction of arrival estimation. Joint learning has performance degradation compared to learning each sub-task separately. For this reason, we propose a reassembly learning to design a single network that deals with dependent sub-tasks together. Reassembly learning is a method to divide multi-task into individual sub-tasks, to train each sub-task, and then to reassemble and fine-tune into a single network. Experimental results show that the reassembly learning has good performance in the sound event localization and detection. Besides, the convolutional recurrent neural networks have been known as a state of art in both sound classification and detection applications. In DCASE 2019 challenge task 3, we suggest new architecture, trellis network based on temporal convolution networks, which can replace the convolutional recurrent neural networks. Trellis network shows a strong point in the direction of arrival estimation and has the possibility of being applied to a variety of sound classification and detection applications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Perezlopez2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Perezlopez2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A HYBRID PARAMETRIC-DEEP LEARNING APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Andres Perez-Lopez, Eduardo Fonseca, Xavier Serra
       </p>
<p style="text-align:left">
<em>
         Centre Tecnologic de Catalunya, Universitat Pompeu Fabra
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Perezlopez_UPF_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Perezlopez2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Perezlopez2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Perezlopez2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Perezlopez_106.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Perezlopez2019').collapse('show');window.location.hash='#Perezlopez2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Perezlopez2019" class="panel-collapse collapse" id="collapse-Perezlopez2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A HYBRID PARAMETRIC-DEEP LEARNING APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Andres Perez-Lopez, Eduardo Fonseca, Xavier Serra
       </small>
<br/>
<small>
<em>
         Centre Tecnologic de Catalunya, Universitat Pompeu Fabra
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes and discusses the algorithm submitted to the Sound Event Localization and Detection Task of DCASE2019 Challenge. The proposed methodology combines a parametric spatial audio analysis approach for localization estimation, a simple heuristic for event segmentation, and a deep learning based monophonic event classifier. The evaluation of the proposed algorithm with the development dataset yields overall results slightly outperforming the baseline system. The main highlight is a reduction of the localization error over 65\%.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Perezlopez2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Perezlopez_106.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/andresperezlopez/DCASE2019_task3" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Perezlopez2019label" class="modal fade" id="bibtex-Perezlopez2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPerezlopez2019label">
        A HYBRID PARAMETRIC-DEEP LEARNING APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Perezlopez2019,
    Author = "Perez-Lopez, Andres and Fonseca, Eduardo and Serra, Xavier",
    title = "A HYBRID PARAMETRIC-DEEP LEARNING APPROACH FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes and discusses the algorithm submitted to the Sound Event Localization and Detection Task of DCASE2019 Challenge. The proposed methodology combines a parametric spatial audio analysis approach for localization estimation, a simple heuristic for event segmentation, and a deep learning based monophonic event classifier. The evaluation of the proposed algorithm with the development dataset yields overall results slightly outperforming the baseline system. The main highlight is a reduction of the localization error over 65\\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ranjan2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Ranjan2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENTS DETECTION AND DIRECTION OF ARRIVAL ESTIMATION USING RESIDUAL NET AND RECURRENT NEURAL NETWORKS
       </h4>
<p style="text-align:left">
        Rishabh Ranjan, Sathish Jayabalan, Woon-Seng Gan
       </p>
<p style="text-align:left">
<em>
         Nanyang Technological University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ranjan_NTU_task3_1</span> <span class="label label-primary">Ranjan_NTU_task3_2</span> <span class="label label-primary">Ranjan_NTU_task3_3</span> <span class="label label-primary">Ranjan_NTU_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ranjan2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ranjan2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ranjan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Ranjan_36.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ranjan2019" class="panel-collapse collapse" id="collapse-Ranjan2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENTS DETECTION AND DIRECTION OF ARRIVAL ESTIMATION USING RESIDUAL NET AND RECURRENT NEURAL NETWORKS
      </h4>
<p style="text-align:left">
<small>
        Rishabh Ranjan, Sathish Jayabalan, Woon-Seng Gan
       </small>
<br/>
<small>
<em>
         Nanyang Technological University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents deep learning approach for sound events detection and localization, which is also a part of detection and classification of acoustic scenes and events (DCASE) challenge 2019 Task 3. Deep residual nets originally used for image classification are adapted and combined with recurrent neural networks (RNN) to estimate the onset-offset of sound events, sound events class, and their direction in a reverberant environment. Additionally, data augmentation and post processing techniques are applied to generalize the system performance to unseen data. Using our best model on validation dataset, sound events detection achieves F1-score of 0.89 and error rate of 0.18, whereas sound source localization task achieves angular error of 9 degree and 0.90 frame recall.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ranjan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Ranjan_36.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ranjan2019label" class="modal fade" id="bibtex-Ranjan2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRanjan2019label">
        SOUND EVENTS DETECTION AND DIRECTION OF ARRIVAL ESTIMATION USING RESIDUAL NET AND RECURRENT NEURAL NETWORKS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ranjan2019,
    Author = "Ranjan, Rishabh and Jayabalan, Sathish and Gan, Woon-Seng",
    title = "SOUND EVENTS DETECTION AND DIRECTION OF ARRIVAL ESTIMATION USING RESIDUAL NET AND RECURRENT NEURAL NETWORKS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper presents deep learning approach for sound events detection and localization, which is also a part of detection and classification of acoustic scenes and events (DCASE) challenge 2019 Task 3. Deep residual nets originally used for image classification are adapted and combined with recurrent neural networks (RNN) to estimate the onset-offset of sound events, sound events class, and their direction in a reverberant environment. Additionally, data augmentation and post processing techniques are applied to generalize the system performance to unseen data. Using our best model on validation dataset, sound events detection achieves F1-score of 0.89 and error rate of 0.18, whereas sound source localization task achieves angular error of 9 degree and 0.90 frame recall."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Rough2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Rough2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        POLYPHONIC SOUND EVENT DETECTION AND LOCALIZATION USING A TWO-STAGE STRATEGY
       </h4>
<p style="text-align:left">
        Pi LiHong, Zheng Xue, Chen Ping, Wang Zhe, Zhang Chun
       </p>
<p style="text-align:left">
<em>
         Tsinghua University, Beijing Yiemed Medical Technology Co. Ltd, Beijing Sanping Technology Co.Ltd
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Rough_EMED_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Rough2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Rough2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Rough2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Rough_10.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Rough2019" class="panel-collapse collapse" id="collapse-Rough2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       POLYPHONIC SOUND EVENT DETECTION AND LOCALIZATION USING A TWO-STAGE STRATEGY
      </h4>
<p style="text-align:left">
<small>
        Pi LiHong, Zheng Xue, Chen Ping, Wang Zhe, Zhang Chun
       </small>
<br/>
<small>
<em>
         Tsinghua University, Beijing Yiemed Medical Technology Co. Ltd, Beijing Sanping Technology Co.Ltd
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The joint training of SED and DOAE affects the performance of both. We adopt a two-stage polyphonic sound event detection and localization method. The method learns SED first, after which the learned feature layers are transferred for DOAE. It then uses the SED ground truth as a mask to train DOAE. We select Log mel spectrograms and GCCPHAT as the input features, and the GCCPHAT feature which contains phase difference information between any of the two microphones improves the performance of DOAE.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Rough2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Rough_10.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Rough2019label" class="modal fade" id="bibtex-Rough2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRough2019label">
        POLYPHONIC SOUND EVENT DETECTION AND LOCALIZATION USING A TWO-STAGE STRATEGY
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Rough2019,
    Author = "LiHong, Pi and Xue, Zheng and Ping, Chen and Zhe, Wang and Chun, Zhang",
    title = "POLYPHONIC SOUND EVENT DETECTION AND LOCALIZATION USING A TWO-STAGE STRATEGY",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The joint training of SED and DOAE affects the performance of both. We adopt a two-stage polyphonic sound event detection and localization method. The method learns SED first, after which the learned feature layers are transferred for DOAE. It then uses the SED ground truth as a mask to train DOAE. We select Log mel spectrograms and GCCPHAT as the input features, and the GCCPHAT feature which contains phase difference information between any of the two microphones improves the performance of DOAE."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tan2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Tan2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection and Localization Using ResNet RNN and Time-Delay DOA
       </h4>
<p style="text-align:left">
        Ee Leng Tan, Rishabh Ranjan, Sathish Jayabalan
       </p>
<p style="text-align:left">
<em>
         Nanyang Technological University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Tan_NTU_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Tan2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tan2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Tan_42.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tan2019" class="panel-collapse collapse" id="collapse-Tan2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection and Localization Using ResNet RNN and Time-Delay DOA
      </h4>
<p style="text-align:left">
<small>
        Ee Leng Tan, Rishabh Ranjan, Sathish Jayabalan
       </small>
<br/>
<small>
<em>
         Nanyang Technological University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents a deep learning approach for sound events detection and time-delay direction-of-arrival (TDOA) for localization, which is also a part of detection and classification of acoustic scenes and events (DCASE) challenge 2019 Task 3. Deep residual nets originally used for image classification are adapted and combined with recurrent neural networks (RNN) to estimate the onset-offset of sound events, sound events class. Data augmentation and postprocessing techniques are applied to generalize the system performance to unseen data. Direction of sound events in a reverberant environment is estimated using a time-delay direction-of-arrival TDOA algorithm. Using our best model on validation dataset, sound events detection achieves F1-score of 0.84 and error rate of 0.25, whereas sound source localization task achieves angular error of 16.56 degree and 0.82 frame recall.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Tan_42.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tan2019label" class="modal fade" id="bibtex-Tan2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTan2019label">
        Sound Event Detection and Localization Using ResNet RNN and Time-Delay DOA
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Tan2019,
    Author = "Tan, Ee Leng and Ranjan, Rishabh and Jayabalan, Sathish",
    title = "Sound Event Detection and Localization Using ResNet RNN and Time-Delay DOA",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper presents a deep learning approach for sound events detection and time-delay direction-of-arrival (TDOA) for localization, which is also a part of detection and classification of acoustic scenes and events (DCASE) challenge 2019 Task 3. Deep residual nets originally used for image classification are adapted and combined with recurrent neural networks (RNN) to estimate the onset-offset of sound events, sound events class. Data augmentation and postprocessing techniques are applied to generalize the system performance to unseen data. Direction of sound events in a reverberant environment is estimated using a time-delay direction-of-arrival TDOA algorithm. Using our best model on validation dataset, sound events detection achieves F1-score of 0.84 and error rate of 0.25, whereas sound source localization task achieves angular error of 16.56 degree and 0.82 frame recall."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xue2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Xue2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTI-BEAM AND MULTI-TASK LEARNING FOR JOINT SOUND EVENT DETECTION AND LOCALIZATION
       </h4>
<p style="text-align:left">
        Wei Xue, Tong Ying, Zhang Chao, Ding Guohong
       </p>
<p style="text-align:left">
<em>
         JD.COM
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xue_JDAI_task3_1</span> <span class="label label-primary">Xue_JDAI_task3_2</span> <span class="label label-primary">Xue_JDAI_task3_3</span> <span class="label label-primary">Xue_JDAI_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xue2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xue2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xue2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Xue_91.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xue2019" class="panel-collapse collapse" id="collapse-Xue2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTI-BEAM AND MULTI-TASK LEARNING FOR JOINT SOUND EVENT DETECTION AND LOCALIZATION
      </h4>
<p style="text-align:left">
<small>
        Wei Xue, Tong Ying, Zhang Chao, Ding Guohong
       </small>
<br/>
<small>
<em>
         JD.COM
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Joint sound event detection (SED) and sound source localization (SSL) is essential since it provides both the temporal and spatial information of the events that appear in an acoustic scene. Although the problem can be tackled by designing a system based on the deep neural networks (DNNs) and fundamental spectral and spatial features, in this paper, we largely leverage the conventional microphone array signal processing techniques to generate more comprehensive representations for both SED and SSL, and to perform post-processing such that stable SED and SSL results can be obtained. Specifically, the features extracted from signals of multiple beams are utilized, which orient towards different directions of arrival (DOAs), and are formed according to the estimated steering vector of each DOA. Smoothed cross-power spectra (CPS) are computed based on the signal presence probability (SPP), and are used both as the input features of the DNNs, and for estimating the steering vectors of different DOAs. A triple-task learning scheme is developed, which jointly exploits the classification and regression based criterion for DOA estimation, and uses the classification based criterion as a regularization for the DNN. Experimental results demonstrate that the proposed method yields substantial improvements compared with the baseline method for the task 3 of the DCASE challenge 2019.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xue2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Xue_91.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xue2019label" class="modal fade" id="bibtex-Xue2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXue2019label">
        MULTI-BEAM AND MULTI-TASK LEARNING FOR JOINT SOUND EVENT DETECTION AND LOCALIZATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xue2019,
    Author = "Xue, Wei and Ying, Tong and Chao, Zhang and Guohong, Ding",
    title = "MULTI-BEAM AND MULTI-TASK LEARNING FOR JOINT SOUND EVENT DETECTION AND LOCALIZATION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Joint sound event detection (SED) and sound source localization (SSL) is essential since it provides both the temporal and spatial information of the events that appear in an acoustic scene. Although the problem can be tackled by designing a system based on the deep neural networks (DNNs) and fundamental spectral and spatial features, in this paper, we largely leverage the conventional microphone array signal processing techniques to generate more comprehensive representations for both SED and SSL, and to perform post-processing such that stable SED and SSL results can be obtained. Specifically, the features extracted from signals of multiple beams are utilized, which orient towards different directions of arrival (DOAs), and are formed according to the estimated steering vector of each DOA. Smoothed cross-power spectra (CPS) are computed based on the signal presence probability (SPP), and are used both as the input features of the DNNs, and for estimating the steering vectors of different DOAs. A triple-task learning scheme is developed, which jointly exploits the classification and regression based criterion for DOA estimation, and uses the classification based criterion as a regularization for the DNN. Experimental results demonstrate that the proposed method yields substantial improvements compared with the baseline method for the task 3 of the DCASE challenge 2019."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="ZhaoLu2019" style="box-shadow: none">
<div class="panel-heading" id="heading-ZhaoLu2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound event detection and localization based on CNN and LSTM
       </h4>
<p style="text-align:left">
        Zhao Lu
       </p>
<p style="text-align:left">
<em>
         University of Electronic Science and Technology of China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">ZhaoLu_UESTC_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-ZhaoLu2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-ZhaoLu2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-ZhaoLu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_ZhaoLu_109.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-ZhaoLu2019" class="panel-collapse collapse" id="collapse-ZhaoLu2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound event detection and localization based on CNN and LSTM
      </h4>
<p style="text-align:left">
<small>
        Zhao Lu
       </small>
<br/>
<small>
<em>
         University of Electronic Science and Technology of China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge is a topic seminar for speech feature classification. Task 3 is the location and detection of sound events. In this field, the learning method based on deep neural network is becoming more and more popular. On the basis of CNN, the spectrum and cross-correlation information of multichannel microphone array are learned based on CNN and LSTM, and the detection of sound events and the estimation of arrival direction are obtained. Compared with the baseline method, this method improves the estimation accuracy of DOA and the recognition ability of SED by using DCASE2019 dataset and PyTorch deep learning tool. The combination of CNN and LSTM works very well on this kind of feature classification problem with time series.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-ZhaoLu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_ZhaoLu_109.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-ZhaoLu2019label" class="modal fade" id="bibtex-ZhaoLu2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhaoLu2019label">
        Sound event detection and localization based on CNN and LSTM
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{ZhaoLu2019,
    Author = "Lu, Zhao",
    title = "Sound event detection and localization based on CNN and LSTM",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge is a topic seminar for speech feature classification. Task 3 is the location and detection of sound events. In this field, the learning method based on deep neural network is becoming more and more popular. On the basis of CNN, the spectrum and cross-correlation information of multichannel microphone array are learned based on CNN and LSTM, and the detection of sound events and the estimation of arrival direction are obtained. Compared with the baseline method, this method improves the estimation accuracy of DOA and the recognition ability of SED by using DCASE2019 dataset and PyTorch deep learning tool. The combination of CNN and LSTM works very well on this kind of feature classification problem with time series."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>