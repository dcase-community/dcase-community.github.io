<!DOCTYPE html><html lang="en">
<head>
    <title>Sound event detection in domestic environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2019/task-sound-event-detection-in-domestic-environments-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2019</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2019/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-urban-sound-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2019/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2019/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2019/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/dunes-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-large-scale fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Large-scale</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2019_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound event detection in domestic environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#supplementary-metrics">Supplementary metrics</a></li>
</ul>
</li>
<li><a href="#teams-ranking">Teams ranking</a>
<ul>
<li><a href="#supplementary-metrics-1">Supplementary metrics</a></li>
</ul>
</li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong> given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong> together with a small weakly annotated training set to improve system performance. <strong>The labels in the annotated subset are verified and can be considered as reliable.</strong></p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_dev" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>16.8</td>
<td>23.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.5</td>
<td>22.4</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.2</td>
<td>22.5</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.2</td>
<td>22.7</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>DCASE2019 mean-teacher with shifted and noisy data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>43.6</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_1</td>
<td>DCASE2019 mean-teacher with shifted data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>38.3</td>
<td>42.1</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>22.3</td>
<td>21.3</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_2</td>
<td>CTK_NU_task4_2</td>
<td>Chan2019</td>
<td>29.7</td>
<td>29.7</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_3</td>
<td>CTK_NU_task4_3</td>
<td>Chan2019</td>
<td>27.7</td>
<td>27.8</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_4</td>
<td>CTK_NU_task4_4</td>
<td>Chan2019</td>
<td>26.9</td>
<td>27.2</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>30.4</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_3</td>
<td>msm_ResNet_3_augmentation</td>
<td>Mishima2019</td>
<td>18.3</td>
<td>25.9</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>msm_ResNet_4_augmentation_pseudo</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>24.7</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_2</td>
<td>msm_ResNet_2_pseudo</td>
<td>Mishima2019</td>
<td>17.7</td>
<td>24.8</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_1</td>
<td>msm_ResNet_1_simple</td>
<td>Mishima2019</td>
<td>16.7</td>
<td>24.0</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>CANCES multi-task</td>
<td>Cances2019</td>
<td>28.4</td>
<td>33.8</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>CANCES multi-task</td>
<td>Cances2019</td>
<td>26.1</td>
<td>28.8</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>PELLEGRINI multi-task</td>
<td>Cances2019</td>
<td>39.7</td>
<td>39.9</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_2</td>
<td>Guiding_learning_2</td>
<td>Lin2019</td>
<td>40.9</td>
<td>44.0</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_4</td>
<td>Guiding_learning_4</td>
<td>Lin2019</td>
<td>41.8</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Guiding_learning_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>45.3</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_1</td>
<td>Guiding_learning_1</td>
<td>Lin2019</td>
<td>40.7</td>
<td>44.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>DCASE2019 baseline system</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>23.7</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_1</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>21.7</td>
<td>25.0</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_4</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>25.3</td>
<td>31.9</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_3</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>23.8</td>
<td>25.0</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>31.9</td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Mean VAT Teacher</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>59.6</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_1</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>27.8</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_3</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>29.4</td>
<td>34.5</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_2</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>28.3</td>
<td>31.8</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.5</td>
<td>35.3</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>29.0</td>
<td>34.4</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>29.4</td>
<td>35.0</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>34.6</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>BossLee_FRDC_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>42.5</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_3</td>
<td>BossLee_FRDC_3</td>
<td>Shi2019</td>
<td>40.9</td>
<td>38.9</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_4</td>
<td>BossLee_FRDC_4</td>
<td>Shi2019</td>
<td>41.5</td>
<td>41.7</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_1</td>
<td>BossLee_FRDC_1</td>
<td>Shi2019</td>
<td>37.0</td>
<td>36.7</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_1</td>
<td>UESTC_SICE_task4_1</td>
<td>Zhang2019</td>
<td>29.4</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>UESTC_SICE_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.5</td>
<td>19.4</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_2</td>
<td>Wang_YSU_task4_2</td>
<td>Yang2019</td>
<td>6.2</td>
<td>20.9</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_3</td>
<td>Wang_YSU_task4_3</td>
<td>Yang2019</td>
<td>6.7</td>
<td>22.7</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_1</td>
<td>USTC_CRNN_MT system1</td>
<td>Yan2019</td>
<td>35.8</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_3</td>
<td>USTC_CRNN_MT system3</td>
<td>Yan2019</td>
<td>35.6</td>
<td>42.1</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_4</td>
<td>USTC_CRNN_MT system4</td>
<td>Yan2019</td>
<td>33.5</td>
<td>39.4</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>USTC_CRNN_MT system2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>42.6</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_2</td>
<td>KNUwaveCNN2</td>
<td>Lee2019</td>
<td>25.8</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_4</td>
<td>KNUwaveCNN4</td>
<td>Lee2019</td>
<td>24.6</td>
<td>28.7</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>KNUwaveCNN3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_1</td>
<td>KNUwaveCNN1</td>
<td>Lee2019</td>
<td>26.4</td>
<td>28.8</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Regularized Surrey9</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>24.3</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim_task4_1</td>
<td>Lim2019</td>
<td>32.6</td>
<td>38.8</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim_task4_2</td>
<td>Lim2019</td>
<td>33.2</td>
<td>39.5</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim_task4_3</td>
<td>Lim2019</td>
<td>32.5</td>
<td>39.4</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>40.9</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval_segment_1s" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Youtube dataset)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Youtube dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Vimeo dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Segment-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval_segment_1s" data-sortable="true" data-value-type="float1-percentage">
                Segment-based<br/>F-score <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>16.8</td>
<td>18.3</td>
<td>13.2</td>
<td>64.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.5</td>
<td>19.2</td>
<td>13.3</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.2</td>
<td>18.4</td>
<td>14.4</td>
<td>65.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.2</td>
<td>18.7</td>
<td>13.5</td>
<td>64.4</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>DCASE2019 mean-teacher with shifted and noisy data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>45.8</td>
<td>33.3</td>
<td>71.4</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_1</td>
<td>DCASE2019 mean-teacher with shifted data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>38.3</td>
<td>41.9</td>
<td>29.2</td>
<td>68.6</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>22.3</td>
<td>24.1</td>
<td>17.0</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_2</td>
<td>CTK_NU_task4_2</td>
<td>Chan2019</td>
<td>29.7</td>
<td>33.2</td>
<td>21.0</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_3</td>
<td>CTK_NU_task4_3</td>
<td>Chan2019</td>
<td>27.7</td>
<td>30.8</td>
<td>19.8</td>
<td>50.5</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_4</td>
<td>CTK_NU_task4_4</td>
<td>Chan2019</td>
<td>26.9</td>
<td>30.1</td>
<td>18.8</td>
<td>48.7</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>34.7</td>
<td>21.6</td>
<td>58.2</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_3</td>
<td>msm_ResNet_3_augmentation</td>
<td>Mishima2019</td>
<td>18.3</td>
<td>20.6</td>
<td>12.6</td>
<td>58.8</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>msm_ResNet_4_augmentation_pseudo</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>21.8</td>
<td>15.0</td>
<td>58.7</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_2</td>
<td>msm_ResNet_2_pseudo</td>
<td>Mishima2019</td>
<td>17.7</td>
<td>19.0</td>
<td>14.1</td>
<td>56.1</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_1</td>
<td>msm_ResNet_1_simple</td>
<td>Mishima2019</td>
<td>16.7</td>
<td>18.8</td>
<td>11.7</td>
<td>56.2</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>CANCES multi-task</td>
<td>Cances2019</td>
<td>28.4</td>
<td>31.1</td>
<td>21.3</td>
<td>61.2</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>CANCES multi-task</td>
<td>Cances2019</td>
<td>26.1</td>
<td>29.2</td>
<td>18.1</td>
<td>62.5</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>PELLEGRINI multi-task</td>
<td>Cances2019</td>
<td>39.7</td>
<td>43.0</td>
<td>30.9</td>
<td>64.7</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_2</td>
<td>Guiding_learning_2</td>
<td>Lin2019</td>
<td>40.9</td>
<td>45.0</td>
<td>29.8</td>
<td>62.7</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_4</td>
<td>Guiding_learning_4</td>
<td>Lin2019</td>
<td>41.8</td>
<td>46.7</td>
<td>28.6</td>
<td>64.5</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Guiding_learning_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>47.7</td>
<td>29.4</td>
<td>64.8</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_1</td>
<td>Guiding_learning_1</td>
<td>Lin2019</td>
<td>40.7</td>
<td>45.5</td>
<td>27.6</td>
<td>61.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>DCASE2019 baseline system</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>29.0</td>
<td>18.1</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_1</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>21.7</td>
<td>23.0</td>
<td>18.2</td>
<td>63.3</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_4</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>25.3</td>
<td>28.6</td>
<td>16.1</td>
<td>58.7</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_3</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>23.8</td>
<td>26.2</td>
<td>17.5</td>
<td>61.7</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>30.1</td>
<td>21.7</td>
<td>61.6</td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Mean VAT Teacher</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>27.1</td>
<td>20.0</td>
<td>60.4</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_1</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>27.8</td>
<td>30.4</td>
<td>22.1</td>
<td>66.1</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>36.2</td>
<td>23.8</td>
<td>65.3</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_3</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>29.4</td>
<td>32.9</td>
<td>21.2</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_2</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>28.3</td>
<td>32.1</td>
<td>19.3</td>
<td>62.4</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.5</td>
<td>32.5</td>
<td>24.7</td>
<td>53.5</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>29.0</td>
<td>31.2</td>
<td>23.0</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>29.4</td>
<td>31.2</td>
<td>24.4</td>
<td>52.4</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>33.2</td>
<td>23.8</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>BossLee_FRDC_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>46.1</td>
<td>31.5</td>
<td>69.8</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_3</td>
<td>BossLee_FRDC_3</td>
<td>Shi2019</td>
<td>40.9</td>
<td>45.5</td>
<td>29.8</td>
<td>68.7</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_4</td>
<td>BossLee_FRDC_4</td>
<td>Shi2019</td>
<td>41.5</td>
<td>46.4</td>
<td>29.3</td>
<td>67.8</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_1</td>
<td>BossLee_FRDC_1</td>
<td>Shi2019</td>
<td>37.0</td>
<td>40.2</td>
<td>28.9</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_1</td>
<td>UESTC_SICE_task4_1</td>
<td>Zhang2019</td>
<td>29.4</td>
<td>31.9</td>
<td>23.3</td>
<td>62.0</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>UESTC_SICE_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>34.5</td>
<td>21.1</td>
<td>60.9</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.5</td>
<td>7.4</td>
<td>4.1</td>
<td>26.1</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_2</td>
<td>Wang_YSU_task4_2</td>
<td>Yang2019</td>
<td>6.2</td>
<td>7.2</td>
<td>4.0</td>
<td>25.4</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_3</td>
<td>Wang_YSU_task4_3</td>
<td>Yang2019</td>
<td>6.7</td>
<td>7.6</td>
<td>4.6</td>
<td>26.3</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_1</td>
<td>USTC_CRNN_MT system1</td>
<td>Yan2019</td>
<td>35.8</td>
<td>38.2</td>
<td>29.3</td>
<td>66.1</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_3</td>
<td>USTC_CRNN_MT system3</td>
<td>Yan2019</td>
<td>35.6</td>
<td>38.2</td>
<td>28.2</td>
<td>64.6</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_4</td>
<td>USTC_CRNN_MT system4</td>
<td>Yan2019</td>
<td>33.5</td>
<td>35.6</td>
<td>27.3</td>
<td>64.1</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>USTC_CRNN_MT system2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>38.8</td>
<td>28.7</td>
<td>65.2</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_2</td>
<td>KNUwaveCNN2</td>
<td>Lee2019</td>
<td>25.8</td>
<td>27.4</td>
<td>21.5</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_4</td>
<td>KNUwaveCNN4</td>
<td>Lee2019</td>
<td>24.6</td>
<td>26.1</td>
<td>20.5</td>
<td>48.3</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>KNUwaveCNN3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>28.1</td>
<td>22.9</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_1</td>
<td>KNUwaveCNN1</td>
<td>Lee2019</td>
<td>26.4</td>
<td>27.8</td>
<td>22.6</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Regularized Surrey9</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>26.2</td>
<td>19.2</td>
<td>63.4</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim_task4_1</td>
<td>Lim2019</td>
<td>32.6</td>
<td>35.3</td>
<td>25.8</td>
<td>67.1</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim_task4_2</td>
<td>Lim2019</td>
<td>33.2</td>
<td>36.7</td>
<td>24.8</td>
<td>69.2</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim_task4_3</td>
<td>Lim2019</td>
<td>32.5</td>
<td>36.3</td>
<td>22.4</td>
<td>63.2</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>38.6</td>
<td>23.7</td>
<td>66.4</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_dev" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.5</td>
<td>22.4</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>DCASE2019 mean-teacher with shifted and noisy data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>43.6</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>22.3</td>
<td>21.3</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>30.4</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>msm_ResNet_4_augmentation_pseudo</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>24.7</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>PELLEGRINI multi-task</td>
<td>Cances2019</td>
<td>39.7</td>
<td>39.9</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Guiding_learning_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>45.3</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>DCASE2019 baseline system</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>23.7</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>31.9</td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Mean VAT Teacher</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>59.6</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>34.6</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>BossLee_FRDC_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>42.5</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>UESTC_SICE_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.7</td>
<td>19.4</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>USTC_CRNN_MT system2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>42.6</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>KNUwaveCNN3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Regularized Surrey9</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>24.3</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>40.9</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics-1">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_youtube" data-scatter-y="f_score_vimeo" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="f_score_eval_segment_1s" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Youtube dataset)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Youtube dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Vimeo dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Segment-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval_segment_1s" data-sortable="true" data-value-type="float1-percentage">
                Segment-based<br/>F-score <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.5</td>
<td>19.2</td>
<td>13.3</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>DCASE2019 mean-teacher with shifted and noisy data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>45.8</td>
<td>33.3</td>
<td>71.4</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>22.3</td>
<td>24.1</td>
<td>17.0</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>34.7</td>
<td>21.6</td>
<td>58.2</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>msm_ResNet_4_augmentation_pseudo</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>21.8</td>
<td>15.0</td>
<td>58.7</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>PELLEGRINI multi-task</td>
<td>Cances2019</td>
<td>39.7</td>
<td>43.0</td>
<td>30.9</td>
<td>64.7</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Guiding_learning_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>47.7</td>
<td>29.4</td>
<td>64.8</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>DCASE2019 baseline system</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>29.0</td>
<td>18.1</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>30.1</td>
<td>21.7</td>
<td>61.6</td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Mean VAT Teacher</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>27.1</td>
<td>20.0</td>
<td>60.4</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>36.2</td>
<td>23.8</td>
<td>65.3</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>33.2</td>
<td>23.8</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>BossLee_FRDC_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>46.1</td>
<td>31.5</td>
<td>69.8</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>UESTC_SICE_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>34.5</td>
<td>21.1</td>
<td>60.9</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.7</td>
<td>7.6</td>
<td>4.6</td>
<td>26.3</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>USTC_CRNN_MT system2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>38.8</td>
<td>28.7</td>
<td>65.2</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>KNUwaveCNN3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>28.1</td>
<td>22.9</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Regularized Surrey9</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>26.2</td>
<td>19.2</td>
<td>63.4</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>38.6</td>
<td>23.7</td>
<td>66.4</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="Baseline_dcase2019" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Lin_ICT_task4_3" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["Class_f_score_Alarm_bell_ringing", "Class_f_score_Blender", "Class_f_score_Cat", "Class_f_score_Dishes", "Class_f_score_Dog", "Class_f_score_Electric_shaver_toothbrush", "Class_f_score_Frying", "Class_f_score_Running_water", "Class_f_score_Speech", "Class_f_score_Vacuum_cleaner"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission<br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="Class_f_score_Alarm_bell_ringing" data-sortable="true" data-value-type="float1-percentage">
                Alarm<br/>Bell<br/>Ringing
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Blender" data-sortable="true" data-value-type="float1-percentage">
                Blender
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Cat" data-sortable="true" data-value-type="float1-percentage">
                Cat
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dishes" data-sortable="true" data-value-type="float1-percentage">
                Dishes
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dog" data-sortable="true" data-value-type="float1-percentage">
                Dog
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Electric_shaver_toothbrush" data-sortable="true" data-value-type="float1-percentage">
                Electric<br/>shave<br/>toothbrush
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Frying" data-sortable="true" data-value-type="float1-percentage">
                Frying
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Running_water" data-sortable="true" data-value-type="float1-percentage">
                Running<br/>water
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Speech" data-sortable="true" data-value-type="float1-percentage">
                Speech
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Vacuum_cleaner" data-sortable="true" data-value-type="float1-percentage">
                Vacuum<br/>cleaner
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>16.8</td>
<td>14.0</td>
<td>21.5</td>
<td>0.4</td>
<td>0.2</td>
<td>0.3</td>
<td>21.5</td>
<td>25.0</td>
<td>24.6</td>
<td>10.7</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.5</td>
<td>14.0</td>
<td>26.1</td>
<td>0.4</td>
<td>0.0</td>
<td>0.3</td>
<td>22.5</td>
<td>26.8</td>
<td>26.3</td>
<td>10.7</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.2</td>
<td>13.2</td>
<td>22.1</td>
<td>0.4</td>
<td>0.2</td>
<td>0.3</td>
<td>23.6</td>
<td>27.8</td>
<td>23.1</td>
<td>11.6</td>
<td>49.8</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>NUDT System for DCASE2019 Task4</td>
<td>Wang2019</td>
<td>17.2</td>
<td>11.9</td>
<td>20.9</td>
<td>0.4</td>
<td>0.2</td>
<td>0.0</td>
<td>24.9</td>
<td>27.7</td>
<td>27.0</td>
<td>11.7</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>DCASE2019 mean-teacher with shifted and noisy data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>42.6</td>
<td>49.2</td>
<td>52.9</td>
<td>35.2</td>
<td>40.9</td>
<td>47.5</td>
<td>41.4</td>
<td>31.9</td>
<td>43.9</td>
<td>35.7</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_1</td>
<td>DCASE2019 mean-teacher with shifted data augmentation system</td>
<td>Delphin-Poulat2019</td>
<td>38.3</td>
<td>41.6</td>
<td>40.8</td>
<td>51.9</td>
<td>37.2</td>
<td>37.8</td>
<td>41.1</td>
<td>39.0</td>
<td>22.1</td>
<td>41.2</td>
<td>29.8</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>22.3</td>
<td>6.2</td>
<td>14.2</td>
<td>41.7</td>
<td>11.1</td>
<td>17.1</td>
<td>28.7</td>
<td>3.0</td>
<td>20.8</td>
<td>50.3</td>
<td>30.3</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_2</td>
<td>CTK_NU_task4_2</td>
<td>Chan2019</td>
<td>29.7</td>
<td>20.5</td>
<td>42.9</td>
<td>40.3</td>
<td>0.7</td>
<td>22.9</td>
<td>37.4</td>
<td>30.5</td>
<td>20.0</td>
<td>39.7</td>
<td>41.8</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_3</td>
<td>CTK_NU_task4_3</td>
<td>Chan2019</td>
<td>27.7</td>
<td>32.5</td>
<td>38.8</td>
<td>33.8</td>
<td>0.0</td>
<td>17.6</td>
<td>40.2</td>
<td>29.7</td>
<td>19.5</td>
<td>23.0</td>
<td>42.1</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_4</td>
<td>CTK_NU_task4_4</td>
<td>Chan2019</td>
<td>26.9</td>
<td>24.0</td>
<td>38.9</td>
<td>30.7</td>
<td>0.7</td>
<td>17.2</td>
<td>35.3</td>
<td>27.6</td>
<td>18.5</td>
<td>36.3</td>
<td>39.9</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>25.1</td>
<td>38.2</td>
<td>27.2</td>
<td>7.7</td>
<td>25.6</td>
<td>50.0</td>
<td>35.0</td>
<td>24.2</td>
<td>26.6</td>
<td>50.7</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_3</td>
<td>msm_ResNet_3_augmentation</td>
<td>Mishima2019</td>
<td>18.3</td>
<td>17.9</td>
<td>5.3</td>
<td>36.5</td>
<td>24.9</td>
<td>28.7</td>
<td>13.9</td>
<td>5.0</td>
<td>4.6</td>
<td>38.0</td>
<td>8.4</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>msm_ResNet_4_augmentation_pseudo</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>18.7</td>
<td>7.9</td>
<td>41.2</td>
<td>25.1</td>
<td>18.2</td>
<td>14.5</td>
<td>9.5</td>
<td>3.6</td>
<td>48.6</td>
<td>10.5</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_2</td>
<td>msm_ResNet_2_pseudo</td>
<td>Mishima2019</td>
<td>17.7</td>
<td>22.4</td>
<td>0.8</td>
<td>40.9</td>
<td>18.1</td>
<td>31.5</td>
<td>7.5</td>
<td>0.5</td>
<td>1.3</td>
<td>51.7</td>
<td>2.4</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_1</td>
<td>msm_ResNet_1_simple</td>
<td>Mishima2019</td>
<td>16.7</td>
<td>18.3</td>
<td>2.4</td>
<td>35.8</td>
<td>18.6</td>
<td>27.1</td>
<td>8.0</td>
<td>0.8</td>
<td>1.4</td>
<td>50.5</td>
<td>4.5</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>CANCES multi-task</td>
<td>Cances2019</td>
<td>28.4</td>
<td>23.2</td>
<td>24.8</td>
<td>38.0</td>
<td>22.0</td>
<td>24.5</td>
<td>25.2</td>
<td>29.6</td>
<td>21.3</td>
<td>44.0</td>
<td>31.1</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>CANCES multi-task</td>
<td>Cances2019</td>
<td>26.1</td>
<td>18.8</td>
<td>26.9</td>
<td>20.5</td>
<td>19.4</td>
<td>11.1</td>
<td>27.6</td>
<td>40.9</td>
<td>14.1</td>
<td>45.5</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>PELLEGRINI multi-task</td>
<td>Cances2019</td>
<td>39.7</td>
<td>35.8</td>
<td>35.1</td>
<td>60.2</td>
<td>32.5</td>
<td>35.5</td>
<td>35.9</td>
<td>37.5</td>
<td>27.7</td>
<td>47.4</td>
<td>49.1</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_2</td>
<td>Guiding_learning_2</td>
<td>Lin2019</td>
<td>40.9</td>
<td>36.4</td>
<td>40.5</td>
<td>54.2</td>
<td>27.0</td>
<td>41.5</td>
<td>42.0</td>
<td>41.7</td>
<td>25.7</td>
<td>46.2</td>
<td>54.2</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_4</td>
<td>Guiding_learning_4</td>
<td>Lin2019</td>
<td>41.8</td>
<td>42.5</td>
<td>36.8</td>
<td>55.1</td>
<td>26.5</td>
<td>43.1</td>
<td>41.8</td>
<td>39.3</td>
<td>20.4</td>
<td>54.2</td>
<td>57.9</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Guiding_learning_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>42.3</td>
<td>40.5</td>
<td>55.1</td>
<td>26.4</td>
<td>42.0</td>
<td>44.6</td>
<td>41.5</td>
<td>21.8</td>
<td>54.6</td>
<td>58.6</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_1</td>
<td>Guiding_learning_1</td>
<td>Lin2019</td>
<td>40.7</td>
<td>40.2</td>
<td>35.8</td>
<td>55.3</td>
<td>24.6</td>
<td>38.7</td>
<td>43.1</td>
<td>42.0</td>
<td>23.6</td>
<td>55.9</td>
<td>48.3</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>DCASE2019 baseline system</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>26.6</td>
<td>32.2</td>
<td>53.6</td>
<td>13.7</td>
<td>9.9</td>
<td>13.3</td>
<td>24.1</td>
<td>10.9</td>
<td>37.7</td>
<td>35.5</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_1</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>21.7</td>
<td>6.4</td>
<td>18.6</td>
<td>40.9</td>
<td>12.3</td>
<td>9.0</td>
<td>32.8</td>
<td>12.7</td>
<td>19.0</td>
<td>46.0</td>
<td>19.5</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_4</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>25.3</td>
<td>3.7</td>
<td>14.6</td>
<td>51.6</td>
<td>5.9</td>
<td>5.6</td>
<td>39.8</td>
<td>37.4</td>
<td>23.6</td>
<td>45.7</td>
<td>24.8</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_3</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>23.8</td>
<td>17.0</td>
<td>14.6</td>
<td>34.6</td>
<td>14.8</td>
<td>13.8</td>
<td>31.9</td>
<td>16.3</td>
<td>23.6</td>
<td>46.0</td>
<td>25.5</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>DCASE2019 task4 system</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>16.6</td>
<td>18.6</td>
<td>46.3</td>
<td>20.1</td>
<td>21.5</td>
<td>34.9</td>
<td>28.6</td>
<td>19.0</td>
<td>46.8</td>
<td>25.9</td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Mean VAT Teacher</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>33.9</td>
<td>34.9</td>
<td>44.0</td>
<td>19.5</td>
<td>2.8</td>
<td>12.6</td>
<td>23.4</td>
<td>11.8</td>
<td>39.4</td>
<td>27.4</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_1</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>27.8</td>
<td>21.0</td>
<td>32.1</td>
<td>34.1</td>
<td>22.6</td>
<td>20.2</td>
<td>25.1</td>
<td>14.5</td>
<td>22.1</td>
<td>46.4</td>
<td>40.1</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>32.4</td>
<td>33.2</td>
<td>38.3</td>
<td>24.3</td>
<td>27.6</td>
<td>32.7</td>
<td>17.0</td>
<td>25.0</td>
<td>45.8</td>
<td>48.1</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_3</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>29.4</td>
<td>32.4</td>
<td>27.0</td>
<td>38.3</td>
<td>24.3</td>
<td>27.6</td>
<td>25.1</td>
<td>13.7</td>
<td>19.4</td>
<td>45.8</td>
<td>40.1</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_2</td>
<td>DCASE2019 SED ResNet self-mask kiyo</td>
<td>Kiyokawa2019</td>
<td>28.3</td>
<td>32.4</td>
<td>31.7</td>
<td>32.8</td>
<td>20.5</td>
<td>27.6</td>
<td>19.5</td>
<td>18.9</td>
<td>20.8</td>
<td>45.8</td>
<td>33.5</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.5</td>
<td>23.3</td>
<td>46.9</td>
<td>29.5</td>
<td>16.4</td>
<td>41.6</td>
<td>20.9</td>
<td>30.2</td>
<td>19.4</td>
<td>43.6</td>
<td>32.8</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>29.0</td>
<td>22.1</td>
<td>43.9</td>
<td>32.5</td>
<td>12.7</td>
<td>36.7</td>
<td>18.9</td>
<td>25.8</td>
<td>19.4</td>
<td>43.5</td>
<td>34.4</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>29.4</td>
<td>23.1</td>
<td>46.0</td>
<td>32.0</td>
<td>14.8</td>
<td>37.4</td>
<td>19.7</td>
<td>28.2</td>
<td>17.8</td>
<td>44.6</td>
<td>30.4</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU DCASE2019 task4 system</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>21.2</td>
<td>45.2</td>
<td>30.6</td>
<td>16.4</td>
<td>40.8</td>
<td>20.7</td>
<td>28.8</td>
<td>23.6</td>
<td>42.5</td>
<td>36.7</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>BossLee_FRDC_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>45.8</td>
<td>44.2</td>
<td>63.2</td>
<td>32.2</td>
<td>27.5</td>
<td>41.9</td>
<td>27.6</td>
<td>21.8</td>
<td>58.7</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_3</td>
<td>BossLee_FRDC_3</td>
<td>Shi2019</td>
<td>40.9</td>
<td>45.5</td>
<td>44.4</td>
<td>65.2</td>
<td>27.1</td>
<td>32.5</td>
<td>36.1</td>
<td>29.1</td>
<td>19.1</td>
<td>58.0</td>
<td>52.4</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_4</td>
<td>BossLee_FRDC_4</td>
<td>Shi2019</td>
<td>41.5</td>
<td>48.0</td>
<td>42.6</td>
<td>62.6</td>
<td>29.4</td>
<td>33.6</td>
<td>35.5</td>
<td>29.9</td>
<td>19.0</td>
<td>59.1</td>
<td>55.0</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_1</td>
<td>BossLee_FRDC_1</td>
<td>Shi2019</td>
<td>37.0</td>
<td>45.1</td>
<td>39.3</td>
<td>58.4</td>
<td>27.2</td>
<td>33.9</td>
<td>28.2</td>
<td>28.9</td>
<td>10.0</td>
<td>56.0</td>
<td>43.0</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_1</td>
<td>UESTC_SICE_task4_1</td>
<td>Zhang2019</td>
<td>29.4</td>
<td>32.8</td>
<td>41.5</td>
<td>54.7</td>
<td>8.1</td>
<td>1.0</td>
<td>25.5</td>
<td>21.6</td>
<td>17.3</td>
<td>56.6</td>
<td>35.3</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>UESTC_SICE_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>30.9</td>
<td>29.3</td>
<td>52.4</td>
<td>17.1</td>
<td>21.0</td>
<td>31.4</td>
<td>21.1</td>
<td>21.9</td>
<td>44.8</td>
<td>38.1</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.5</td>
<td>1.7</td>
<td>0.0</td>
<td>3.3</td>
<td>1.4</td>
<td>3.4</td>
<td>7.0</td>
<td>11.9</td>
<td>3.1</td>
<td>19.2</td>
<td>13.6</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_2</td>
<td>Wang_YSU_task4_2</td>
<td>Yang2019</td>
<td>6.2</td>
<td>2.3</td>
<td>6.2</td>
<td>7.6</td>
<td>2.9</td>
<td>0.7</td>
<td>4.3</td>
<td>7.7</td>
<td>3.4</td>
<td>16.0</td>
<td>11.4</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_3</td>
<td>Wang_YSU_task4_3</td>
<td>Yang2019</td>
<td>6.7</td>
<td>1.9</td>
<td>6.2</td>
<td>7.6</td>
<td>2.9</td>
<td>1.0</td>
<td>4.3</td>
<td>9.6</td>
<td>6.0</td>
<td>16.0</td>
<td>11.0</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_1</td>
<td>USTC_CRNN_MT system1</td>
<td>Yan2019</td>
<td>35.8</td>
<td>23.3</td>
<td>36.6</td>
<td>52.8</td>
<td>28.0</td>
<td>41.4</td>
<td>22.6</td>
<td>42.6</td>
<td>34.9</td>
<td>37.0</td>
<td>39.2</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_3</td>
<td>USTC_CRNN_MT system3</td>
<td>Yan2019</td>
<td>35.6</td>
<td>23.3</td>
<td>33.5</td>
<td>57.1</td>
<td>32.1</td>
<td>41.8</td>
<td>23.1</td>
<td>42.6</td>
<td>31.9</td>
<td>32.6</td>
<td>37.5</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_4</td>
<td>USTC_CRNN_MT system4</td>
<td>Yan2019</td>
<td>33.5</td>
<td>23.3</td>
<td>33.5</td>
<td>57.1</td>
<td>32.1</td>
<td>41.8</td>
<td>23.1</td>
<td>22.2</td>
<td>31.9</td>
<td>32.6</td>
<td>37.5</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>USTC_CRNN_MT system2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>18.5</td>
<td>39.2</td>
<td>52.3</td>
<td>26.8</td>
<td>41.4</td>
<td>30.5</td>
<td>42.6</td>
<td>34.9</td>
<td>38.3</td>
<td>37.3</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_2</td>
<td>KNUwaveCNN2</td>
<td>Lee2019</td>
<td>25.8</td>
<td>25.8</td>
<td>21.2</td>
<td>24.4</td>
<td>11.4</td>
<td>25.7</td>
<td>28.0</td>
<td>34.7</td>
<td>16.6</td>
<td>38.2</td>
<td>32.4</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_4</td>
<td>KNUwaveCNN4</td>
<td>Lee2019</td>
<td>24.6</td>
<td>26.9</td>
<td>30.0</td>
<td>20.1</td>
<td>10.4</td>
<td>26.5</td>
<td>27.7</td>
<td>30.9</td>
<td>16.4</td>
<td>31.4</td>
<td>26.0</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>KNUwaveCNN3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>25.8</td>
<td>23.4</td>
<td>24.3</td>
<td>11.4</td>
<td>25.8</td>
<td>28.9</td>
<td>36.3</td>
<td>16.7</td>
<td>38.4</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_1</td>
<td>KNUwaveCNN1</td>
<td>Lee2019</td>
<td>26.4</td>
<td>25.8</td>
<td>27.0</td>
<td>24.4</td>
<td>11.4</td>
<td>25.7</td>
<td>26.9</td>
<td>35.1</td>
<td>16.2</td>
<td>38.2</td>
<td>33.5</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Regularized Surrey9</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>25.6</td>
<td>21.6</td>
<td>25.1</td>
<td>14.4</td>
<td>12.9</td>
<td>25.7</td>
<td>24.9</td>
<td>17.5</td>
<td>50.6</td>
<td>23.7</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim_task4_1</td>
<td>Lim2019</td>
<td>32.6</td>
<td>22.2</td>
<td>41.7</td>
<td>53.1</td>
<td>17.2</td>
<td>29.2</td>
<td>12.6</td>
<td>36.0</td>
<td>21.8</td>
<td>50.8</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim_task4_2</td>
<td>Lim2019</td>
<td>33.2</td>
<td>26.9</td>
<td>36.7</td>
<td>53.7</td>
<td>19.3</td>
<td>27.1</td>
<td>14.0</td>
<td>35.9</td>
<td>23.0</td>
<td>52.4</td>
<td>42.9</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim_task4_3</td>
<td>Lim2019</td>
<td>32.5</td>
<td>25.7</td>
<td>31.6</td>
<td>52.6</td>
<td>20.1</td>
<td>35.2</td>
<td>15.9</td>
<td>33.2</td>
<td>19.5</td>
<td>58.4</td>
<td>32.6</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>26.2</td>
<td>35.5</td>
<td>57.2</td>
<td>24.1</td>
<td>33.1</td>
<td>17.4</td>
<td>33.3</td>
<td>21.5</td>
<td>58.5</td>
<td>37.1</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>Wang2019</td>
<td>16.8</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>Wang2019</td>
<td>17.5</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>Wang2019</td>
<td>17.2</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>Wang2019</td>
<td>17.2</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>22.05kHz</td>
<td>noise addition, time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_1</td>
<td>Delphin-Poulat2019</td>
<td>38.3</td>
<td>22.05kHz</td>
<td>time shifting, frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>Kong2019</td>
<td>22.3</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_2</td>
<td>Chan2019</td>
<td>29.7</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_3</td>
<td>Chan2019</td>
<td>27.7</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_4</td>
<td>Chan2019</td>
<td>26.9</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_3</td>
<td>Mishima2019</td>
<td>18.3</td>
<td>44.1kHz</td>
<td>block mixing,mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>44.1kHz</td>
<td>block mixing,mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_2</td>
<td>Mishima2019</td>
<td>17.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_1</td>
<td>Mishima2019</td>
<td>16.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>Cances2019</td>
<td>28.4</td>
<td>22KHz</td>
<td>pitch_shifting, time stretching, level, noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>Cances2019</td>
<td>26.1</td>
<td>22KHz</td>
<td>pitch_shifting, time stretching, level, noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>Cances2019</td>
<td>39.7</td>
<td>22KHz</td>
<td>pitch_shifting, time stretching, level</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_2</td>
<td>Lin2019</td>
<td>40.9</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_4</td>
<td>Lin2019</td>
<td>41.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_1</td>
<td>Lin2019</td>
<td>40.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_1</td>
<td>Bolun2019</td>
<td>21.7</td>
<td>32kHz</td>
<td>event adding</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_4</td>
<td>Bolun2019</td>
<td>25.3</td>
<td>32kHz</td>
<td>event adding</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_3</td>
<td>Bolun2019</td>
<td>23.8</td>
<td>32kHz</td>
<td>event adding</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>32kHz</td>
<td>event adding</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>44.1kHz</td>
<td>VAT</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_1</td>
<td>Kiyokawa2019</td>
<td>27.8</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_3</td>
<td>Kiyokawa2019</td>
<td>29.4</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_2</td>
<td>Kiyokawa2019</td>
<td>28.3</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>Kothinti2019</td>
<td>30.5</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>Kothinti2019</td>
<td>29.0</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>Kothinti2019</td>
<td>29.4</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>44.1kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_3</td>
<td>Shi2019</td>
<td>40.9</td>
<td>44.1kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_4</td>
<td>Shi2019</td>
<td>41.5</td>
<td>44.1kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_1</td>
<td>Shi2019</td>
<td>37.0</td>
<td>44.1kHz</td>
<td>Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_1</td>
<td>Zhang2019</td>
<td>29.4</td>
<td>44.1kHz</td>
<td></td>
<td>mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>44.1kHz</td>
<td></td>
<td>mel-spectrogram</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.5</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_2</td>
<td>Yang2019</td>
<td>6.2</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_3</td>
<td>Yang2019</td>
<td>6.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_1</td>
<td>Yan2019</td>
<td>35.8</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_3</td>
<td>Yan2019</td>
<td>35.6</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_4</td>
<td>Yan2019</td>
<td>33.5</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_2</td>
<td>Lee2019</td>
<td>25.8</td>
<td>44.1kHz</td>
<td></td>
<td>waveform</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_4</td>
<td>Lee2019</td>
<td>24.6</td>
<td>44.1kHz</td>
<td>notch filter</td>
<td>waveform</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>44.1kHz</td>
<td></td>
<td>waveform</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_1</td>
<td>Lee2019</td>
<td>26.4</td>
<td>44.1kHz</td>
<td></td>
<td>waveform</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>32kHz</td>
<td>occlusions</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim2019</td>
<td>32.6</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim2019</td>
<td>33.2</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim2019</td>
<td>32.5</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>44.1kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-field="machine_learning_semi_supervised" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Semi-supervised approach
            </th>
<th class="text-center narrow-col" data-field="post-processing" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Post-processing
            </th>
<th class="text-center narrow-col" data-field="segmentation_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Segmentation<br/>method
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>Wang2019</td>
<td>16.8</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td></td>
<td></td>
<td>mean probabilities</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>Wang2019</td>
<td>17.5</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td></td>
<td></td>
<td>mean probabilities</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>Wang2019</td>
<td>17.2</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td></td>
<td></td>
<td>mean probabilities</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>Wang2019</td>
<td>17.2</td>
<td>CRNN</td>
<td>pseudo-labelling</td>
<td></td>
<td></td>
<td>mean probabilities</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_1</td>
<td>Delphin-Poulat2019</td>
<td>38.3</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (class-dependent)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>Kong2019</td>
<td>22.3</td>
<td>CNN</td>
<td>supervised</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_2</td>
<td>Chan2019</td>
<td>29.7</td>
<td>NMF, CNN</td>
<td>non-negative matrix factorization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_3</td>
<td>Chan2019</td>
<td>27.7</td>
<td>NMF, CNN</td>
<td>non-negative matrix factorization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_4</td>
<td>Chan2019</td>
<td>26.9</td>
<td>NMF, CNN</td>
<td>non-negative matrix factorization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>NMF, CNN</td>
<td>non-negative matrix factorization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_3</td>
<td>Mishima2019</td>
<td>18.3</td>
<td>ResNet</td>
<td></td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>ResNet</td>
<td>pseudo-labelling</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_2</td>
<td>Mishima2019</td>
<td>17.7</td>
<td>ResNet</td>
<td>pseudo-labelling</td>
<td>time aggregation, median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_1</td>
<td>Mishima2019</td>
<td>16.7</td>
<td>ResNet</td>
<td></td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>Cances2019</td>
<td>28.4</td>
<td>CRNN</td>
<td></td>
<td>smoothed moving average</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>Cances2019</td>
<td>26.1</td>
<td>CRNN</td>
<td></td>
<td>smoothed moving average</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>Cances2019</td>
<td>39.7</td>
<td>CRNN</td>
<td></td>
<td>smoothed moving average</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_2</td>
<td>Lin2019</td>
<td>40.9</td>
<td>CNN</td>
<td>guiding learning with a more professional teacher</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_4</td>
<td>Lin2019</td>
<td>41.8</td>
<td>CNN</td>
<td>guiding learning with a more professional teacher</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>CNN</td>
<td>guiding learning with a more professional teacher</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_1</td>
<td>Lin2019</td>
<td>40.7</td>
<td>CNN</td>
<td>guiding learning with a more professional teacher</td>
<td>median filtering (with adaptive window size)</td>
<td>attention layer</td>
<td></td>
</tr>
<tr data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_1</td>
<td>Bolun2019</td>
<td>21.7</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_4</td>
<td>Bolun2019</td>
<td>25.3</td>
<td>CNN, RNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_3</td>
<td>Bolun2019</td>
<td>23.8</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>Bolun2019</td>
<td>27.8</td>
<td>CNN, RNN, ensemble</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>CRNN</td>
<td>mean-teacher student, VAT</td>
<td>median filtering</td>
<td>attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_1</td>
<td>Kiyokawa2019</td>
<td>27.8</td>
<td>ResNet, SENet</td>
<td></td>
<td>time thresholding</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>ResNet, SENet</td>
<td></td>
<td>time thresholding</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_3</td>
<td>Kiyokawa2019</td>
<td>29.4</td>
<td>ResNet, SENet</td>
<td></td>
<td>time thresholding</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_2</td>
<td>Kiyokawa2019</td>
<td>28.3</td>
<td>ResNet, SENet</td>
<td></td>
<td>time thresholding</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>Kothinti2019</td>
<td>30.5</td>
<td>CRNN, RBM, CRBM, PCA</td>
<td>mean-teacher student, pseudo-labelling</td>
<td></td>
<td>saliency</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>Kothinti2019</td>
<td>29.0</td>
<td>CRNN, RBM, CRBM, PCA, Kalman Filter</td>
<td>mean-teacher student, pseudo-labelling</td>
<td></td>
<td>saliency</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>Kothinti2019</td>
<td>29.4</td>
<td>CRNN, RBM, CRBM, PCA, Kalman Filter</td>
<td>mean-teacher student, pseudo-labelling</td>
<td></td>
<td>saliency</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>CRNN, RBM, CRBM, PCA</td>
<td>mean-teacher student, pseudo-labelling</td>
<td></td>
<td>saliency</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>CRNN</td>
<td>interpolation consistency training</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_3</td>
<td>Shi2019</td>
<td>40.9</td>
<td>CRNN</td>
<td>MixMatch</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_4</td>
<td>Shi2019</td>
<td>41.5</td>
<td>CRNN</td>
<td>mean-teacher student, interpolation consistency training, MixMatch</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_1</td>
<td>Shi2019</td>
<td>37.0</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_1</td>
<td>Zhang2019</td>
<td>29.4</td>
<td>CNN,ResNet,RNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>CNN,ResNet,RNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.5</td>
<td>CMRANN-MT</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_2</td>
<td>Yang2019</td>
<td>6.2</td>
<td>CMRANN-MT</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_3</td>
<td>Yang2019</td>
<td>6.7</td>
<td>CMRANN-MT</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_1</td>
<td>Yan2019</td>
<td>35.8</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_3</td>
<td>Yan2019</td>
<td>35.6</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_4</td>
<td>Yan2019</td>
<td>33.5</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_2</td>
<td>Lee2019</td>
<td>25.8</td>
<td>CNN</td>
<td>pseudo-labelling, mean-teacher student</td>
<td>minimum gap/length compensation</td>
<td></td>
<td>double thresholding</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_4</td>
<td>Lee2019</td>
<td>24.6</td>
<td>CNN</td>
<td>pseudo-labelling, mean-teacher student</td>
<td>minimum gap/length compensation</td>
<td></td>
<td>double thresholding</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>CNN</td>
<td>pseudo-labelling, mean-teacher student</td>
<td>minimum gap/length compensation</td>
<td></td>
<td>double thresholding</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_1</td>
<td>Lee2019</td>
<td>26.4</td>
<td>CNN</td>
<td>pseudo-labelling, mean-teacher student</td>
<td></td>
<td></td>
<td>double thresholding</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>CNN</td>
<td></td>
<td>voice activity detection</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim2019</td>
<td>32.6</td>
<td>CRNN, Ensemble</td>
<td></td>
<td>median filtering</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim2019</td>
<td>33.2</td>
<td>CRNN, Ensemble</td>
<td></td>
<td>median filtering</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim2019</td>
<td>32.5</td>
<td>CRNN, Ensemble</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>CRNN, Ensemble</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>mean probabilities, thresholding</td>
</tr>
</tbody>
</table>
<h2 id="complexity">Complexity</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_complexity_time" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Training time
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>Wang2019</td>
<td>16.8</td>
<td>4034268</td>
<td>3</td>
<td>2h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>Wang2019</td>
<td>17.5</td>
<td>4034268</td>
<td>3</td>
<td>2h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>Wang2019</td>
<td>17.2</td>
<td>4034268</td>
<td>3</td>
<td>2h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>Wang2019</td>
<td>17.2</td>
<td>4034268</td>
<td>3</td>
<td>2h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_2</td>
<td>Delphin-Poulat2019</td>
<td>42.1</td>
<td>1582036</td>
<td></td>
<td>21h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Delphin_OL_task4_1</td>
<td>Delphin-Poulat2019</td>
<td>38.3</td>
<td>1582036</td>
<td></td>
<td>17h (1 GTX 1080)</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task4_1</td>
<td>Kong2019</td>
<td>22.3</td>
<td>4686144</td>
<td></td>
<td>1h (1 Titan XP)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_2</td>
<td>Chan2019</td>
<td>29.7</td>
<td>4309450</td>
<td></td>
<td>0.5h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_3</td>
<td>Chan2019</td>
<td>27.7</td>
<td>4309450</td>
<td></td>
<td>0.5h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_4</td>
<td>Chan2019</td>
<td>26.9</td>
<td>4309450</td>
<td></td>
<td>0.5h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>CTK_NU_task4_1</td>
<td>Chan2019</td>
<td>31.0</td>
<td>4309450</td>
<td></td>
<td>0.5h (1 GTX 1060)</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_3</td>
<td>Mishima2019</td>
<td>18.3</td>
<td>23865546</td>
<td></td>
<td>6h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_4</td>
<td>Mishima2019</td>
<td>19.8</td>
<td>23865546</td>
<td></td>
<td>6h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_2</td>
<td>Mishima2019</td>
<td>17.7</td>
<td>23865546</td>
<td></td>
<td>8h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Mishima_NEC_task4_1</td>
<td>Mishima2019</td>
<td>16.7</td>
<td>23865546</td>
<td></td>
<td>6h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>Cances2019</td>
<td>28.4</td>
<td>420116</td>
<td></td>
<td>1h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_2</td>
<td>Cances2019</td>
<td>26.1</td>
<td>470036</td>
<td></td>
<td>1h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_1</td>
<td>Cances2019</td>
<td>39.7</td>
<td>165460</td>
<td></td>
<td>12h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_2</td>
<td>Lin2019</td>
<td>40.9</td>
<td>1209744</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_4</td>
<td>Lin2019</td>
<td>41.8</td>
<td>6048720</td>
<td>5</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_3</td>
<td>Lin2019</td>
<td>42.7</td>
<td>7258464</td>
<td>6</td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Lin_ICT_task4_1</td>
<td>Lin2019</td>
<td>40.7</td>
<td>1209744</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_dcase2019</td>
<td>Turpault2019</td>
<td>25.8</td>
<td>214356</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_1</td>
<td>Bolun2019</td>
<td>21.7</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_4</td>
<td>Bolun2019</td>
<td>25.3</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_3</td>
<td>Bolun2019</td>
<td>23.8</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>bolun_NWPU_task4_2</td>
<td>Bolun2019</td>
<td>27.8</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Agnone_PDL_task4_1</td>
<td>Agnone2019</td>
<td>25.0</td>
<td>214356</td>
<td></td>
<td>2h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_1</td>
<td>Kiyokawa2019</td>
<td>27.8</td>
<td>11408962</td>
<td></td>
<td>8h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_4</td>
<td>Kiyokawa2019</td>
<td>32.4</td>
<td>11408962</td>
<td></td>
<td>12h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_3</td>
<td>Kiyokawa2019</td>
<td>29.4</td>
<td>11408962</td>
<td></td>
<td>12h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kiyokawa_NEC_task4_2</td>
<td>Kiyokawa2019</td>
<td>28.3</td>
<td>11408962</td>
<td></td>
<td>12h (4 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>Kothinti2019</td>
<td>30.5</td>
<td>1200000</td>
<td>3</td>
<td>2h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>Kothinti2019</td>
<td>29.0</td>
<td>520000</td>
<td></td>
<td>2h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>Kothinti2019</td>
<td>29.4</td>
<td>1200000</td>
<td>3</td>
<td>2h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>Kothinti2019</td>
<td>30.7</td>
<td>520000</td>
<td></td>
<td>2h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_2</td>
<td>Shi2019</td>
<td>42.0</td>
<td>6878340</td>
<td>9</td>
<td>24h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_3</td>
<td>Shi2019</td>
<td>40.9</td>
<td>4585560</td>
<td>6</td>
<td>24h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_4</td>
<td>Shi2019</td>
<td>41.5</td>
<td>18342240</td>
<td>24</td>
<td>24h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>Shi_FRDC_task4_1</td>
<td>Shi2019</td>
<td>37.0</td>
<td>6878340</td>
<td>9</td>
<td>24h (1 TITAN Xp)</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_1</td>
<td>Zhang2019</td>
<td>29.4</td>
<td>298122</td>
<td></td>
<td>2.5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>ZYL_UESTC_task4_2</td>
<td>Zhang2019</td>
<td>30.8</td>
<td>298698</td>
<td></td>
<td>4h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_1</td>
<td>Yang2019</td>
<td>6.5</td>
<td>126090</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_2</td>
<td>Yang2019</td>
<td>6.2</td>
<td>126090</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Wang_YSU_task4_3</td>
<td>Yang2019</td>
<td>6.7</td>
<td>126090</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_1</td>
<td>Yan2019</td>
<td>35.8</td>
<td>7068540</td>
<td>5</td>
<td>3.5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_3</td>
<td>Yan2019</td>
<td>35.6</td>
<td>7068540</td>
<td>5</td>
<td>3.5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_4</td>
<td>Yan2019</td>
<td>33.5</td>
<td>7068540</td>
<td>5</td>
<td>3.5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Yan_USTC_task4_2</td>
<td>Yan2019</td>
<td>36.2</td>
<td>7068540</td>
<td>5</td>
<td>3.5h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_2</td>
<td>Lee2019</td>
<td>25.8</td>
<td>3425776</td>
<td></td>
<td>6h (1 GTX Titan V)</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_4</td>
<td>Lee2019</td>
<td>24.6</td>
<td>3425776</td>
<td></td>
<td>6h (1 GTX Titan V)</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_3</td>
<td>Lee2019</td>
<td>26.7</td>
<td>3425776</td>
<td></td>
<td>6h (1 GTX Titan V)</td>
</tr>
<tr>
<td></td>
<td>Lee_KNU_task4_1</td>
<td>Lee2019</td>
<td>26.4</td>
<td>3425776</td>
<td></td>
<td>6h (1 GTX Titan V)</td>
</tr>
<tr>
<td></td>
<td>Rakowski_SRPOL_task4_1</td>
<td>Rakowski2019</td>
<td>24.2</td>
<td>4691274</td>
<td></td>
<td>6h (1 Tesla P40)</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim2019</td>
<td>32.6</td>
<td>10572448</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim2019</td>
<td>33.2</td>
<td>42289792</td>
<td>16</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim2019</td>
<td>32.5</td>
<td>10572448</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim2019</td>
<td>34.4</td>
<td>42289792</td>
<td>16</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2019/technical_reports_task4.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Agnone2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Agnone2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        VIRTUAL ADVERSARIAL TRAINING SYSTEM FOR DCASE 2019 TASK 4
       </h4>
<p style="text-align:left">
        Agnone, Anthony and Altaf, Umair
       </p>
<p style="text-align:left">
<em>
         Pindrop, Audio Research department, Atlanta, United States
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Agnone_PDL_task4_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Agnone2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Agnone2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Agnone2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Agnone_78.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Agnone2019" class="panel-collapse collapse" id="collapse-Agnone2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       VIRTUAL ADVERSARIAL TRAINING SYSTEM FOR DCASE 2019 TASK 4
      </h4>
<p style="text-align:left">
<small>
        Agnone, Anthony and Altaf, Umair
       </small>
<br/>
<small>
<em>
         Pindrop, Audio Research department, Atlanta, United States
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes the approach used for Task 4 of the DCASE 2019 Challenge. This tasks challenges systems to learn from a combination of labeled and unlabeled data. Furthermore, the labeled data is itself a combination of strongly-informed, coarse time-based data and weakly-informed, fine time-based synthetic data. The baseline system builds off of the winning solution from last year, and adds the synthetic data, which was not provided in that iteration of the challenge. Our solution uses the semi-supervised virtual adversarial training method, in addition to the Mean Teacher consistency loss, to encourage generalization from weakly-labeled and unlabeled data. The chosen system parametrization achieves a 59.57% macro F1 score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         VAT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, mean-teacher student + VAT
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Agnone2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Agnone_78.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Agnone2019label" class="modal fade" id="bibtex-Agnone2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAgnone2019label">
        VIRTUAL ADVERSARIAL TRAINING SYSTEM FOR DCASE 2019 TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Agnone2019,
    Author = "Agnone, Anthony and Altaf, Umair",
    title = "VIRTUAL ADVERSARIAL TRAINING SYSTEM FOR DCASE 2019 TASK 4",
    institution = "Pindrop, Audio Research department, Atlanta, United States",
    year = "2019",
    month = "June",
    abstract = "This paper describes the approach used for Task 4 of the DCASE 2019 Challenge. This tasks challenges systems to learn from a combination of labeled and unlabeled data. Furthermore, the labeled data is itself a combination of strongly-informed, coarse time-based data and weakly-informed, fine time-based synthetic data. The baseline system builds off of the winning solution from last year, and adds the synthetic data, which was not provided in that iteration of the challenge. Our solution uses the semi-supervised virtual adversarial training method, in addition to the Mean Teacher consistency loss, to encourage generalization from weakly-labeled and unlabeled data. The chosen system parametrization achieves a 59.57\% macro F1 score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bolun2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Bolun2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CLASS WISE FUSION SYSTEM FOR DCASE 2019 TASK4
       </h4>
<p style="text-align:left">
        Wang, Bolun and Wu, Hao and Bai, Jisheng and Chen, Chen and Wang, Mou and Wang, Rui and Fu, Zhonghua and Chen, Jianfeng and Rahardja, Susanto and Zhang, Xiaolei
       </p>
<p style="text-align:left">
<em>
         Northwestern Polytechnical University, School of Computer Science, Xi'an, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">bolun_NWPU_task4_1</span> <span class="label label-primary">bolun_NWPU_task4_2</span> <span class="label label-primary">bolun_NWPU_task4_3</span> <span class="label label-primary">bolun_NWPU_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bolun2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bolun2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bolun2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Bolun_41_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bolun2019" class="panel-collapse collapse" id="collapse-Bolun2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CLASS WISE FUSION SYSTEM FOR DCASE 2019 TASK4
      </h4>
<p style="text-align:left">
<small>
        Wang, Bolun and Wu, Hao and Bai, Jisheng and Chen, Chen and Wang, Mou and Wang, Rui and Fu, Zhonghua and Chen, Jianfeng and Rahardja, Susanto and Zhang, Xiaolei
       </small>
<br/>
<small>
<em>
         Northwestern Polytechnical University, School of Computer Science, Xi'an, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we introduce our system for Task4 of Dcase 2019 challenges (Sound event detection in domestic environments). The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled, along with strong labeled simulated data. With the aim of improving performance with large amount of unlabeled data, and a small labeled training data. We focus on three parts: data augmentation, loss function, and network fusion.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         event adding
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bolun2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Bolun_41_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bolun2019label" class="modal fade" id="bibtex-Bolun2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBolun2019label">
        CLASS WISE FUSION SYSTEM FOR DCASE 2019 TASK4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bolun2019,
    Author = "Wang, Bolun and Wu, Hao and Bai, Jisheng and Chen, Chen and Wang, Mou and Wang, Rui and Fu, Zhonghua and Chen, Jianfeng and Rahardja, Susanto and Zhang, Xiaolei",
    title = "CLASS WISE FUSION SYSTEM FOR DCASE 2019 TASK4",
    institution = "Northwestern Polytechnical University, School of Computer Science, Xi'an, China",
    year = "2019",
    month = "June",
    abstract = "In this report, we introduce our system for Task4 of Dcase 2019 challenges (Sound event detection in domestic environments). The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled, along with strong labeled simulated data. With the aim of improving performance with large amount of unlabeled data, and a small labeled training data. We focus on three parts: data augmentation, loss function, and network fusion."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cances2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Cances2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTI TASK LEARNING AND POST PROCESSING OPTIMIZATION FOR SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Cances, LÃ©o and Pellegrini, Thomas and Guyot, Patrice
       </p>
<p style="text-align:left">
<em>
         IRIT, UniversitÃ© de Toulouse, CNRS, Toulouse, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">PELLEGRINI_IRIT_task4_1</span> <span class="label label-primary">CANCES_IRIT_task4_2</span> <span class="label label-primary">CANCES_IRIT_task4_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cances2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cances2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cances2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Cances_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Cances2019').collapse('show');window.location.hash='#Cances2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cances2019" class="panel-collapse collapse" id="collapse-Cances2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTI TASK LEARNING AND POST PROCESSING OPTIMIZATION FOR SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Cances, LÃ©o and Pellegrini, Thomas and Guyot, Patrice
       </small>
<br/>
<small>
<em>
         IRIT, UniversitÃ© de Toulouse, CNRS, Toulouse, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we report our experiments in Sound Event Detection in domestic environments in the framework of the DCASE 2019 Task 4 challenge. The novelty, this year, lies in the availability of three different subsets for development: a weakly annotated dataset, a strongly annotated synthetic subset, and an unlabeled subset. The weak annotations, unlike the strong ones, provide tags from audio events but do not provide temporal boundaries. The task objective is twofold: detecting audio events (multi label classification at recording level), and localizing the events precisely within the recordings. First, we explore multi task training to take advantage of the synthetic and unlabeled in domain subsets. Then, we applied various temporal segmentation methods using optimization algorithms to obtain the best performing segmentation parameters. On the multi-task itself, we explored two strategies based on convolutional recurrent neural networks (CRNN): 1) a single branch model with two outputs, 2) multi branch models with two or three outputs. These approaches outperform the baseline of 23.7% in F measure by a large margin, with values of respectively 39.9% and 33.8% for the first and second strategy, on the official validation subset comprised of 1103 recordings.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch_shifting, time stretching, level, noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cances2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Cances_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/topel/dcase19-RCNN-task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cances2019label" class="modal fade" id="bibtex-Cances2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCances2019label">
        MULTI TASK LEARNING AND POST PROCESSING OPTIMIZATION FOR SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cances2019,
    Author = "Cances, LÃ©o and Pellegrini, Thomas and Guyot, Patrice",
    title = "MULTI TASK LEARNING AND POST PROCESSING OPTIMIZATION FOR SOUND EVENT DETECTION",
    institution = "IRIT, UniversitÃ© de Toulouse, CNRS, Toulouse, France",
    year = "2019",
    month = "June",
    abstract = "In this paper, we report our experiments in Sound Event Detection in domestic environments in the framework of the DCASE 2019 Task 4 challenge. The novelty, this year, lies in the availability of three different subsets for development: a weakly annotated dataset, a strongly annotated synthetic subset, and an unlabeled subset. The weak annotations, unlike the strong ones, provide tags from audio events but do not provide temporal boundaries. The task objective is twofold: detecting audio events (multi label classification at recording level), and localizing the events precisely within the recordings. First, we explore multi task training to take advantage of the synthetic and unlabeled in domain subsets. Then, we applied various temporal segmentation methods using optimization algorithms to obtain the best performing segmentation parameters. On the multi-task itself, we explored two strategies based on convolutional recurrent neural networks (CRNN): 1) a single branch model with two outputs, 2) multi branch models with two or three outputs. These approaches outperform the baseline of 23.7\% in F measure by a large margin, with values of respectively 39.9\% and 33.8\% for the first and second strategy, on the official validation subset comprised of 1103 recordings."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chan2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Chan2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        NON-NEGATIVE MATRIX FACTORIZATION-CONVOLUTION NEURAL NETWORK (NMF-CNN) FOR SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Chan, Teck Kai and Chin, Cheng Siong and Li, Ye
       </p>
<p style="text-align:left">
<em>
         Newcastle University, Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">CTK_NU_task4_1</span> <span class="label label-primary">CTK_NU_task4_2</span> <span class="label label-primary">CTK_NU_task4_3</span> <span class="label label-primary">CTK_NU_task4_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chan2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chan2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chan_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chan2019" class="panel-collapse collapse" id="collapse-Chan2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       NON-NEGATIVE MATRIX FACTORIZATION-CONVOLUTION NEURAL NETWORK (NMF-CNN) FOR SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Chan, Teck Kai and Chin, Cheng Siong and Li, Ye
       </small>
<br/>
<small>
<em>
         Newcastle University, Singapore
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The main scientific question of this year DCASE challenge, Task 4 - Sound Event Detection in Domestic Environments, is to investigate the types of data (strongly labeled synthetic data, weakly labeled data, unlabeled in domain data) required to achieve the best performing system. In this paper, we proposed a deep learning model that integrates Convolution Neural Network (CNN) with Non-Negative Matrix Factorization (NMF). The best performing model can achieve a higher event based F1-score of 30.39% as compared to the baseline system that achieved an F1-score of 23.7% on the validation dataset. Based on the results, even though synthetic data is strongly labeled, it cannot be used as a sole source of training data and resulted in the worst performance. Although, using a combination of weakly and strongly labeled data can achieve the highest F1-score, but the increment was not significant and may not be worth- while to include synthetic data into the training set. Results have also suggested that the quality of labeling unlabeled in domain data is essential and can have an adverse effect on the accuracy rather than improving the model performance if labeling was not done accurately.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         NMF, CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Chan_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chan2019label" class="modal fade" id="bibtex-Chan2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChan2019label">
        NON-NEGATIVE MATRIX FACTORIZATION-CONVOLUTION NEURAL NETWORK (NMF-CNN) FOR SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chan2019,
    Author = "Chan, Teck Kai and Chin, Cheng Siong and Li, Ye",
    title = "NON-NEGATIVE MATRIX FACTORIZATION-CONVOLUTION NEURAL NETWORK (NMF-CNN) FOR SOUND EVENT DETECTION",
    institution = "Newcastle University, Singapore",
    year = "2019",
    month = "June",
    abstract = "The main scientific question of this year DCASE challenge, Task 4 - Sound Event Detection in Domestic Environments, is to investigate the types of data (strongly labeled synthetic data, weakly labeled data, unlabeled in domain data) required to achieve the best performing system. In this paper, we proposed a deep learning model that integrates Convolution Neural Network (CNN) with Non-Negative Matrix Factorization (NMF). The best performing model can achieve a higher event based F1-score of 30.39\% as compared to the baseline system that achieved an F1-score of 23.7\% on the validation dataset. Based on the results, even though synthetic data is strongly labeled, it cannot be used as a sole source of training data and resulted in the worst performance. Although, using a combination of weakly and strongly labeled data can achieve the highest F1-score, but the increment was not significant and may not be worth- while to include synthetic data into the training set. Results have also suggested that the quality of labeling unlabeled in domain data is essential and can have an adverse effect on the accuracy rather than improving the model performance if labeling was not done accurately."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Delphin-Poulat2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Delphin-Poulat2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4
       </h4>
<p style="text-align:left">
        Delphin-Poulat, Lionel and Plapous, Cyril
       </p>
<p style="text-align:left">
<em>
         Orange Labs Lannion, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Delphin_OL_task4_1</span> <span class="label label-primary">Delphin_OL_task4_2</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Delphin-Poulat2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Delphin-Poulat2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Delphin-Poulat2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Delphin_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Delphin-Poulat2019" class="panel-collapse collapse" id="collapse-Delphin-Poulat2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4
      </h4>
<p style="text-align:left">
<small>
        Delphin-Poulat, Lionel and Plapous, Cyril
       </small>
<br/>
<small>
<em>
         Orange Labs Lannion, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present our neural network for the DCASE 2019 challengeâ€™s Task 4 (Sound event detection in domestic environments) [1]. The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled. We propose a mean-teacher model with convolutional neural network (CNN) and recurrent neural network (RNN) together with data augmentation and a median window tuned for each class based on prior knowledge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time shifting, frequency shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Delphin-Poulat2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Delphin_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Delphin-Poulat2019label" class="modal fade" id="bibtex-Delphin-Poulat2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDelphin-Poulat2019label">
        MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Delphin-Poulat2019,
    Author = "Delphin-Poulat, Lionel and Plapous, Cyril",
    title = "MEAN TEACHER WITH DATA AUGMENTATION FOR DCASE 2019 TASK 4",
    institution = "Orange Labs Lannion, France",
    year = "2019",
    month = "June",
    abstract = "In this paper, we present our neural network for the DCASE 2019 challengeâ€™s Task 4 (Sound event detection in domestic environments) [1]. The goal of the task is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled. We propose a mean-teacher model with convolutional neural network (CNN) and recurrent neural network (RNN) together with data augmentation and a median window tuned for each class based on prior knowledge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kiyokawa2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kiyokawa2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT DETECTION WITH RESNET AND SELF-MASK MODULE FOR DCASE 2019 TASK 4
       </h4>
<p style="text-align:left">
        Kiyokawa, Yu and Mishima, Sakiko and Toizumi, Takahiro and Sagi, Kazutoshi and Kondo, Reishi and Nomura, Toshiyuki
       </p>
<p style="text-align:left">
<em>
         Data Science Research Laboratories, NEC Corporation, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kiyokawa_NEC_task4_1</span> <span class="label label-primary">Kiyokawa_NEC_task4_2</span> <span class="label label-primary">Kiyokawa_NEC_task4_3</span> <span class="label label-primary">Kiyokawa_NEC_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kiyokawa2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kiyokawa2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kiyokawa2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kiyokawa_101.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kiyokawa2019" class="panel-collapse collapse" id="collapse-Kiyokawa2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT DETECTION WITH RESNET AND SELF-MASK MODULE FOR DCASE 2019 TASK 4
      </h4>
<p style="text-align:left">
<small>
        Kiyokawa, Yu and Mishima, Sakiko and Toizumi, Takahiro and Sagi, Kazutoshi and Kondo, Reishi and Nomura, Toshiyuki
       </small>
<br/>
<small>
<em>
         Data Science Research Laboratories, NEC Corporation, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we propose a sound event detection system using a residual network (ResNet) with a self-mask module for a task 4 of detection and classification of acoustic scenes and events 2019 (DCASE 2019) challenge. Our system is constructed with a convolutional neural network based on a ResNet. We introduce a self-mask module as a region proposal network in order to detect event time boundaries. The self-mask module constrains time duration of silent and sound events by proposing candidates of the sound event region. These constraints improve detection accuracy of the sound event regions. Evaluation results show that our system obtains 36.09% of event-based F1-score for a sound event detection on a validation dataset of the task 4.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ResNet, SENet
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kiyokawa2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kiyokawa_101.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kiyokawa2019label" class="modal fade" id="bibtex-Kiyokawa2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKiyokawa2019label">
        SOUND EVENT DETECTION WITH RESNET AND SELF-MASK MODULE FOR DCASE 2019 TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kiyokawa2019,
    Author = "Kiyokawa, Yu and Mishima, Sakiko and Toizumi, Takahiro and Sagi, Kazutoshi and Kondo, Reishi and Nomura, Toshiyuki",
    title = "SOUND EVENT DETECTION WITH RESNET AND SELF-MASK MODULE FOR DCASE 2019 TASK 4",
    institution = "Data Science Research Laboratories, NEC Corporation, Japan",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we propose a sound event detection system using a residual network (ResNet) with a self-mask module for a task 4 of detection and classification of acoustic scenes and events 2019 (DCASE 2019) challenge. Our system is constructed with a convolutional neural network based on a ResNet. We introduce a self-mask module as a region proposal network in order to detect event time boundaries. The self-mask module constrains time duration of silent and sound events by proposing candidates of the sound event region. These constraints improve detection accuracy of the sound event regions. Evaluation results show that our system obtains 36.09\% of event-based F1-score for a sound event detection on a validation dataset of the task 4."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
       </h4>
<p style="text-align:left">
        Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.
       </p>
<p style="text-align:left">
<em>
         Centre for Vision, Speech and Signal Processing, University of Surrey, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kong_SURREY_task4_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2019').collapse('show');window.location.hash='#Kong2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2019" class="panel-collapse collapse" id="collapse-Kong2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
      </h4>
<p style="text-align:left">
<small>
        Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.
       </small>
<br/>
<small>
<em>
         Centre for Vision, Speech and Signal Processing, University of Surrey, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2019_task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2019label" class="modal fade" id="bibtex-Kong2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2019label">
        CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2019,
    Author = "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.",
    title = "CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS",
    institution = "Centre for Vision, Speech and Signal Processing, University of Surrey, UK",
    year = "2019",
    month = "June",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kothinti2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kothinti2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        INTEGRATED BOTTOM-UP AND TOP-DOWN INFERENCE FOR SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Kothinti, Sandeep and Sell, Gregory and Watanabe, Shinji and Elhilali, Mounya
       </p>
<p style="text-align:left">
<em>
         Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kothinti_JHU_task4_1</span> <span class="label label-primary">Kothinti_JHU_task4_2</span> <span class="label label-primary">Kothinti_JHU_task4_3</span> <span class="label label-primary">Mishima_NEC_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kothinti2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kothinti2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kothinti2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kothinti_79.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kothinti2019" class="panel-collapse collapse" id="collapse-Kothinti2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       INTEGRATED BOTTOM-UP AND TOP-DOWN INFERENCE FOR SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Kothinti, Sandeep and Sell, Gregory and Watanabe, Shinji and Elhilali, Mounya
       </small>
<br/>
<small>
<em>
         Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       While supervised methods have been highly effective at defining boundaries of sound events, the characteristics of the acoustic scene itself can provide complementary information about the changing profile of the scene and presence of new events. This work explores an integrated supervised and unsupervised approach to weakly labeled sound event detection by complementing a class-based inference system with a bottom-up, salience-based analysis. The two systems work conjointly in two ways: 1) Class information from the supervised model is used to tune the parameters of the bottom-up salience detection; and 2) Salience-based boundaries are leveraged to create pseudo-labels for weakly labeled data to generate more samples of strongly annotated data. These operations reflect the interplay between stimulus driven analysis and semantic driven analysis. The proposed method gives an absolute improvement of 11% on macro-averaged F-score on the development set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, auditory spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, RBM, CRBM, PCA, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kothinti2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kothinti_79.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kothinti2019label" class="modal fade" id="bibtex-Kothinti2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKothinti2019label">
        INTEGRATED BOTTOM-UP AND TOP-DOWN INFERENCE FOR SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kothinti2019,
    Author = "Kothinti, Sandeep and Sell, Gregory and Watanabe, Shinji and Elhilali, Mounya",
    title = "INTEGRATED BOTTOM-UP AND TOP-DOWN INFERENCE FOR SOUND EVENT DETECTION",
    institution = "Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, USA",
    year = "2019",
    month = "June",
    abstract = "While supervised methods have been highly effective at defining boundaries of sound events, the characteristics of the acoustic scene itself can provide complementary information about the changing profile of the scene and presence of new events. This work explores an integrated supervised and unsupervised approach to weakly labeled sound event detection by complementing a class-based inference system with a bottom-up, salience-based analysis. The two systems work conjointly in two ways: 1) Class information from the supervised model is used to tune the parameters of the bottom-up salience detection; and 2) Salience-based boundaries are leveraged to create pseudo-labels for weakly labeled data to generate more samples of strongly annotated data. These operations reflect the interplay between stimulus driven analysis and semantic driven analysis. The proposed method gives an absolute improvement of 11\% on macro-averaged F-score on the development set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Lee2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        END-TO-END DEEP CONVOLUTIONAL NEURAL NETWORK WITH MULTI-SCALE STRUCTURE FOR WEAKLY LABELED SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Lee, Seokjin and Kim, Minhan and Jeong, Youngho
       </p>
<p style="text-align:left">
<em>
         Kyungpook National University, School of Electronics Engineering, Daegu, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lee_KNU_task4_1</span> <span class="label label-primary">Lee_KNU_task4_2</span> <span class="label label-primary">Lee_KNU_task4_3</span> <span class="label label-primary">Lee_KNU_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lee2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lee_30.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2019" class="panel-collapse collapse" id="collapse-Lee2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       END-TO-END DEEP CONVOLUTIONAL NEURAL NETWORK WITH MULTI-SCALE STRUCTURE FOR WEAKLY LABELED SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Lee, Seokjin and Kim, Minhan and Jeong, Youngho
       </small>
<br/>
<small>
<em>
         Kyungpook National University, School of Electronics Engineering, Daegu, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, an end-to-end sound event detection algorithm that detects and classifies the sound events from the waveform itself. The proposed model consists of multi-scale time frames and networks to handle both short and long signal characteristics; the frame slides 0.1 second to provide sufficiently fine resolution. The element network for each time frame data consists of several one-dimensional convolutional neural networks (1D-CNNs) with deeply stacked structure. The results of element networks are averaged and gated by sound activity detection. The decision is made by performing the double thresholding, and the results are enhanced by class-wise minimum gap/length compensation. To evaluate our proposed network, the simulation was performed with development data from DCASE 2019 Task 4, and the results show that the proposed algorithm has a macro-averaged f1-score of 31.7% for the development dataset of DCASE 2019 and 30.2% for the evaluation dataset of DCASE 2018.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         waveform
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, pseudo-labelling, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lee_30.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2019label" class="modal fade" id="bibtex-Lee2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2019label">
        END-TO-END DEEP CONVOLUTIONAL NEURAL NETWORK WITH MULTI-SCALE STRUCTURE FOR WEAKLY LABELED SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lee2019,
    Author = "Lee, Seokjin and Kim, Minhan and Jeong, Youngho",
    title = "END-TO-END DEEP CONVOLUTIONAL NEURAL NETWORK WITH MULTI-SCALE STRUCTURE FOR WEAKLY LABELED SOUND EVENT DETECTION",
    institution = "Kyungpook National University, School of Electronics Engineering, Daegu, Republic of Korea",
    year = "2019",
    month = "June",
    abstract = "In this paper, an end-to-end sound event detection algorithm that detects and classifies the sound events from the waveform itself. The proposed model consists of multi-scale time frames and networks to handle both short and long signal characteristics; the frame slides 0.1 second to provide sufficiently fine resolution. The element network for each time frame data consists of several one-dimensional convolutional neural networks (1D-CNNs) with deeply stacked structure. The results of element networks are averaged and gated by sound activity detection. The decision is made by performing the double thresholding, and the results are enhanced by class-wise minimum gap/length compensation. To evaluate our proposed network, the simulation was performed with development data from DCASE 2019 Task 4, and the results show that the proposed algorithm has a macro-averaged f1-score of 31.7\% for the development dataset of DCASE 2019 and 30.2\% for the evaluation dataset of DCASE 2018."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lim2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Lim2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS USING ENSEMBLE OF CONVOLUTIONAL RECURRENT NEURAL NETWORKS
       </h4>
<p style="text-align:left">
        Lim, Wootaek and Suh, Sangwon and Park, Sooyoung and Jeong, Youngho
       </p>
<p style="text-align:left">
<em>
         Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lim_ETRI_task4_1</span> <span class="label label-primary">Lim_ETRI_task4_2</span> <span class="label label-primary">Lim_ETRI_task4_3</span> <span class="label label-primary">Lim_ETRI_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lim2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lim2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lim2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lim_77.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lim2019" class="panel-collapse collapse" id="collapse-Lim2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS USING ENSEMBLE OF CONVOLUTIONAL RECURRENT NEURAL NETWORKS
      </h4>
<p style="text-align:left">
<small>
        Lim, Wootaek and Suh, Sangwon and Park, Sooyoung and Jeong, Youngho
       </small>
<br/>
<small>
<em>
         Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present a method to detect sound events in domestic environments using small weakly labeled data, large unlabeled data, and strongly labeled synthetic data as proposed in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge task 4. To solve the problem, we use convolutional recurrent neural network (CRNN), as it stacks convolutional neural networks (CNN) and bi-directional gated recurrent unit (Bi-GRU). Moreover, we propose various methods such as data augmentation, event activity detection, multi-median filtering, mean-teacher student model, and the ensemble of neural networks to improve performance. By combining the proposed method, sound event detection performance can be enhanced, compared with the baseline algorithm. As a result, performance evaluation shows that the proposed method provides detection results of 40.89% for event-based metrics and 66.17% for segment-based metrics.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probabilities, thresholding
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lim2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lim_77.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lim2019label" class="modal fade" id="bibtex-Lim2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLim2019label">
        SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS USING ENSEMBLE OF CONVOLUTIONAL RECURRENT NEURAL NETWORKS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lim2019,
    Author = "Lim, Wootaek and Suh, Sangwon and Park, Sooyoung and Jeong, Youngho",
    title = "SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS USING ENSEMBLE OF CONVOLUTIONAL RECURRENT NEURAL NETWORKS",
    institution = "Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea",
    year = "2019",
    month = "June",
    abstract = "In this paper, we present a method to detect sound events in domestic environments using small weakly labeled data, large unlabeled data, and strongly labeled synthetic data as proposed in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge task 4. To solve the problem, we use convolutional recurrent neural network (CRNN), as it stacks convolutional neural networks (CNN) and bi-directional gated recurrent unit (Bi-GRU). Moreover, we propose various methods such as data augmentation, event activity detection, multi-median filtering, mean-teacher student model, and the ensemble of neural networks to improve performance. By combining the proposed method, sound event detection performance can be enhanced, compared with the baseline algorithm. As a result, performance evaluation shows that the proposed method provides detection results of 40.89\% for event-based metrics and 66.17\% for segment-based metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lin2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Lin2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GUIDED LEARNING CONVOLUTION SYSTEM FOR DCASE 2019 TASK 4
       </h4>
<p style="text-align:left">
        Lin, Liwei and Wang, Xiangdong
       </p>
<p style="text-align:left">
<em>
         Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lin_ICT_task4_1</span> <span class="label label-primary">Lin_ICT_task4_2</span> <span class="label label-primary">Lin_ICT_task4_3</span> <span class="label label-primary">Lin_ICT_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lin2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lin2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lin_25.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Lin2019').collapse('show');window.location.hash='#Lin2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lin2019" class="panel-collapse collapse" id="collapse-Lin2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GUIDED LEARNING CONVOLUTION SYSTEM FOR DCASE 2019 TASK 4
      </h4>
<p style="text-align:left">
<small>
        Lin, Liwei and Wang, Xiangdong
       </small>
<br/>
<small>
<em>
         Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe in detail the system we submitted to DCASE2019 task 4: sound event detection (SED) in domestic environments. We approach SED as a multiple instance learning (MIL) problem and employ a convolutional neural network (CNN) with class-wise attention pooling (cATP) module to solve it. By considering the interference caused by the co-occurrence of multiple events in the unbalanced dataset, we combine the cATP-MIL framework with the disentangled feature. To take advantage of the unlabeled data, we adopt the guided learning with a more professional teacher for semi-supervised learning. A group of median filters with adaptive window sizes is utilized in post-processing. We also analyze the effect of the synthetic data on the performance of the model and finally achieve an F-measure of 45.43% on the validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Lin_25.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Kikyo-16/Sound_event_detection" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lin2019label" class="modal fade" id="bibtex-Lin2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLin2019label">
        GUIDED LEARNING CONVOLUTION SYSTEM FOR DCASE 2019 TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lin2019,
    Author = "Lin, Liwei and Wang, Xiangdong",
    title = "GUIDED LEARNING CONVOLUTION SYSTEM FOR DCASE 2019 TASK 4",
    institution = "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we describe in detail the system we submitted to DCASE2019 task 4: sound event detection (SED) in domestic environments. We approach SED as a multiple instance learning (MIL) problem and employ a convolutional neural network (CNN) with class-wise attention pooling (cATP) module to solve it. By considering the interference caused by the co-occurrence of multiple events in the unbalanced dataset, we combine the cATP-MIL framework with the disentangled feature. To take advantage of the unlabeled data, we adopt the guided learning with a more professional teacher for semi-supervised learning. A group of median filters with adaptive window sizes is utilized in post-processing. We also analyze the effect of the synthetic data on the performance of the model and finally achieve an F-measure of 45.43\% on the validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mishima2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Mishima2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        TRAINING METHOD USING CLASS-FRAME PSEUDO LABEL FOR WEAKLY LABELED DATASET ON DCASE2019
       </h4>
<p style="text-align:left">
        Mishima, Sakiko and Kiyokawa, Yu and Toizumi, Takahiro and Sagi, Kazutoshi and Kondo, Reishi and Nomura, Toshiyuki
       </p>
<p style="text-align:left">
<em>
         Data Science Research Laboratories, NEC Corporation, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mishima_NEC_task4_1</span> <span class="label label-primary">Mishima_NEC_task4_2</span> <span class="label label-primary">Mishima_NEC_task4_3</span> <span class="label label-primary">Mishima_NEC_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mishima2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mishima2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mishima2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Mishima_108.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mishima2019" class="panel-collapse collapse" id="collapse-Mishima2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       TRAINING METHOD USING CLASS-FRAME PSEUDO LABEL FOR WEAKLY LABELED DATASET ON DCASE2019
      </h4>
<p style="text-align:left">
<small>
        Mishima, Sakiko and Kiyokawa, Yu and Toizumi, Takahiro and Sagi, Kazutoshi and Kondo, Reishi and Nomura, Toshiyuki
       </small>
<br/>
<small>
<em>
         Data Science Research Laboratories, NEC Corporation, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We propose a training method using class-frame pseudo label for weakly labeled datasets given by the IEEE AASP challenge on detection and classification of acoustic scenes and events 2019 (DCASE2019) task 4. Our model is constructed based on a residual network (ResNet) and trained by datasets including strong and weak labels. The strong label has event classes and their presences at each frame, and the weak label has only event classes. In order to train the model effectively, we propose class-frame pseudo labels for weakly labeled datasets. The class-frame pseudo label contributes to improvement of the event presence prediction at each frame by avoidance of overfitting to strongly labeled datasets. A result shows that F1-scores by our proposed method are 25.9% and 62.0% in the event-based and segment-based evaluations, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ResNet
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mishima2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Mishima_108.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mishima2019label" class="modal fade" id="bibtex-Mishima2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMishima2019label">
        TRAINING METHOD USING CLASS-FRAME PSEUDO LABEL FOR WEAKLY LABELED DATASET ON DCASE2019
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mishima2019,
    Author = "Mishima, Sakiko and Kiyokawa, Yu and Toizumi, Takahiro and Sagi, Kazutoshi and Kondo, Reishi and Nomura, Toshiyuki",
    title = "TRAINING METHOD USING CLASS-FRAME PSEUDO LABEL FOR WEAKLY LABELED DATASET ON DCASE2019",
    institution = "Data Science Research Laboratories, NEC Corporation, Japan",
    year = "2019",
    month = "June",
    abstract = "We propose a training method using class-frame pseudo label for weakly labeled datasets given by the IEEE AASP challenge on detection and classification of acoustic scenes and events 2019 (DCASE2019) task 4. Our model is constructed based on a residual network (ResNet) and trained by datasets including strong and weak labels. The strong label has event classes and their presences at each frame, and the weak label has only event classes. In order to train the model effectively, we propose class-frame pseudo labels for weakly labeled datasets. The class-frame pseudo label contributes to improvement of the event presence prediction at each frame by avoidance of overfitting to strongly labeled datasets. A result shows that F1-scores by our proposed method are 25.9\% and 62.0\% in the event-based and segment-based evaluations, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Rakowski2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Rakowski2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        REGULARIZED CNN FOR SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
       </h4>
<p style="text-align:left">
        Rakowski, Alexander
       </p>
<p style="text-align:left">
<em>
         Samsung R&amp;D Poland, Audio Intelligence Dept., Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Rakowski_SRPOL_task4_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Rakowski2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Rakowski2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Rakowski2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Rakowski_58_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Rakowski2019" class="panel-collapse collapse" id="collapse-Rakowski2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       REGULARIZED CNN FOR SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
      </h4>
<p style="text-align:left">
<small>
        Rakowski, Alexander
       </small>
<br/>
<small>
<em>
         Samsung R&amp;D Poland, Audio Intelligence Dept., Warsaw, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes a system used for Task 4 of the DCASE 2019 Challenge - Sound Event Detection in Domestic Environments. The system consists of a 9-layer convolutional neural network which yields frame-level predictions. These are then aggregated using a Voice Activity Detection algorithm in order to extract sound events. To prevent the system from overfitting two techniques are applied. The first one consists of training the model with channel- and pixel-wise dropout. The second one removes information from a randomly selected subset of frames.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         occlusions
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Rakowski2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Rakowski_58_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Rakowski2019label" class="modal fade" id="bibtex-Rakowski2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRakowski2019label">
        REGULARIZED CNN FOR SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Rakowski2019,
    Author = "Rakowski, Alexander",
    title = "REGULARIZED CNN FOR SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS",
    institution = "Samsung R\&amp;D Poland, Audio Intelligence Dept., Warsaw, Poland",
    year = "2019",
    month = "June",
    abstract = "This report describes a system used for Task 4 of the DCASE 2019 Challenge - Sound Event Detection in Domestic Environments. The system consists of a 9-layer convolutional neural network which yields frame-level predictions. These are then aggregated using a Voice Activity Detection algorithm in order to extract sound events. To prevent the system from overfitting two techniques are applied. The first one consists of training the model with channel- and pixel-wise dropout. The second one removes information from a randomly selected subset of frames."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Shi2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Shi2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        HODGEPODGE: SOUND EVENT DETECTION BASED ON ENSEMBLE OF SEMI-SUPERVISED LEARNING METHODS
       </h4>
<p style="text-align:left">
        Shi, Ziqiang
       </p>
<p style="text-align:left">
<em>
         Fujitsu Research and Development Center, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Shi_FRDC_task4_1</span> <span class="label label-primary">Shi_FRDC_task4_2</span> <span class="label label-primary">Shi_FRDC_task4_3</span> <span class="label label-primary">Shi_FRDC_task4_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Shi2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Shi2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Shi2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Shi_11.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Shi2019" class="panel-collapse collapse" id="collapse-Shi2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       HODGEPODGE: SOUND EVENT DETECTION BASED ON ENSEMBLE OF SEMI-SUPERVISED LEARNING METHODS
      </h4>
<p style="text-align:left">
<small>
        Shi, Ziqiang
       </small>
<br/>
<small>
<em>
         Fujitsu Research and Development Center, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present the techniques and models applied to our submission for DCASE 2019 task 4: Sound event detection in domestic environments. We aim to focus primarily on how to apply semi-supervise learning methods efficiently to deal with large amount of unlabeled in-domain data. Three semi-supervised learning principles have been used in our system, including: 1) Consistency regularization applies data augmentation; 2) MixUp regularizer requiring that the prediction for a interpolation of two inputs is close to the interpolation of the prediction for each individual input; 3) MixUp regularization applies to interpolation between data augmentations. We also tried an ensemble of various models, trained by using different semi-supervised learning principles.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Shi2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Shi_11.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Shi2019label" class="modal fade" id="bibtex-Shi2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexShi2019label">
        HODGEPODGE: SOUND EVENT DETECTION BASED ON ENSEMBLE OF SEMI-SUPERVISED LEARNING METHODS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Shi2019,
    Author = "Shi, Ziqiang",
    title = "HODGEPODGE: SOUND EVENT DETECTION BASED ON ENSEMBLE OF SEMI-SUPERVISED LEARNING METHODS",
    institution = "Fujitsu Research and Development Center, Beijing, China",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we present the techniques and models applied to our submission for DCASE 2019 task 4: Sound event detection in domestic environments. We aim to focus primarily on how to apply semi-supervise learning methods efficiently to deal with large amount of unlabeled in-domain data. Three semi-supervised learning principles have been used in our system, including: 1) Consistency regularization applies data augmentation; 2) MixUp regularizer requiring that the prediction for a interpolation of two inputs is close to the interpolation of the prediction for each individual input; 3) MixUp regularization applies to interpolation between data augmentations. We also tried an ensemble of various models, trained by using different semi-supervised learning principles."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Turpault2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Turpault2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound event detection in domestic environments with weakly labeled data and soundscape synthesis
       </h4>
<p style="text-align:left">
        Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin
       </p>
<p style="text-align:left">
<em>
         UniversitÃ© de Lorraine, CNRS, Inria, Loria, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Task 4 baseline</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Turpault2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Turpault2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Turpault2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02160855" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Turpault2019').collapse('show');window.location.hash='#Turpault2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:3px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Turpault2019" class="panel-collapse collapse" id="collapse-Turpault2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound event detection in domestic environments with weakly labeled data and soundscape synthesis
      </h4>
<p style="text-align:left">
<small>
        Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin
       </small>
<br/>
<small>
<em>
         UniversitÃ© de Lorraine, CNRS, Inria, Loria, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents DCASE 2019 task 4 and proposes a first analysis of the results. The task is the follow up to DCASE 2018 task 4 and evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The paper focuses in particular on the additional synthetic, strongly labeled, dataset provided this year.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Turpault2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-02160855" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/turpaultn/DCASE2019_task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Turpault2019label" class="modal fade" id="bibtex-Turpault2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTurpault2019label">
        Sound event detection in domestic environments with weakly labeled data and soundscape synthesis
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Turpault2019,
    AUTHOR = "Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in domestic environments with weakly labeled data and soundscape synthesis}",
    institution = "UniversitÃ© de Lorraine, CNRS, Inria, Loria, France",
    note = "working paper or preprint",
    year = "2019",
    month = "June",
    abstract = "This paper presents DCASE 2019 task 4 and proposes a first analysis of the results. The task is the follow up to DCASE 2018 task 4 and evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The paper focuses in particular on the additional synthetic, strongly labeled, dataset provided this year."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT DETECTION USING WEAKLY LABELED AND UNLABELED DATA WITH SELF-ADAPTIVE EVENT THRESHOLD
       </h4>
<p style="text-align:left">
        Wang, Dezhi and Zhang, Lilun and Bao, Changchun and Wang, Yongxian and Xu, Kele and Zhu, Boqing
       </p>
<p style="text-align:left">
<em>
         National University of Defense Technology, College of Meteorology and Oceanography, Changsha, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_NUDT_task4_1</span> <span class="label label-primary">Wang_NUDT_task4_2</span> <span class="label label-primary">Wang_NUDT_task4_3</span> <span class="label label-primary">Wang_NUDT_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wang_76.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2019" class="panel-collapse collapse" id="collapse-Wang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT DETECTION USING WEAKLY LABELED AND UNLABELED DATA WITH SELF-ADAPTIVE EVENT THRESHOLD
      </h4>
<p style="text-align:left">
<small>
        Wang, Dezhi and Zhang, Lilun and Bao, Changchun and Wang, Yongxian and Xu, Kele and Zhu, Boqing
       </small>
<br/>
<small>
<em>
         National University of Defense Technology, College of Meteorology and Oceanography, Changsha, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The details of our method submitted to the task 4 of DCASE challenge 2019 are described in this technical report. This task evaluates systems for the detection of sound events in domestic environments using large-scale weakly labeled data. In particular, an architecture based on the framework of convolutional recurrent neural network (CRNN) is utilized to detect the timestamps of all the events in given audio clips where the training audio files have only clip-level labels. In order to take advantage of the large-scale unlabeled in-domain training data, an audio tagging system using deep residual network (ResNext) is first employed to make predictions for weak labels of the unlabeled data before the sound event detection process. In addition, a self-adaptive searching strategy for best sound-event thresholds is applied in the model testing process, which is believed to have some benefits on the improvement of model performance and generalization capability. Finally, the system achieves 23.79% F1-value in class-wise average metrics for the sound event detection on the provided testing dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, delta feature
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Wang_76.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2019label" class="modal fade" id="bibtex-Wang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2019label">
        SOUND EVENT DETECTION USING WEAKLY LABELED AND UNLABELED DATA WITH SELF-ADAPTIVE EVENT THRESHOLD
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2019,
    Author = "Wang, Dezhi and Zhang, Lilun and Bao, Changchun and Wang, Yongxian and Xu, Kele and Zhu, Boqing",
    title = "SOUND EVENT DETECTION USING WEAKLY LABELED AND UNLABELED DATA WITH SELF-ADAPTIVE EVENT THRESHOLD",
    institution = "National University of Defense Technology, College of Meteorology and Oceanography, Changsha, China",
    year = "2019",
    month = "June",
    abstract = "The details of our method submitted to the task 4 of DCASE challenge 2019 are described in this technical report. This task evaluates systems for the detection of sound events in domestic environments using large-scale weakly labeled data. In particular, an architecture based on the framework of convolutional recurrent neural network (CRNN) is utilized to detect the timestamps of all the events in given audio clips where the training audio files have only clip-level labels. In order to take advantage of the large-scale unlabeled in-domain training data, an audio tagging system using deep residual network (ResNext) is first employed to make predictions for weak labels of the unlabeled data before the sound event detection process. In addition, a self-adaptive searching strategy for best sound-event thresholds is applied in the model testing process, which is believed to have some benefits on the improvement of model performance and generalization capability. Finally, the system achieves 23.79\% F1-value in class-wise average metrics for the sound event detection on the provided testing dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yan2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Yan2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        WEAKLY LABELED SOUND EVENT DETECTION WITH RESDUAL CRNN USING SEMI-SUPERVISED METHOD
       </h4>
<p style="text-align:left">
        Yan, Jie and Song, Yan
       </p>
<p style="text-align:left">
<em>
         University of Science and Technology of China, National Engineering Laboratory for Speech and Language Information Processing, Hefei, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yan_USTC_task4_1</span> <span class="label label-primary">Yan_USTC_task4_2</span> <span class="label label-primary">Yan_USTC_task4_3</span> <span class="label label-primary">Yan_USTC_task4_4</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yan2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yan2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Yan_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yan2019" class="panel-collapse collapse" id="collapse-Yan2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       WEAKLY LABELED SOUND EVENT DETECTION WITH RESDUAL CRNN USING SEMI-SUPERVISED METHOD
      </h4>
<p style="text-align:left">
<small>
        Yan, Jie and Song, Yan
       </small>
<br/>
<small>
<em>
         University of Science and Technology of China, National Engineering Laboratory for Speech and Language Information Processing, Hefei, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our system for the task 4 of DCASE 2019 challenge (Sound event detection in domestic environments). The goal of the task is to evaluate systems with real data either weakly labeled or unlabeled and simulated data that is strongly labeled. To perform this task, we propose resdual CRNN as our system. We also use mean-teacher model based on confidence thresholding and smooth embedding method. In addition, we also apply specaugment for the labeled data shortage problem. Finaly, we achieve better performance than DCASE2019 baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         specaugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yan2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Yan_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yan2019label" class="modal fade" id="bibtex-Yan2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYan2019label">
        WEAKLY LABELED SOUND EVENT DETECTION WITH RESDUAL CRNN USING SEMI-SUPERVISED METHOD
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yan2019,
    Author = "Yan, Jie and Song, Yan",
    title = "WEAKLY LABELED SOUND EVENT DETECTION WITH RESDUAL CRNN USING SEMI-SUPERVISED METHOD",
    institution = "University of Science and Technology of China, National Engineering Laboratory for Speech and Language Information Processing, Hefei, China",
    year = "2019",
    month = "June",
    abstract = "In this report, we present our system for the task 4 of DCASE 2019 challenge (Sound event detection in domestic environments). The goal of the task is to evaluate systems with real data either weakly labeled or unlabeled and simulated data that is strongly labeled. To perform this task, we propose resdual CRNN as our system. We also use mean-teacher model based on confidence thresholding and smooth embedding method. In addition, we also apply specaugment for the labeled data shortage problem. Finaly, we achieve better performance than DCASE2019 baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Yang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MEAN TEACHER MODEL BASED ON CMRANN NETWORK FOR SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Yang, Qian and Xia, Jing and Wang, Jinjia
       </p>
<p style="text-align:left">
<em>
         College of Information Science and Engineering, Yan shan University, Qinhuangdao, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_YSU_task4_1</span> <span class="label label-primary">Wang_YSU_task4_2</span> <span class="label label-primary">Wang_YSU_task4_3</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Yang_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yang2019" class="panel-collapse collapse" id="collapse-Yang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MEAN TEACHER MODEL BASED ON CMRANN NETWORK FOR SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Yang, Qian and Xia, Jing and Wang, Jinjia
       </small>
<br/>
<small>
<em>
         College of Information Science and Engineering, Yan shan University, Qinhuangdao, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper proposes an improved mean teacher model for sound event detection tasks in a domestic environment. The model consists of CNN network, ML-LoBCoD network, RNN network and attention mechanism. To evaluate our method, we tested on the DCASE 2019 Challenge Task 4 dataset. The results show that the average score of F1 in the evaluation 2018 dataset is 22.7%, and the F1 score in the validation 2019 dataset is 23.4%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CMRANN-MT, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Yang_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yang2019label" class="modal fade" id="bibtex-Yang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYang2019label">
        MEAN TEACHER MODEL BASED ON CMRANN NETWORK FOR SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yang2019,
    Author = "Yang, Qian and Xia, Jing and Wang, Jinjia",
    title = "MEAN TEACHER MODEL BASED ON CMRANN NETWORK FOR SOUND EVENT DETECTION",
    institution = "College of Information Science and Engineering, Yan shan University, Qinhuangdao, China",
    year = "2019",
    month = "June",
    abstract = "This paper proposes an improved mean teacher model for sound event detection tasks in a domestic environment. The model consists of CNN network, ML-LoBCoD network, RNN network and attention mechanism. To evaluate our method, we tested on the DCASE 2019 Challenge Task 4 dataset. The results show that the average score of F1 in the evaluation 2018 dataset is 22.7\%, and the F1 score in the validation 2019 dataset is 23.4\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AN IMPROVED SYSTEM FOR DCASE 2019 CHALLENGE TASK 4
       </h4>
<p style="text-align:left">
        Zhang, Zhenyuan Zhang and Yang, Mingxue and Liu, Li
       </p>
<p style="text-align:left">
<em>
         University of Electronic Science and Technology of China ence and Technology of China, School of Information and Communication Engineering, Chengdu, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">ZYL_UESTC_task4_1</span> <span class="label label-primary">ZYL_UESTC_task4_2</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhang2019" class="panel-collapse collapse" id="collapse-Zhang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AN IMPROVED SYSTEM FOR DCASE 2019 CHALLENGE TASK 4
      </h4>
<p style="text-align:left">
<small>
        Zhang, Zhenyuan Zhang and Yang, Mingxue and Liu, Li
       </small>
<br/>
<small>
<em>
         University of Electronic Science and Technology of China ence and Technology of China, School of Information and Communication Engineering, Chengdu, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present an improved system for DCASE2019 challenge task 4, with the goal to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled .We use the multi-scale Mel-spectra as the feature and do the detection with the 3 layers convolu- tional neural network(CNN) and 2 layers recurrent neural network (RNN), after each layer of CNN, we apply a Res-Net (Residual Neural Network) block to increase learning depth. Aim to use data without labels or with weak labels, we apply the meanâ€“teacher model to do the sound event detection.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN,ResNet,RNN, mean-teacher student
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhang2019label" class="modal fade" id="bibtex-Zhang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhang2019label">
        AN IMPROVED SYSTEM FOR DCASE 2019 CHALLENGE TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhang2019,
    Author = "Zhang, Zhenyuan Zhang and Yang, Mingxue and Liu, Li",
    title = "AN IMPROVED SYSTEM FOR DCASE 2019 CHALLENGE TASK 4",
    institution = "University of Electronic Science and Technology of China ence and Technology of China, School of Information and Communication Engineering, Chengdu, China",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we present an improved system for DCASE2019 challenge task 4, with the goal to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled and simulated data that is strongly labeled .We use the multi-scale Mel-spectra as the feature and do the detection with the 3 layers convolu- tional neural network(CNN) and 2 layers recurrent neural network (RNN), after each layer of CNN, we apply a Res-Net (Residual Neural Network) block to increase learning depth. Aim to use data without labels or with weak labels, we apply the meanâ€“teacher model to do the sound event detection."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>