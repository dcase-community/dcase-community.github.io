<!DOCTYPE html><html lang="en">
<head>
    <title>DCASE2013 Challenge - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2013/index">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="Challenge has ended. This site is collecting information from original DCASE2013 Challenge website to document DCASE challenges in a uniform way. Challenge results and analysis of submitted systems have been published in: Publication D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D. Plumbley. Detection and classification of acoustic …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/bnews.min.css">
    <link rel="stylesheet" href="/theme/css/bdates.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group  active">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class=" active">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2013</strong></a>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2013/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2013/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2013/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2013/task-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2013/task-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthetic text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2013/task-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2013/task-sound-event-detection-results-ol"><i class="fa fa-bar-chart"></i>&nbsp;Subtask OL</a>
    </li>
            <li class="">
        <a href="/challenge2013/task-sound-event-detection-results-os"><i class="fa fa-bar-chart"></i>&nbsp;Subtask OS</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Download data">
        <a href="/challenge2013/download"><i class="fa fa-download"></i>&nbsp;Download</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2013/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge organizers">
        <a href="/challenge2013/organizers"><i class="fa fa-users"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/stones-01.jpg);box-shadow: 0px 1000px rgba(0, 0, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-right"><object class="img img-responsive sr-header-overlay" type="image/svg+xml" data="..//images/overlays/wave.svg"></object></div><h1 class="bold">DCASE2013 Challenge</h1><span class="subheading">IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events</span><hr class="small right bold">
                        <span class="subheading subheading-secondary">31 March - 14 April 2013</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default hidden-print">
<div class="panel-heading">
<h3 class="panel-title">Important dates</h3>
</div>
<ul class="bdates-container list-group">
<a class="list-group-item" href="/challenge2013/download">
<div class="row">
<div class="col-md-12">
<h5 class="list-group-item-heading text-muted">
<strong>01 Dec 2012</strong>
</h5>
</div>
<div class="col-md-12">
<h5 class="list-group-item-heading text-muted">
                        Release of development datasets
                    
                    </h5>
</div>
</div>
</a>
<a class="list-group-item" href="/challenge2013/submission">
<div class="row">
<div class="col-md-12">
<h5 class="list-group-item-heading text-muted">
<strong>14 Apr 2013</strong>
</h5>
</div>
<div class="col-md-12">
<h5 class="list-group-item-heading text-muted">
                        Challenge submission (system code)
                    
                    </h5>
</div>
</div>
</a>
</ul>
</div>

 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Contact</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>DCASE Challenge</strong>
<a class="icon" href="mailto:dcase.challenge@gmail.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
</div>
</td>
</tr>
</table>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="alert alert-info">
<strong>Challenge has ended.</strong> <br/>This site is collecting information from original <a href="http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge/" target="_blank">DCASE2013 Challenge website</a> to document DCASE challenges in a uniform way.
</p>
<p>Challenge results and analysis of submitted systems have been published in:</p>
<div class="btex-item" data-item="Stowell2015" data-source="content/data/challenge2013/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Stowell2015"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            D. <span class="bibtex-protected">Stowell</span>, D. <span class="bibtex-protected">Giannoulis</span>, E. <span class="bibtex-protected">Benetos</span>, M. <span class="bibtex-protected">Lagrange</span>, and M. D. <span class="bibtex-protected">Plumbley</span>.
<em>Detection and classification of acoustic scenes and events.</em>
<em>IEEE Transactions on Multimedia</em>, 17(10):1733–1746, Oct 2015.
<a href="https://doi.org/10.1109/TMM.2015.2428998">doi:10.1109/TMM.2015.2428998</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexStowell20154a1af77cfdd44f6e811f7457c29d8195" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.archives-ouvertes.fr/hal-01253912/document" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseStowell20154a1af77cfdd44f6e811f7457c29d8195" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseStowell20154a1af77cfdd44f6e811f7457c29d8195" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingStowell20154a1af77cfdd44f6e811f7457c29d8195" class="panel-collapse collapse" id="collapseStowell20154a1af77cfdd44f6e811f7457c29d8195" role="tabpanel">
<h4>Detection and Classification of Acoustic Scenes and Events</h4>
<h5>Abstract</h5>
<p class="text-justify">For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.</p>
<h5>Keywords</h5>
<p class="text-justify">acoustic signal processing;knowledge based systems;speech recognition;acoustic scenes detection;acoustic scenes classification;intelligent systems;audio modality;speech recognition;music;IEEE Audio and Acoustic Signal Processing Technical Committee;DCASE;Event detection;Speech;Speech recognition;Music;Microphones;Licenses;Audio databases;event detection;machine intelligence;pattern recognition</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexStowell20154a1af77cfdd44f6e811f7457c29d8195" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.archives-ouvertes.fr/hal-01253912/document" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexStowell20154a1af77cfdd44f6e811f7457c29d8195label" class="modal fade" id="bibtexStowell20154a1af77cfdd44f6e811f7457c29d8195" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexStowell20154a1af77cfdd44f6e811f7457c29d8195label">Detection and Classification of Acoustic Scenes and Events</h4>
</div>
<div class="modal-body">
<pre>@article{Stowell2015,
    author = "{Stowell}, D. and {Giannoulis}, D. and {Benetos}, E. and {Lagrange}, M. and {Plumbley}, M. D.",
    journal = "IEEE Transactions on Multimedia",
    title = "Detection and Classification of Acoustic Scenes and Events",
    year = "2015",
    volume = "17",
    number = "10",
    pages = "1733-1746",
    abstract = "For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.",
    keywords = "acoustic signal processing;knowledge based systems;speech recognition;acoustic scenes detection;acoustic scenes classification;intelligent systems;audio modality;speech recognition;music;IEEE Audio and Acoustic Signal Processing Technical Committee;DCASE;Event detection;Speech;Speech recognition;Music;Microphones;Licenses;Audio databases;event detection;machine intelligence;pattern recognition",
    doi = "10.1109/TMM.2015.2428998",
    issn = "1520-9210",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>Results for each tasks are presented in task specific results pages:</p>
<div class="row">
<div class="col-lg-12">
<a class="btn btn-primary" href="/challenge2013/task-acoustic-scene-classification-results">Task 1</a>
<div aria-label="..." class="btn-group" role="group">
<a class="btn btn-success" href="/challenge2013/task-sound-event-detection-results-ol" style="">Task 2 / Office live</a>
<a class="btn btn-info" href="/challenge2013/task-sound-event-detection-results-os" style="">Task 2 / Office synthetic</a>
</div>
</div>
</div>
<h1>Introduction</h1>
<p>We invite researchers in signal processing, machine learning and other fields to participate in our challenge, which consists of a set of related tasks on automatic detection and classification of acoustic scenes and acoustic events.</p>
<p>The tasks fall into the field of computational auditory scene analysis (CASA). Humans are able to follow specific sound sources in a complex audio environment with ease and the development of systems that try to mimic this behaviour is an open problem, especially in the case of overlapping sound events.</p>
<h1>Tasks</h1>
<div class="row">
<div class="col-lg-12"><h3>Acoustic scene classification</h3></div>
</div>
<div class="row">
<div class="col-md-2">
<a class="icon" href="/challenge2013/task-acoustic-scene-classification">
<span class="fa-stack fa-4x">
<i class="fa fa-square fa-stack-2x text-primary"></i>
<i class="fa dc-scene fa-stack-1x fa-inverse"></i>
<strong class="fa-stack-1x dcase-icon-top-text">Scenes</strong>
<span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span>
</span>
</a>
</div>
<div class="col-md-10">
<p>
    The goal of acoustic scene classification is to classify a test recording into one of predefined classes that characterizes the environment in which it was recorded -- for example "park", "busy street", "office". The acoustic data will include recordings from 10 contexts.
    </p>
<a class="btn btn-primary" href="/challenge2013/task-acoustic-scene-classification">Task description <i class="fa fa-caret-right"></i></a>
<a class="btn btn-primary" href="/challenge2013/task-acoustic-scene-classification-results">Results <i class="fa fa-caret-right"></i></a>
</div>
</div>
<div class="row">
<div class="col-lg-12"><h3>Sound event detection</h3></div>
</div>
<div class="row">
<div class="col-md-2">
<a class="icon" href="/challenge2013/task-sound-event-detection">
<span class="fa-stack fa-4x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa dc-synthetic fa-stack-1x fa-inverse"></i>
<strong class="fa-stack-1x dcase-icon-top-text">Events</strong>
<span class="fa-stack-1x dcase-icon-bottom-text">Task 2</span>
</span>
</a>
</div>
<div class="col-md-10">
<p>
    The event detection challenge will address the problem of identifying individual sound events that are prominent in an acoustic scene. Two distinct experiments will take, one for simple acoustic scenes without overlapping sounds and the other using complex scenes in a polyphonic scenario:
    </p>
<div class="row">
<div class="col-md-2">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<strong class="fa-stack-1x icon-text">OL</strong>
<strong class="fa-stack-1x dcase-icon-top-text">Live</strong>
</span>
</div>
<div class="col-md-10">
<strong>Subtask OL: Office Live</strong>
<p>
                The first dataset for event detection will consist of 3 subsets (for development, training, and testing). The training set will contain instantiations of individual events for every class. The developement and testing datasets, denoted as office live (OL), will consist of 1 min recordings of every-day audio events in a number of office environments. The audio events for these recordings will be annotated and they will include: door knock, door slam, speech, laughter, keyboard clicks, objects hitting table, keys clinging, phone ringing, turning page, cough, printer, short alert-beeping, clearing throat, mouse click, drawer, and switches.
            </p>
</div>
</div>
<div class="row">
<div class="col-md-2">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-info"></i>
<strong class="fa-stack-1x icon-text">OS</strong>
<strong class="fa-stack-1x dcase-icon-top-text">Synthetic</strong>
</span>
</div>
<div class="col-md-10">
<strong>Subtask OS: Office Synthetic</strong>
<p>
                The second dataset will contain artificially sequenced sounds provided by the Analysis-Synthesis team of IRCAM, termed Office Synthetic (OS). The training set will be identical to the one for the first dataset. The development and testing sets will consist of artificial scenes built by sequencing recordings of individual events (different recordings from the ones used for the training dataset) and background recordings provided by C4DM.
            </p>
</div>
</div>
<a class="btn btn-success" href="/challenge2013/task-sound-event-detection">Task description <i class="fa fa-caret-right"></i></a>
<div class="btn-group">
<a class="btn btn-success" href="/challenge2013/task-sound-event-detection-results-ol" style="">Subtask OL results</a>
<a class="btn btn-info" href="/challenge2013/task-sound-event-detection-results-os" style="">Subtask OS results</a>
</div>
</div>
</div>
<h1>WASPAA2013 Poster session</h1>
<p>Participants were encouraged to submit novel work as a regular paper at the <a href="http://www.waspaa.com/waspaa13/d-case-challenge/">WASPAA 2013</a> conference. Approved papers were presented as <a href="http://www.waspaa.com/waspaa13/information-for-attendees/schedule/index.html#PT">D-CASE Poster Session</a>. </p>
<div class="btex" data-source="content/data/challenge2013/waspaa_poster_session.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Geiger2013" style="box-shadow: none">
<div class="panel-heading" id="headingGeiger2013" role="tab">
<div class="row">
<div class="col-xs-9">
<p style="text-align:left">
        J. T. <span class="bibtex-protected">Geiger</span>, B. <span class="bibtex-protected">Schuller</span>, and G. <span class="bibtex-protected">Rigoll</span>.
<em>Large-scale audio feature extraction and svm for acoustic scene classification.</em>
In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, volume, 1–4. Oct 2013.
<a href="https://doi.org/10.1109/WASPAA.2013.6701857">doi:10.1109/WASPAA.2013.6701857</a>.
                                    
                                    
       </p>
<button aria-controls="collapse-Geiger2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Geiger2013" type="button">
<i class="fa fa-caret-down">
</i>
        Read more...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Geiger2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6701857" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Geiger2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Geiger2013" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Geiger2013" class="panel-collapse collapse" id="collapse-Geiger2013" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This work describes a system for acoustic scene classification using large-scale audio feature extraction. It is our contribution to the Scene Classification track of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (D-CASE). The system classifies 30 second long recordings of 10 different acoustic scenes. From the highly variable recordings, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Using a sliding window approach, classification is performed on short windows. SVM are used to classify these short segments, and a majority voting scheme is employed to get a decision for longer recordings. On the official development set of the challenge, an accuracy of 73 % is achieved. SVM are compared with a nearest neighbour classifier and an approach called Latent Perceptual Indexing, whereby SVM achieve the best results. A feature analysis using the t-statistic shows that mainly Mel spectra are the most relevant features.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio signals;feature extraction;signal classification;statistical analysis;support vector machines;Mel spectra;t-statistic;latent perceptual indexing;nearest neighbour classifier;short segments;sliding window approach;variable recordings;D-CASE;acoustic scenes;IEEE AASP;scene classification track;acoustic scene classification;SVM;support vector machines;large-scale audio feature extraction;Support vector machines;Mel frequency cepstral coefficient;Feature extraction;Accuracy;Training data;Training;Computational auditory scene analysis;acoustic scene recognition;feature extraction
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Geiger2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6701857" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Geiger2013label" class="modal fade" id="bibtex-Geiger2013" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGeiger2013label">
        Large-scale audio feature extraction and SVM for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Geiger2013,
    author = "{Geiger}, J. T. and {Schuller}, B. and {Rigoll}, G.",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Large-scale audio feature extraction and SVM for acoustic scene classification",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    abstract = "This work describes a system for acoustic scene classification using large-scale audio feature extraction. It is our contribution to the Scene Classification track of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (D-CASE). The system classifies 30 second long recordings of 10 different acoustic scenes. From the highly variable recordings, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Using a sliding window approach, classification is performed on short windows. SVM are used to classify these short segments, and a majority voting scheme is employed to get a decision for longer recordings. On the official development set of the challenge, an accuracy of 73 \% is achieved. SVM are compared with a nearest neighbour classifier and an approach called Latent Perceptual Indexing, whereby SVM achieve the best results. A feature analysis using the t-statistic shows that mainly Mel spectra are the most relevant features.",
    keywords = "audio signals;feature extraction;signal classification;statistical analysis;support vector machines;Mel spectra;t-statistic;latent perceptual indexing;nearest neighbour classifier;short segments;sliding window approach;variable recordings;D-CASE;acoustic scenes;IEEE AASP;scene classification track;acoustic scene classification;SVM;support vector machines;large-scale audio feature extraction;Support vector machines;Mel frequency cepstral coefficient;Feature extraction;Accuracy;Training data;Training;Computational auditory scene analysis;acoustic scene recognition;feature extraction",
    doi = "10.1109/WASPAA.2013.6701857",
    issn = "1931-1168",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gemmeke2013" style="box-shadow: none">
<div class="panel-heading" id="headingGemmeke2013" role="tab">
<div class="row">
<div class="col-xs-9">
<p style="text-align:left">
        J. F. <span class="bibtex-protected">Gemmeke</span>, L. <span class="bibtex-protected">Vuegen</span>, P. <span class="bibtex-protected">Karsmakers</span>, B. <span class="bibtex-protected">Vanrumste</span>, and H. <span class="bibtex-protected">Van hamme</span>.
<em>An exemplar-based nmf approach to audio event detection.</em>
In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, volume, 1–4. Oct 2013.
<a href="https://doi.org/10.1109/WASPAA.2013.6701847">doi:10.1109/WASPAA.2013.6701847</a>.
                                    
                                    
       </p>
<button aria-controls="collapse-Gemmeke2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gemmeke2013" type="button">
<i class="fa fa-caret-down">
</i>
        Read more...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gemmeke2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://www.esat.kuleuven.be/psi/spraak/cgi-bin/get_file.cgi?/jgemmeke/Gemmeke2013_WASPAA.pdf&amp;auto&amp;Gemmeke2013_WASPAA" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Gemmeke2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gemmeke2013" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gemmeke2013" class="panel-collapse collapse" id="collapse-Gemmeke2013" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present a novel, exemplar-based method for audio event detection based on non-negative matrix factorisation. Building on recent work in noise robust automatic speech recognition, we model events as a linear combination of dictionary atoms, and mixtures as a linear combination of overlapping events. The weights of activated atoms in an observation serve directly as evidence for the underlying event classes. The atoms in the dictionary span multiple frames and are created by extracting all possible fixed-length exemplars from the training data. To combat data scarcity of small training datasets, we propose to artificially augment the amount of training data by linear time warping in the feature domain at multiple rates. The method is evaluated on the Office Live and Office Synthetic datasets released by the AASP Challenge on Detection and Classification of Acoustic Scenes and Events.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       acoustic signal detection;acoustic signal processing;audio signal processing;matrix decomposition;signal classification;speech recognition;exemplar-based NMF approach;nonnegative matrix factorisation;audio event detection;noise robust automatic speech recognition;dictionary atoms;linear overlapping event combination;dictionary span multiple frames;possible fixed-length exemplar extraction;linear time warping;Office Live datasets;Office Synthetic datasets;AASP Challenge;acoustic scene detection;acoustic scene classification;acoustic event detection;acoustic event classification;data scarcity;Acoustics;Dictionaries;Event detection;Hidden Markov models;Training data;Measurement;Noise;Audio event detection;exemplars;NMF
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gemmeke2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://www.esat.kuleuven.be/psi/spraak/cgi-bin/get_file.cgi?/jgemmeke/Gemmeke2013_WASPAA.pdf&amp;auto&amp;Gemmeke2013_WASPAA" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gemmeke2013label" class="modal fade" id="bibtex-Gemmeke2013" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGemmeke2013label">
        An exemplar-based NMF approach to audio event detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gemmeke2013,
    author = "{Gemmeke}, J. F. and {Vuegen}, L. and {Karsmakers}, P. and {Vanrumste}, B. and {Van hamme}, H.",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "An exemplar-based NMF approach to audio event detection",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    abstract = "We present a novel, exemplar-based method for audio event detection based on non-negative matrix factorisation. Building on recent work in noise robust automatic speech recognition, we model events as a linear combination of dictionary atoms, and mixtures as a linear combination of overlapping events. The weights of activated atoms in an observation serve directly as evidence for the underlying event classes. The atoms in the dictionary span multiple frames and are created by extracting all possible fixed-length exemplars from the training data. To combat data scarcity of small training datasets, we propose to artificially augment the amount of training data by linear time warping in the feature domain at multiple rates. The method is evaluated on the Office Live and Office Synthetic datasets released by the AASP Challenge on Detection and Classification of Acoustic Scenes and Events.",
    keywords = "acoustic signal detection;acoustic signal processing;audio signal processing;matrix decomposition;signal classification;speech recognition;exemplar-based NMF approach;nonnegative matrix factorisation;audio event detection;noise robust automatic speech recognition;dictionary atoms;linear overlapping event combination;dictionary span multiple frames;possible fixed-length exemplar extraction;linear time warping;Office Live datasets;Office Synthetic datasets;AASP Challenge;acoustic scene detection;acoustic scene classification;acoustic event detection;acoustic event classification;data scarcity;Acoustics;Dictionaries;Event detection;Hidden Markov models;Training data;Measurement;Noise;Audio event detection;exemplars;NMF",
    doi = "10.1109/WASPAA.2013.6701847",
    issn = "1931-1168",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2013" style="box-shadow: none">
<div class="panel-heading" id="headingLee2013" role="tab">
<div class="row">
<div class="col-xs-9">
<p style="text-align:left">
        K. <span class="bibtex-protected">Lee</span>, Z. <span class="bibtex-protected">Hyung</span>, and J. <span class="bibtex-protected">Nam</span>.
<em>Acoustic scene classification using sparse feature learning and event-based pooling.</em>
In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, volume, 1–4. Oct 2013.
<a href="https://doi.org/10.1109/WASPAA.2013.6701893">doi:10.1109/WASPAA.2013.6701893</a>.
                                    
                                    
       </p>
<button aria-controls="collapse-Lee2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2013" type="button">
<i class="fa fa-caret-down">
</i>
        Read more...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="http://mac.kaist.ac.kr/pubs/LeeHyungNam-waspaa2013.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lee2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2013" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2013" class="panel-collapse collapse" id="collapse-Lee2013" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Recently unsupervised learning algorithms have been successfully used to represent data in many of machine recognition tasks. In particular, sparse feature learning algorithms have shown that they can not only discover meaningful structures from raw data but also outperform many hand-engineered features. In this paper, we apply the sparse feature learning approach to acoustic scene classification. We use a sparse restricted Boltzmann machine to capture manyfold local acoustic structures from audio data and represent the data in a high-dimensional sparse feature space given the learned structures. For scene classification, we summarize the local features by pooling over audio scene data. While the feature pooling is typically performed over uniformly divided segments, we suggest a new pooling method, which first detects audio events and then performs pooling only over detected events, considering the irregular occurrence of audio events in acoustic scene data. We evaluate the learned features on the IEEE AASP Challenge development set, comparing them with a baseline model using mel-frequency cepstral coefficients (MFCCs). The results show that learned features outperform MFCCs, event-based pooling achieves higher accuracy than uniform pooling and, furthermore, a combination of the two methods performs even better than either one used alone.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       acoustic signal processing;Boltzmann machines;cepstral analysis;learning (artificial intelligence);signal classification;acoustic scene classification;event-based pooling;unsupervised learning algorithm;machine recognition tasks;sparse feature learning algorithm;sparse restricted Boltzmann machine;IEEE AASP Challenge development set;mel-frequency cepstral coefficients;MFCC;Acoustics;Feature extraction;Training;Electron tubes;Accuracy;Mathematical model;Conferences;acoustic scene classification;environmental sound;feature learning;restricted Boltzmann machine;sparse feature representation;max-pooling;event detection
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="http://mac.kaist.ac.kr/pubs/LeeHyungNam-waspaa2013.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2013label" class="modal fade" id="bibtex-Lee2013" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2013label">
        Acoustic scene classification using sparse feature learning and event-based pooling
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2013,
    author = "{Lee}, K. and {Hyung}, Z. and {Nam}, J.",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Acoustic scene classification using sparse feature learning and event-based pooling",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    abstract = "Recently unsupervised learning algorithms have been successfully used to represent data in many of machine recognition tasks. In particular, sparse feature learning algorithms have shown that they can not only discover meaningful structures from raw data but also outperform many hand-engineered features. In this paper, we apply the sparse feature learning approach to acoustic scene classification. We use a sparse restricted Boltzmann machine to capture manyfold local acoustic structures from audio data and represent the data in a high-dimensional sparse feature space given the learned structures. For scene classification, we summarize the local features by pooling over audio scene data. While the feature pooling is typically performed over uniformly divided segments, we suggest a new pooling method, which first detects audio events and then performs pooling only over detected events, considering the irregular occurrence of audio events in acoustic scene data. We evaluate the learned features on the IEEE AASP Challenge development set, comparing them with a baseline model using mel-frequency cepstral coefficients (MFCCs). The results show that learned features outperform MFCCs, event-based pooling achieves higher accuracy than uniform pooling and, furthermore, a combination of the two methods performs even better than either one used alone.",
    keywords = "acoustic signal processing;Boltzmann machines;cepstral analysis;learning (artificial intelligence);signal classification;acoustic scene classification;event-based pooling;unsupervised learning algorithm;machine recognition tasks;sparse feature learning algorithm;sparse restricted Boltzmann machine;IEEE AASP Challenge development set;mel-frequency cepstral coefficients;MFCC;Acoustics;Feature extraction;Training;Electron tubes;Accuracy;Mathematical model;Conferences;acoustic scene classification;environmental sound;feature learning;restricted Boltzmann machine;sparse feature representation;max-pooling;event detection",
    doi = "10.1109/WASPAA.2013.6701893",
    issn = "1931-1168",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Niessen2013" style="box-shadow: none">
<div class="panel-heading" id="headingNiessen2013" role="tab">
<div class="row">
<div class="col-xs-9">
<p style="text-align:left">
        M. E. <span class="bibtex-protected">Niessen</span>, T. L. M. <span class="bibtex-protected">Van Kasteren</span>, and A. <span class="bibtex-protected">Merentitis</span>.
<em>Hierarchical modeling using automated sub-clustering for sound event recognition.</em>
In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, volume, 1–4. Oct 2013.
<a href="https://doi.org/10.1109/WASPAA.2013.6701862">doi:10.1109/WASPAA.2013.6701862</a>.
                                    
                                    
       </p>
<button aria-controls="collapse-Niessen2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Niessen2013" type="button">
<i class="fa fa-caret-down">
</i>
        Read more...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Niessen2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://www.researchgate.net/publication/258168712_Hierarchical_modeling_using_automated_sub-clustering_for_sound_event_recognition" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Niessen2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Niessen2013" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Niessen2013" class="panel-collapse collapse" id="collapse-Niessen2013" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The automatic recognition of sound events allows for novel applications in areas such as security, mobile and multimedia. In this work we present a hierarchical hidden Markov model for sound event detection that automatically clusters the inherent structure of the events into sub-events. We evaluate our approach on an IEEE audio challenge dataset consisting of office sound events and provide a systematic comparison of the various building blocks of our approach to demonstrate the effectiveness of incorporating certain dependencies in the model. The hierarchical hidden Markov model achieves an average frame-based F-measure recognition performance of 45.5% on a test dataset that was used to evaluate challenge submissions. We also show how the hierarchical model can be used as a meta-classifier, although in the particular application this did not lead to an increase in performance on the test dataset.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio signal processing;hidden Markov models;metaclassifier;frame based F measure recognition;sound event detection;hierarchical hidden Markov model;automatic recognition;sound event recognition;automated subclustering;hierarchical modeling;Hidden Markov models;Data models;Acoustics;Feature extraction;Conferences;Event detection;Speech;sound event detection;hierarchical models;meta-classifier
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Niessen2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://www.researchgate.net/publication/258168712_Hierarchical_modeling_using_automated_sub-clustering_for_sound_event_recognition" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Niessen2013label" class="modal fade" id="bibtex-Niessen2013" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNiessen2013label">
        Hierarchical modeling using automated sub-clustering for sound event recognition
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Niessen2013,
    author = "{Niessen}, M. E. and {Van Kasteren}, T. L. M. and {Merentitis}, A.",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Hierarchical modeling using automated sub-clustering for sound event recognition",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    abstract = "The automatic recognition of sound events allows for novel applications in areas such as security, mobile and multimedia. In this work we present a hierarchical hidden Markov model for sound event detection that automatically clusters the inherent structure of the events into sub-events. We evaluate our approach on an IEEE audio challenge dataset consisting of office sound events and provide a systematic comparison of the various building blocks of our approach to demonstrate the effectiveness of incorporating certain dependencies in the model. The hierarchical hidden Markov model achieves an average frame-based F-measure recognition performance of 45.5\% on a test dataset that was used to evaluate challenge submissions. We also show how the hierarchical model can be used as a meta-classifier, although in the particular application this did not lead to an increase in performance on the test dataset.",
    keywords = "audio signal processing;hidden Markov models;metaclassifier;frame based F measure recognition;sound event detection;hierarchical hidden Markov model;automatic recognition;sound event recognition;automated subclustering;hierarchical modeling;Hidden Markov models;Data models;Acoustics;Feature extraction;Conferences;Event detection;Speech;sound event detection;hierarchical models;meta-classifier",
    doi = "10.1109/WASPAA.2013.6701862",
    issn = "1931-1168",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Roma2013" style="box-shadow: none">
<div class="panel-heading" id="headingRoma2013" role="tab">
<div class="row">
<div class="col-xs-9">
<p style="text-align:left">
        G. <span class="bibtex-protected">Roma</span>, W. <span class="bibtex-protected">Nogueira</span>, and P. <span class="bibtex-protected">Herrera</span>.
<em>Recurrence quantification analysis features for environmental sound recognition.</em>
In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, volume, 1–4. Oct 2013.
<a href="https://doi.org/10.1109/WASPAA.2013.6701890">doi:10.1109/WASPAA.2013.6701890</a>.
                                    
                                    
       </p>
<button aria-controls="collapse-Roma2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Roma2013" type="button">
<i class="fa fa-caret-down">
</i>
        Read more...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Roma2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6701890" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Roma2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Roma2013" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Roma2013" class="panel-collapse collapse" id="collapse-Roma2013" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper tackles the problem of feature aggregation for recognition of auditory scenes in unlabeled audio. We describe a new set of descriptors based on Recurrence Quantification Analysis (RQA), which can be extracted from the similarity matrix of a time series of audio descriptors. We analyze their usefulness for environmental audio recognition combined with traditional feature statistics in the context of the AASP D-CASE[1] challenge. Our results show the potential of non-linear time series analysis techniques for dealing with environmental sounds.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio signal processing;time series;nonlinear time series analysis technique;environmental audio recognition;unlabeled audio;auditory scene recognition;feature aggregation;environmental sound recognition;RQA;recurrence quantification analysis;Feature extraction;Mel frequency cepstral coefficient;Databases;Accuracy;Time series analysis;Conferences
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Roma2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6701890" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Roma2013label" class="modal fade" id="bibtex-Roma2013" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRoma2013label">
        Recurrence quantification analysis features for environmental sound recognition
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Roma2013,
    author = "{Roma}, G. and {Nogueira}, W. and {Herrera}, P.",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "Recurrence quantification analysis features for environmental sound recognition",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    abstract = "This paper tackles the problem of feature aggregation for recognition of auditory scenes in unlabeled audio. We describe a new set of descriptors based on Recurrence Quantification Analysis (RQA), which can be extracted from the similarity matrix of a time series of audio descriptors. We analyze their usefulness for environmental audio recognition combined with traditional feature statistics in the context of the AASP D-CASE[1] challenge. Our results show the potential of non-linear time series analysis techniques for dealing with environmental sounds.",
    keywords = "audio signal processing;time series;nonlinear time series analysis technique;environmental audio recognition;unlabeled audio;auditory scene recognition;feature aggregation;environmental sound recognition;RQA;recurrence quantification analysis;Feature extraction;Mel frequency cepstral coefficient;Databases;Accuracy;Time series analysis;Conferences",
    doi = "10.1109/WASPAA.2013.6701890",
    issn = "1931-1168",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schroder2013" style="box-shadow: none">
<div class="panel-heading" id="headingSchroder2013" role="tab">
<div class="row">
<div class="col-xs-9">
<p style="text-align:left">
        J. <span class="bibtex-protected">Schröder</span>, N. <span class="bibtex-protected">Moritz</span>, M. R. <span class="bibtex-protected">Schädler</span>, B. <span class="bibtex-protected">Cauchi</span>, K. <span class="bibtex-protected">Adiloglu</span>, J. <span class="bibtex-protected">Anemüller</span>, S. <span class="bibtex-protected">Doclo</span>, B. <span class="bibtex-protected">Kollmeier</span>, and S. <span class="bibtex-protected">Goetze</span>.
<em>On the use of spectro-temporal features for the ieee aasp challenge ‘detection and classification of acoustic scenes and events’.</em>
In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, volume, 1–4. Oct 2013.
<a href="https://doi.org/10.1109/WASPAA.2013.6701868">doi:10.1109/WASPAA.2013.6701868</a>.
                                    
                                    
       </p>
<button aria-controls="collapse-Schroder2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schroder2013" type="button">
<i class="fa fa-caret-down">
</i>
        Read more...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schroder2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://uol.de/fileadmin/user_upload/mediphysik/ag/sigproc/download/papers/SP2013_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Schroder2013" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schroder2013" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schroder2013" class="panel-collapse collapse" id="collapse-Schroder2013" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this contribution, an acoustic event detection system based on spectro-temporal features and a two-layer hidden Markov model as back-end is proposed within the framework of the IEEE AASP challenge `Detection and Classification of Acoustic Scenes and Events' (D-CASE). Noise reduction based on the log-spectral amplitude estimator by [1] and noise power density estimation by [2] is used for signal enhancement. Performance based on three different kinds of features is compared, i.e. for amplitude modulation spectrogram, Gabor filterbank-features and conventional Mel-frequency cepstral coefficients (MFCCs), all of them known from automatic speech recognition (ASR). The evaluation is based on the office live recordings provided within the D-CASE challenge. The influence of the signal enhancement is investigated and the increase in recognition rate by the proposed features in comparison to MFCC-features is shown. It is demonstrated that the proposed spectro-temporal features achieve a better recognition accuracy than MFCCs.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Gabor filters;hidden Markov models;signal classification;speech enhancement;speech recognition;spectro-temporal features;acoustic event detection system;two-layer hidden Markov model;IEEE AASP challenge;detection and classification of acoustic scenes and events;noise reduction;log-spectral amplitude estimator;noise power density estimation;signal enhancement;Gabor filterbank-features;Mel-frequency cepstral coefficients;automatic speech recognition;office live recordings;D-CASE challenge;Acoustics;Hidden Markov models;Feature extraction;Speech;Event detection;Noise;Frequency modulation;acoustic event detection;Gabor filterbank;amplitude modulation spectrogram;IEEE AASP D-CASE challenge
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schroder2013" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://uol.de/fileadmin/user_upload/mediphysik/ag/sigproc/download/papers/SP2013_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schroder2013label" class="modal fade" id="bibtex-Schroder2013" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchroder2013label">
        On the use of spectro-temporal features for the IEEE AASP challenge ‘detection and classification of acoustic scenes and events’
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schroder2013,
    author = "{Schröder}, J. and {Moritz}, N. and {Schädler}, M. R. and {Cauchi}, B. and {Adiloglu}, K. and {Anemüller}, J. and {Doclo}, S. and {Kollmeier}, B. and {Goetze}, S.",
    booktitle = "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    title = "On the use of spectro-temporal features for the IEEE AASP challenge ‘detection and classification of acoustic scenes and events’",
    year = "2013",
    volume = "",
    number = "",
    pages = "1-4",
    abstract = "In this contribution, an acoustic event detection system based on spectro-temporal features and a two-layer hidden Markov model as back-end is proposed within the framework of the IEEE AASP challenge `Detection and Classification of Acoustic Scenes and Events' (D-CASE). Noise reduction based on the log-spectral amplitude estimator by [1] and noise power density estimation by [2] is used for signal enhancement. Performance based on three different kinds of features is compared, i.e. for amplitude modulation spectrogram, Gabor filterbank-features and conventional Mel-frequency cepstral coefficients (MFCCs), all of them known from automatic speech recognition (ASR). The evaluation is based on the office live recordings provided within the D-CASE challenge. The influence of the signal enhancement is investigated and the increase in recognition rate by the proposed features in comparison to MFCC-features is shown. It is demonstrated that the proposed spectro-temporal features achieve a better recognition accuracy than MFCCs.",
    keywords = "Gabor filters;hidden Markov models;signal classification;speech enhancement;speech recognition;spectro-temporal features;acoustic event detection system;two-layer hidden Markov model;IEEE AASP challenge;detection and classification of acoustic scenes and events;noise reduction;log-spectral amplitude estimator;noise power density estimation;signal enhancement;Gabor filterbank-features;Mel-frequency cepstral coefficients;automatic speech recognition;office live recordings;D-CASE challenge;Acoustics;Hidden Markov models;Feature extraction;Speech;Event detection;Noise;Frequency modulation;acoustic event detection;Gabor filterbank;amplitude modulation spectrogram;IEEE AASP D-CASE challenge",
    doi = "10.1109/WASPAA.2013.6701868",
    issn = "1931-1168",
    month = "Oct"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/timeago.min.js"></script>
<script type="text/javascript" src="/theme/js/bnews.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>