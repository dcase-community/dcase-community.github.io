<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Localization and Detection - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2019/task-sound-event-localization-and-detection">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of this task is to jointly localize and recognize individual sound events and their respective temporal onset and offset times. Challenge has ended. Full results for this task can be found in the Results page. Description Given a multichannel audio input, the goal of a sound event localization …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2019</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2019/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2019/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2019/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-urban-sound-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2019/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2019/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2019/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wall-12.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-warning"></i><i class="fa dc-localization fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Localization</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 3</span></span><img src="../images/logos/dcase/dcase2019_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Localization and Detection</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Sharath Adavanne" class="img img-circle" src="/images/person/sharath_adavanne.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Sharath Adavanne</strong>
<a class="icon" href="mailto:sharath.adavanne@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Archontis Politis" class="img img-circle" src="/images/person/archontis_politis.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Archontis Politis</strong>
<a class="icon" href="mailto:archontis.politis@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Tuomas Virtanen" class="img img-circle" src="/images/person/tuomas_virtanen.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Tuomas Virtanen</strong>
<a class="icon" href="mailto:tuomas.virtanen@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#recording-procedure">Recording procedure</a></li>
<li><a href="#recording-format-and-dataset-specifications">Recording format and dataset specifications</a></li>
<li><a href="#reference-labels">Reference labels</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#submission">Submission</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#awards">Awards</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#repository">Repository</a></li>
<li><a href="#results-for-the-development-dataset">Results for the development dataset</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of this task is to jointly localize and recognize individual sound events and their respective temporal onset and offset times.</p>
<p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found in the <a class="btn btn-default btn-xs" href="/challenge2019/task-sound-event-localization-and-detection-results">Results <i class="fa fa-caret-right"></i></a> page.
</p>
<h1 id="description">Description</h1>
<p>Given a multichannel audio input, the goal of a sound event localization and detection (SELD) method is to output all instances of the sound labels in the recording, their respective onset-offset times, and their directions-of-arrival (DOAs) in azimuth and elevation angles. Effective implementations of such a SELD method enable an automated description of human activities with a spatial dimension, and help machines to interact with the world more seamlessly. More specifically, SELD can be an important module in assisted listening systems, scene information visualization systems, immersive interactive media, and spatial machine cognition for scene-based deployment of services. A straightforward practical application is a robot that recognizes and tracks the sound source of interest. In the current challenge, only static scenes are considered, meaning that each individual sound event instance in the provided recordings is spatially stationary with a fixed location during its entire duration.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2019/task3_sound_event_localization_and_detection.png"/>
<figcaption>Figure 1: Overview of sound event localization and detection system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<h1 id="audio-dataset">Audio dataset</h1>
<p>The task provides two datasets, <strong>TAU Spatial Sound Events 2019 - Ambisonic</strong> or <strong>TAU Spatial Sound Events 2019 - Microphone Array</strong>, of an identical sound scene with the only difference in the format of the audio. The <strong>TAU Spatial Sound Events 2019 - Ambisonic</strong> provides four-channel First-Order Ambisonic (FOA) recordings while the <strong>TAU Spatial Sound Events 2019 - Microphone Array</strong> provides four-channel directional microphone recordings from a tetrahedral array configuration. Both formats are extracted from the same microphone array, and additional information on the spatial characteristics of each format can be found below. The participants can choose one of the two or both the datasets based on the audio format they prefer. Both the datasets, consists of a development and evaluation set. The development set consists of 400, one minute long recordings sampled at 48000 Hz, divided into four cross-validation splits of 100 recordings each. The evaluation set consists of 100, one-minute recordings. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor locations, at 504 unique combinations of azimuth-elevation-distance. Furthermore, in order to synthesize the recordings the collected IRs were convolved with <a href="http://www.cs.tut.fi/sgn/arg/dcase2016/task-sound-event-detection-in-synthetic-audio#audio-dataset">isolated sound events dataset from DCASE 2016 task 2</a>. Finally, to create a realistic sound scene recording, natural ambient noise collected in the IR recording locations was added to the synthesized recordings such that the average SNR of the sound events was 30 dB.</p>
<p>The IRs were collected in Finland by Tampere University between 12/2017 - 06/2018. The data collection received funding from the European Research Council, grant agreement 637422 EVERYSOUND.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="../images/sponsors/erc.jpg" title="ERC"/></a></p>
<h2 id="recording-procedure">Recording procedure</h2>
<p>The real-life IR recordings were collected using an <a href="https://mhacoustics.com/products">Eigenmike</a> spherical microphone array. A <a href="https://www.genelec.com/home-speakers/g-series-active-speakers">Genelec G Two loudspeaker</a> was used to playback a maximum length sequences (MLS) around the Eigenmike. The MLS playback level was ensured to be 30 dB greater than the ambient sound level during the recording. The IRs were obtained in the STFT domain using a least-squares regression between the known measurement signal (MLS) and far-field recording independently at each frequency. These IRs were collected in the following directions:</p>
<ul>
<li>36 IRs at every 10° azimuth angle, for 9 elevations from -40° to 40° at 10° increments, at 1 m distance from the Eigenmike, resulting in 324 discrete DOAs.</li>
<li>36 IRs at every 10° azimuth angle, for 5 elevations from -20° to 20° at 10° increments, at 2 m distance from the Eigenmike, resulting in 180 discrete DOAs.</li>
</ul>
<p>The IRs were recorded at five different indoor locations inside the Tampere University campus at Hervanta, Finland. Additionally, we also collected 30 minutes of ambient noise recordings from these five locations with the IR recording setup unchanged. The description of the indoor locations are as following:</p>
<ol>
<li>Language Center - Large common area with multiple seating tables and carpet flooring. People chatting and working.</li>
<li>Reaktori Building - Large cafeteria with multiple seating tables and carpet flooring. People chatting and having food.</li>
<li>Festia Building - High ceiling corridor with hard flooring. People walking around and chatting.</li>
<li>Tietotalo Building - Corridor with classrooms around and hard flooring. People walking around and chatting.</li>
<li>Sähkötalo Building - Large corridor with multiple sofas and tables, hard and carpet flooring at different parts. People walking around and chatting.</li>
</ol>
<h2 id="recording-format-and-dataset-specifications">Recording format and dataset specifications</h2>
<p>The <a href="http://www.cs.tut.fi/sgn/arg/dcase2016/task-sound-event-detection-in-synthetic-audio#audio-dataset">isolated sound events dataset from DCASE 2016 task 2</a> consists of 11 classes, each with 20 examples. These examples are randomly split into five sets with an equal number of examples for each class, the first four sets are used for synthesizing the four splits of development dataset, while the remaining one set is used for evaluation dataset. For each split of the dataset, we synthesize 100 recordings. Each of these recordings is generated by randomly choosing sound event examples from the corresponding set and assigning a start time, and one of the collected IRs randomly. Finally, by convolving each of these assigned sound examples with their respective IRs, we spatially position them at a given distance, azimuth and elevation angles from the Eigenmike. We make sure to use IRs from a single location for all sound events in a recording. Further, half of the recordings in each split are synthesized with up to two temporally overlapping sound events while the others are synthesized with no overlapping sound events. Finally, the ambient noise collected at the respective IR location was added to the synthesized recording such that the average SNR of the sound events is 30 dB.</p>
<p>Since the number of channels in the IRs is equal to the number of microphones in Eigenmike (32), in order to create the <strong>TAU Spatial Sound Events 2019 - Microphone Array</strong> dataset we use the channels 6, 10, 26, and 22  that corresponds to microphone positions (45°, 35°, 4.2cm), (-45°, -35°, 4.2cm), (135°, -35°, 4.2cm) and (-135°, 35°, 4.2cm). The spherical coordinate system in use is right-handed with the front at (0°, 0°), left at (90°, 0°) and top at (0°, 90°). Further, the <strong>TAU Spatial Sound Events 2019 - Ambisonic</strong> dataset is obtained by converting the 32 channel microphone signals to FOA, by means of encoding filters based on anechoic measurements of the Eigenmike array response.</p>
<p>For model-based localization approaches the array response may be considered known. The following theoretical spatial responses (steering vectors) modeling the two -formats describe the directional response of each channel to a source incident from DOA given by azimuth angle <span class="math">\(\phi\)</span> and elevation angle <span class="math">\(\theta\)</span>.</p>
<p>For the first-order ambisonics:</p>
<div class="math">\begin{eqnarray}
H_1(\phi, \theta, f) &amp;=&amp; 1 \\
H_2(\phi, \theta, f) &amp;=&amp; \sqrt{3} * \sin(\phi) * \cos(\theta) \\
H_3(\phi, \theta, f) &amp;=&amp; \sqrt{3} * \sin(\theta) \\
H_4(\phi, \theta, f) &amp;=&amp; \sqrt{3} * \cos(\phi) * \cos(\theta)
\end{eqnarray}</div>
<p>For the tetrahedral array of microphones mounted on spherical baffle, an analytical expression for the directional array response is given by the expansion:
</p>
<div class="math">\begin{equation}
H_m(\phi_m, \theta_m, \phi, \theta, \omega) = \frac{1}{(\omega R/c)^2}\sum_{n=0}^{30} \frac{i^{n-1}}{h_n'^{(2)}(\omega R/c)}(2n+1)P_n(\cos(\gamma_m))
\end{equation}</div>
<p>where <span class="math">\(m\)</span> is the channel number, <span class="math">\((\phi_m, \theta_m)\)</span> are the specific microphone's azimuth and elevation position, <span class="math">\(\omega = 2\pi f\)</span> is the angular frequency, <span class="math">\(R = 0.042\)</span>m is the array radius, <span class="math">\(c = 343\)</span>m/s is the speed of sound, <span class="math">\(\cos(\gamma_m)\)</span> is the cosine angle between the microphone and the DOA, and <span class="math">\(P_n\)</span> is the unnormalized Legendre polynomial of degree <span class="math">\(n\)</span>, and <span class="math">\(h_n'^{(2)}\)</span> is the derivative with respect to the argument of a spherical Hankel function of the second kind. The expansion is limited to 30 terms which provides negligible modeling error up to 20kHz.
Note that the Ambisonics format is frequency-independent, something that holds true up to around 9kHz with the specific microphone array, while the the actual encoded response starting to deviate gradually from the ideal one provided above at higher frequencies.</p>
<p>In summary, there are 100 recordings in total in the evaluation dataset, and in each of the four splits of the development dataset. These 100 recordings are comprised of 10 recordings that have either up to two, or no temporally overlapping sound events, synthesized using the IRs from the five locations (10 * 2 * 5 = 100). The only explicit difference between each of the development dataset splits and evaluation dataset is the isolated sound event examples employed.  Although each of the development dataset splits and evaluation dataset consists of IRs from all the five locations, the dataset only guarantees a balanced distribution of sound events in each of the 36 azimuths and 9 elevation angles within the splits but does not guarantee the use of IRs collected at a single location to be entirely present in a single split. For example, some of the IRs of  Reaktori building might not be in the first split but might occur in any of the other splits.</p>
<p>More details on the IR recordings collections and synthesis of the dataset can be read in:</p>
<div class="btex-item" data-item="Adavanne2019_DCASE" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Adavanne2019_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Sharath Adavanne, Archontis Politis, and Tuomas Virtanen.
<em>A multi-room reverberant dataset for sound event localization and detection.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), 10–14. New York University, NY, USA, October 2019.
URL: <a href="https://dcase.community/workshop2019/proceedings">https://dcase.community/workshop2019/proceedings</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Adavanne_46.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" class="panel-collapse collapse" id="collapseAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" role="tabpanel">
<h4>A Multi-room Reverberant Dataset for Sound Event Localization and Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset where each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate benchmark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Adavanne_46.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881label" class="modal fade" id="bibtexAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAdavanne2019_DCASE4d309f7a44e8441790da26b3e3010881label">A Multi-room Reverberant Dataset for Sound Event Localization and Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Adavanne2019_DCASE,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    title = "A Multi-room Reverberant Dataset for Sound Event Localization and Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    address = "New York University, NY, USA",
    month = "October",
    year = "2019",
    pages = "10--14",
    abstract = "This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset where each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate benchmark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup.",
    url = "https://dcase.community/workshop2019/proceedings"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="reference-labels">Reference labels</h2>
<p>As labels, for each recording in the development dataset, we provide a CSV format file, that enlists the sound events, their respective temporal onset-offset times, azimuth and elevation angles. Since the development dataset provides four cross-validation splits, it can be used as a standalone dataset for future work. If you are preparing a publication based on the DCASE challenge set up and you want to evaluate your proposed system with official challenge evaluation setup, contact the task coordinators. The task coordinators can provide unofficial scoring for a limited amount of system outputs.</p>
<h2 id="download">Download</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.2599196" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.2599196" target="_blank">
<span style="font-size:20px;">TAU Spatial Sound Events 2019 - Ambisonic and Microphone Array, Development dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(7.9 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.2599196">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.2599196.svg"/>
</a>
<span class="text-muted">
                
                version 2
                
                
                </span>
</div>
</div>
<p><br/></p>
<p class="alert alert-info">
Dataset was updated on <strong>20 March 2019</strong> to remove labels of sound events that were missing in the audio (version 2). In order to update already downloaded dataset version 1, download only the <code>metadata_dev.zip</code> file from version 2.
</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.3377088" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.3377088" target="_blank">
<span style="font-size:20px;">TAU Spatial Sound Events 2019 - Ambisonic and Microphone Array, Evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(2 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3377088">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3377088.svg"/>
</a>
<span class="text-muted">
                
                version 2
                
                
                </span>
</div>
</div>
<p><br/></p>
<p class="alert alert-info">
Dataset was updated on <strong>26 August 2019</strong>: Now that the task has ended, we are releasing the reference labels for the evaluation dataset (version 2).
</p>
<h1 id="task-setup">Task setup</h1>
<p>The development dataset consists of a pre-defined four cross-validation split as shown in the table below. These splits consist of audio recordings and the corresponding metadata describing the sound events and their respective locations within each recording. Participants are required to report the performance of their method on the testing splits of the four folds. In order to allow a fair comparison of methods on the development dataset participants are not allowed to change the defined splits.</p>
<div class="table-responsive col-md-8">
<table class="table table-striped">
<thead>
<tr>
<th>Folds</th>
<th>Training splits</th>
<th>Validation split</th>
<th>Testing split</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fold 1</td>
<td>3, 4</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>Fold 2</td>
<td>4, 1</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>Fold 3</td>
<td>1, 2</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>Fold 4</td>
<td>2, 3</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p>The evaluation dataset is released a few weeks before the final submission deadline. This dataset consists of only audio recordings without any metadata/labels. Participants can decide the training procedure, i.e. the amount of training and validation files in the development dataset, the number of ensemble models, and submit the results of the SELD performance on the evaluation dataset.</p>
<h2 id="development-dataset">Development dataset</h2>
<p>The recordings in the development dataset follow the naming convention:</p>
<div class="highlight"><pre><span></span><code><span class="n">split</span><span class="p">[</span><span class="n">number</span><span class="p">]</span><span class="n">_ir</span><span class="p">[</span><span class="n">location</span><span class="w"> </span><span class="n">number</span><span class="p">]</span><span class="n">_ov</span><span class="p">[</span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">overlapping</span><span class="w"> </span><span class="n">sound</span><span class="w"> </span><span class="n">events</span><span class="p">]</span><span class="n">_</span><span class="p">[</span><span class="n">recording</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">split</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<p>The information of the location whose impulse response has been used to synthesize the recording or the number of overlapping sound events in the recording is only provided for the participant to understand the performance of their method with respect to different conditions. We encourage participants to do individual studies for such conditions and report as a publication in the DCASE 2019 workshop. But for the challenge, we only consider generic methods that do not use location or number of overlapping sound events information during training or inference.</p>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>The evaluation dataset consists of 100 recordings without any information on the location, or the number of overlapping sound events in the naming convention as below:</p>
<div class="highlight"><pre><span></span><code><span class="n">split</span><span class="o">[</span><span class="n">number</span><span class="o">]</span><span class="n">_</span><span class="o">[</span><span class="n">recording number per split</span><span class="o">]</span><span class="p">.</span><span class="n">wav</span>
</code></pre></div>
<h1 id="task-rules">Task rules</h1>
<ul>
<li>Use of external data is <strong>not allowed</strong>.</li>
<li>Manipulation of provided cross-validation split for development dataset is <strong>not allowed</strong>.</li>
<li>The development dataset can be augmented without the use of external data (e.g. using techniques such as pitch shifting or time stretching).</li>
<li>Participants are <strong>not allowed</strong> to make subjective judgments of the evaluation data, nor to annotate it. The evaluation dataset cannot be used to train the submitted system.</li>
</ul>
<h1 id="submission">Submission</h1>
<p>The results for each of the 100 recordings in the evaluation dataset should be collected in individual file-wise CSV. Similarly, we also collect file-wise CSV for each of the 400 recordings (testing split results of the four folds) in the development dataset. This file-wise CSVs has the same name as the audio recording, but with <code>.csv</code> extension, and contains the following information in each row.</p>
<div class="highlight"><pre><span></span><code>[frame number (int)],[active class index (int)],[azimuth (int)],[elevation (int)]
</code></pre></div>
<p>An example output file will look like below. If you use the baseline code, then this output is automatically produced for you.</p>
<div class="highlight"><pre><span></span><code><span class="mf">10</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="mf">10</span><span class="p">,</span><span class="o">-</span><span class="mf">20</span>
<span class="mf">10</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="mf">30</span>
<span class="mf">11</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="mf">10</span><span class="p">,</span><span class="o">-</span><span class="mf">20</span>
<span class="mf">11</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="mf">30</span>
<span class="mf">12</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="mf">10</span><span class="p">,</span><span class="o">-</span><span class="mf">20</span>
<span class="mf">12</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="mf">30</span>
<span class="mf">13</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="mf">10</span><span class="p">,</span><span class="o">-</span><span class="mf">20</span>
<span class="mf">13</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="mf">30</span>
<span class="mf">13</span><span class="p">,</span><span class="mf">2</span><span class="p">,</span><span class="mf">30</span><span class="p">,</span><span class="mf">0</span>
<span class="mf">112</span><span class="p">,</span><span class="mf">4</span><span class="p">,</span><span class="o">-</span><span class="mf">40</span><span class="p">,</span><span class="mf">0</span>
<span class="mf">113</span><span class="p">,</span><span class="mf">4</span><span class="p">,</span><span class="o">-</span><span class="mf">40</span><span class="p">,</span><span class="mf">0</span>
<span class="mf">114</span><span class="p">,</span><span class="mf">4</span><span class="p">,</span><span class="o">-</span><span class="mf">40</span><span class="p">,</span><span class="mf">0</span>
</code></pre></div>
<p>The output file describes that there are two instances of class 1 that is active in locations (10° , -20°) and (-50° , 30°)  for four continuous frames 10 to 13. In the 13th frame, in addition to class 1, class 2 is also active. Finally, class 4 is active in frames 112-114 at location (-40° , 0°).</p>
<p>The evaluation is performed at <strong>hop length of 20 ms</strong>, that results in <strong>3000 frames for a 60 s long audio recording</strong>. In case the participants use a different hop length for their study, we expect the participants to use a suitable post-processing method to extract the information at 20 ms hop length and submit it as evaluation results. The class index for each of the 11 classes in the provided dataset is available in the metadata provided with the dataset and the baseline code. The azimuth angles are expected in the range of -180° to 170° while the elevation angles are expected in the range of -40° to 40°, any value beyond these limits will be clipped to the respective minimum or maximum values.</p>
<p>In addition to the CSV files, the participants are asked to update the information of their method in the provided file and submit a technical report describing the method. We allow upto 4 system output submissions per participant/team. For each system, meta information should be provided in a separate file, containing the task specific information. All files should be packaged into a zip file for submission. The detailed information regarding the challenge information can be found in the submission page.</p>
<p>Detailed information for the submission can be found on the <a href="/challenge2019/submission">Submission page</a>.</p>
<h1 id="evaluation">Evaluation</h1>
<p>The SELD task is evaluated with individual metrics for SED and DOA estimation. For SED, we use the F-score and error rate (ER) calculated in one-second segments. A short description of the SED metrics is found <a href="/challenge2017/metrics">here</a>, and the detailed information is available in:</p>
<div class="btex-item" data-item="Mesaros2016_MDPI" data-source="content/data/external_publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_MDPI"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>Metrics for polyphonic sound event detection.</em>
<em>Applied Sciences</em>, 6(6):162, 2016.
URL: <a href="http://www.mdpi.com/2076-3417/6/6/162">http://www.mdpi.com/2076-3417/6/6/162</a>, <a href="https://doi.org/10.3390/app6060162">doi:10.3390/app6060162</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://github.com/TUT-ARG/sed_eval" rel="tooltip" title="Toolbox"><i class="fa fa-file-code-o"></i></a>
<button aria-controls="collapseMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" class="panel-collapse collapse" id="collapseMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" role="tabpanel">
<h4>Metrics for Polyphonic Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" href="http://www.mdpi.com/2076-3417/6/6/162" title="Journal page"><i class="fa fa-book"></i> Web publication</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://github.com/TUT-ARG/sed_eval" rel="tooltip" title="Toolbox"><i class="fa fa-file-code-o"></i> Toolbox</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5label" class="modal fade" id="bibtexMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_MDPI8cf2e7575baf40b4b0ccbf86e654c5c5label">Metrics for Polyphonic Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2016_MDPI,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "Metrics for Polyphonic Sound Event Detection",
    journal = "Applied Sciences",
    volume = "6",
    year = "2016",
    number = "6",
    pages = "162",
    url = "http://www.mdpi.com/2076-3417/6/6/162",
    issn = "2076-3417",
    abstract = "This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.",
    doi = "10.3390/app6060162"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>For DOA estimation we use two frame-wise metrics: DOA error and frame recall. The DOA error is the average angular error in degrees between the predicted and reference DOAs. For a recording of length <span class="math">\(T\)</span> time-frames, let <span class="math">\(\mathbf{DOA}^t_R\)</span> be the list of all reference DOAs at time-frame <span class="math">\(t\)</span> and <span class="math">\(\mathbf{DOA}^t_E\)</span> be the list of all estimated DOAs. The DOA error is now defined as</p>
<div class="math">\begin{equation}
DOA\,error = \frac{1}{\sum_{t=1}^{T}{D^t_E}}\sum_{t=1}^{T}{\mathcal{H}(\mathbf{DOA}^t_R, \mathbf{DOA}^t_E)},
\end{equation}</div>
<p>where <span class="math">\(D^t_E\)</span> is the number of DOAs in <span class="math">\(\mathbf{DOA}^t_E\)</span> at <span class="math">\(t\)</span>-th frame, and <span class="math">\(\mathcal{H}\)</span> is the Hungarian algorithm for solving assignment problem, i.e., matching the individual estimated DOAs with the respective reference DOAs. The Hungarian algorithm solves this by estimating the pair-wise costs between individual predicted and reference DOA using the central angle between them,</p>
<div class="math">\begin{equation}
    \sigma = \arccos(\sin\lambda_{E}\sin\lambda_{R} + \cos\lambda_{E}\cos\lambda_{R}\cos(|\phi_{R}-\phi_{E}|))
\end{equation}</div>
<p>
where the reference DOA is represented by the azimuth angle <span class="math">\(\phi_R \in [-\pi, \pi)\)</span> and elevation angle <span class="math">\(\lambda_R \in [-\pi/2, \pi/2]\)</span>, and the estimated DOA is represented with <span class="math">\((\phi_E, \lambda_E)\)</span> in the similar range as reference DOA.</p>
<p>In order to account for time frames where the number of estimated and reference DOAs are unequal, we report the second metric frame recall, which is calculated as,</p>
<div class="math">\begin{equation}
Frame\,recall =\frac{\sum_{t=1}^{T}{\mathbb{1}(D^t_R = D^t_E)}}{T},
\end{equation}</div>
<p>where <span class="math">\(D^t_R\)</span> is the number of DOAs in <span class="math">\(\mathbf{DOA}^t_R\)</span> at <span class="math">\(t\)</span>-th frame, <span class="math">\(\mathbb{1}()\)</span> is the indicator function resulting in an output one if the <span class="math">\((D^t_R = D^t_E)\)</span> condition is met else returns zero. More details regarding the DOA metrics can be found in:</p>
<div class="btex-item" data-item="adavanne2018doa_EUSIPCO" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="adavanne2018doa_EUSIPCO"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Sharath Adavanne, Archontis Politis, and Tuomas Virtanen.
<em>Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network.</em>
In 2018 26th European Signal Processing Conference (EUSIPCO), 1462–1466. IEEE, 2018.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1710.10059.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" class="panel-collapse collapse" id="collapseadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" role="tabpanel">
<h4>Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper proposes a deep neural network for estimating the directions of arrival (DOA) of multiple sound sources. The proposed stacked convolutional and recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS) along with the DOA estimates in both azimuth and elevation. We avoid any explicit feature extraction step by using the magnitudes and phases of the spectrograms of all the channels as input to the network. The proposed DOAnet is evaluated by estimating the DOAs of multiple concurrently present sources in anechoic, matched and unmatched reverberant conditions. The results show that the proposed DOAnet is capable of estimating the number of sources and their respective DOAs with good precision and generate SPS with high signal-to-noise ratio.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1710.10059.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8label" class="modal fade" id="bibtexadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexadavanne2018doa_EUSIPCObaebb0c932f042afbc03517dde7e50d8label">Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{adavanne2018doa_EUSIPCO,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    title = "Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network",
    booktitle = "2018 26th European Signal Processing Conference (EUSIPCO)",
    pages = "1462--1466",
    year = "2018",
    organization = "IEEE",
    abstract = "This paper proposes a deep neural network for estimating the directions of arrival (DOA) of multiple sound sources. The proposed stacked convolutional and recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS) along with the DOA estimates in both azimuth and elevation. We avoid any explicit feature extraction step by using the magnitudes and phases of the spectrograms of all the channels as input to the network. The proposed DOAnet is evaluated by estimating the DOAs of multiple concurrently present sources in anechoic, matched and unmatched reverberant conditions. The results show that the proposed DOAnet is capable of estimating the number of sources and their respective DOAs with good precision and generate SPS with high signal-to-noise ratio."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>An ideal SELD method will have an error rate of zero, F score of 1 (reported in %), DOA error of 0° and frame recall of 1 (reported in %). In order to compare the submitted methods, we will rank each method individually for all the four metrics, and the final positions will be the obtained using the cumulative minimum of the ranks.</p>
<p><strong>PLEASE NOTE:</strong> The four cross-validation folds are treated as a single experiment, meaning that metrics are calculated only after training and testing all folds, not as the average of the individual folds nor as the average of individual class performance. Intermediate measures (insertions, deletions, substitutions) from all folds are accumulated before calculating metrics. For more information on why so, please refer to the following paper:</p>
<div class="btex-item" data-item="Forman2010" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Forman2010"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            George Forman and Martin Scholz.
<em>Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement.</em>
<em>SIGKDD Explor. Newsl.</em>, 12(1):49–57, November 2010.
URL: <a href="http://doi.acm.org/10.1145/1882471.1882479">http://doi.acm.org/10.1145/1882471.1882479</a>, <a href="https://doi.org/10.1145/1882471.1882479">doi:10.1145/1882471.1882479</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexForman20101762f27240984ac49ed466ce2b23ede3" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.kdd.org/exploration_files/v12-1-p49-forman-sigkdd.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseForman20101762f27240984ac49ed466ce2b23ede3" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseForman20101762f27240984ac49ed466ce2b23ede3" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingForman20101762f27240984ac49ed466ce2b23ede3" class="panel-collapse collapse" id="collapseForman20101762f27240984ac49ed466ce2b23ede3" role="tabpanel">
<h4>Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement</h4>
<h5>Abstract</h5>
<p class="text-justify">Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexForman20101762f27240984ac49ed466ce2b23ede3" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.kdd.org/exploration_files/v12-1-p49-forman-sigkdd.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexForman20101762f27240984ac49ed466ce2b23ede3label" class="modal fade" id="bibtexForman20101762f27240984ac49ed466ce2b23ede3" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexForman20101762f27240984ac49ed466ce2b23ede3label">Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement</h4>
</div>
<div class="modal-body">
<pre>@article{Forman2010,
    author = "Forman, George and Scholz, Martin",
    title = "Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement",
    abstract = "Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.",
    journal = "SIGKDD Explor. Newsl.",
    issue_date = "June 2010",
    volume = "12",
    number = "1",
    month = "November",
    year = "2010",
    issn = "1931-0145",
    pages = "49--57",
    numpages = "9",
    url = "http://doi.acm.org/10.1145/1882471.1882479",
    doi = "10.1145/1882471.1882479",
    acmid = "1882479",
    publisher = "ACM",
    address = "New York, NY, USA"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h1 id="results">Results</h1>
<p>The SELD task received 58 submissions in total from 22 teams across the world. The results for these submissions are as following.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="anchor" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_er" data-scatter-y="eval_doa" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="5">Evaluation dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
                Submission name
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="anchor_rank" data-sortable="true" data-value-type="int">
                Official <br/>rank
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_er" data-reversed="true" data-sortable="true" data-value-type="float2">
                Error <br/>Rate
            </th>
<th class="text-center" data-chartable="true" data-field="eval_f" data-sortable="true" data-value-type="float1-percentage">
                F-score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eval_doa" data-reversed="true" data-sortable="true" data-value-type="float1">
                DOA<br/>error
            </th>
<th class="text-center" data-chartable="true" data-field="eval_fr" data-sortable="true" data-value-type="float1-percentage">
                Frame <br/>recall
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_2</td>
<td>Slawomir Kapka</td>
<td>Samsung R&amp;D Institute Poland</td>
<td>task-sound-event-localization-and-detection-results#Kapka2019</td>
<td>1</td>
<td>0.08</td>
<td>94.7</td>
<td>3.7</td>
<td>96.8</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_4</td>
<td>Slawomir Kapka</td>
<td>Samsung R&amp;D Institute Poland</td>
<td>task-sound-event-localization-and-detection-results#Kapka2019</td>
<td>2</td>
<td>0.08</td>
<td>94.7</td>
<td>3.7</td>
<td>96.8</td>
</tr>
<tr>
<td></td>
<td>Kapka_SRPOL_task3_3</td>
<td>Slawomir Kapka</td>
<td>Samsung R&amp;D Institute Poland</td>
<td>task-sound-event-localization-and-detection-results#Kapka2019</td>
<td>3</td>
<td>0.10</td>
<td>93.5</td>
<td>4.6</td>
<td>96.0</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_4</td>
<td>Qiuqiang Kong</td>
<td>University of Surrey</td>
<td>task-sound-event-localization-and-detection-results#Cao2019</td>
<td>4</td>
<td>0.08</td>
<td>95.5</td>
<td>5.5</td>
<td>92.2</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_1</td>
<td>Wei Xue</td>
<td>JD.COM</td>
<td>task-sound-event-localization-and-detection-results#Xue2019</td>
<td>5</td>
<td>0.06</td>
<td>96.3</td>
<td>9.7</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_2</td>
<td>Liang He</td>
<td>Tsinghua University</td>
<td>task-sound-event-localization-and-detection-results#He2019</td>
<td>6</td>
<td>0.06</td>
<td>96.7</td>
<td>22.4</td>
<td>94.1</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_1</td>
<td>Liang He</td>
<td>Tsinghua University</td>
<td>task-sound-event-localization-and-detection-results#He2019</td>
<td>7</td>
<td>0.06</td>
<td>96.6</td>
<td>23.8</td>
<td>94.4</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_1</td>
<td>Qiuqiang Kong</td>
<td>University of Surrey</td>
<td>task-sound-event-localization-and-detection-results#Cao2019</td>
<td>8</td>
<td>0.09</td>
<td>95.1</td>
<td>5.5</td>
<td>91.0</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_4</td>
<td>Wei Xue</td>
<td>JD.COM</td>
<td>task-sound-event-localization-and-detection-results#Xue2019</td>
<td>9</td>
<td>0.07</td>
<td>95.9</td>
<td>10.0</td>
<td>92.6</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_1</td>
<td>Wen Jie Jee</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Jee2019</td>
<td>10</td>
<td>0.12</td>
<td>93.7</td>
<td>4.2</td>
<td>91.8</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_3</td>
<td>Wei Xue</td>
<td>JD.COM</td>
<td>task-sound-event-localization-and-detection-results#Xue2019</td>
<td>11</td>
<td>0.08</td>
<td>95.6</td>
<td>10.1</td>
<td>92.2</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_4</td>
<td>Liang He</td>
<td>Tsinghua University</td>
<td>task-sound-event-localization-and-detection-results#He2019</td>
<td>12</td>
<td>0.06</td>
<td>96.3</td>
<td>26.1</td>
<td>93.4</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_3</td>
<td>Qiuqiang Kong</td>
<td>University of Surrey</td>
<td>task-sound-event-localization-and-detection-results#Cao2019</td>
<td>13</td>
<td>0.10</td>
<td>94.9</td>
<td>5.8</td>
<td>90.4</td>
</tr>
<tr>
<td></td>
<td>Xue_JDAI_task3_2</td>
<td>Wei Xue</td>
<td>JD.COM</td>
<td>task-sound-event-localization-and-detection-results#Xue2019</td>
<td>14</td>
<td>0.09</td>
<td>95.2</td>
<td>9.2</td>
<td>91.5</td>
</tr>
<tr>
<td></td>
<td>He_THU_task3_3</td>
<td>Liang He</td>
<td>Tsinghua University</td>
<td>task-sound-event-localization-and-detection-results#He2019</td>
<td>15</td>
<td>0.08</td>
<td>95.6</td>
<td>24.4</td>
<td>92.9</td>
</tr>
<tr>
<td></td>
<td>Cao_Surrey_task3_2</td>
<td>Qiuqiang Kong</td>
<td>University of Surrey</td>
<td>task-sound-event-localization-and-detection-results#Cao2019</td>
<td>16</td>
<td>0.12</td>
<td>93.8</td>
<td>5.5</td>
<td>89.0</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Nguyen2019</td>
<td>17</td>
<td>0.11</td>
<td>93.4</td>
<td>5.4</td>
<td>88.8</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_3</td>
<td>Yuma Koizumi</td>
<td>NTT Media Intelligence Laboratories</td>
<td>task-sound-event-localization-and-detection-results#MazzonYasuda2019</td>
<td>18</td>
<td>0.10</td>
<td>94.2</td>
<td>6.4</td>
<td>88.8</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_3</td>
<td>Chang Joon-Hyuk</td>
<td>Hanyang University</td>
<td>task-sound-event-localization-and-detection-results#Chang2019</td>
<td>19</td>
<td>0.14</td>
<td>91.9</td>
<td>2.7</td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_4</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Nguyen2019</td>
<td>20</td>
<td>0.12</td>
<td>93.2</td>
<td>5.5</td>
<td>88.7</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_4</td>
<td>Chang Joon-Hyuk</td>
<td>Hanyang University</td>
<td>task-sound-event-localization-and-detection-results#Chang2019</td>
<td>21</td>
<td>0.17</td>
<td>90.5</td>
<td>3.1</td>
<td>94.1</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_2</td>
<td>Yuma Koizumi</td>
<td>NTT Media Intelligence Laboratories</td>
<td>task-sound-event-localization-and-detection-results#MazzonYasuda2019</td>
<td>22</td>
<td>0.13</td>
<td>93.0</td>
<td>5.0</td>
<td>88.2</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_2</td>
<td>Chang Joon-Hyuk</td>
<td>Hanyang University</td>
<td>task-sound-event-localization-and-detection-results#Chang2019</td>
<td>23</td>
<td>0.14</td>
<td>92.3</td>
<td>9.7</td>
<td>95.3</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task3_1</td>
<td>Chang Joon-Hyuk</td>
<td>Hanyang University</td>
<td>task-sound-event-localization-and-detection-results#Chang2019</td>
<td>24</td>
<td>0.13</td>
<td>92.8</td>
<td>8.4</td>
<td>91.4</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_1</td>
<td>Yuma Koizumi</td>
<td>NTT Media Intelligence Laboratories</td>
<td>task-sound-event-localization-and-detection-results#MazzonYasuda2019</td>
<td>25</td>
<td>0.12</td>
<td>93.3</td>
<td>7.1</td>
<td>88.1</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_3</td>
<td>Rishabh Ranjan</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Ranjan2019</td>
<td>26</td>
<td>0.16</td>
<td>90.9</td>
<td>5.7</td>
<td>91.8</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_4</td>
<td>Rishabh Ranjan</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Ranjan2019</td>
<td>27</td>
<td>0.16</td>
<td>90.7</td>
<td>6.4</td>
<td>92.0</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_1</td>
<td>Sooyoung Park</td>
<td>Electronics and Telecommunications Research Institute</td>
<td>task-sound-event-localization-and-detection-results#Park2019</td>
<td>28</td>
<td>0.15</td>
<td>91.9</td>
<td>5.1</td>
<td>87.4</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_1</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Nguyen2019</td>
<td>29</td>
<td>0.15</td>
<td>91.1</td>
<td>5.6</td>
<td>89.8</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_2</td>
<td>Shuangran Leung</td>
<td>DBSonics</td>
<td>task-sound-event-localization-and-detection-results#Leung2019</td>
<td>30</td>
<td>0.12</td>
<td>93.3</td>
<td>25.9</td>
<td>91.1</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_2</td>
<td>Sooyoung Park</td>
<td>Electronics and Telecommunications Research Institute</td>
<td>task-sound-event-localization-and-detection-results#Park2019</td>
<td>31</td>
<td>0.15</td>
<td>91.8</td>
<td>5.0</td>
<td>87.2</td>
</tr>
<tr>
<td></td>
<td>Grondin_MIT_task3_1</td>
<td>Francois Grondin</td>
<td>Massachusetts Institute of Technology</td>
<td>task-sound-event-localization-and-detection-results#Grondin2019</td>
<td>32</td>
<td>0.14</td>
<td>92.2</td>
<td>7.4</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>Leung_DBS_task3_1</td>
<td>Shuangran Leung</td>
<td>DBSonics</td>
<td>task-sound-event-localization-and-detection-results#Leung2019</td>
<td>33</td>
<td>0.12</td>
<td>93.4</td>
<td>27.2</td>
<td>90.7</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_3</td>
<td>Sooyoung Park</td>
<td>Electronics and Telecommunications Research Institute</td>
<td>task-sound-event-localization-and-detection-results#Park2019</td>
<td>34</td>
<td>0.15</td>
<td>91.9</td>
<td>7.0</td>
<td>87.4</td>
</tr>
<tr>
<td></td>
<td>MazzonYasuda_NTT_task3_4</td>
<td>Yuma Koizumi</td>
<td>NTT Media Intelligence Laboratories</td>
<td>task-sound-event-localization-and-detection-results#MazzonYasuda2019</td>
<td>35</td>
<td>0.14</td>
<td>92.0</td>
<td>7.3</td>
<td>87.1</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_4</td>
<td>Sooyoung Park</td>
<td>Electronics and Telecommunications Research Institute</td>
<td>task-sound-event-localization-and-detection-results#Park2019</td>
<td>36</td>
<td>0.15</td>
<td>91.8</td>
<td>7.0</td>
<td>87.2</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_1</td>
<td>Rishabh Ranjan</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Ranjan2019</td>
<td>37</td>
<td>0.18</td>
<td>89.9</td>
<td>8.6</td>
<td>90.1</td>
</tr>
<tr>
<td></td>
<td>Ranjan_NTU_task3_2</td>
<td>Rishabh Ranjan</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Ranjan2019</td>
<td>38</td>
<td>0.22</td>
<td>86.8</td>
<td>7.8</td>
<td>90.0</td>
</tr>
<tr>
<td></td>
<td>ZhaoLu_UESTC_task3_1</td>
<td>Zhao Lu</td>
<td>University of Electronic Science and Technology of China</td>
<td>task-sound-event-localization-and-detection-results#ZhaoLu2019</td>
<td>39</td>
<td>0.18</td>
<td>89.3</td>
<td>6.8</td>
<td>84.3</td>
</tr>
<tr>
<td></td>
<td>Rough_EMED_task3_2</td>
<td>Pi LiHong</td>
<td>Tsinghua University</td>
<td>task-sound-event-localization-and-detection-results#Rough2019</td>
<td>40</td>
<td>0.18</td>
<td>89.7</td>
<td>9.4</td>
<td>85.5</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_2</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Nguyen2019</td>
<td>41</td>
<td>0.17</td>
<td>89.7</td>
<td>8.0</td>
<td>77.3</td>
</tr>
<tr>
<td></td>
<td>Jee_NTU_task3_2</td>
<td>Wen Jie Jee</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Jee2019</td>
<td>42</td>
<td>0.19</td>
<td>89.1</td>
<td>8.1</td>
<td>85.0</td>
</tr>
<tr>
<td></td>
<td>Tan_NTU_task3_1</td>
<td>Ee Leng Tan</td>
<td>Nanyang Technological University</td>
<td>task-sound-event-localization-and-detection-results#Tan2019</td>
<td>43</td>
<td>0.17</td>
<td>89.8</td>
<td>15.4</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Lewandowski_SRPOL_task3_1</td>
<td>Mateusz Lewandowski</td>
<td>Samsung R&amp;D Institute Poland</td>
<td>task-sound-event-localization-and-detection-results#Kapka2019</td>
<td>44</td>
<td>0.19</td>
<td>89.4</td>
<td>36.2</td>
<td>87.7</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_2</td>
<td>Hector Cordourier Maruri</td>
<td>Intel Corporation</td>
<td>task-sound-event-localization-and-detection-results#Cordourier2019</td>
<td>45</td>
<td>0.22</td>
<td>86.5</td>
<td>20.8</td>
<td>85.7</td>
</tr>
<tr>
<td></td>
<td>Cordourier_IL_task3_1</td>
<td>Hector Cordourier Maruri</td>
<td>Intel Corporation</td>
<td>task-sound-event-localization-and-detection-results#Cordourier2019</td>
<td>46</td>
<td>0.22</td>
<td>86.3</td>
<td>19.9</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_4</td>
<td>Daniel Krause</td>
<td>AGH University of Science and Technology</td>
<td>task-sound-event-localization-and-detection-results#Krause2019</td>
<td>47</td>
<td>0.22</td>
<td>87.4</td>
<td>31.0</td>
<td>87.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_FOA_baseline</td>
<td>Sharath Adavanne</td>
<td>Tampere University</td>
<td>task-sound-event-localization-and-detection-results#Adavanne2019</td>
<td>48</td>
<td>0.28</td>
<td>85.4</td>
<td>24.6</td>
<td>85.7</td>
</tr>
<tr>
<td></td>
<td>Perezlopez_UPF_task3_1</td>
<td>Andres Perez-Lopez</td>
<td>Centre Tecnologic de Catalunya</td>
<td>task-sound-event-localization-and-detection-results#Perezlopez2019</td>
<td>49</td>
<td>0.29</td>
<td>82.1</td>
<td>9.3</td>
<td>75.8</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_1</td>
<td>Sotirios Panagiotis Chytas</td>
<td>University of Thessaly</td>
<td>task-sound-event-localization-and-detection-results#Chytas2019</td>
<td>50</td>
<td>0.29</td>
<td>82.4</td>
<td>18.6</td>
<td>75.6</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_3</td>
<td>Jorn Anemuller</td>
<td>University of Oldenburg</td>
<td>task-sound-event-localization-and-detection-results#Anemueller2019</td>
<td>51</td>
<td>0.28</td>
<td>83.8</td>
<td>29.2</td>
<td>84.1</td>
</tr>
<tr>
<td></td>
<td>Chytas_UTH_task3_2</td>
<td>Sotirios Panagiotis Chytas</td>
<td>University of Thessaly</td>
<td>task-sound-event-localization-and-detection-results#Chytas2019</td>
<td>52</td>
<td>0.29</td>
<td>82.3</td>
<td>18.7</td>
<td>75.7</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_2</td>
<td>Daniel Krause</td>
<td>AGH University of Science and Technology</td>
<td>task-sound-event-localization-and-detection-results#Krause2019</td>
<td>53</td>
<td>0.32</td>
<td>82.9</td>
<td>31.7</td>
<td>85.7</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_1</td>
<td>Daniel Krause</td>
<td>AGH University of Science and Technology</td>
<td>task-sound-event-localization-and-detection-results#Krause2019</td>
<td>54</td>
<td>0.30</td>
<td>83.0</td>
<td>32.5</td>
<td>85.3</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_1</td>
<td>Jorn Anemuller</td>
<td>University of Oldenburg</td>
<td>task-sound-event-localization-and-detection-results#Anemueller2019</td>
<td>55</td>
<td>0.33</td>
<td>81.3</td>
<td>28.2</td>
<td>84.5</td>
</tr>
<tr>
<td></td>
<td>Kong_SURREY_task3_1</td>
<td>Qiuqiang Kong</td>
<td>University of Surrey</td>
<td>task-sound-event-localization-and-detection-results#Kong2019</td>
<td>56</td>
<td>0.29</td>
<td>83.4</td>
<td>37.6</td>
<td>81.3</td>
</tr>
<tr>
<td></td>
<td>Anemueller_UOL_task3_2</td>
<td>Jorn Anemuller</td>
<td>University of Oldenburg</td>
<td>task-sound-event-localization-and-detection-results#Anemueller2019</td>
<td>57</td>
<td>0.36</td>
<td>79.8</td>
<td>25.0</td>
<td>84.1</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2019_MIC_baseline</td>
<td>Sharath Adavanne</td>
<td>Tampere University</td>
<td>task-sound-event-localization-and-detection-results#Adavanne2019</td>
<td>58</td>
<td>0.30</td>
<td>83.2</td>
<td>38.1</td>
<td>83.4</td>
</tr>
<tr>
<td></td>
<td>Lin_YYZN_task3_1</td>
<td>Yifeng Lin</td>
<td>Esound corporation</td>
<td>task-sound-event-localization-and-detection-results#Lin2019</td>
<td>59</td>
<td>1.03</td>
<td>2.6</td>
<td>21.9</td>
<td>31.6</td>
</tr>
<tr>
<td></td>
<td>Krause_AGH_task3_3</td>
<td>Daniel Krause</td>
<td>AGH University of Science and Technology</td>
<td>task-sound-event-localization-and-detection-results#Krause2019</td>
<td>60</td>
<td>0.35</td>
<td>80.3</td>
<td>52.6</td>
<td>83.6</td>
</tr>
</tbody>
</table>
<p>Complete results and technical reports can be found in the <a class="btn btn-primary" href="/challenge2019/task-sound-event-localization-and-detection-results">results page</a></p>
<h1 id="awards">Awards</h1>
<p>This task will offer two awards, not necessarily based on the evaluation set performance ranking. These awards aim to encourage contestants to openly publish their code, and to use novel and problem-specific approaches which leverage knowledge of the audio domain. We also highly encourage student authorship.</p>
<div class="row">
<div class="col-md-2 col-xs-top text-center">
<a href="/challenge2019/awards#reproducible-system-award">
<span class="fa-stack fa-4x">
<i class="fa fa-circle fa-stack-2x" style="color:#75ce75;"></i>
<i class="fa fa-trophy fa-stack-1x fa-inverse" style="color:#82ec82;"></i>
<span class="fa-stack-1x" style="font-size:48%;color:white;font-weight:bold;line-height:20px;margin-top:1em;">Open source</span>
<span class="fa-stack-1x dcase-icon-bottom-text">Award</span>
</span>
</a>
</div>
<div class="col-md-10">
<a href="/challenge2019/awards#reproducible-system-award">
<h3 id="open-source-award">Reproducible system award</h3>
</a>
<p>Reproducible system award of <strong>500 USD</strong> will be offered for the highest scoring method that is open-source and fully reproducible. For full reproducibility, the authors must provide all the information needed to run the system and achieve the reported performance. The choice of licence is left to the author, but should ideally be selected among the ones approved by the <a href="https://opensource.org/licenses" target="_blank">Open Source Initiative</a>.
</p>
</div>
</div>
<div class="row">
<div class="col-md-2 col-xs-top text-center">
<a href="/challenge2019/awards#judges’-award">
<span class="fa-stack fa-4x">
<i class="fa fa-circle fa-stack-2x" style="color:#75ce75;"></i>
<i class="fa fa-trophy fa-stack-1x fa-inverse" style="color:#82ec82;"></i>
<span class="fa-stack-1x" style="font-size:48%;color:white;font-weight:bold;">Judges</span>
<span class="fa-stack-1x dcase-icon-bottom-text">Award</span>
</span>
</a>
</div>
<div class="col-md-10">
<a href="/challenge2019/awards#judges’-award">
<h3 id="judges-award">Judges’ award</h3>
</a>
<p>Judges’ award of <strong>500 USD</strong> will be offered for the method considered by the judges to be the most interesting or innovative. Criteria considered for this award include but are not limited to: originality, complexity, student participation, open-source, etc. Single model approaches are strongly preferred over ensembles;  occasionally, small ensembles of different models can be considered, if the approach is innovative.</p>
</div>
</div>
<p>More information can be found on the <a href="/challenge2019/awards">Award page</a>.</p>
<p><br/></p>
<h4 class="text-center">The awards are sponsored by</h4>
<table style="background-color:#fafafa;border-collapse:collapse;border-radius:1em;overflow:hidden;margin-bottom:20px;">
<tbody>
<tr>
<td colspan="6" style="width:50%;padding-top:10px;padding-bottom:0px;padding-left:10px;"><span class="text-muted">Gold sponsor</span></td>
<td colspan="6" style="width:50%;padding-top:10px;padding-bottom:0px;padding-left:10px;"><span class="text-muted">Silver sponsor</span></td>
</tr>
<tr>
<td colspan="6" style="width:50%;padding-top:0px;padding-bottom:0px;padding-left:10px;padding-right:20px;">
<a href="https://www.sonos.com/" target="_blank">
<img alt="Sonos" class="img img-responsive" src="/images/sponsors/sonos_logo.png" style="margin-left: auto;margin-right: auto;"/>
</a>
</td>
<td colspan="6" style="width:50%;padding-top:0px;padding-bottom:0px;padding-left:20px;padding-right:10px;">
<a href="https://www.harman.com/" target="_blank">
<img alt="Harman" class="img img-responsive" src="/images/sponsors/harman_logo.png" style="margin-left: auto;margin-right: auto;"/>
</a>
</td>
</tr>
<tr>
<td colspan="12" style="width:100%;padding-top:0px;padding-bottom:0px;padding-left:10px;"><span class="text-muted">Bronze sponsors</span></td>
</tr>
<tr>
<td colspan="4" style="width:33.3333%;padding-top:0px;padding-bottom:0px;padding-left:10px;padding-right:20px;">
<a href="http://cochlear.ai/" target="_blank">
<img alt="Cochlear.ai" class="img img-responsive" src="/images/sponsors/cochlearai_logo_2019.png" style="margin-left: auto;margin-right: auto;"/>
</a>
</td>
<td colspan="4" style="width:33.3333%;padding-top:0px;padding-bottom:0px;padding-right:20px;padding-left:20px;">
<a href="https://www.oticon.global/" target="_blank">
<img alt="Oticon" class="img img-responsive" src="/images/sponsors/oticon_logo.png" style="margin-left: auto;margin-right: auto;"/>
</a>
</td>
<td colspan="4" style="width:33.3333%;padding-top:0px;padding-bottom:0px;padding-left:20px;padding-right:10px;">
<a href="https://www.soundintel.com/" target="_blank">
<img alt="Sound Intelligence" class="img img-responsive" src="/images/sponsors/sound_intelligence_logo.png" style="margin-left: auto;margin-right: auto;"/>
</a>
</td>
</tr>
<tr>
<td colspan="12" style="width:100%;padding-top:10px;padding-bottom:0px;padding-left:10px;"><span class="text-muted">Technical sponsor</span></td>
</tr>
<tr>
<td colspan="4" style="width:33.3333%;padding-top:0px;padding-left:10px;padding-right:20px;padding-bottom:10px;">
<a href="https://www.inria.fr/en/" target="_blank">
<img alt="Inria" class="img img-responsive" src="/images/logos/organizers/inria.png" style="margin-left: auto;margin-right: auto;"/>
</a>
</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="baseline-system">Baseline system</h1>
<p>As the baseline, we use the recently published SELDnet, a CRNN based method that uses the confidence of the SED to estimate one DOA for each sound class. The SED is obtained as a multiclass multilabel classification, whereas DOA is performed as a multioutput regression. The SELDnet uses the magnitude and phase component of the FFT as input feature, SED labels represented as one-hot encoding and DOAs represented as azimuth and elevation angles in radians. More details about SELDnet can be read in:</p>
<div class="btex-item" data-item="Adavanne2018_JSTSP" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Adavanne2018_JSTSP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Sharath Adavanne, Archontis Politis, Joonas Nikunen, and Tuomas Virtanen.
<em>Sound event localization and detection of overlapping sources using convolutional recurrent neural networks.</em>
<em>IEEE Journal of Selected Topics in Signal Processing</em>, 13(1):34–48, March 2018.
<a href="https://doi.org/10.1109/JSTSP.2018.2885636">doi:10.1109/JSTSP.2018.2885636</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1807.00129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" class="panel-collapse collapse" id="collapseAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" role="tabpanel">
<h4>Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h4>
<h5>Abstract</h5>
<p class="text-justify">In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.</p>
<h5>Keywords</h5>
<p class="text-justify">Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1807.00129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409flabel" class="modal fade" id="bibtexAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409f" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAdavanne2018_JSTSPdd7a557a630f4310aa33a7dab88a409flabel">Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h4>
</div>
<div class="modal-body">
<pre>@article{Adavanne2018_JSTSP,
    author = "Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    title = "Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks",
    year = "2018",
    volume = "13",
    number = "1",
    pages = "34--48",
    keywords = "Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network",
    abstract = "In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.",
    doi = "10.1109/JSTSP.2018.2885636",
    issn = "1932-4553",
    month = "March"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>A difference of the baseline implementation here and the implementation in the above publication is that the baseline outputs directly azimuth and elevation angles, rather than cartesian components of the DOA vector.</p>
<p><strong>PLEASE NOTE:</strong> The SELD task is in a nascent stage. Although the SELDnet architecture used as baseline has been found to perform effectively on the SELD task for a range of conditions, there are still a lot of approaches that are unexplored in both data-driven and model-based estimation. We believe the proposed task and dataset will help explore new methods, and the results obtained, poorer or better, should be treated as valid research and shared with the community. This will only benefit future researchers on what works and what doesn't.</p>
<h2 id="repository">Repository</h2>
<p>This repository implements SELDnet and performs cross-validation in the manner we recommend. We also provide scripts to visualize your SELD results and estimate the relevant metric scores before final submission.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/sharathadavanne/seld-dcase2019" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/sharathadavanne/seld-dcase2019" target="_blank">
<span style="font-size:20px;">DCASE 2019 SELD task baseline repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="results-for-the-development-dataset">Results for the development dataset</h2>
<div class="table-responsive col-md-12">
<table class="table table-striped">
<thead>
<tr>
<th>Dataset</th>
<th>Error rate</th>
<th>F score</th>
<th>DOA error</th>
<th>Frame recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ambisonic</td>
<td>0.34</td>
<td>79.9 %</td>
<td>28.5°</td>
<td>85.4 %</td>
</tr>
<tr>
<td>Microphone array</td>
<td>0.35</td>
<td>80.0 %</td>
<td>30.8°</td>
<td>84.0 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p><strong>Note:</strong> The reported baseline system performance is not exactly reproducible due to varying setups. However, you should be able to obtain very similar results.</p>
<h1 id="citation">Citation</h1>
<p>If you are participating in this task or using the dataset and code please consider citing the following papers:</p>
<div class="btex-item" data-item="Adavanne2019_DCASE" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Adavanne2019_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Sharath Adavanne, Archontis Politis, and Tuomas Virtanen.
<em>A multi-room reverberant dataset for sound event localization and detection.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), 10–14. New York University, NY, USA, October 2019.
URL: <a href="https://dcase.community/workshop2019/proceedings">https://dcase.community/workshop2019/proceedings</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Adavanne_46.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" class="panel-collapse collapse" id="collapseAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" role="tabpanel">
<h4>A Multi-room Reverberant Dataset for Sound Event Localization and Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset where each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate benchmark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2019/proceedings/DCASE2019Workshop_Adavanne_46.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bflabel" class="modal fade" id="bibtexAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bf" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAdavanne2019_DCASE085f8c0313034630a5d94a813c1408bflabel">A Multi-room Reverberant Dataset for Sound Event Localization and Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Adavanne2019_DCASE,
    author = "Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas",
    title = "A Multi-room Reverberant Dataset for Sound Event Localization and Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)",
    address = "New York University, NY, USA",
    month = "October",
    year = "2019",
    pages = "10--14",
    abstract = "This paper presents the sound event localization and detection (SELD) task setup for the DCASE 2019 challenge. The goal of the SELD task is to detect the temporal activities of a known set of sound event classes, and further localize them in space when active. As part of the challenge, a synthesized dataset where each sound event associated with a spatial coordinate represented using azimuth and elevation angles is provided. These sound events are spatialized using real-life impulse responses collected at multiple spatial coordinates in five different rooms with varying dimensions and material properties. A baseline SELD method employing a convolutional recurrent neural network is used to generate benchmark scores for this reverberant dataset. The benchmark scores are obtained using the recommended cross-validation setup.",
    url = "https://dcase.community/workshop2019/proceedings"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Adavanne2018_JSTSP" data-source="content/data/challenge2019/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Adavanne2018_JSTSP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Sharath Adavanne, Archontis Politis, Joonas Nikunen, and Tuomas Virtanen.
<em>Sound event localization and detection of overlapping sources using convolutional recurrent neural networks.</em>
<em>IEEE Journal of Selected Topics in Signal Processing</em>, 13(1):34–48, March 2018.
<a href="https://doi.org/10.1109/JSTSP.2018.2885636">doi:10.1109/JSTSP.2018.2885636</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1807.00129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" class="panel-collapse collapse" id="collapseAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" role="tabpanel">
<h4>Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h4>
<h5>Abstract</h5>
<p class="text-justify">In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.</p>
<h5>Keywords</h5>
<p class="text-justify">Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1807.00129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93label" class="modal fade" id="bibtexAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAdavanne2018_JSTSP89696da2ed0b41e9b586151aceb2ae93label">Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h4>
</div>
<div class="modal-body">
<pre>@article{Adavanne2018_JSTSP,
    author = "Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    title = "Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks",
    year = "2018",
    volume = "13",
    number = "1",
    pages = "34--48",
    keywords = "Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network",
    abstract = "In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.",
    doi = "10.1109/JSTSP.2018.2885636",
    issn = "1932-4553",
    month = "March"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>