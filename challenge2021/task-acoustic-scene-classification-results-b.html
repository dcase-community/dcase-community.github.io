<!DOCTYPE html><html lang="en">
<head>
    <title>Audio-Visual Scene Classification - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2021/task-acoustic-scene-classification-results-b">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description This subtask is concerned with classification using audio and video modalities. Since audio-visual machine learning has gained popularity in the last years, we aim to provide a multidisciplinary task that may attract researchers from the machine vision community. We imposed no restrictions on the modality or combinations of â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2021</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2021/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2021/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class=" active">
        <a href="/challenge2021/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2021/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2021/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/grid-03.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><strong class="fa-stack-1x icon-text">B</strong><strong class="fa-stack-1x dcase-icon-top-text">Complexity</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2021_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Audio-Visual Scene Classification</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#modality">Modality</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a>
<ul>
<li><a href="#log-loss">Log loss</a></li>
<li><a href="#accuracy">Accuracy</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>This subtask is concerned with classification using audio and video modalities. Since audio-visual machine learning has gained popularity in the last years, we aim to provide a multidisciplinary task that may attract researchers from the machine vision community. </p>
<p>We imposed no restrictions on the modality or combinations of modalities used in the system. We encouraged participants to also submit single-modality systems (audio-only or video-only methods for scene classification).</p>
<p>The development set contains synchronized audio and video recordings from 10 European cities in 10 different scenes. The total amount of audio in the development set is 34 hours. The evaluation set contains data from 12 cities (2 cities unseen in the development set). Evaluation data contains 20 hours of audio. </p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2021/task-acoustic-scene-classification#subtask-b" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="logloss_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="3">Submission information</th>
<th class="sep-left-cell text-center" colspan="3">Evaluation dataset</th>
<th class="sep-left-cell text-center" colspan="2">Development dataset</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small><small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss (Development dataset)" data-chartable="true" data-field="logloss_dev" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Development dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Development dataset)" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> (Development dataset)</small>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>muls_tr(1)</td>
<td>Boes2021</td>
<td>23</td>
<td>0.653</td>
<td>74.5 (74.2 - 74.8)</td>
<td>0.620</td>
<td>75.9</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_2</td>
<td>muls_tr(2)</td>
<td>Boes2021</td>
<td>25</td>
<td>0.683</td>
<td>76.0 (75.7 - 76.3)</td>
<td>0.652</td>
<td>76.6</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_3</td>
<td>muls_tr(3)</td>
<td>Boes2021</td>
<td>26</td>
<td>0.701</td>
<td>76.3 (76.0 - 76.6)</td>
<td>0.665</td>
<td>77.0</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_4</td>
<td>muls_tr(4)</td>
<td>Boes2021</td>
<td>24</td>
<td>0.681</td>
<td>76.0 (75.6 - 76.3)</td>
<td>0.682</td>
<td>77.1</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_1</td>
<td>AholabASC1</td>
<td>Diez2021</td>
<td>35</td>
<td>1.061</td>
<td>65.2 (64.8 - 65.5)</td>
<td>1.038</td>
<td>65.6</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_2</td>
<td>AhonoiseASC1</td>
<td>Diez2021</td>
<td>38</td>
<td>1.096</td>
<td>64.4 (64.1 - 64.8)</td>
<td>1.006</td>
<td>63.3</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>AhonoiseASC1</td>
<td>Diez2021</td>
<td>34</td>
<td>1.060</td>
<td>64.7 (64.4 - 65.1)</td>
<td>1.023</td>
<td>63.2</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_1</td>
<td>USTC_t1b_1</td>
<td>Wang2021</td>
<td>8</td>
<td>0.241</td>
<td>92.9 (92.7 - 93.1)</td>
<td>0.147</td>
<td>94.7</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_2</td>
<td>USTC_t1b_2</td>
<td>Wang2021</td>
<td>7</td>
<td>0.238</td>
<td>92.7 (92.5 - 92.9)</td>
<td>0.145</td>
<td>95.1</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_3</td>
<td>USTC_t1b_3</td>
<td>Wang2021</td>
<td>6</td>
<td>0.222</td>
<td>93.2 (93.0 - 93.4)</td>
<td>0.143</td>
<td>95.2</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>USTC_t1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>0.221</td>
<td>93.2 (93.0 - 93.4)</td>
<td>0.141</td>
<td>95.5</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_1</td>
<td>WS Small</td>
<td>Fedorishin2021</td>
<td>37</td>
<td>1.077</td>
<td>67.2 (66.8 - 67.5)</td>
<td>0.907</td>
<td>69.6</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>WS Fusion</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>1.028</td>
<td>68.7 (68.4 - 69.1)</td>
<td>0.990</td>
<td>68.5</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_1</td>
<td>HTCH_1</td>
<td>Hou2021</td>
<td>20</td>
<td>0.555</td>
<td>81.5 (81.2 - 81.8)</td>
<td>0.346</td>
<td>87.4</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_2</td>
<td>HTCH_2</td>
<td>Hou2021</td>
<td>29</td>
<td>0.771</td>
<td>81.8 (81.6 - 82.1)</td>
<td>0.351</td>
<td>89.5</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_3</td>
<td>HTCH_3</td>
<td>Hou2021</td>
<td>19</td>
<td>0.523</td>
<td>84.0 (83.7 - 84.3)</td>
<td>0.318</td>
<td>88.6</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>HTCH_4</td>
<td>Hou2021</td>
<td>16</td>
<td>0.416</td>
<td>85.6 (85.3 - 85.8)</td>
<td>0.328</td>
<td>91.7</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>AVSC_SE_CRNN</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>0.495</td>
<td>86.5 (86.3 - 86.8)</td>
<td>0.556</td>
<td>90.0</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_2</td>
<td>AVSC_SE_CRNN</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>22</td>
<td>0.640</td>
<td>83.2 (82.9 - 83.4)</td>
<td>0.616</td>
<td>87.0</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_3</td>
<td>AVSC_SE_CRNN</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>32</td>
<td>1.006</td>
<td>66.8 (66.5 - 67.1)</td>
<td>0.969</td>
<td>69.0</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_1</td>
<td>S01</td>
<td>Okazaki2021</td>
<td>12</td>
<td>0.312</td>
<td>91.6 (91.4 - 91.8)</td>
<td>0.260</td>
<td>92.5</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_2</td>
<td>S02</td>
<td>Okazaki2021</td>
<td>13</td>
<td>0.320</td>
<td>93.2 (93.0 - 93.3)</td>
<td>0.149</td>
<td>96.1</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_3</td>
<td>S03</td>
<td>Okazaki2021</td>
<td>11</td>
<td>0.303</td>
<td>93.5 (93.3 - 93.7)</td>
<td>0.238</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>S04</td>
<td>Okazaki2021</td>
<td>9</td>
<td>0.257</td>
<td>93.5 (93.3 - 93.7)</td>
<td>0.149</td>
<td>96.1</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_1</td>
<td>CRFDS</td>
<td>Peng2021</td>
<td>45</td>
<td>1.395</td>
<td>68.2 (67.9 - 68.5)</td>
<td>1.614</td>
<td>71.8</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>CRFDS</td>
<td>Peng2021</td>
<td>40</td>
<td>1.172</td>
<td>67.8 (67.5 - 68.1)</td>
<td>1.627</td>
<td>71.0</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_3</td>
<td>CRFDS</td>
<td>Peng2021</td>
<td>41</td>
<td>1.172</td>
<td>67.8 (67.5 - 68.1)</td>
<td>1.635</td>
<td>70.5</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_4</td>
<td>CRFDS</td>
<td>Peng2021</td>
<td>43</td>
<td>1.233</td>
<td>68.5 (68.1 - 68.8)</td>
<td>1.824</td>
<td>70.2</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_1</td>
<td>Pham_AIT</td>
<td>Pham2021</td>
<td>44</td>
<td>1.311</td>
<td>73.0 (72.7 - 73.3)</td>
<td></td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_2</td>
<td>Pham_AIT</td>
<td>Pham2021</td>
<td>21</td>
<td>0.589</td>
<td>88.3 (88.1 - 88.6)</td>
<td></td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham_AIT</td>
<td>Pham2021</td>
<td>17</td>
<td>0.434</td>
<td>88.4 (88.2 - 88.7)</td>
<td></td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_4</td>
<td>Pham_AIT</td>
<td>Pham2021</td>
<td>28</td>
<td>0.738</td>
<td>91.5 (91.3 - 91.7)</td>
<td></td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_1</td>
<td>WT</td>
<td>Triantafyllopoulos2021</td>
<td>39</td>
<td>1.157</td>
<td>58.4 (58.1 - 58.8)</td>
<td>1.153</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>MWT-FiLM</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>0.735</td>
<td>73.6 (73.3 - 73.9)</td>
<td>0.568</td>
<td>79.5</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_3</td>
<td>MWT-Bias</td>
<td>Triantafyllopoulos2021</td>
<td>30</td>
<td>0.785</td>
<td>73.7 (73.3 - 74.0)</td>
<td>0.704</td>
<td>76.2</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_4</td>
<td>MWT-Wave</td>
<td>Triantafyllopoulos2021</td>
<td>31</td>
<td>0.872</td>
<td>70.3 (70.0 - 70.7)</td>
<td>0.796</td>
<td>72.6</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang_BIT1</td>
<td>Wang2021a</td>
<td>36</td>
<td>1.061</td>
<td>74.1 (73.7 - 74.4)</td>
<td>1.072</td>
<td>91.5</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_2</td>
<td>Wang_BIT2</td>
<td>Liang2021</td>
<td>42</td>
<td>1.180</td>
<td>62.4 (62.0 - 62.7)</td>
<td>0.744</td>
<td>80.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td>Baseline</td>
<td></td>
<td></td>
<td>0.662</td>
<td>77.1 (76.8 - 77.5)</td>
<td>0.658</td>
<td>77.0</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_1</td>
<td>cnn14_cvt</td>
<td>Yang2021</td>
<td>15</td>
<td>0.332</td>
<td>90.8 (90.6 - 91.1)</td>
<td>0.261</td>
<td>92.6</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_2</td>
<td>trans_cvt</td>
<td>Yang2021</td>
<td>14</td>
<td>0.321</td>
<td>90.8 (90.6 - 91.0)</td>
<td>0.230</td>
<td>93.1</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>2trans_cnn</td>
<td>Yang2021</td>
<td>10</td>
<td>0.279</td>
<td>92.1 (91.9 - 92.3)</td>
<td>0.223</td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_1</td>
<td>ZhangIOA1</td>
<td>Wang2021b</td>
<td>3</td>
<td>0.201</td>
<td>93.5 (93.3 - 93.7)</td>
<td>0.146</td>
<td>95.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_2</td>
<td>ZhangIOA2</td>
<td>Wang2021b</td>
<td>4</td>
<td>0.205</td>
<td>93.6 (93.4 - 93.8)</td>
<td>0.153</td>
<td>95.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>ZhangIOA3</td>
<td>Wang2021b</td>
<td>1</td>
<td>0.195</td>
<td>93.8 (93.6 - 93.9)</td>
<td>0.145</td>
<td>95.1</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_4</td>
<td>ZhangIOA4</td>
<td>Wang2021b</td>
<td>2</td>
<td>0.199</td>
<td>93.9 (93.7 - 94.1)</td>
<td>0.156</td>
<td>95.0</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="logloss_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="3">Submission information</th>
<th class="sep-left-cell text-center" colspan="4">Evaluation dataset</th>
<th class="sep-left-cell text-center" colspan="2">Development dataset</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system rank
            </th>
<th class="text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_team" data-sortable="true" data-value-type="int">
                Team rank
            </th>
<th class="sep-left-cell text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small><small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss (Development dataset)" data-chartable="true" data-field="logloss_dev" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Development dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Development dataset)" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> (Development dataset)</small>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>muls_tr(1)</td>
<td>Boes2021</td>
<td>23</td>
<td>8</td>
<td>0.653</td>
<td>74.5 (74.2 - 74.8)</td>
<td>0.620</td>
<td>75.9</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>AhonoiseASC1</td>
<td>Diez2021</td>
<td>34</td>
<td>11</td>
<td>1.060</td>
<td>64.7 (64.4 - 65.1)</td>
<td>1.023</td>
<td>63.2</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>USTC_t1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>2</td>
<td>0.221</td>
<td>93.2 (93.0 - 93.4)</td>
<td>0.141</td>
<td>95.5</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>WS Fusion</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>10</td>
<td>1.028</td>
<td>68.7 (68.4 - 69.1)</td>
<td>0.990</td>
<td>68.5</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>HTCH_4</td>
<td>Hou2021</td>
<td>16</td>
<td>5</td>
<td>0.416</td>
<td>85.6 (85.3 - 85.8)</td>
<td>0.328</td>
<td>91.7</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>AVSC_SE_CRNN</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>7</td>
<td>0.495</td>
<td>86.5 (86.3 - 86.8)</td>
<td>0.556</td>
<td>90.0</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>S04</td>
<td>Okazaki2021</td>
<td>9</td>
<td>3</td>
<td>0.257</td>
<td>93.5 (93.3 - 93.7)</td>
<td>0.149</td>
<td>96.1</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>CRFDS</td>
<td>Peng2021</td>
<td>40</td>
<td>13</td>
<td>1.172</td>
<td>67.8 (67.5 - 68.1)</td>
<td>1.627</td>
<td>71.0</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham_AIT</td>
<td>Pham2021</td>
<td>17</td>
<td>6</td>
<td>0.434</td>
<td>88.4 (88.2 - 88.7)</td>
<td></td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>MWT-FiLM</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>9</td>
<td>0.735</td>
<td>73.6 (73.3 - 73.9)</td>
<td>0.568</td>
<td>79.5</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang_BIT1</td>
<td>Wang2021a</td>
<td>36</td>
<td>12</td>
<td>1.061</td>
<td>74.1 (73.7 - 74.4)</td>
<td>1.072</td>
<td>91.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td>Baseline</td>
<td></td>
<td></td>
<td></td>
<td>0.662</td>
<td>77.1 (76.8 - 77.5)</td>
<td>0.658</td>
<td>77.0</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>2trans_cnn</td>
<td>Yang2021</td>
<td>10</td>
<td>4</td>
<td>0.279</td>
<td>92.1 (91.9 - 92.3)</td>
<td>0.223</td>
<td>93.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>ZhangIOA3</td>
<td>Wang2021b</td>
<td>1</td>
<td>1</td>
<td>0.195</td>
<td>93.8 (93.6 - 93.9)</td>
<td>0.145</td>
<td>95.1</td>
</tr>
</tbody>
</table>
<h1 id="modality">Modality</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label 
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss <br/>(Eval)
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_modalities" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Used modalities
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_modality_combination" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Method to combine information from modalities
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>Boes2021</td>
<td>23</td>
<td>0.653</td>
<td>74.5</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_2</td>
<td>Boes2021</td>
<td>25</td>
<td>0.683</td>
<td>76.0</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_3</td>
<td>Boes2021</td>
<td>26</td>
<td>0.701</td>
<td>76.3</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_4</td>
<td>Boes2021</td>
<td>24</td>
<td>0.681</td>
<td>76.0</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_1</td>
<td>Diez2021</td>
<td>35</td>
<td>1.061</td>
<td>65.2</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_2</td>
<td>Diez2021</td>
<td>38</td>
<td>1.096</td>
<td>64.4</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>Diez2021</td>
<td>34</td>
<td>1.060</td>
<td>64.7</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_1</td>
<td>Wang2021</td>
<td>8</td>
<td>0.241</td>
<td>92.9</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_2</td>
<td>Wang2021</td>
<td>7</td>
<td>0.238</td>
<td>92.7</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_3</td>
<td>Wang2021</td>
<td>6</td>
<td>0.222</td>
<td>93.2</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>0.221</td>
<td>93.2</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_1</td>
<td>Fedorishin2021</td>
<td>37</td>
<td>1.077</td>
<td>67.2</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>1.028</td>
<td>68.7</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_1</td>
<td>Hou2021</td>
<td>20</td>
<td>0.555</td>
<td>81.5</td>
<td>audio + video</td>
<td>late fusion</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_2</td>
<td>Hou2021</td>
<td>29</td>
<td>0.771</td>
<td>81.8</td>
<td>audio + video</td>
<td>late fusion</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_3</td>
<td>Hou2021</td>
<td>19</td>
<td>0.523</td>
<td>84.0</td>
<td>audio + video</td>
<td>late fusion</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>Hou2021</td>
<td>16</td>
<td>0.416</td>
<td>85.6</td>
<td>audio + video</td>
<td>late fusion</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>0.495</td>
<td>86.5</td>
<td>audio + video</td>
<td>early fusion, late fusion</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_2</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>22</td>
<td>0.640</td>
<td>83.2</td>
<td>video</td>
<td>video only</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_3</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>32</td>
<td>1.006</td>
<td>66.8</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_1</td>
<td>Okazaki2021</td>
<td>12</td>
<td>0.312</td>
<td>91.6</td>
<td>video</td>
<td>video only</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_2</td>
<td>Okazaki2021</td>
<td>13</td>
<td>0.320</td>
<td>93.2</td>
<td>audio + video</td>
<td>audio-visual</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_3</td>
<td>Okazaki2021</td>
<td>11</td>
<td>0.303</td>
<td>93.5</td>
<td>audio + video</td>
<td>audio-visual</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>Okazaki2021</td>
<td>9</td>
<td>0.257</td>
<td>93.5</td>
<td>audio + video</td>
<td>audio-visual</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_1</td>
<td>Peng2021</td>
<td>45</td>
<td>1.395</td>
<td>68.2</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>Peng2021</td>
<td>40</td>
<td>1.172</td>
<td>67.8</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_3</td>
<td>Peng2021</td>
<td>41</td>
<td>1.172</td>
<td>67.8</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_4</td>
<td>Peng2021</td>
<td>43</td>
<td>1.233</td>
<td>68.5</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_1</td>
<td>Pham2021</td>
<td>44</td>
<td>1.311</td>
<td>73.0</td>
<td></td>
<td>PROD late fusion</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_2</td>
<td>Pham2021</td>
<td>21</td>
<td>0.589</td>
<td>88.3</td>
<td></td>
<td>PROD late fusion</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham2021</td>
<td>17</td>
<td>0.434</td>
<td>88.4</td>
<td></td>
<td>PROD late fusion</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_4</td>
<td>Pham2021</td>
<td>28</td>
<td>0.738</td>
<td>91.5</td>
<td></td>
<td>PROD late fusion</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_1</td>
<td>Triantafyllopoulos2021</td>
<td>39</td>
<td>1.157</td>
<td>58.4</td>
<td>audio</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>0.735</td>
<td>73.6</td>
<td>audio + video</td>
<td>FiLM conditioning</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_3</td>
<td>Triantafyllopoulos2021</td>
<td>30</td>
<td>0.785</td>
<td>73.7</td>
<td>audio + video</td>
<td>Bias conditioning</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_4</td>
<td>Triantafyllopoulos2021</td>
<td>31</td>
<td>0.872</td>
<td>70.3</td>
<td>audio + video</td>
<td>Wave conditioning</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang2021a</td>
<td>36</td>
<td>1.061</td>
<td>74.1</td>
<td>video</td>
<td>video only</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_2</td>
<td>Liang2021</td>
<td>42</td>
<td>1.180</td>
<td>62.4</td>
<td>audio</td>
<td>audio only</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td></td>
<td></td>
<td>0.662</td>
<td>77.1</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_1</td>
<td>Yang2021</td>
<td>15</td>
<td>0.332</td>
<td>90.8</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_2</td>
<td>Yang2021</td>
<td>14</td>
<td>0.321</td>
<td>90.8</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>Yang2021</td>
<td>10</td>
<td>0.279</td>
<td>92.1</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_1</td>
<td>Wang2021b</td>
<td>3</td>
<td>0.201</td>
<td>93.5</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_2</td>
<td>Wang2021b</td>
<td>4</td>
<td>0.205</td>
<td>93.6</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>Wang2021b</td>
<td>1</td>
<td>0.195</td>
<td>93.8</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_4</td>
<td>Wang2021b</td>
<td>2</td>
<td>0.199</td>
<td>93.9</td>
<td>audio + video</td>
<td>early fusion</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="class-wise-performance">Class-wise performance</h1>
<h2 id="log-loss">Log loss</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2021 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Kim_QTI_task1a_2" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Log loss",
        "fields": ["class_logloss_eval_airport", "class_logloss_eval_bus", "class_logloss_eval_metro", "class_logloss_eval_metro_station", "class_logloss_eval_park", "class_logloss_eval_public_square", "class_logloss_eval_shopping_mall", "class_logloss_eval_street_pedestrian", "class_logloss_eval_street_traffic", "class_logloss_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Log loss", "fields": ["class_logloss_eval_airport", "class_logloss_eval_metro_station", "class_logloss_eval_shopping_mall"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Log loss", "fields": ["class_logloss_eval_park", "class_logloss_eval_public_square", "class_logloss_eval_street_pedestrian", "class_logloss_eval_street_traffic"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Log loss", "fields": ["class_logloss_eval_bus","class_logloss_eval_metro","class_logloss_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="class_logloss_eval_airport" data-reversed="true" data-sortable="true" data-value-type="float3">
                Airport
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_bus" data-reversed="true" data-sortable="true" data-value-type="float3">
                Bus
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_metro" data-reversed="true" data-sortable="true" data-value-type="float3">
                Metro
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_metro_station" data-reversed="true" data-sortable="true" data-value-type="float3">
                Metro <br/>station
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_park" data-reversed="true" data-sortable="true" data-value-type="float3">
                Park
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_public_square" data-reversed="true" data-sortable="true" data-value-type="float3">
                Public <br/>square
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_shopping_mall" data-reversed="true" data-sortable="true" data-value-type="float3">
                Shopping <br/>mall
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_street_pedestrian" data-reversed="true" data-sortable="true" data-value-type="float3">
                Street <br/>pedestrian
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_street_traffic" data-reversed="true" data-sortable="true" data-value-type="float3">
                Street <br/>traffic
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_tram" data-reversed="true" data-sortable="true" data-value-type="float3">
                Tram
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>Boes2021</td>
<td>23</td>
<td>0.653</td>
<td>0.769</td>
<td>0.600</td>
<td>0.724</td>
<td>0.705</td>
<td>0.175</td>
<td>0.751</td>
<td>0.495</td>
<td>0.679</td>
<td>0.452</td>
<td>1.175</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_2</td>
<td>Boes2021</td>
<td>25</td>
<td>0.683</td>
<td>0.615</td>
<td>0.759</td>
<td>0.815</td>
<td>0.517</td>
<td>0.214</td>
<td>0.835</td>
<td>0.621</td>
<td>0.860</td>
<td>0.421</td>
<td>1.177</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_3</td>
<td>Boes2021</td>
<td>26</td>
<td>0.701</td>
<td>0.578</td>
<td>0.838</td>
<td>0.785</td>
<td>0.440</td>
<td>0.158</td>
<td>1.128</td>
<td>0.617</td>
<td>0.917</td>
<td>0.417</td>
<td>1.133</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_4</td>
<td>Boes2021</td>
<td>24</td>
<td>0.681</td>
<td>0.573</td>
<td>0.836</td>
<td>0.868</td>
<td>0.493</td>
<td>0.192</td>
<td>0.870</td>
<td>0.643</td>
<td>0.821</td>
<td>0.405</td>
<td>1.110</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_1</td>
<td>Diez2021</td>
<td>35</td>
<td>1.061</td>
<td>2.085</td>
<td>0.962</td>
<td>1.087</td>
<td>1.203</td>
<td>0.340</td>
<td>1.113</td>
<td>1.203</td>
<td>1.272</td>
<td>0.581</td>
<td>0.763</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_2</td>
<td>Diez2021</td>
<td>38</td>
<td>1.096</td>
<td>1.654</td>
<td>0.807</td>
<td>1.524</td>
<td>1.740</td>
<td>0.384</td>
<td>0.924</td>
<td>1.362</td>
<td>1.158</td>
<td>0.496</td>
<td>0.909</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>Diez2021</td>
<td>34</td>
<td>1.060</td>
<td>1.266</td>
<td>0.843</td>
<td>1.413</td>
<td>1.531</td>
<td>0.299</td>
<td>1.148</td>
<td>1.585</td>
<td>1.204</td>
<td>0.450</td>
<td>0.858</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_1</td>
<td>Wang2021</td>
<td>8</td>
<td>0.241</td>
<td>0.267</td>
<td>0.113</td>
<td>0.257</td>
<td>0.018</td>
<td>0.025</td>
<td>0.453</td>
<td>0.241</td>
<td>0.620</td>
<td>0.092</td>
<td>0.325</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_2</td>
<td>Wang2021</td>
<td>7</td>
<td>0.238</td>
<td>0.237</td>
<td>0.116</td>
<td>0.232</td>
<td>0.017</td>
<td>0.031</td>
<td>0.469</td>
<td>0.279</td>
<td>0.578</td>
<td>0.085</td>
<td>0.339</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_3</td>
<td>Wang2021</td>
<td>6</td>
<td>0.222</td>
<td>0.234</td>
<td>0.136</td>
<td>0.211</td>
<td>0.023</td>
<td>0.028</td>
<td>0.456</td>
<td>0.223</td>
<td>0.553</td>
<td>0.082</td>
<td>0.273</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>0.221</td>
<td>0.250</td>
<td>0.133</td>
<td>0.214</td>
<td>0.023</td>
<td>0.028</td>
<td>0.432</td>
<td>0.220</td>
<td>0.542</td>
<td>0.088</td>
<td>0.277</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_1</td>
<td>Fedorishin2021</td>
<td>37</td>
<td>1.077</td>
<td>2.391</td>
<td>0.552</td>
<td>1.287</td>
<td>1.139</td>
<td>0.241</td>
<td>1.272</td>
<td>0.786</td>
<td>1.522</td>
<td>0.526</td>
<td>1.052</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>1.028</td>
<td>1.671</td>
<td>0.615</td>
<td>1.195</td>
<td>1.118</td>
<td>0.200</td>
<td>1.745</td>
<td>1.008</td>
<td>1.291</td>
<td>0.524</td>
<td>0.910</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_1</td>
<td>Hou2021</td>
<td>20</td>
<td>0.555</td>
<td>0.703</td>
<td>0.581</td>
<td>0.373</td>
<td>0.211</td>
<td>0.021</td>
<td>1.113</td>
<td>0.610</td>
<td>0.680</td>
<td>0.271</td>
<td>0.985</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_2</td>
<td>Hou2021</td>
<td>29</td>
<td>0.771</td>
<td>0.917</td>
<td>0.902</td>
<td>0.477</td>
<td>0.231</td>
<td>0.006</td>
<td>1.810</td>
<td>0.704</td>
<td>1.458</td>
<td>0.339</td>
<td>0.863</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_3</td>
<td>Hou2021</td>
<td>19</td>
<td>0.523</td>
<td>0.571</td>
<td>0.459</td>
<td>0.300</td>
<td>0.231</td>
<td>0.054</td>
<td>0.931</td>
<td>0.671</td>
<td>0.730</td>
<td>0.367</td>
<td>0.911</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>Hou2021</td>
<td>16</td>
<td>0.416</td>
<td>0.524</td>
<td>0.424</td>
<td>0.257</td>
<td>0.161</td>
<td>0.019</td>
<td>0.846</td>
<td>0.474</td>
<td>0.641</td>
<td>0.219</td>
<td>0.592</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>0.495</td>
<td>0.771</td>
<td>0.475</td>
<td>0.628</td>
<td>0.223</td>
<td>0.102</td>
<td>0.664</td>
<td>0.371</td>
<td>0.714</td>
<td>0.191</td>
<td>0.810</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_2</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>22</td>
<td>0.640</td>
<td>0.809</td>
<td>0.424</td>
<td>0.646</td>
<td>0.128</td>
<td>0.087</td>
<td>0.831</td>
<td>0.526</td>
<td>0.960</td>
<td>0.305</td>
<td>1.677</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_3</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>32</td>
<td>1.006</td>
<td>1.777</td>
<td>0.605</td>
<td>0.979</td>
<td>1.177</td>
<td>0.345</td>
<td>1.396</td>
<td>1.093</td>
<td>1.039</td>
<td>0.656</td>
<td>0.990</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_1</td>
<td>Okazaki2021</td>
<td>12</td>
<td>0.312</td>
<td>0.227</td>
<td>0.314</td>
<td>0.308</td>
<td>0.144</td>
<td>0.107</td>
<td>0.477</td>
<td>0.180</td>
<td>0.532</td>
<td>0.182</td>
<td>0.645</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_2</td>
<td>Okazaki2021</td>
<td>13</td>
<td>0.320</td>
<td>0.284</td>
<td>0.161</td>
<td>0.307</td>
<td>0.102</td>
<td>0.043</td>
<td>0.657</td>
<td>0.228</td>
<td>0.880</td>
<td>0.124</td>
<td>0.411</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_3</td>
<td>Okazaki2021</td>
<td>11</td>
<td>0.303</td>
<td>0.279</td>
<td>0.282</td>
<td>0.304</td>
<td>0.172</td>
<td>0.105</td>
<td>0.506</td>
<td>0.213</td>
<td>0.544</td>
<td>0.171</td>
<td>0.457</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>Okazaki2021</td>
<td>9</td>
<td>0.257</td>
<td>0.257</td>
<td>0.181</td>
<td>0.222</td>
<td>0.078</td>
<td>0.040</td>
<td>0.472</td>
<td>0.165</td>
<td>0.662</td>
<td>0.106</td>
<td>0.387</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_1</td>
<td>Peng2021</td>
<td>45</td>
<td>1.395</td>
<td>2.363</td>
<td>0.934</td>
<td>1.157</td>
<td>1.568</td>
<td>0.194</td>
<td>3.260</td>
<td>1.239</td>
<td>1.561</td>
<td>0.585</td>
<td>1.090</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>Peng2021</td>
<td>40</td>
<td>1.172</td>
<td>1.414</td>
<td>0.722</td>
<td>0.907</td>
<td>1.140</td>
<td>0.198</td>
<td>2.190</td>
<td>1.515</td>
<td>1.662</td>
<td>0.620</td>
<td>1.353</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_3</td>
<td>Peng2021</td>
<td>41</td>
<td>1.172</td>
<td>1.414</td>
<td>0.722</td>
<td>0.907</td>
<td>1.140</td>
<td>0.198</td>
<td>2.190</td>
<td>1.515</td>
<td>1.662</td>
<td>0.620</td>
<td>1.353</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_4</td>
<td>Peng2021</td>
<td>43</td>
<td>1.233</td>
<td>2.224</td>
<td>0.668</td>
<td>1.004</td>
<td>1.340</td>
<td>0.278</td>
<td>2.258</td>
<td>1.269</td>
<td>1.803</td>
<td>0.517</td>
<td>0.966</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_1</td>
<td>Pham2021</td>
<td>44</td>
<td>1.311</td>
<td>1.588</td>
<td>0.667</td>
<td>1.989</td>
<td>1.202</td>
<td>0.321</td>
<td>2.089</td>
<td>1.738</td>
<td>1.346</td>
<td>1.094</td>
<td>1.074</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_2</td>
<td>Pham2021</td>
<td>21</td>
<td>0.589</td>
<td>0.444</td>
<td>0.472</td>
<td>0.538</td>
<td>0.044</td>
<td>0.080</td>
<td>1.265</td>
<td>0.286</td>
<td>0.980</td>
<td>0.412</td>
<td>1.367</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham2021</td>
<td>17</td>
<td>0.434</td>
<td>0.455</td>
<td>0.225</td>
<td>0.394</td>
<td>0.107</td>
<td>0.067</td>
<td>0.830</td>
<td>0.457</td>
<td>0.833</td>
<td>0.273</td>
<td>0.704</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_4</td>
<td>Pham2021</td>
<td>28</td>
<td>0.738</td>
<td>0.389</td>
<td>0.393</td>
<td>0.695</td>
<td>0.034</td>
<td>0.004</td>
<td>1.952</td>
<td>0.515</td>
<td>1.554</td>
<td>0.483</td>
<td>1.361</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_1</td>
<td>Triantafyllopoulos2021</td>
<td>39</td>
<td>1.157</td>
<td>1.442</td>
<td>0.678</td>
<td>1.336</td>
<td>1.959</td>
<td>0.283</td>
<td>1.492</td>
<td>0.677</td>
<td>1.430</td>
<td>0.757</td>
<td>1.520</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>0.735</td>
<td>1.038</td>
<td>0.702</td>
<td>0.931</td>
<td>0.306</td>
<td>0.075</td>
<td>0.978</td>
<td>0.730</td>
<td>1.400</td>
<td>0.401</td>
<td>0.792</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_3</td>
<td>Triantafyllopoulos2021</td>
<td>30</td>
<td>0.785</td>
<td>1.503</td>
<td>0.604</td>
<td>0.512</td>
<td>0.832</td>
<td>0.164</td>
<td>0.623</td>
<td>0.502</td>
<td>0.921</td>
<td>0.979</td>
<td>1.211</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_4</td>
<td>Triantafyllopoulos2021</td>
<td>31</td>
<td>0.872</td>
<td>1.556</td>
<td>0.815</td>
<td>0.956</td>
<td>1.130</td>
<td>0.427</td>
<td>1.009</td>
<td>0.232</td>
<td>1.572</td>
<td>0.521</td>
<td>0.503</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang2021a</td>
<td>36</td>
<td>1.061</td>
<td>2.558</td>
<td>0.529</td>
<td>2.405</td>
<td>1.250</td>
<td>0.007</td>
<td>1.965</td>
<td>0.047</td>
<td>0.699</td>
<td>0.591</td>
<td>0.562</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_2</td>
<td>Liang2021</td>
<td>42</td>
<td>1.180</td>
<td>1.114</td>
<td>1.121</td>
<td>1.223</td>
<td>1.516</td>
<td>0.722</td>
<td>1.516</td>
<td>1.035</td>
<td>1.516</td>
<td>0.887</td>
<td>1.151</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td></td>
<td></td>
<td>0.662</td>
<td>1.313</td>
<td>0.496</td>
<td>0.683</td>
<td>0.517</td>
<td>0.151</td>
<td>1.002</td>
<td>0.586</td>
<td>0.907</td>
<td>0.215</td>
<td>0.751</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_1</td>
<td>Yang2021</td>
<td>15</td>
<td>0.332</td>
<td>0.774</td>
<td>0.270</td>
<td>0.288</td>
<td>0.098</td>
<td>0.032</td>
<td>0.536</td>
<td>0.207</td>
<td>0.664</td>
<td>0.080</td>
<td>0.375</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_2</td>
<td>Yang2021</td>
<td>14</td>
<td>0.321</td>
<td>0.699</td>
<td>0.169</td>
<td>0.331</td>
<td>0.052</td>
<td>0.026</td>
<td>0.553</td>
<td>0.219</td>
<td>0.419</td>
<td>0.095</td>
<td>0.642</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>Yang2021</td>
<td>10</td>
<td>0.279</td>
<td>0.602</td>
<td>0.155</td>
<td>0.292</td>
<td>0.042</td>
<td>0.030</td>
<td>0.549</td>
<td>0.212</td>
<td>0.442</td>
<td>0.074</td>
<td>0.395</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_1</td>
<td>Wang2021b</td>
<td>3</td>
<td>0.201</td>
<td>0.463</td>
<td>0.099</td>
<td>0.113</td>
<td>0.043</td>
<td>0.009</td>
<td>0.417</td>
<td>0.053</td>
<td>0.373</td>
<td>0.066</td>
<td>0.369</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_2</td>
<td>Wang2021b</td>
<td>4</td>
<td>0.205</td>
<td>0.493</td>
<td>0.086</td>
<td>0.141</td>
<td>0.045</td>
<td>0.013</td>
<td>0.392</td>
<td>0.063</td>
<td>0.388</td>
<td>0.066</td>
<td>0.358</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>Wang2021b</td>
<td>1</td>
<td>0.195</td>
<td>0.454</td>
<td>0.092</td>
<td>0.113</td>
<td>0.041</td>
<td>0.009</td>
<td>0.402</td>
<td>0.047</td>
<td>0.379</td>
<td>0.064</td>
<td>0.352</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_4</td>
<td>Wang2021b</td>
<td>2</td>
<td>0.199</td>
<td>0.454</td>
<td>0.079</td>
<td>0.138</td>
<td>0.044</td>
<td>0.015</td>
<td>0.376</td>
<td>0.061</td>
<td>0.389</td>
<td>0.069</td>
<td>0.363</td>
</tr>
</tbody>
</table>
<h2 id="accuracy">Accuracy</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2021 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Kim_QTI_task1a_2" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_bus", "class_accuracy_eval_metro", "class_accuracy_eval_metro_station", "class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_shopping_mall", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic", "class_accuracy_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_metro_station", "class_accuracy_eval_shopping_mall"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_bus","class_accuracy_eval_metro","class_accuracy_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="class_accuracy_eval_airport" data-sortable="true" data-value-type="float1-percentage">
                Airport
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_bus" data-sortable="true" data-value-type="float1-percentage">
                Bus
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro" data-sortable="true" data-value-type="float1-percentage">
                Metro
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro_station" data-sortable="true" data-value-type="float1-percentage">
                Metro <br/>station
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_park" data-sortable="true" data-value-type="float1-percentage">
                Park
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_public_square" data-sortable="true" data-value-type="float1-percentage">
                Public <br/>square
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_shopping_mall" data-sortable="true" data-value-type="float1-percentage">
                Shopping <br/>mall
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_pedestrian" data-sortable="true" data-value-type="float1-percentage">
                Street <br/>pedestrian
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_traffic" data-sortable="true" data-value-type="float1-percentage">
                Street <br/>traffic
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_tram" data-sortable="true" data-value-type="float1-percentage">
                Tram
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>Boes2021</td>
<td>23</td>
<td>74.5</td>
<td>63.2</td>
<td>80.4</td>
<td>76.7</td>
<td>75.5</td>
<td>94.2</td>
<td>70.1</td>
<td>83.4</td>
<td>79.7</td>
<td>85.1</td>
<td>36.5</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_2</td>
<td>Boes2021</td>
<td>25</td>
<td>76.0</td>
<td>79.7</td>
<td>72.4</td>
<td>73.8</td>
<td>83.6</td>
<td>92.4</td>
<td>70.1</td>
<td>77.8</td>
<td>74.1</td>
<td>85.5</td>
<td>50.6</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_3</td>
<td>Boes2021</td>
<td>26</td>
<td>76.3</td>
<td>80.8</td>
<td>71.8</td>
<td>74.4</td>
<td>86.5</td>
<td>94.6</td>
<td>63.4</td>
<td>79.8</td>
<td>71.4</td>
<td>87.6</td>
<td>53.0</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_4</td>
<td>Boes2021</td>
<td>24</td>
<td>76.0</td>
<td>81.8</td>
<td>69.4</td>
<td>69.8</td>
<td>84.3</td>
<td>94.2</td>
<td>68.5</td>
<td>77.6</td>
<td>73.5</td>
<td>86.8</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_1</td>
<td>Diez2021</td>
<td>35</td>
<td>65.2</td>
<td>37.9</td>
<td>67.8</td>
<td>59.7</td>
<td>59.4</td>
<td>89.4</td>
<td>58.0</td>
<td>73.8</td>
<td>53.2</td>
<td>80.7</td>
<td>71.8</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_2</td>
<td>Diez2021</td>
<td>38</td>
<td>64.4</td>
<td>44.1</td>
<td>74.9</td>
<td>44.8</td>
<td>47.9</td>
<td>88.9</td>
<td>64.3</td>
<td>71.8</td>
<td>55.1</td>
<td>83.7</td>
<td>68.8</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>Diez2021</td>
<td>34</td>
<td>64.7</td>
<td>53.2</td>
<td>73.0</td>
<td>47.4</td>
<td>52.1</td>
<td>91.2</td>
<td>55.4</td>
<td>62.9</td>
<td>57.6</td>
<td>85.3</td>
<td>69.3</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_1</td>
<td>Wang2021</td>
<td>8</td>
<td>92.9</td>
<td>90.2</td>
<td>96.8</td>
<td>93.2</td>
<td>99.7</td>
<td>99.9</td>
<td>87.3</td>
<td>93.6</td>
<td>81.4</td>
<td>96.8</td>
<td>89.7</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_2</td>
<td>Wang2021</td>
<td>7</td>
<td>92.7</td>
<td>90.9</td>
<td>96.4</td>
<td>94.2</td>
<td>99.8</td>
<td>99.6</td>
<td>86.1</td>
<td>93.0</td>
<td>81.0</td>
<td>97.2</td>
<td>89.2</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_3</td>
<td>Wang2021</td>
<td>6</td>
<td>93.2</td>
<td>91.1</td>
<td>95.9</td>
<td>95.5</td>
<td>99.4</td>
<td>99.8</td>
<td>85.2</td>
<td>93.8</td>
<td>81.7</td>
<td>97.5</td>
<td>92.3</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>93.2</td>
<td>90.7</td>
<td>96.2</td>
<td>95.3</td>
<td>99.5</td>
<td>99.9</td>
<td>85.4</td>
<td>94.1</td>
<td>81.5</td>
<td>97.2</td>
<td>92.1</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_1</td>
<td>Fedorishin2021</td>
<td>37</td>
<td>67.2</td>
<td>42.3</td>
<td>81.4</td>
<td>58.3</td>
<td>63.4</td>
<td>92.9</td>
<td>55.0</td>
<td>77.2</td>
<td>51.2</td>
<td>83.7</td>
<td>66.1</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>68.7</td>
<td>54.2</td>
<td>80.9</td>
<td>63.7</td>
<td>65.2</td>
<td>94.6</td>
<td>45.9</td>
<td>68.6</td>
<td>57.8</td>
<td>84.6</td>
<td>71.7</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_1</td>
<td>Hou2021</td>
<td>20</td>
<td>81.5</td>
<td>79.5</td>
<td>75.9</td>
<td>85.5</td>
<td>91.9</td>
<td>99.1</td>
<td>63.4</td>
<td>80.6</td>
<td>80.1</td>
<td>90.7</td>
<td>68.5</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_2</td>
<td>Hou2021</td>
<td>29</td>
<td>81.8</td>
<td>70.4</td>
<td>75.7</td>
<td>86.2</td>
<td>93.1</td>
<td>99.8</td>
<td>65.1</td>
<td>83.7</td>
<td>73.9</td>
<td>93.2</td>
<td>77.3</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_3</td>
<td>Hou2021</td>
<td>19</td>
<td>84.0</td>
<td>82.1</td>
<td>88.1</td>
<td>88.1</td>
<td>92.5</td>
<td>99.3</td>
<td>74.0</td>
<td>78.4</td>
<td>81.6</td>
<td>88.8</td>
<td>67.0</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>Hou2021</td>
<td>16</td>
<td>85.6</td>
<td>82.8</td>
<td>83.5</td>
<td>88.1</td>
<td>95.1</td>
<td>100.0</td>
<td>70.4</td>
<td>86.7</td>
<td>81.8</td>
<td>94.1</td>
<td>73.3</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>86.5</td>
<td>78.6</td>
<td>86.9</td>
<td>82.6</td>
<td>97.7</td>
<td>99.4</td>
<td>78.2</td>
<td>91.8</td>
<td>80.2</td>
<td>95.7</td>
<td>74.3</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_2</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>22</td>
<td>83.2</td>
<td>75.9</td>
<td>87.5</td>
<td>84.4</td>
<td>95.1</td>
<td>95.5</td>
<td>82.5</td>
<td>86.4</td>
<td>77.0</td>
<td>92.6</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_3</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>32</td>
<td>66.8</td>
<td>45.8</td>
<td>79.4</td>
<td>67.0</td>
<td>63.4</td>
<td>89.3</td>
<td>51.1</td>
<td>63.2</td>
<td>66.0</td>
<td>78.8</td>
<td>64.0</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_1</td>
<td>Okazaki2021</td>
<td>12</td>
<td>91.6</td>
<td>98.1</td>
<td>89.9</td>
<td>91.2</td>
<td>97.8</td>
<td>100.0</td>
<td>85.9</td>
<td>97.0</td>
<td>85.6</td>
<td>96.6</td>
<td>73.5</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_2</td>
<td>Okazaki2021</td>
<td>13</td>
<td>93.2</td>
<td>96.1</td>
<td>96.6</td>
<td>92.8</td>
<td>98.7</td>
<td>99.9</td>
<td>85.6</td>
<td>95.4</td>
<td>81.6</td>
<td>97.9</td>
<td>87.0</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_3</td>
<td>Okazaki2021</td>
<td>11</td>
<td>93.5</td>
<td>96.0</td>
<td>95.1</td>
<td>96.0</td>
<td>99.0</td>
<td>100.0</td>
<td>85.2</td>
<td>96.8</td>
<td>85.0</td>
<td>97.9</td>
<td>84.3</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>Okazaki2021</td>
<td>9</td>
<td>93.5</td>
<td>96.0</td>
<td>95.1</td>
<td>96.0</td>
<td>99.0</td>
<td>100.0</td>
<td>85.2</td>
<td>96.8</td>
<td>85.0</td>
<td>97.9</td>
<td>84.3</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_1</td>
<td>Peng2021</td>
<td>45</td>
<td>68.2</td>
<td>49.5</td>
<td>78.6</td>
<td>69.5</td>
<td>61.2</td>
<td>94.8</td>
<td>46.8</td>
<td>66.2</td>
<td>57.9</td>
<td>84.4</td>
<td>73.1</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>Peng2021</td>
<td>40</td>
<td>67.8</td>
<td>53.8</td>
<td>79.5</td>
<td>72.8</td>
<td>65.9</td>
<td>94.4</td>
<td>49.3</td>
<td>58.6</td>
<td>54.3</td>
<td>82.6</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_3</td>
<td>Peng2021</td>
<td>41</td>
<td>67.8</td>
<td>53.8</td>
<td>79.5</td>
<td>72.8</td>
<td>65.9</td>
<td>94.4</td>
<td>49.3</td>
<td>58.6</td>
<td>54.3</td>
<td>82.6</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_4</td>
<td>Peng2021</td>
<td>43</td>
<td>68.5</td>
<td>45.0</td>
<td>81.0</td>
<td>70.5</td>
<td>66.3</td>
<td>92.0</td>
<td>47.8</td>
<td>67.8</td>
<td>58.6</td>
<td>84.8</td>
<td>70.9</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_1</td>
<td>Pham2021</td>
<td>44</td>
<td>73.0</td>
<td>64.5</td>
<td>87.9</td>
<td>63.2</td>
<td>74.7</td>
<td>93.2</td>
<td>54.4</td>
<td>67.3</td>
<td>70.2</td>
<td>78.9</td>
<td>75.7</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_2</td>
<td>Pham2021</td>
<td>21</td>
<td>88.3</td>
<td>88.0</td>
<td>88.1</td>
<td>87.6</td>
<td>98.8</td>
<td>96.0</td>
<td>79.0</td>
<td>93.1</td>
<td>84.4</td>
<td>93.5</td>
<td>75.0</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham2021</td>
<td>17</td>
<td>88.4</td>
<td>84.8</td>
<td>92.3</td>
<td>88.7</td>
<td>96.7</td>
<td>97.3</td>
<td>77.4</td>
<td>89.6</td>
<td>82.9</td>
<td>93.7</td>
<td>81.0</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_4</td>
<td>Pham2021</td>
<td>28</td>
<td>91.5</td>
<td>94.2</td>
<td>94.1</td>
<td>89.6</td>
<td>99.2</td>
<td>99.9</td>
<td>78.1</td>
<td>95.0</td>
<td>85.1</td>
<td>95.4</td>
<td>84.1</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_1</td>
<td>Triantafyllopoulos2021</td>
<td>39</td>
<td>58.4</td>
<td>37.2</td>
<td>77.4</td>
<td>52.1</td>
<td>36.1</td>
<td>93.5</td>
<td>41.9</td>
<td>76.8</td>
<td>48.8</td>
<td>78.8</td>
<td>41.9</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>73.6</td>
<td>59.9</td>
<td>75.2</td>
<td>64.7</td>
<td>90.9</td>
<td>99.0</td>
<td>65.6</td>
<td>72.8</td>
<td>55.0</td>
<td>87.3</td>
<td>65.3</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_3</td>
<td>Triantafyllopoulos2021</td>
<td>30</td>
<td>73.7</td>
<td>51.7</td>
<td>79.4</td>
<td>82.0</td>
<td>78.3</td>
<td>95.1</td>
<td>76.6</td>
<td>82.1</td>
<td>66.1</td>
<td>74.4</td>
<td>51.1</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_4</td>
<td>Triantafyllopoulos2021</td>
<td>31</td>
<td>70.3</td>
<td>45.8</td>
<td>71.6</td>
<td>60.9</td>
<td>69.5</td>
<td>86.9</td>
<td>71.3</td>
<td>92.0</td>
<td>41.4</td>
<td>84.7</td>
<td>79.2</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang2021a</td>
<td>36</td>
<td>74.1</td>
<td>45.6</td>
<td>80.2</td>
<td>39.9</td>
<td>64.5</td>
<td>99.9</td>
<td>62.8</td>
<td>98.2</td>
<td>81.7</td>
<td>84.9</td>
<td>82.9</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_2</td>
<td>Liang2021</td>
<td>42</td>
<td>62.4</td>
<td>63.3</td>
<td>62.6</td>
<td>66.2</td>
<td>49.9</td>
<td>90.9</td>
<td>35.3</td>
<td>65.8</td>
<td>51.6</td>
<td>79.0</td>
<td>59.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td></td>
<td></td>
<td>77.1</td>
<td>56.6</td>
<td>83.4</td>
<td>75.8</td>
<td>83.9</td>
<td>95.8</td>
<td>63.6</td>
<td>78.2</td>
<td>70.6</td>
<td>93.3</td>
<td>70.4</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_1</td>
<td>Yang2021</td>
<td>15</td>
<td>90.8</td>
<td>83.5</td>
<td>90.6</td>
<td>90.6</td>
<td>97.0</td>
<td>99.3</td>
<td>85.1</td>
<td>93.4</td>
<td>82.0</td>
<td>97.8</td>
<td>89.0</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_2</td>
<td>Yang2021</td>
<td>14</td>
<td>90.8</td>
<td>81.5</td>
<td>94.8</td>
<td>89.4</td>
<td>98.4</td>
<td>98.7</td>
<td>85.0</td>
<td>92.7</td>
<td>89.8</td>
<td>97.4</td>
<td>80.6</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>Yang2021</td>
<td>10</td>
<td>92.1</td>
<td>84.8</td>
<td>94.8</td>
<td>90.5</td>
<td>98.6</td>
<td>98.8</td>
<td>84.6</td>
<td>93.2</td>
<td>89.0</td>
<td>97.7</td>
<td>88.5</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_1</td>
<td>Wang2021b</td>
<td>3</td>
<td>93.5</td>
<td>88.0</td>
<td>96.2</td>
<td>97.7</td>
<td>99.3</td>
<td>100.0</td>
<td>83.1</td>
<td>98.0</td>
<td>88.3</td>
<td>97.7</td>
<td>86.7</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_2</td>
<td>Wang2021b</td>
<td>4</td>
<td>93.6</td>
<td>87.0</td>
<td>96.9</td>
<td>97.8</td>
<td>99.3</td>
<td>100.0</td>
<td>84.9</td>
<td>97.6</td>
<td>88.0</td>
<td>97.9</td>
<td>86.4</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>Wang2021b</td>
<td>1</td>
<td>93.8</td>
<td>88.1</td>
<td>96.6</td>
<td>97.9</td>
<td>99.3</td>
<td>100.0</td>
<td>83.7</td>
<td>98.5</td>
<td>88.2</td>
<td>97.8</td>
<td>87.6</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_4</td>
<td>Wang2021b</td>
<td>2</td>
<td>93.9</td>
<td>87.4</td>
<td>97.4</td>
<td>98.2</td>
<td>99.4</td>
<td>100.0</td>
<td>85.3</td>
<td>98.0</td>
<td>88.0</td>
<td>98.0</td>
<td>87.2</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label 
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss <br/>(Eval)
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
<th class="text-center narrow-col" data-field="system_audio_embeddings" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Embeddings / audio
            </th>
<th class="text-center narrow-col" data-field="system_visual_embeddings" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Embeddings / visual
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>Boes2021</td>
<td>23</td>
<td>0.653</td>
<td>74.5</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
<td>VGGish</td>
<td>VGG16</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_2</td>
<td>Boes2021</td>
<td>25</td>
<td>0.683</td>
<td>76.0</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
<td>VGGish</td>
<td>VGG16</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_3</td>
<td>Boes2021</td>
<td>26</td>
<td>0.701</td>
<td>76.3</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
<td>VGGish</td>
<td>VGG16</td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_4</td>
<td>Boes2021</td>
<td>24</td>
<td>0.681</td>
<td>76.0</td>
<td>22.05kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
<td>VGGish</td>
<td>VGG16</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_1</td>
<td>Diez2021</td>
<td>35</td>
<td>1.061</td>
<td>65.2</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel spectrogram</td>
<td>OpenL3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_2</td>
<td>Diez2021</td>
<td>38</td>
<td>1.096</td>
<td>64.4</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel spectrogram</td>
<td>OpenL3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>Diez2021</td>
<td>34</td>
<td>1.060</td>
<td>64.7</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel spectrogram</td>
<td>OpenL3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_1</td>
<td>Wang2021</td>
<td>8</td>
<td>0.241</td>
<td>92.9</td>
<td>48.0kHz</td>
<td>mixup, channel confusion, SpecAugment, pitch shifting, speed change, random noise, mix audios, contrast, sharpness</td>
<td>log-mel energies</td>
<td>FCNN, ResNet17, HMM</td>
<td>DenseNet161, ResNet50, ResNeSt50, HMM, VGG19</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_2</td>
<td>Wang2021</td>
<td>7</td>
<td>0.238</td>
<td>92.7</td>
<td>48.0kHz</td>
<td>mixup, channel confusion, SpecAugment, pitch shifting, speed change, random noise, mix audios, contrast, sharpness</td>
<td>log-mel energies</td>
<td>FCNN, ResNet17, HMM</td>
<td>DenseNet161, ResNet50, ResNeSt50, HMM, VGG19</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_3</td>
<td>Wang2021</td>
<td>6</td>
<td>0.222</td>
<td>93.2</td>
<td>48.0kHz</td>
<td>mixup, channel confusion, SpecAugment, pitch shifting, speed change, random noise, mix audios, contrast, sharpness</td>
<td>log-mel energies</td>
<td>FCNN, ResNet17</td>
<td>DenseNet161, ResNet50, ResNeSt50</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>0.221</td>
<td>93.2</td>
<td>48.0kHz</td>
<td>mixup, channel confusion, SpecAugment, pitch shifting, speed change, random noise, mix audios, contrast, sharpness</td>
<td>log-mel energies</td>
<td>FCNN, ResNet17</td>
<td>DenseNet161, ResNet50, ResNeSt50</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_1</td>
<td>Fedorishin2021</td>
<td>37</td>
<td>1.077</td>
<td>67.2</td>
<td>48.0kHz</td>
<td>mixup, time-shifting</td>
<td>log-mel energies, raw waveform</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>1.028</td>
<td>68.7</td>
<td>48.0kHz</td>
<td>mixup, time-shifting</td>
<td>log-mel energies, raw waveform</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_1</td>
<td>Hou2021</td>
<td>20</td>
<td>0.555</td>
<td>81.5</td>
<td>48.0kHz</td>
<td></td>
<td>OpenL3</td>
<td>OpenL3</td>
<td>ResNet50</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_2</td>
<td>Hou2021</td>
<td>29</td>
<td>0.771</td>
<td>81.8</td>
<td>48.0kHz</td>
<td></td>
<td>OpenL3</td>
<td>OpenL3</td>
<td>ResNet50</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_3</td>
<td>Hou2021</td>
<td>19</td>
<td>0.523</td>
<td>84.0</td>
<td>48.0kHz</td>
<td></td>
<td>OpenL3</td>
<td>OpenL3</td>
<td>ResNet50</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>Hou2021</td>
<td>16</td>
<td>0.416</td>
<td>85.6</td>
<td>48.0kHz</td>
<td></td>
<td>OpenL3</td>
<td>OpenL3</td>
<td>ResNet50</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>0.495</td>
<td>86.5</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>gammatone spectrogram</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_2</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>22</td>
<td>0.640</td>
<td>83.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_3</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>32</td>
<td>1.006</td>
<td>66.8</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>gammatone spectrogram</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_1</td>
<td>Okazaki2021</td>
<td>12</td>
<td>0.312</td>
<td>91.6</td>
<td></td>
<td>(Video) random affine, color jitter, gaussian blur, random erasing</td>
<td></td>
<td></td>
<td>CNN, CLIP CNN/ViT</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_2</td>
<td>Okazaki2021</td>
<td>13</td>
<td>0.320</td>
<td>93.2</td>
<td>48.0kHz</td>
<td>(Audio) frequency masking, random gain (Video) random affine, color jitter, gaussian blur, random erasing</td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>CNN, CLIP CNN/ViT</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_3</td>
<td>Okazaki2021</td>
<td>11</td>
<td>0.303</td>
<td>93.5</td>
<td>48.0kHz</td>
<td>(Audio) frequency masking, random gain (Video) random affine, color jitter, gaussian blur, random erasing</td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>CNN, CLIP CNN/ViT</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>Okazaki2021</td>
<td>9</td>
<td>0.257</td>
<td>93.5</td>
<td>48.0kHz</td>
<td>(Audio) frequency masking, random gain (Video) random affine, color jitter, gaussian blur, random erasing</td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>CNN, CLIP CNN/ViT</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_1</td>
<td>Peng2021</td>
<td>45</td>
<td>1.395</td>
<td>68.2</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRFDS</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>Peng2021</td>
<td>40</td>
<td>1.172</td>
<td>67.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRFDS</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_3</td>
<td>Peng2021</td>
<td>41</td>
<td>1.172</td>
<td>67.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRFDS</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_4</td>
<td>Peng2021</td>
<td>43</td>
<td>1.233</td>
<td>68.5</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRFDS</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_1</td>
<td>Pham2021</td>
<td>44</td>
<td>1.311</td>
<td>73.0</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>CQT, Gammatonegram, log-mel</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_2</td>
<td>Pham2021</td>
<td>21</td>
<td>0.589</td>
<td>88.3</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>CQT, Gammatonegram, log-mel</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham2021</td>
<td>17</td>
<td>0.434</td>
<td>88.4</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>CQT, Gammatonegram, log-mel</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_4</td>
<td>Pham2021</td>
<td>28</td>
<td>0.738</td>
<td>91.5</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>CQT, Gammatonegram, log-mel</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_1</td>
<td>Triantafyllopoulos2021</td>
<td>39</td>
<td>1.157</td>
<td>58.4</td>
<td>48.0kHz</td>
<td>frequency masking, time masking, random cropping</td>
<td>log-mel spectrogram</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>0.735</td>
<td>73.6</td>
<td>48.0kHz</td>
<td>frequency masking, time masking, random cropping</td>
<td>log-mel spectrogram</td>
<td>OpenL3</td>
<td>OpenL3</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_3</td>
<td>Triantafyllopoulos2021</td>
<td>30</td>
<td>0.785</td>
<td>73.7</td>
<td>48.0kHz</td>
<td>frequency masking, time masking, random cropping</td>
<td>log-mel spectrogram</td>
<td>OpenL3</td>
<td>OpenL3</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_4</td>
<td>Triantafyllopoulos2021</td>
<td>31</td>
<td>0.872</td>
<td>70.3</td>
<td>48.0kHz</td>
<td>frequency masking, time masking, random cropping</td>
<td>log-mel spectrogram</td>
<td>OpenL3</td>
<td>OpenL3</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang2021a</td>
<td>36</td>
<td>1.061</td>
<td>74.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_2</td>
<td>Liang2021</td>
<td>42</td>
<td>1.180</td>
<td>62.4</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td></td>
<td></td>
<td>0.662</td>
<td>77.1</td>
<td>48.0kHz</td>
<td></td>
<td>log-mel energies</td>
<td>OpenL3</td>
<td>OpenL3</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_1</td>
<td>Yang2021</td>
<td>15</td>
<td>0.332</td>
<td>90.8</td>
<td>44.1kHz</td>
<td>mixup, specaugment</td>
<td>log-mel energies</td>
<td>VGGish</td>
<td>Transformer</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_2</td>
<td>Yang2021</td>
<td>14</td>
<td>0.321</td>
<td>90.8</td>
<td>44.1kHz</td>
<td>mixup, specaugment</td>
<td>log-mel energies</td>
<td>Transformer</td>
<td>Transformer</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>Yang2021</td>
<td>10</td>
<td>0.279</td>
<td>92.1</td>
<td>44.1kHz</td>
<td>mixup, specaugment</td>
<td>log-mel energies</td>
<td>Transformer, VGGish</td>
<td>Transformer</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_1</td>
<td>Wang2021b</td>
<td>3</td>
<td>0.201</td>
<td>93.5</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel energies, CQT, bark</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_2</td>
<td>Wang2021b</td>
<td>4</td>
<td>0.205</td>
<td>93.6</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel energies, CQT, bark</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>Wang2021b</td>
<td>1</td>
<td>0.195</td>
<td>93.8</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel energies, CQT, bark</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_4</td>
<td>Wang2021b</td>
<td>2</td>
<td>0.199</td>
<td>93.9</td>
<td>48.0kHz</td>
<td>mixup</td>
<td>log-mel energies, CQT, bark</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="system_complexity_total" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss <br/>(Eval)
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_external_data_usage" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                External <br/>data usage
            </th>
<th class="text-center narrow-col" data-field="external_data_sources" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                External <br/>data sources
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total_audio" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity / Audio
            </th>
<th class="text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total_visual" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity / Visual
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Boes_KUL_task1b_1</td>
<td>Boes2021</td>
<td>23</td>
<td>0.653</td>
<td>74.5</td>
<td>embeddings</td>
<td></td>
<td>2302441</td>
<td></td>
<td></td>
<td>CNN, transformer</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_2</td>
<td>Boes2021</td>
<td>25</td>
<td>0.683</td>
<td>76.0</td>
<td>embeddings</td>
<td></td>
<td>2302441</td>
<td></td>
<td></td>
<td>CNN, transformer</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_3</td>
<td>Boes2021</td>
<td>26</td>
<td>0.701</td>
<td>76.3</td>
<td>embeddings</td>
<td></td>
<td>2302441</td>
<td></td>
<td></td>
<td>CNN, transformer</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Boes_KUL_task1b_4</td>
<td>Boes2021</td>
<td>24</td>
<td>0.681</td>
<td>76.0</td>
<td>embeddings</td>
<td></td>
<td>2302441</td>
<td></td>
<td></td>
<td>CNN, transformer</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_1</td>
<td>Diez2021</td>
<td>35</td>
<td>1.061</td>
<td>65.2</td>
<td>embeddings</td>
<td></td>
<td>970294</td>
<td>970294</td>
<td></td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_2</td>
<td>Diez2021</td>
<td>38</td>
<td>1.096</td>
<td>64.4</td>
<td>embeddings</td>
<td></td>
<td>751690</td>
<td>751690</td>
<td></td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Diez_Noismart_task1b_3</td>
<td>Diez2021</td>
<td>34</td>
<td>1.060</td>
<td>64.7</td>
<td>embeddings</td>
<td></td>
<td>972598</td>
<td>972598</td>
<td></td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_1</td>
<td>Wang2021</td>
<td>8</td>
<td>0.241</td>
<td>92.9</td>
<td>pre-trained model</td>
<td>Places365</td>
<td>933932599</td>
<td>250090842</td>
<td>678276533</td>
<td>GMM, HMM, VGG, CNN, ResNet, DNN, ResNeSt, DenseNet, ensemble</td>
<td>4</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_2</td>
<td>Wang2021</td>
<td>7</td>
<td>0.238</td>
<td>92.7</td>
<td>pre-trained model</td>
<td>Places365</td>
<td>1185866410</td>
<td>404955720</td>
<td>780910690</td>
<td>GMM, HMM, VGG, CNN, ResNet, DNN, ResNeSt, DenseNet, ensemble</td>
<td>8</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_3</td>
<td>Wang2021</td>
<td>6</td>
<td>0.222</td>
<td>93.2</td>
<td>pre-trained model</td>
<td>Places365</td>
<td>263064259</td>
<td>154864878</td>
<td>102634157</td>
<td>CNN, ResNet, DNN, ResNeSt, DenseNet, ensemble</td>
<td>4</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Du_USTC_task1b_4</td>
<td>Wang2021</td>
<td>5</td>
<td>0.221</td>
<td>93.2</td>
<td>pre-trained model</td>
<td>Places365, ImageNet</td>
<td>373738849</td>
<td>215535992</td>
<td>149650221</td>
<td>CNN, ResNet, DNN, DenseNet, ResNeSt, ensemble</td>
<td>6</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_1</td>
<td>Fedorishin2021</td>
<td>37</td>
<td>1.077</td>
<td>67.2</td>
<td></td>
<td></td>
<td>1351562</td>
<td>1351562</td>
<td>0</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Fedorishin_UB_task1b_2</td>
<td>Fedorishin2021</td>
<td>33</td>
<td>1.028</td>
<td>68.7</td>
<td></td>
<td></td>
<td>5422730</td>
<td>5422730</td>
<td>0</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_1</td>
<td>Hou2021</td>
<td>20</td>
<td>0.555</td>
<td>81.5</td>
<td>embeddings, pre-trained model</td>
<td></td>
<td>28329010</td>
<td>2113024</td>
<td>26082088</td>
<td>CNN, ResNet</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_2</td>
<td>Hou2021</td>
<td>29</td>
<td>0.771</td>
<td>81.8</td>
<td>embeddings, pre-trained model</td>
<td></td>
<td>28329010</td>
<td>2113024</td>
<td>26082088</td>
<td>CNN, ResNet</td>
<td>3</td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_3</td>
<td>Hou2021</td>
<td>19</td>
<td>0.523</td>
<td>84.0</td>
<td>embeddings, pre-trained model</td>
<td></td>
<td>28329010</td>
<td>2113024</td>
<td>26082088</td>
<td>CNN, ResNet</td>
<td>2</td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Hou_UGent_task1b_4</td>
<td>Hou2021</td>
<td>16</td>
<td>0.416</td>
<td>85.6</td>
<td>embeddings, pre-trained model</td>
<td></td>
<td>28329010</td>
<td>2113024</td>
<td>26082088</td>
<td>CNN, ResNet</td>
<td>3</td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_1</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>18</td>
<td>0.495</td>
<td>86.5</td>
<td>pre-trained model Places365</td>
<td>Places365</td>
<td>15416148</td>
<td>323274</td>
<td>14820170</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_2</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>22</td>
<td>0.640</td>
<td>83.2</td>
<td>pre-trained model Places365</td>
<td>Places365</td>
<td>14820170</td>
<td></td>
<td>14820170</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task1b_3</td>
<td>Naranjo-Alcazar2021_t1b</td>
<td>32</td>
<td>1.006</td>
<td>66.8</td>
<td></td>
<td></td>
<td>323274</td>
<td>323274</td>
<td></td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_1</td>
<td>Okazaki2021</td>
<td>12</td>
<td>0.312</td>
<td>91.6</td>
<td>pre-trained model</td>
<td>ImageNet pre-trained models, CLIP models</td>
<td>70676012</td>
<td>0</td>
<td>70676012</td>
<td>CNN, ViT, ensemble</td>
<td>4</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_2</td>
<td>Okazaki2021</td>
<td>13</td>
<td>0.320</td>
<td>93.2</td>
<td>pre-trained model</td>
<td>ImageNet pre-trained models, CLIP models</td>
<td>127257242</td>
<td>56581230</td>
<td>70676012</td>
<td>CNN, ViT, ensemble</td>
<td>7</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_3</td>
<td>Okazaki2021</td>
<td>11</td>
<td>0.303</td>
<td>93.5</td>
<td>pre-trained model</td>
<td>ImageNet pre-trained models, CLIP models</td>
<td>636286210</td>
<td>282906150</td>
<td>353380060</td>
<td>CNN, ViT, ensemble</td>
<td>35</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Okazaki_LDSLVision_task1b_4</td>
<td>Okazaki2021</td>
<td>9</td>
<td>0.257</td>
<td>93.5</td>
<td>pre-trained model</td>
<td>ImageNet pre-trained models, CLIP models</td>
<td>636286210</td>
<td>282906150</td>
<td>353380060</td>
<td>CNN, ViT, ensemble</td>
<td>35</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_1</td>
<td>Peng2021</td>
<td>45</td>
<td>1.395</td>
<td>68.2</td>
<td>directly</td>
<td></td>
<td>1817152</td>
<td>1817152</td>
<td>0</td>
<td>CNN</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_2</td>
<td>Peng2021</td>
<td>40</td>
<td>1.172</td>
<td>67.8</td>
<td>directly</td>
<td></td>
<td>1817152</td>
<td>1817152</td>
<td>0</td>
<td>CNN</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_3</td>
<td>Peng2021</td>
<td>41</td>
<td>1.172</td>
<td>67.8</td>
<td>directly</td>
<td></td>
<td>1817152</td>
<td>1817152</td>
<td>0</td>
<td>CNN</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Peng_CQU_task1b_4</td>
<td>Peng2021</td>
<td>43</td>
<td>1.233</td>
<td>68.5</td>
<td>directly</td>
<td></td>
<td>1817152</td>
<td>1817152</td>
<td>0</td>
<td>CNN</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_1</td>
<td>Pham2021</td>
<td>44</td>
<td>1.311</td>
<td>73.0</td>
<td></td>
<td></td>
<td>180626842</td>
<td>47582256</td>
<td>133044586</td>
<td>CNN</td>
<td>6</td>
<td>Average</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_2</td>
<td>Pham2021</td>
<td>21</td>
<td>0.589</td>
<td>88.3</td>
<td></td>
<td></td>
<td>180626842</td>
<td>47582256</td>
<td>133044586</td>
<td>CNN</td>
<td>6</td>
<td>Average</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_3</td>
<td>Pham2021</td>
<td>17</td>
<td>0.434</td>
<td>88.4</td>
<td></td>
<td></td>
<td>180626842</td>
<td>47582256</td>
<td>133044586</td>
<td>CNN</td>
<td>6</td>
<td>Average</td>
</tr>
<tr>
<td></td>
<td>Pham_AIT_task1b_4</td>
<td>Pham2021</td>
<td>28</td>
<td>0.738</td>
<td>91.5</td>
<td></td>
<td></td>
<td>180626842</td>
<td>47582256</td>
<td>133044586</td>
<td>CNN</td>
<td>6</td>
<td>Average</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_1</td>
<td>Triantafyllopoulos2021</td>
<td>39</td>
<td>1.157</td>
<td>58.4</td>
<td></td>
<td></td>
<td>1468139</td>
<td>1468139</td>
<td></td>
<td>WaveTransformer</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_2</td>
<td>Triantafyllopoulos2021</td>
<td>27</td>
<td>0.735</td>
<td>73.6</td>
<td></td>
<td></td>
<td>2747339</td>
<td>11897999</td>
<td>4691020</td>
<td>MultimodalWaveTransformer</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_3</td>
<td>Triantafyllopoulos2021</td>
<td>30</td>
<td>0.785</td>
<td>73.7</td>
<td></td>
<td></td>
<td>2107739</td>
<td>11258399</td>
<td>4691020</td>
<td>MultimodalWaveTransformer</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Triantafyllopoulos_AUD_task1b_4</td>
<td>Triantafyllopoulos2021</td>
<td>31</td>
<td>0.872</td>
<td>70.3</td>
<td></td>
<td></td>
<td>2107739</td>
<td>11258399</td>
<td>4691020</td>
<td>MultimodalWaveTransformer</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_1</td>
<td>Wang2021a</td>
<td>36</td>
<td>1.061</td>
<td>74.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Transformer</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Wang_BIT_task1b_2</td>
<td>Liang2021</td>
<td>42</td>
<td>1.180</td>
<td>62.4</td>
<td>embeddings</td>
<td></td>
<td>14553134</td>
<td>9489294</td>
<td>5029654</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2021 baseline</td>
<td></td>
<td></td>
<td>0.662</td>
<td>77.1</td>
<td>embeddings</td>
<td></td>
<td>711454</td>
<td>338634</td>
<td>338634</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_1</td>
<td>Yang2021</td>
<td>15</td>
<td>0.332</td>
<td>90.8</td>
<td>directly</td>
<td>ImageNet</td>
<td>102000000</td>
<td>81000000</td>
<td>19800000</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_2</td>
<td>Yang2021</td>
<td>14</td>
<td>0.321</td>
<td>90.8</td>
<td>directly</td>
<td>ImageNet</td>
<td>40000000</td>
<td>18974474</td>
<td>19800000</td>
<td>Multi-head Attention, MLP</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Yang_THU_task1b_3</td>
<td>Yang2021</td>
<td>10</td>
<td>0.279</td>
<td>92.1</td>
<td>directly</td>
<td>ImageNet</td>
<td>121000000</td>
<td>99974474</td>
<td>19800000</td>
<td>Multi-head Attention, MLP, CNN</td>
<td></td>
<td>maximum likelihood</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_1</td>
<td>Wang2021b</td>
<td>3</td>
<td>0.201</td>
<td>93.5</td>
<td>directly, pre-trained model</td>
<td>ImageNet, Places365, EfficientNet, PyTorch Image Models</td>
<td>100865990</td>
<td>17479760</td>
<td>77985102</td>
<td>CNN, EfficientNet, Swin Transformer, ensemble</td>
<td>4</td>
<td>weighted vote</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_2</td>
<td>Wang2021b</td>
<td>4</td>
<td>0.205</td>
<td>93.6</td>
<td>directly, pre-trained model</td>
<td>ImageNet, Places365, EfficientNet, PyTorch Image Models</td>
<td>103370202</td>
<td>17479760</td>
<td>77985102</td>
<td>CNN, EfficientNet, Swin Transformer, ensemble</td>
<td>6</td>
<td>average vote</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_3</td>
<td>Wang2021b</td>
<td>1</td>
<td>0.195</td>
<td>93.8</td>
<td>directly, pre-trained model</td>
<td>ImageNet, Places365, EfficientNet, PyTorch Image Models</td>
<td>110848636</td>
<td>25882876</td>
<td>77985102</td>
<td>CNN, EfficientNet, Swin Transformer, ensemble</td>
<td>5</td>
<td>weighted vote</td>
</tr>
<tr>
<td></td>
<td>Zhang_IOA_task1b_4</td>
<td>Wang2021b</td>
<td>2</td>
<td>0.199</td>
<td>93.9</td>
<td>directly, pre-trained model</td>
<td>ImageNet, Places365, EfficientNet, PyTorch Image Models</td>
<td>115725988</td>
<td>25882876</td>
<td>77985102</td>
<td>CNN, EfficientNet, Swin Transformer, ensemble</td>
<td>9</td>
<td>average vote</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2021/technical_reports_task1b.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Boes2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Boes2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Source Transformer Architectures for Audiovisual Scene Classification
       </h4>
<p style="text-align:left">
        Wim Boes and Hugo Van hamme
       </p>
<p style="text-align:left">
<em>
         ESAT, KU Leuven, Leuven, Belgium
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Boes_KUL_task1b_1</span> <span class="label label-info">Boes_KUL_task1b_2</span> <span class="label label-info">Boes_KUL_task1b_3</span> <span class="label label-info">Boes_KUL_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Boes2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Boes2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Boes2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Boes_77_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Boes2021" class="panel-collapse collapse" id="collapse-Boes2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Source Transformer Architectures for Audiovisual Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Wim Boes and Hugo Van hamme
       </small>
<br/>
<small>
<em>
         ESAT, KU Leuven, Leuven, Belgium
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, the systems we submitted for subtask 1B of the DCASE 2021 challenge, regarding audiovisual scene classi- fication, are described in detail. They are essentially multi-source transformers employing a combination of auditory and visual fea- tures to make predictions. These models are evaluated utilizing the macro-averaged multi-class cross-entropy and accuracy metrics. In terms of the macro-averaged multi-class cross-entropy, our best model achieved a score of 0.620 on the validation data. This is slightly better than the performance of the baseline system (0.658). With regard to the accuracy measure, our best model achieved a score of 77.1% on the validation data, which is about the same as the performance obtained by the baseline system (77.0%).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, transformer
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Boes2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Boes_77_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Boes2021label" class="modal fade" id="bibtex-Boes2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBoes2021label">
        Multi-Source Transformer Architectures for Audiovisual Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Boes2021,
    Author = "Boes, Wim and Van hamme, Hugo",
    title = "Multi-Source Transformer Architectures for Audiovisual Scene Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, the systems we submitted for subtask 1B of the DCASE 2021 challenge, regarding audiovisual scene classi- fication, are described in detail. They are essentially multi-source transformers employing a combination of auditory and visual fea- tures to make predictions. These models are evaluated utilizing the macro-averaged multi-class cross-entropy and accuracy metrics. In terms of the macro-averaged multi-class cross-entropy, our best model achieved a score of 0.620 on the validation data. This is slightly better than the performance of the baseline system (0.658). With regard to the accuracy measure, our best model achieved a score of 77.1\% on the validation data, which is about the same as the performance obtained by the baseline system (77.0\%)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Diez2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Diez2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Audio Scene Classification Using Enhanced Convolutional Neural Networks for DCASE 2021 Challenge
       </h4>
<p style="text-align:left">
        Itxasne Diez<sup>1</sup> and Ibon Saratxaga<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Getxo, Basque Country, Spain, <sup>2</sup>HiTZ Center â€“ Aholab Signal Processing Laboratory, University of the Basque Country, Bilbao, Basque Country, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Diez_Noismart_task1b_1</span> <span class="label label-info">Diez_Noismart_task1b_2</span> <span class="label label-info">Diez_Noismart_task1b_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Diez2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Diez2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Diez2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Diez_113_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Diez2021" class="panel-collapse collapse" id="collapse-Diez2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Audio Scene Classification Using Enhanced Convolutional Neural Networks for DCASE 2021 Challenge
      </h4>
<p style="text-align:left">
<small>
        Itxasne Diez<sup>1</sup> and Ibon Saratxaga<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Getxo, Basque Country, Spain, <sup>2</sup>HiTZ Center â€“ Aholab Signal Processing Laboratory, University of the Basque Country, Bilbao, Basque Country, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our system proposed for Task 1B â€“ Audio-Visual Scene Classification of the DCASE 2021 Challenge. Our system focuses in the audio signal based classification. The system has an architecture based on the combination of Convolutional Neural Networks and OpenL3 embeddings. The CNN consist of three stacked 2D convolutional layers to process the log-melspectrogram parameters obtained from the input signals. Additionally OpenL3 embeddings of the input signals are also calculated and merged with the output of the CNN stack. The resulting vector is fed to a classification block consisting of two fully connected layers. Mixup augmentation technique is applied to the training data and binaural data is also used as input to provide additional information. In this report, we describe the proposed systems in detail and com- pare them to the baseline approach using the provided development datasets.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mixed; binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Diez2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Diez_113_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Diez2021label" class="modal fade" id="bibtex-Diez2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDiez2021label">
        Audio Scene Classification Using Enhanced Convolutional Neural Networks for DCASE 2021 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Diez2021,
    Author = "Diez, Itxasne and Saratxaga, Ibon",
    title = "Audio Scene Classification Using Enhanced Convolutional Neural Networks for {DCASE} 2021 Challenge",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This technical report describes our system proposed for Task 1B â€“ Audio-Visual Scene Classification of the DCASE 2021 Challenge. Our system focuses in the audio signal based classification. The system has an architecture based on the combination of Convolutional Neural Networks and OpenL3 embeddings. The CNN consist of three stacked 2D convolutional layers to process the log-melspectrogram parameters obtained from the input signals. Additionally OpenL3 embeddings of the input signals are also calculated and merged with the output of the CNN stack. The resulting vector is fed to a classification block consisting of two fully connected layers. Mixup augmentation technique is applied to the training data and binaural data is also used as input to provide additional information. In this report, we describe the proposed systems in detail and com- pare them to the baseline approach using the provided development datasets."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fedorishin2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Fedorishin2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Investigating Waveform and Spectrogram Feature Fusion for Audio Classification
       </h4>
<p style="text-align:left">
        Dennis Fedorishin<sup>1</sup>, Nishant Sankaran<sup>1</sup>, Deen Mohan<sup>1</sup>, Justas Birgiolas<sup>2</sup>, Philip Schneider<sup>2</sup>, Srirangaraj Setlur<sup>1</sup> and Venu Govindaraju<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Computer Science, Center for Unified Biometrics and Sensors, University at Buffalo, New York, USA, <sup>2</sup>ACV Auctions, LLC., New York, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Fedorishin_UB_task1b_1</span> <span class="label label-info">Fedorishin_UB_task1b_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fedorishin2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fedorishin2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fedorishin2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Fedorishin_97_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fedorishin2021" class="panel-collapse collapse" id="collapse-Fedorishin2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Investigating Waveform and Spectrogram Feature Fusion for Audio Classification
      </h4>
<p style="text-align:left">
<small>
        Dennis Fedorishin<sup>1</sup>, Nishant Sankaran<sup>1</sup>, Deen Mohan<sup>1</sup>, Justas Birgiolas<sup>2</sup>, Philip Schneider<sup>2</sup>, Srirangaraj Setlur<sup>1</sup> and Venu Govindaraju<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Computer Science, Center for Unified Biometrics and Sensors, University at Buffalo, New York, USA, <sup>2</sup>ACV Auctions, LLC., New York, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents our submitted system for the DCASE 2021 Challenge Task1B: Audio-Visual Scene Classifica- tion. Focusing on the audio modality only, we investigate the use of two common feature representations within the audio understand- ing domain, the raw waveform and Mel-spectrogram, and measure their degree of complementarity when using both representations in a fusion setting. We introduce a new model paradigm for audio clas- sification by fusing features learned from Mel-spectrograms and the raw waveform from separate feature extraction branches. Our ex- perimental results show that our proposed fusion model has a 4.5% increase in validation accuracy and a reduction of .14 in validation loss over the Task 1B baseline audio-only subnetwork. We further show that learned features of raw waveforms and Mel-spectrograms are indeed complementary to each other and that there is a consis- tent classification performance improvement over models trained on Mel-spectrograms alone.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, time-shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, raw waveform
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fedorishin2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Fedorishin_97_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fedorishin2021label" class="modal fade" id="bibtex-Fedorishin2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFedorishin2021label">
        Investigating Waveform and Spectrogram Feature Fusion for Audio Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fedorishin2021,
    Author = "Fedorishin, Dennis and Sankaran, Nishant and Mohan, Deen and Birgiolas, Justas and Schneider, Philip and Setlur, Srirangaraj and Govindaraju, Venu",
    title = "Investigating Waveform and Spectrogram Feature Fusion for Audio Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This technical report presents our submitted system for the DCASE 2021 Challenge Task1B: Audio-Visual Scene Classifica- tion. Focusing on the audio modality only, we investigate the use of two common feature representations within the audio understand- ing domain, the raw waveform and Mel-spectrogram, and measure their degree of complementarity when using both representations in a fusion setting. We introduce a new model paradigm for audio clas- sification by fusing features learned from Mel-spectrograms and the raw waveform from separate feature extraction branches. Our ex- perimental results show that our proposed fusion model has a 4.5\% increase in validation accuracy and a reduction of .14 in validation loss over the Task 1B baseline audio-only subnetwork. We further show that learned features of raw waveforms and Mel-spectrograms are indeed complementary to each other and that there is a consis- tent classification performance improvement over models trained on Mel-spectrograms alone."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hou2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Hou2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CNN-Based Dual-Stream Network for Audio-Visual Scene Classification
       </h4>
<p style="text-align:left">
        Yuanbo Hou<sup>1</sup>, Yizhou Tan<sup>2</sup>, Yue Chang<sup>3</sup>, Tianyang Huang<sup>3</sup>, Shengchen Li<sup>4</sup>, Xi Shao<sup>3</sup> and Dick Botteldooren<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Ghent University, Gent, Belgium, <sup>2</sup>International School, Beijing University of Posts and Telecommunications, Beijing, China, <sup>3</sup>Telecommunications &amp; Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China, <sup>4</sup>Xiâ€™an Jiaotong-Liverpool University, Suzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Hou_UGent_task1b_1</span> <span class="label label-info">Hou_UGent_task1b_2</span> <span class="label label-info">Hou_UGent_task1b_3</span> <span class="label label-info">Hou_UGent_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hou2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hou2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hou2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Hou_89_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Hou2021').collapse('show');window.location.hash='#Hou2021';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hou2021" class="panel-collapse collapse" id="collapse-Hou2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CNN-Based Dual-Stream Network for Audio-Visual Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Yuanbo Hou<sup>1</sup>, Yizhou Tan<sup>2</sup>, Yue Chang<sup>3</sup>, Tianyang Huang<sup>3</sup>, Shengchen Li<sup>4</sup>, Xi Shao<sup>3</sup> and Dick Botteldooren<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Ghent University, Gent, Belgium, <sup>2</sup>International School, Beijing University of Posts and Telecommunications, Beijing, China, <sup>3</sup>Telecommunications &amp; Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China, <sup>4</sup>Xiâ€™an Jiaotong-Liverpool University, Suzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents the CNN-based dual-stream network for audio-visual scene classification in DCASE 2021 Challenge (Task 1 Subtask B). The proposed method in this report is only trained based on the development dataset in Task 1 Subtask B and does not use any external dataset. For the performance, the model proposed in this report gets 0.318 log-loss and 90.0% accuracy for scene classification on the development dataset, and the log-loss and accuracy in the baseline are 0.658 and 77.0%, respectively. Our results are reproducible, source code is available here: https://github.com/Yuanbo2020/DCASE2021-T1B.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         OpenL3
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ResNet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hou2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Hou_89_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Yuanbo2020/DCASE2021-T1B" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hou2021label" class="modal fade" id="bibtex-Hou2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHou2021label">
        CNN-Based Dual-Stream Network for Audio-Visual Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hou2021,
    Author = "Hou, Yuanbo and Tan, Yizhou and Chang, Yue and Huang, Tianyang and Li, Shengchen and Shao, Xi and Botteldooren, Dick",
    title = "{CNN}-Based Dual-Stream Network for Audio-Visual Scene Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This technical report presents the CNN-based dual-stream network for audio-visual scene classification in DCASE 2021 Challenge (Task 1 Subtask B). The proposed method in this report is only trained based on the development dataset in Task 1 Subtask B and does not use any external dataset. For the performance, the model proposed in this report gets 0.318 log-loss and 90.0\% accuracy for scene classification on the development dataset, and the log-loss and accuracy in the baseline are 0.658 and 77.0\%, respectively. Our results are reproducible, source code is available here: https://github.com/Yuanbo2020/DCASE2021-T1B."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Naranjo-Alcazar2021_t1b" style="box-shadow: none">
<div class="panel-heading" id="heading-Naranjo-Alcazar2021_t1b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Task 1B DCASE 2021: Audio-Visual Scene Classification with Squeeze-Excitation Convolutional Recurrent Neural Networks
       </h4>
<p style="text-align:left">
        Javier Naranjo-Alcazar<sup>1,2</sup>, Sergi Perez-Castanos<sup>1</sup>, Maximo Cobos<sup>1</sup>, Francesc J. Ferri<sup>1</sup> and Pedro Zuccarello<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Computer Science, Universitat de Valencia, Burjassot, Spain, <sup>2</sup>Intituto TecnolÃ³gico de InformÃ¡tica, Valencia, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Naranjo-Alcazar_UV_task1b_1</span> <span class="label label-info">Naranjo-Alcazar_UV_task1b_2</span> <span class="label label-info">Naranjo-Alcazar_UV_task1b_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Naranjo-Alcazar2021_t1b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Naranjo-Alcazar2021_t1b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Naranjo-Alcazar2021_t1b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Naranjo-Alcazar_33_t1b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Naranjo-Alcazar2021_t1b').collapse('show');window.location.hash='#Naranjo-Alcazar2021_t1b';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Naranjo-Alcazar2021_t1b" class="panel-collapse collapse" id="collapse-Naranjo-Alcazar2021_t1b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Task 1B DCASE 2021: Audio-Visual Scene Classification with Squeeze-Excitation Convolutional Recurrent Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Javier Naranjo-Alcazar<sup>1,2</sup>, Sergi Perez-Castanos<sup>1</sup>, Maximo Cobos<sup>1</sup>, Francesc J. Ferri<sup>1</sup> and Pedro Zuccarello<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Computer Science, Universitat de Valencia, Burjassot, Spain, <sup>2</sup>Intituto TecnolÃ³gico de InformÃ¡tica, Valencia, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Automatic scene classification has always been one of the core tasks in every edition of the DCASE challenge. Until this edition, such classification was performed using only audio data, and so the problematic was defined as Acoustic Scene Classification (ASC). In this 2021 edition, audio data is accompanied with visual data, providing additional information that can be jointly exploited for achieving higher recognition accuracy. The proposed approach makes use of two separate networks which are respectively trained in isolation on audio and visual data, so that each network specializes in a given modality. After training each network, the fusion of information from the audio and visual subnetworks is performed at two different stages. The early fusion stage combines features resulting from the last convolutional block of the respective subnetworks at different time steps to feed a bidirectional recurrent structure. The late fusion stage combines the output of the early fusion stage with the independent predictions provided by the two subnetworks, resulting in the final prediction. For the visual subnetwork, a VGG16 architecture pretrained on the Places365 dataset is used, applying a fine-tuning strategy over the Challenge dataset. On the other hand, the audio subnetwork is trained from scratch and uses squeeze- excitation techniques as in previous contributions from this team. As a result, the final accuracy of the system is 92% on development split, outperforming the baseline by 15 percentage points.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, difference
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         gammatone spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Naranjo-Alcazar2021_t1b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Naranjo-Alcazar_33_t1b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Machine-Listeners-Valencia/DCASE2021-Task1b" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Naranjo-Alcazar2021_t1blabel" class="modal fade" id="bibtex-Naranjo-Alcazar2021_t1b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNaranjo-Alcazar2021_t1blabel">
        Task 1B DCASE 2021: Audio-Visual Scene Classification with Squeeze-Excitation Convolutional Recurrent Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Naranjo-Alcazar2021_t1b,
    Author = "Naranjo-Alcazar, Javier and Perez-Castanos, Sergi and Cobos, Maximo and Ferri, Francesc J. and Zuccarello, Pedro",
    title = "Task {1B} {DCASE} 2021: Audio-Visual Scene Classification with Squeeze-Excitation Convolutional Recurrent Neural Networks",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Automatic scene classification has always been one of the core tasks in every edition of the DCASE challenge. Until this edition, such classification was performed using only audio data, and so the problematic was defined as Acoustic Scene Classification (ASC). In this 2021 edition, audio data is accompanied with visual data, providing additional information that can be jointly exploited for achieving higher recognition accuracy. The proposed approach makes use of two separate networks which are respectively trained in isolation on audio and visual data, so that each network specializes in a given modality. After training each network, the fusion of information from the audio and visual subnetworks is performed at two different stages. The early fusion stage combines features resulting from the last convolutional block of the respective subnetworks at different time steps to feed a bidirectional recurrent structure. The late fusion stage combines the output of the early fusion stage with the independent predictions provided by the two subnetworks, resulting in the final prediction. For the visual subnetwork, a VGG16 architecture pretrained on the Places365 dataset is used, applying a fine-tuning strategy over the Challenge dataset. On the other hand, the audio subnetwork is trained from scratch and uses squeeze- excitation techniques as in previous contributions from this team. As a result, the final accuracy of the system is 92\% on development split, outperforming the baseline by 15 percentage points."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Okazaki2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Okazaki2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Ldslvision Submissions to Dcaseâ€™21: A Multi-Modal Fusion Approach for Audio-Visual Scene Classification Enhanced by Clip Variants
       </h4>
<p style="text-align:left">
        Soichiro Okazaki, Kong Quan and Tomoaki Yoshinaga
       </p>
<p style="text-align:left">
<em>
         Lumada Data Science Lab., Hitachi, Ltd., Toyko, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Okazaki_LDSLVision_task1b_1</span> <span class="label label-info">Okazaki_LDSLVision_task1b_2</span> <span class="label label-info">Okazaki_LDSLVision_task1b_3</span> <span class="label label-info">Okazaki_LDSLVision_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Okazaki2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Okazaki2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Okazaki2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Okazaki_18_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Okazaki2021" class="panel-collapse collapse" id="collapse-Okazaki2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Ldslvision Submissions to Dcaseâ€™21: A Multi-Modal Fusion Approach for Audio-Visual Scene Classification Enhanced by Clip Variants
      </h4>
<p style="text-align:left">
<small>
        Soichiro Okazaki, Kong Quan and Tomoaki Yoshinaga
       </small>
<br/>
<small>
<em>
         Lumada Data Science Lab., Hitachi, Ltd., Toyko, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we describe our solution for audio-visual scene classification task of DCASE2021 challenge Task1B. Our solution is based on a multi-modal fusion approach consisting of three different domain features: (1) Log-mel spectrogram audio features extracted by CNN variants from audio files. (2) Frame-wise image features extracted by CNN variants from video files. (3) Text-guided frame- wise image features extracted by CLIP variants from video files. We trained three domain models respectively and created final sub- missions by ensembling the class-wise confidences of three domain modelsâ€™ outputs. With ensembling and post-processing for the confidences, our model reached 0.149 log-loss (official baseline: 0.658 log-loss) and 96.1% accuracy (official baseline: 77.0% accuracy) on the officially provided fold1 evaluation dataset of Task1B.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono+delta+delta-delta (3 channels)
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         (Video) random affine, color jitter, gaussian blur, random erasing; (Audio) frequency masking, random gain (Video) random affine, color jitter, gaussian blur, random erasing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ViT, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Okazaki2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Okazaki_18_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Okazaki2021label" class="modal fade" id="bibtex-Okazaki2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexOkazaki2021label">
        Ldslvision Submissions to Dcaseâ€™21: A Multi-Modal Fusion Approach for Audio-Visual Scene Classification Enhanced by Clip Variants
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Okazaki2021,
    Author = "Okazaki, Soichiro and Quan, Kong and Yoshinaga, Tomoaki",
    title = "Ldslvision Submissions to Dcaseâ€™21: A Multi-Modal Fusion Approach for Audio-Visual Scene Classification Enhanced by Clip Variants",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this report, we describe our solution for audio-visual scene classification task of DCASE2021 challenge Task1B. Our solution is based on a multi-modal fusion approach consisting of three different domain features: (1) Log-mel spectrogram audio features extracted by CNN variants from audio files. (2) Frame-wise image features extracted by CNN variants from video files. (3) Text-guided frame- wise image features extracted by CLIP variants from video files. We trained three domain models respectively and created final sub- missions by ensembling the class-wise confidences of three domain modelsâ€™ outputs. With ensembling and post-processing for the confidences, our model reached 0.149 log-loss (official baseline: 0.658 log-loss) and 96.1\% accuracy (official baseline: 77.0\% accuracy) on the officially provided fold1 evaluation dataset of Task1B."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Peng2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Peng2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolutional Receptive Field Dual Selection Mechanism for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Wang Peng<sup>1</sup>, Tianyang Zhang<sup>1</sup> and Zehua Zou<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Intelligent Information Technology and System Lab, CHONGQING UNIVERSITY, Chongqing, China, <sup>2</sup>Image Information Processing Lab, CHONGQING UNIVERSITY, Chongqing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Peng_CQU_task1b_1</span> <span class="label label-info">Peng_CQU_task1b_2</span> <span class="label label-info">Peng_CQU_task1b_3</span> <span class="label label-info">Peng_CQU_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Peng2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Peng2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Peng2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Peng_107_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Peng2021" class="panel-collapse collapse" id="collapse-Peng2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolutional Receptive Field Dual Selection Mechanism for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Wang Peng<sup>1</sup>, Tianyang Zhang<sup>1</sup> and Zehua Zou<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Intelligent Information Technology and System Lab, CHONGQING UNIVERSITY, Chongqing, China, <sup>2</sup>Image Information Processing Lab, CHONGQING UNIVERSITY, Chongqing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Convolution neural network (CNN), which can extract rich semantic information of signal, is a representative feature learing network in acoustic scene classification (ASC). However, since that the receptive field (RF) of a CNN is fixed, it is inefficient to capture the dynamical time-frequency changing characteristic of the input Log-Mel spectrogram. In addition, although the Log-Mel spectrogram can be treated as an image, the time and frequency dimensions, which respectively represent the acoustic event duration and frequency information, have different physical meanings. Therefore, existing receptive field adaptive meth- ods, which get same-sized optimal receptive fields in two dimensions, are not suitable for ASC. To tackle this problem, we proposed a convolution receptive field dual selection mechanism (CRFDS) in this paper. Acoustic scene classification experiments conducted on DCASE 2021 subtask B with audio-only show that the accuracy of CRFDS can achieve 71.45%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Peng2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Peng_107_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Peng2021label" class="modal fade" id="bibtex-Peng2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPeng2021label">
        Convolutional Receptive Field Dual Selection Mechanism for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Peng2021,
    Author = "Peng, Wang and Zhang, Tianyang and Zou, Zehua",
    title = "Convolutional Receptive Field Dual Selection Mechanism for Acoustic Scene Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Convolution neural network (CNN), which can extract rich semantic information of signal, is a representative feature learing network in acoustic scene classification (ASC). However, since that the receptive field (RF) of a CNN is fixed, it is inefficient to capture the dynamical time-frequency changing characteristic of the input Log-Mel spectrogram. In addition, although the Log-Mel spectrogram can be treated as an image, the time and frequency dimensions, which respectively represent the acoustic event duration and frequency information, have different physical meanings. Therefore, existing receptive field adaptive meth- ods, which get same-sized optimal receptive fields in two dimensions, are not suitable for ASC. To tackle this problem, we proposed a convolution receptive field dual selection mechanism (CRFDS) in this paper. Acoustic scene classification experiments conducted on DCASE 2021 subtask B with audio-only show that the accuracy of CRFDS can achieve 71.45\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pham2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Pham2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2021 Task 1B: Technique Report
       </h4>
<p style="text-align:left">
        Lam Pham, Alexander Schindler, Mina Schutz, Jasmin Lampert and Ross King
       </p>
<p style="text-align:left">
<em>
         Center for Digital Safety &amp; Security, Austrian Institute of Technology, Vienna, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Pham_AIT_task1b_1</span> <span class="label label-info">Pham_AIT_task1b_2</span> <span class="label label-info">Pham_AIT_task1b_3</span> <span class="label label-info">Pham_AIT_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Pham2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pham2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pham2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Pham_12_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pham2021" class="panel-collapse collapse" id="collapse-Pham2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2021 Task 1B: Technique Report
      </h4>
<p style="text-align:left">
<small>
        Lam Pham, Alexander Schindler, Mina Schutz, Jasmin Lampert and Ross King
       </small>
<br/>
<small>
<em>
         Center for Digital Safety &amp; Security, Austrian Institute of Technology, Vienna, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Abstractâ€”This report shows a deep learning framework for audio-visual scene classification (SC). Our extensive experiments, which are conducted on DCASE Task 1B development dataset, achieve the best classification accuracy of 82.2%, 91.1%, and 93.9% with audio input only, visual input only, and both audio- visual input, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT, Gammatonegram, log-mel
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         Average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pham2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Pham_12_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pham2021label" class="modal fade" id="bibtex-Pham2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPham2021label">
        DCASE 2021 Task 1B: Technique Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Pham2021,
    Author = "Pham, Lam and Schindler, Alexander and Schutz, Mina and Lampert, Jasmin and King, Ross",
    title = "{DCASE} 2021 Task {1B}: Technique Report",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "Abstractâ€”This report shows a deep learning framework for audio-visual scene classification (SC). Our extensive experiments, which are conducted on DCASE Task 1B development dataset, achieve the best classification accuracy of 82.2\%, 91.1\%, and 93.9\% with audio input only, visual input only, and both audio- visual input, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Triantafyllopoulos2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Triantafyllopoulos2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Multimodal Wavetransformer Architecture Conditioned on Openl3 Embeddings for Audio-Visual Scene Classification
       </h4>
<p style="text-align:left">
        Andreas Triantafyllopoulos<sup>1</sup>, Konstantinos Drossos<sup>2</sup>, Alexander Gebhard<sup>3</sup>, Baird Alice<sup>3</sup> and Schuller BjÃ¶rn<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>audEERING GmbH, Gilching, Germany, <sup>2</sup>Computing Sciences, Tampere University, Tampere, Finland, <sup>3</sup>University of Augsburg, Augsburg, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Triantafyllopoulos_AUD_task1b_1</span> <span class="label label-info">Triantafyllopoulos_AUD_task1b_2</span> <span class="label label-info">Triantafyllopoulos_AUD_task1b_3</span> <span class="label label-info">Triantafyllopoulos_AUD_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Triantafyllopoulos2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Triantafyllopoulos2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Triantafyllopoulos2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Triantafyllopoulos_84_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Triantafyllopoulos2021" class="panel-collapse collapse" id="collapse-Triantafyllopoulos2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Multimodal Wavetransformer Architecture Conditioned on Openl3 Embeddings for Audio-Visual Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Andreas Triantafyllopoulos<sup>1</sup>, Konstantinos Drossos<sup>2</sup>, Alexander Gebhard<sup>3</sup>, Baird Alice<sup>3</sup> and Schuller BjÃ¶rn<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>audEERING GmbH, Gilching, Germany, <sup>2</sup>Computing Sciences, Tampere University, Tampere, Finland, <sup>3</sup>University of Augsburg, Augsburg, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our submission systems to TASK1B of the DCASE2021 Challenge. We submit a total of four systems: one purely audio-based and three multimodal variants of the same architecture. The main module consists of the WaveTransformer architecture, which was recently introduced for automatic audio captioning (AAC). We first adapt the architecture to the task of acoustic scene classification (ASC), and then extend it to handle multimodal signals by globally conditioning all layers on multimodal OpenL3 embeddings. As data augmentation, we apply time- and frequency- bin masking, as well as random cropping. Our best-effort system achieves a log-loss of 0.568 and an accuracy of 79.5%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         frequency masking, time masking, random cropping
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         WaveTransformer; MultimodalWaveTransformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Triantafyllopoulos2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Triantafyllopoulos_84_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Triantafyllopoulos2021label" class="modal fade" id="bibtex-Triantafyllopoulos2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTriantafyllopoulos2021label">
        A Multimodal Wavetransformer Architecture Conditioned on Openl3 Embeddings for Audio-Visual Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Triantafyllopoulos2021,
    Author = "Triantafyllopoulos, Andreas and Drossos, Konstantinos and Gebhard, Alexander and Alice, Baird and BjÃ¶rn, Schuller",
    title = "A Multimodal Wavetransformer Architecture Conditioned on Openl3 Embeddings for Audio-Visual Scene Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this report, we present our submission systems to TASK1B of the DCASE2021 Challenge. We submit a total of four systems: one purely audio-based and three multimodal variants of the same architecture. The main module consists of the WaveTransformer architecture, which was recently introduced for automatic audio captioning (AAC). We first adapt the architecture to the task of acoustic scene classification (ASC), and then extend it to handle multimodal signals by globally conditioning all layers on multimodal OpenL3 embeddings. As data augmentation, we apply time- and frequency- bin masking, as well as random cropping. Our best-effort system achieves a log-loss of 0.568 and an accuracy of 79.5\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Model Ensemble Approach for Audio-Visual Scene Classification
       </h4>
<p style="text-align:left">
        Qing Wang<sup>1</sup>, Siyuan Zheng<sup>1</sup>, Yunqing Li<sup>1</sup>, Yajian Wang<sup>1</sup>, Yuzhong Wu<sup>2</sup>, Hu Hu<sup>3</sup>, Chao-Han Huck Yang<sup>3</sup>, Sabato Marco Siniscalchi<sup>4</sup>, Yannan Wang<sup>5</sup>, Jun Du<sup>1</sup> and Chin-Hui Lee<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>NELSLIP, University of Science and Technology of China, Heifei, China, <sup>2</sup>DSP &amp; Speech Technology Laboratory, The Chinese University of Hong Kong, Hong Kong, China, <sup>3</sup>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA, <sup>4</sup>Kore University of Enna, Italy, <sup>5</sup>Tencent Ethereal Audio Lab, Tencent Corporation, Shenzhen, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Du_USTC_task1b_1</span> <span class="label label-info">Du_USTC_task1b_2</span> <span class="label label-info">Du_USTC_task1b_3</span> <span class="label label-info">Du_USTC_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Du_124_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2021" class="panel-collapse collapse" id="collapse-Wang2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Model Ensemble Approach for Audio-Visual Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Qing Wang<sup>1</sup>, Siyuan Zheng<sup>1</sup>, Yunqing Li<sup>1</sup>, Yajian Wang<sup>1</sup>, Yuzhong Wu<sup>2</sup>, Hu Hu<sup>3</sup>, Chao-Han Huck Yang<sup>3</sup>, Sabato Marco Siniscalchi<sup>4</sup>, Yannan Wang<sup>5</sup>, Jun Du<sup>1</sup> and Chin-Hui Lee<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>NELSLIP, University of Science and Technology of China, Heifei, China, <sup>2</sup>DSP &amp; Speech Technology Laboratory, The Chinese University of Hong Kong, Hong Kong, China, <sup>3</sup>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA, <sup>4</sup>Kore University of Enna, Italy, <sup>5</sup>Tencent Ethereal Audio Lab, Tencent Corporation, Shenzhen, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our approach to Task 1b - Audio- Visual Scene Classification (AVSC) in the DCASE 2021 Challenge. We employ pre-trained networks trained on image datasets to ex- tract video embedding whereas for audio embedding models trained from scratch are more appropriate for feature extraction. We pro- pose several models for the AVSC task based on different audio and video embeddings using early fusion strategy. Besides, we propose to use audio-visual segment model (AVSM) to extract text embed- ding. Data augmentation methods are used during training. Fur- thermore, a two-stage classification strategy is adopted by leverag- ing on score fusion of two classifiers. Finally, model ensemble of two-stage AVSC classifiers is used to obtained more robust predic- tions. The proposed systems are evaluated on the development set of TAU Urban Audio Visual Scenes 2021. Compared with official baseline, our approach can achieve a much lower log loss of 0.141 and a much higher accuracy of 95.3%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, channel confusion, SpecAugment, pitch shifting, speed change, random noise, mix audios, contrast, sharpness
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM, HMM, VGG, CNN, ResNet, DNN, ResNeSt, DenseNet, ensemble; CNN, ResNet, DNN, ResNeSt, DenseNet, ensemble; CNN, ResNet, DNN, DenseNet, ResNeSt, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Du_124_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2021label" class="modal fade" id="bibtex-Wang2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2021label">
        A Model Ensemble Approach for Audio-Visual Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2021,
    Author = "Wang, Qing and Zheng, Siyuan and Li, Yunqing and Wang, Yajian and Wu, Yuzhong and Hu, Hu and Yang, Chao-Han Huck and Siniscalchi, Sabato Marco and Wang, Yannan and Du, Jun and Lee, Chin-Hui",
    title = "A Model Ensemble Approach for Audio-Visual Scene Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we present our approach to Task 1b - Audio- Visual Scene Classification (AVSC) in the DCASE 2021 Challenge. We employ pre-trained networks trained on image datasets to ex- tract video embedding whereas for audio embedding models trained from scratch are more appropriate for feature extraction. We pro- pose several models for the AVSC task based on different audio and video embeddings using early fusion strategy. Besides, we propose to use audio-visual segment model (AVSM) to extract text embed- ding. Data augmentation methods are used during training. Fur- thermore, a two-stage classification strategy is adopted by leverag- ing on score fusion of two classifiers. Finally, model ensemble of two-stage AVSC classifiers is used to obtained more robust predic- tions. The proposed systems are evaluated on the development set of TAU Urban Audio Visual Scenes 2021. Compared with official baseline, our approach can achieve a much lower log loss of 0.141 and a much higher accuracy of 95.3\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2021a" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2021a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Bit Submission for DCASE 2020 Challenge Task1
       </h4>
<p style="text-align:left">
        Yuxiang Wang and Shuang liang
       </p>
<p style="text-align:left">
<em>
         Electronic engineering, Beijing Institute of Technology, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Wang_BIT_task1b_1</span> <span class="label label-info">Wang_BIT_task1b_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2021a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2021a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2021a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Wang_129_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Wang2021a').collapse('show');window.location.hash='#Wang2021a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2021a" class="panel-collapse collapse" id="collapse-Wang2021a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Bit Submission for DCASE 2020 Challenge Task1
      </h4>
<p style="text-align:left">
<small>
        Yuxiang Wang and Shuang liang
       </small>
<br/>
<small>
<em>
         Electronic engineering, Beijing Institute of Technology, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       DCASE2021 challenge task 1 contains two sub-tasks: (i) LowComplexity Acoustic Scene Classification with Multiple Devices and (ii) Audio-Visual Scene Classification. In our submission systems, different methods are used for different tasks. For task 1a, we explore the fsFCNN (frequency sub-sampling controlled fully convolution) with two-stage training. In order to reduce the model size, the knowledge distillation approach is used. For task 1b, different models are used for different modals. For audio classification, the same fsFCNN structures with two-stage training are applied. For video classification, the TimeSformer model is used. Experimental results show that our final model obtain accuracy of 64.6% with 128kb model size. On task 1b development set, the audio modal achieve 80.6% accuracy and 92% for video modal.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Transformer; CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2021a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Wang_129_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/facebookresearch/TimeSformer" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2021alabel" class="modal fade" id="bibtex-Wang2021a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2021alabel">
        Bit Submission for DCASE 2020 Challenge Task1
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2021a,
    Author = "Wang, Yuxiang and liang, Shuang",
    title = "Bit Submission for {DCASE} 2020 Challenge Task1",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "DCASE2021 challenge task 1 contains two sub-tasks: (i) LowComplexity Acoustic Scene Classification with Multiple Devices and (ii) Audio-Visual Scene Classification. In our submission systems, different methods are used for different tasks. For task 1a, we explore the fsFCNN (frequency sub-sampling controlled fully convolution) with two-stage training. In order to reduce the model size, the knowledge distillation approach is used. For task 1b, different models are used for different modals. For audio classification, the same fsFCNN structures with two-stage training are applied. For video classification, the TimeSformer model is used. Experimental results show that our final model obtain accuracy of 64.6\% with 128kb model size. On task 1b development set, the audio modal achieve 80.6\% accuracy and 92\% for video modal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2021b" style="box-shadow: none">
<div class="panel-heading" id="heading-Wang2021b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Audio-Visual Scene Classification Using Transfer Learning and Hybrid Fusion Strategy
       </h4>
<p style="text-align:left">
        Meng Wang, Chengxin Chen, Yuan Xie, Hangting Chen, Yuzhuo Liu and Pengyuan Zhang
       </p>
<p style="text-align:left">
<em>
         Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Zhang_IOA_task1b_1</span> <span class="label label-info">Zhang_IOA_task1b_2</span> <span class="label label-info">Zhang_IOA_task1b_3</span> <span class="label label-info">Zhang_IOA_task1b_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wang2021b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2021b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2021b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zhang_109_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2021b" class="panel-collapse collapse" id="collapse-Wang2021b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Audio-Visual Scene Classification Using Transfer Learning and Hybrid Fusion Strategy
      </h4>
<p style="text-align:left">
<small>
        Meng Wang, Chengxin Chen, Yuan Xie, Hangting Chen, Yuzhuo Liu and Pengyuan Zhang
       </small>
<br/>
<small>
<em>
         Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe the details of our submission for DCASE2021 Task1b. This task focuses on audio-visual scene classification. We use 1D deep convolutional neural network integrated with three different acoustic features in our audio system, and perform a two-stage fine-tuning on some pre-trained models such as ResNet-50 and EfficientNet-b5 in our image system. In model-level fusion, the extracted audio and image embeddings are concatenated as input into a classifier. We also use decision-level fusion to make our system more robust. On the official train/test setup of the development dataset, our best single audio-visual system obtained a 0.159 log loss and 94.1% accuracy compared to 0.623 and 78.5% for the audio-only system and 0.270 and 91.8% for the image-only system. Our final fusion system could achieve a 0.143 log loss and 95.2% accuracy.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, CQT, bark
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, EfficientNet, Swin Transformer, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted vote; average vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2021b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zhang_109_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2021blabel" class="modal fade" id="bibtex-Wang2021b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2021blabel">
        Audio-Visual Scene Classification Using Transfer Learning and Hybrid Fusion Strategy
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wang2021b,
    Author = "Wang, Meng and Chen, Chengxin and Xie, Yuan and Chen, Hangting and Liu, Yuzhuo and Zhang, Pengyuan",
    title = "Audio-Visual Scene Classification Using Transfer Learning and Hybrid Fusion Strategy",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this technical report, we describe the details of our submission for DCASE2021 Task1b. This task focuses on audio-visual scene classification. We use 1D deep convolutional neural network integrated with three different acoustic features in our audio system, and perform a two-stage fine-tuning on some pre-trained models such as ResNet-50 and EfficientNet-b5 in our image system. In model-level fusion, the extracted audio and image embeddings are concatenated as input into a classifier. We also use decision-level fusion to make our system more robust. On the official train/test setup of the development dataset, our best single audio-visual system obtained a 0.159 log loss and 94.1\% accuracy compared to 0.623 and 78.5\% for the audio-only system and 0.270 and 91.8\% for the image-only system. Our final fusion system could achieve a 0.143 log loss and 95.2\% accuracy."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yang2021" style="box-shadow: none">
<div class="panel-heading" id="heading-Yang2021" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Scene Classification Using Acoustic and Visual Feature
       </h4>
<p style="text-align:left">
        Yujie Yang<sup>1</sup> and Yanan Luo<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Tsinghua University, Shenzhen, China, <sup>2</sup>Tencent, Shenzhen, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-info">Yang_THU_task1b_1</span> <span class="label label-info">Yang_THU_task1b_2</span> <span class="label label-info">Yang_THU_task1b_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yang2021" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yang2021" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yang_1_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yang2021" class="panel-collapse collapse" id="collapse-Yang2021" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Scene Classification Using Acoustic and Visual Feature
      </h4>
<p style="text-align:left">
<small>
        Yujie Yang<sup>1</sup> and Yanan Luo<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Tsinghua University, Shenzhen, China, <sup>2</sup>Tencent, Shenzhen, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we provide a brief overview of our submission for the audio-visual scene classification task of the DCASE 2021 challenge. This report focuses on the joint use of audio and video features to improve the performance of scene classification. In order to extract audio features, we train a convolutional neural network similar to the VGG to classify the log-mel spectra. In order to extract video features, we use ResNext to train an image classifier. Subsequently we use both features to do the classification, which can achieve better performance than using only one feature to make the classification.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, specaugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; Multi-head Attention, MLP; Multi-head Attention, MLP, CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yang2021" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yang_1_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yang2021label" class="modal fade" id="bibtex-Yang2021" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYang2021label">
        Scene Classification Using Acoustic and Visual Feature
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yang2021,
    Author = "Yang, Yujie and Luo, Yanan",
    title = "Scene Classification Using Acoustic and Visual Feature",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "In this report, we provide a brief overview of our submission for the audio-visual scene classification task of the DCASE 2021 challenge. This report focuses on the joint use of audio and video features to improve the performance of scene classification. In order to extract audio features, we train a convolutional neural network similar to the VGG to classify the log-mel spectra. In order to extract video features, we use ResNext to train an image classifier. Subsequently we use both features to do the classification, which can achieve better performance than using only one feature to make the classification."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>