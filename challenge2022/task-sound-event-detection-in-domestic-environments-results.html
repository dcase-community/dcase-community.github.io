<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Detection in Domestic Environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2022/task-sound-event-detection-in-domestic-environments-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. The challenge …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2022</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2022/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-automatic-audio-captioning-and-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning-and-language-based-audio-retrieval"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Automatic audio-captioning</strong>
    </li>
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning"><i class="fa dc-captioning fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Language-Based Audio Retrieval</strong>
    </li>
            <li class="">
        <a href="/challenge2022/task-language-based-audio-retrieval"><i class="fa fa-file-text fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2022/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2022/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/tiles-13.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-domestic fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Domestic</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2022_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Detection in Domestic Environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#supplementary-metrics">Supplementary metrics</a></li>
<li><a href="#without-external-resources">Without external resources</a></li>
<li><a href="#with-external-resources">With external resources</a></li>
</ul>
</li>
<li><a href="#teams-ranking">Teams ranking</a>
<ul>
<li><a href="#supplementary-metrics-1">Supplementary metrics</a></li>
<li><a href="#without-external-resources-1">Without external resources</a></li>
<li><a href="#with-external-resources-1">With external resources</a></li>
</ul>
</li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#energy-consumption">Energy Consumption</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
<li><a href="#complexity">Complexity</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The task evaluates systems for the detection of sound events using weakly labeled data (without timestamps).
The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong>
given that multiple events can be present in an audio recording.
The challenge of exploring the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong>
together with a small weakly annotated training set to improve system performance remains. Isolated sound events,
background sound files and scripst to design a <strong>training set with strongly annotated synthetic data</strong> are provided.
<strong>The labels in all the annotated subsets are verified and can be considered as reliable.</strong></p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2022/task-sound-event-detection-in-domestic-environments" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS1_all" data-scatter-y="PSDS1_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Development dataset)" data-chartable="true" data-field="PSDS1_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Development dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Development dataset)" data-chartable="true" data-field="PSDS2_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>0.484</td>
<td>0.697</td>
<td>0.481</td>
<td>0.694</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>DCASE2022 pretrained system 1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>0.472</td>
<td>0.700</td>
<td>0.475</td>
<td>0.688</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
<td>0.431</td>
<td>0.645</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>0.049</td>
<td>0.784</td>
<td>0.051</td>
<td>0.826</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>DCASE2022 PANNs SED 2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>0.000</td>
<td>0.063</td>
<td>0.451</td>
<td>0.734</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>DCASE2022 PANNs SED 3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>0.070</td>
<td>0.194</td>
<td>0.457</td>
<td>0.767</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
<td>0.437</td>
<td>0.680</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>DCASE2022 PANNs SED 4</td>
<td>Liu2022</td>
<td>0.21</td>
<td>0.046</td>
<td>0.151</td>
<td>0.465</td>
<td>0.760</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>0.81</td>
<td>0.062</td>
<td>0.774</td>
<td>0.063</td>
<td>0.814</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>rtzr_strong-real</td>
<td>Suh2022</td>
<td>1.39</td>
<td>0.458</td>
<td>0.721</td>
<td>0.473</td>
<td>0.723</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh2022</td>
<td>1.42</td>
<td>0.478</td>
<td>0.719</td>
<td>0.445</td>
<td>0.704</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>DCASE2022_CRNN_ADJ</td>
<td>Cheng2022</td>
<td>0.93</td>
<td>0.276</td>
<td>0.543</td>
<td>0.356</td>
<td>0.601</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
<td>0.362</td>
<td>0.635</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>DCASE2022 task4 Pre-Trained 2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>0.129</td>
<td>0.758</td>
<td>0.177</td>
<td>0.801</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>0.051</td>
<td>0.777</td>
<td>0.067</td>
<td>0.827</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
<td>0.037</td>
<td>0.244</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu2022</td>
<td>1.25</td>
<td>0.425</td>
<td>0.634</td>
<td>0.443</td>
<td>0.660</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>DCASE2022 Kim system 1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
<td>0.473</td>
<td>0.693</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>0.062</td>
<td>0.781</td>
<td>0.068</td>
<td>0.830</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>DCASE2022 Kim system 4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>0.305</td>
<td>0.750</td>
<td>0.354</td>
<td>0.756</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.695</td>
<td>0.473</td>
<td>0.695</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
<td>0.269</td>
<td>0.446</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>SKATTN_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>0.156</td>
<td>0.449</td>
<td>0.161</td>
<td>0.452</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
<td>0.046</td>
<td>0.165</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
<td>0.129</td>
<td>0.241</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>0.82</td>
<td>0.062</td>
<td>0.787</td>
<td>0.075</td>
<td>0.852</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi2022</td>
<td>1.26</td>
<td>0.439</td>
<td>0.611</td>
<td>0.449</td>
<td>0.662</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>PCO_task4_SED_C</td>
<td>Mizobuchi2022</td>
<td>0.88</td>
<td>0.197</td>
<td>0.620</td>
<td>0.231</td>
<td>0.714</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>PCO_task4_SED_A</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>0.398</td>
<td>0.571</td>
<td>0.425</td>
<td>0.625</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.421</td>
<td>0.664</td>
<td>0.422</td>
<td>0.667</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>0.423</td>
<td>0.651</td>
<td>0.480</td>
<td>0.726</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>train_ensemble1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>0.390</td>
<td>0.620</td>
<td>0.434</td>
<td>0.675</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>train_ensemble2</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>0.415</td>
<td>0.634</td>
<td>0.494</td>
<td>0.748</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
<td>0.342</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>SCRATCH</td>
<td>Dinkel2022</td>
<td>1.29</td>
<td>0.422</td>
<td>0.679</td>
<td>0.456</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>SMALL</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>0.373</td>
<td>0.613</td>
<td>0.395</td>
<td>0.631</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>0.104</td>
<td>0.824</td>
<td>0.126</td>
<td>0.877</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>0.451</td>
<td>0.727</td>
<td>0.482</td>
<td>0.757</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>0.78</td>
<td>0.078</td>
<td>0.723</td>
<td>0.448</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.24</td>
<td>0.425</td>
<td>0.615</td>
<td>0.448</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.09</td>
<td>0.373</td>
<td>0.547</td>
<td>0.448</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>0.158</td>
<td>0.633</td>
<td>0.088</td>
<td>0.837</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>0.082</td>
<td>0.731</td>
<td>0.102</td>
<td>0.840</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>0.410</td>
<td>0.664</td>
<td>0.472</td>
<td>0.721</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
<td>0.474</td>
<td>0.730</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
<td>0.428</td>
<td>0.655</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>10-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.400</td>
<td>0.646</td>
<td>0.410</td>
<td>0.665</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>10-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>0.310</td>
<td>0.642</td>
<td>0.347</td>
<td>0.663</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>7-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.407</td>
<td>0.643</td>
<td>0.422</td>
<td>0.656</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
<td>0.477</td>
<td>0.734</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>ATST-RCT SED system ATST small</td>
<td>Shao2022</td>
<td>1.36</td>
<td>0.476</td>
<td>0.666</td>
<td>0.460</td>
<td>0.698</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>ATST-RCT SED system ATST base</td>
<td>Shao2022</td>
<td>1.40</td>
<td>0.482</td>
<td>0.693</td>
<td>0.468</td>
<td>0.702</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
<td>0.398</td>
<td>0.611</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>0.500</td>
<td>0.695</td>
<td>0.452</td>
<td>0.682</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
<td>0.458</td>
<td>0.688</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>0.510</td>
<td>0.711</td>
<td>0.456</td>
<td>0.685</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
<td>0.459</td>
<td>0.744</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
<td>0.492</td>
<td>0.721</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>0.047</td>
<td>0.824</td>
<td>0.080</td>
<td>0.868</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>0.552</td>
<td>0.786</td>
<td>0.512</td>
<td>0.772</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>tag-conditioned CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>0.527</td>
<td>0.679</td>
<td>0.483</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>0.482</td>
<td>0.702</td>
<td>0.485</td>
<td>0.725</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>PANNs-FDY-CRNN-wrTCL system 1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>0.452</td>
<td>0.662</td>
<td>0.481</td>
<td>0.710</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.79</td>
<td>0.054</td>
<td>0.774</td>
<td>0.065</td>
<td>0.835</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
<td>0.058</td>
<td>0.813</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>0.409</td>
<td>0.656</td>
<td>0.470</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.057</td>
<td>0.747</td>
<td>0.061</td>
<td>0.822</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.055</td>
<td>0.747</td>
<td>0.058</td>
<td>0.820</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>0.404</td>
<td>0.653</td>
<td>0.470</td>
<td>0.687</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
<td>0.341</td>
<td>0.596</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>0.365</td>
<td>0.584</td>
<td>0.374</td>
<td>0.583</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>0.069</td>
<td>0.728</td>
<td>0.070</td>
<td>0.794</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>0.480</td>
<td>0.713</td>
<td>0.479</td>
<td>0.735</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Mean teacher Pseudo labeling system 4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>0.429</td>
<td>0.723</td>
<td>0.436</td>
<td>0.778</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>0.451</td>
<td>0.740</td>
<td>0.462</td>
<td>0.785</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Mean teacher Pseudo labeling system 3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>0.450</td>
<td>0.699</td>
<td>0.456</td>
<td>0.726</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
<td>0.356</td>
<td>0.554</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>0.82</td>
<td>0.053</td>
<td>0.810</td>
<td>0.071</td>
<td>0.857</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>DCASE2022 SED mean teacher system 2</td>
<td>He2022</td>
<td>1.48</td>
<td>0.503</td>
<td>0.749</td>
<td>0.521</td>
<td>0.771</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He2022</td>
<td>1.52</td>
<td>0.525</td>
<td>0.748</td>
<td>0.533</td>
<td>0.762</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>DCASE2022 SED mean teacher system 1</td>
<td>He2022</td>
<td>1.36</td>
<td>0.454</td>
<td>0.696</td>
<td>0.474</td>
<td>0.692</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.79</td>
<td>0.090</td>
<td>0.709</td>
<td>0.115</td>
<td>0.816</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.75</td>
<td>0.075</td>
<td>0.692</td>
<td>0.099</td>
<td>0.783</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.26</td>
<td>0.439</td>
<td>0.612</td>
<td>0.449</td>
<td>0.645</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.20</td>
<td>0.411</td>
<td>0.597</td>
<td>0.420</td>
<td>0.618</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>0.83</td>
<td>0.062</td>
<td>0.800</td>
<td>0.072</td>
<td>0.856</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>CBAM-T CRNN scratch</td>
<td>Xie2022</td>
<td>1.06</td>
<td>0.300</td>
<td>0.641</td>
<td>0.360</td>
<td>0.674</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>CBAM-T CRNN 1</td>
<td>Xie2022</td>
<td>1.36</td>
<td>0.418</td>
<td>0.757</td>
<td>0.460</td>
<td>0.768</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie2022</td>
<td>1.38</td>
<td>0.426</td>
<td>0.766</td>
<td>0.460</td>
<td>0.768</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
<td>0.342</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>0.317</td>
<td>0.565</td>
<td>0.372</td>
<td>0.592</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.340</td>
<td>0.544</td>
<td>0.377</td>
<td>0.585</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>DCASE2022 SED system3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.338</td>
<td>0.554</td>
<td>0.373</td>
<td>0.571</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>DCASE2022 SED system 1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>0.364</td>
<td>0.570</td>
<td>0.408</td>
<td>0.607</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>0.371</td>
<td>0.635</td>
<td>0.398</td>
<td>0.640</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>0.195</td>
<td>0.683</td>
<td>0.215</td>
<td>0.735</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>DCASE2022 SED system 2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>0.086</td>
<td>0.671</td>
<td>0.095</td>
<td>0.754</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Strong and Max-Weak balanced</td>
<td>Castorena2022</td>
<td>0.91</td>
<td>0.267</td>
<td>0.531</td>
<td>0.305</td>
<td>0.587</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena2022</td>
<td>1.01</td>
<td>0.334</td>
<td>0.524</td>
<td>0.343</td>
<td>0.538</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>0.63</td>
<td>0.072</td>
<td>0.559</td>
<td>0.067</td>
<td>0.641</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS_1_youtube" data-scatter-y="PSDS_1_vimeo" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="PSDS_1_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
                PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="PSDS 1 (Public evaluation)" data-chartable="true" data-field="PSDS_1_youtube" data-sortable="true" data-value-type="float3">
                PSDS 1 <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 1 (Vimeo dataset)" data-chartable="true" data-field="PSDS_1_vimeo" data-sortable="true" data-value-type="float3">
                PSDS 1 <br/>(Vimeo dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
                PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="PSDS 2 (Public evaluation)" data-chartable="true" data-field="PSDS_2_youtube" data-sortable="true" data-value-type="float3">
                PSDS 2 <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Vimeo dataset)" data-chartable="true" data-field="PSDS_2_vimeo" data-sortable="true" data-value-type="float3">
                PSDS 2 <br/>(Vimeo dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                F-score <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="F-score 2 (Public evaluation)" data-chartable="true" data-field="f_score_youtube" data-sortable="true" data-value-type="float1-percentage">
                F-score <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="F-score (Vimeo dataset)" data-chartable="true" data-field="f_score_vimeo" data-sortable="true" data-value-type="float1-percentage">
                F-score <br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Xiao2022</td>
<td>0.484</td>
<td>0.525</td>
<td>0.396</td>
<td>0.697</td>
<td>0.725</td>
<td>0.612</td>
<td>56.5</td>
<td>60.2</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>DCASE2022 pretrained system 1</td>
<td>Xiao2022</td>
<td>0.472</td>
<td>0.519</td>
<td>0.384</td>
<td>0.700</td>
<td>0.748</td>
<td>0.577</td>
<td>56.2</td>
<td>61.3</td>
<td>44.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao2022</td>
<td>0.420</td>
<td>0.468</td>
<td>0.304</td>
<td>0.599</td>
<td>0.649</td>
<td>0.470</td>
<td>51.3</td>
<td>55.4</td>
<td>40.1</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>0.049</td>
<td>0.057</td>
<td>0.019</td>
<td>0.784</td>
<td>0.836</td>
<td>0.651</td>
<td>15.0</td>
<td>17.0</td>
<td>10.7</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>DCASE2022 PANNs SED 2</td>
<td>Liu2022</td>
<td>0.000</td>
<td>0.003</td>
<td>0.000</td>
<td>0.063</td>
<td>0.077</td>
<td>0.024</td>
<td>10.5</td>
<td>11.8</td>
<td>6.8</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>DCASE2022 PANNs SED 3</td>
<td>Liu2022</td>
<td>0.070</td>
<td>0.095</td>
<td>0.013</td>
<td>0.194</td>
<td>0.237</td>
<td>0.087</td>
<td>8.3</td>
<td>9.2</td>
<td>5.9</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>0.434</td>
<td>0.483</td>
<td>0.324</td>
<td>0.650</td>
<td>0.702</td>
<td>0.521</td>
<td>47.6</td>
<td>50.7</td>
<td>39.3</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>DCASE2022 PANNs SED 4</td>
<td>Liu2022</td>
<td>0.046</td>
<td>0.069</td>
<td>0.003</td>
<td>0.151</td>
<td>0.180</td>
<td>0.070</td>
<td>7.5</td>
<td>8.3</td>
<td>5.1</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh2022</td>
<td>0.393</td>
<td>0.432</td>
<td>0.324</td>
<td>0.650</td>
<td>0.686</td>
<td>0.560</td>
<td>46.8</td>
<td>50.0</td>
<td>38.7</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>0.062</td>
<td>0.072</td>
<td>0.026</td>
<td>0.774</td>
<td>0.807</td>
<td>0.674</td>
<td>12.9</td>
<td>13.9</td>
<td>10.6</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>rtzr_strong-real</td>
<td>Suh2022</td>
<td>0.458</td>
<td>0.495</td>
<td>0.370</td>
<td>0.721</td>
<td>0.768</td>
<td>0.612</td>
<td>53.1</td>
<td>57.6</td>
<td>42.1</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh2022</td>
<td>0.478</td>
<td>0.512</td>
<td>0.390</td>
<td>0.719</td>
<td>0.772</td>
<td>0.592</td>
<td>53.8</td>
<td>57.7</td>
<td>44.1</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>DCASE2022_CRNN_ADJ</td>
<td>Cheng2022</td>
<td>0.276</td>
<td>0.308</td>
<td>0.212</td>
<td>0.543</td>
<td>0.568</td>
<td>0.470</td>
<td>40.9</td>
<td>43.5</td>
<td>34.3</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>0.314</td>
<td>0.361</td>
<td>0.223</td>
<td>0.582</td>
<td>0.611</td>
<td>0.497</td>
<td>43.2</td>
<td>46.7</td>
<td>34.5</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>DCASE2022 task4 Pre-Trained 2</td>
<td>Liu2022</td>
<td>0.129</td>
<td>0.139</td>
<td>0.100</td>
<td>0.758</td>
<td>0.791</td>
<td>0.682</td>
<td>19.3</td>
<td>20.0</td>
<td>17.9</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>0.051</td>
<td>0.063</td>
<td>0.015</td>
<td>0.777</td>
<td>0.803</td>
<td>0.696</td>
<td>13.6</td>
<td>14.3</td>
<td>12.0</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu2022</td>
<td>0.025</td>
<td>0.023</td>
<td>0.011</td>
<td>0.219</td>
<td>0.224</td>
<td>0.183</td>
<td>5.2</td>
<td>5.8</td>
<td>3.6</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu2022</td>
<td>0.425</td>
<td>0.471</td>
<td>0.319</td>
<td>0.634</td>
<td>0.674</td>
<td>0.512</td>
<td>49.3</td>
<td>52.2</td>
<td>41.6</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>DCASE2022 Kim system 1</td>
<td>Kim2022a</td>
<td>0.444</td>
<td>0.503</td>
<td>0.323</td>
<td>0.697</td>
<td>0.740</td>
<td>0.588</td>
<td>51.0</td>
<td>54.8</td>
<td>41.0</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>0.062</td>
<td>0.069</td>
<td>0.030</td>
<td>0.781</td>
<td>0.809</td>
<td>0.691</td>
<td>12.8</td>
<td>13.5</td>
<td>11.4</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>DCASE2022 Kim system 4</td>
<td>Kim2022a</td>
<td>0.305</td>
<td>0.333</td>
<td>0.234</td>
<td>0.750</td>
<td>0.778</td>
<td>0.683</td>
<td>27.4</td>
<td>28.5</td>
<td>25.1</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim2022a</td>
<td>0.444</td>
<td>0.502</td>
<td>0.334</td>
<td>0.695</td>
<td>0.738</td>
<td>0.585</td>
<td>51.1</td>
<td>55.0</td>
<td>41.0</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.257</td>
<td>0.280</td>
<td>0.207</td>
<td>0.461</td>
<td>0.514</td>
<td>0.345</td>
<td>30.5</td>
<td>32.8</td>
<td>25.1</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>SKATTN_2</td>
<td>Ryu2022</td>
<td>0.156</td>
<td>0.171</td>
<td>0.129</td>
<td>0.449</td>
<td>0.477</td>
<td>0.356</td>
<td>19.3</td>
<td>20.0</td>
<td>18.3</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.029</td>
<td>0.033</td>
<td>0.015</td>
<td>0.184</td>
<td>0.214</td>
<td>0.102</td>
<td>9.4</td>
<td>10.2</td>
<td>7.2</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.104</td>
<td>0.121</td>
<td>0.048</td>
<td>0.196</td>
<td>0.216</td>
<td>0.130</td>
<td>26.5</td>
<td>29.4</td>
<td>19.0</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>0.062</td>
<td>0.071</td>
<td>0.029</td>
<td>0.787</td>
<td>0.818</td>
<td>0.693</td>
<td>13.7</td>
<td>14.5</td>
<td>11.5</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi2022</td>
<td>0.439</td>
<td>0.489</td>
<td>0.324</td>
<td>0.611</td>
<td>0.656</td>
<td>0.498</td>
<td>49.7</td>
<td>53.0</td>
<td>40.4</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>PCO_task4_SED_C</td>
<td>Mizobuchi2022</td>
<td>0.197</td>
<td>0.218</td>
<td>0.164</td>
<td>0.620</td>
<td>0.660</td>
<td>0.517</td>
<td>21.8</td>
<td>24.7</td>
<td>15.3</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>PCO_task4_SED_A</td>
<td>Mizobuchi2022</td>
<td>0.398</td>
<td>0.450</td>
<td>0.285</td>
<td>0.571</td>
<td>0.617</td>
<td>0.452</td>
<td>47.6</td>
<td>50.4</td>
<td>39.5</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>0.421</td>
<td>0.470</td>
<td>0.314</td>
<td>0.664</td>
<td>0.724</td>
<td>0.524</td>
<td>49.6</td>
<td>53.4</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>Sojeong2022</td>
<td>0.423</td>
<td>0.476</td>
<td>0.308</td>
<td>0.651</td>
<td>0.707</td>
<td>0.509</td>
<td>50.4</td>
<td>55.2</td>
<td>38.0</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>train_ensemble1</td>
<td>Sojeong2022</td>
<td>0.390</td>
<td>0.437</td>
<td>0.284</td>
<td>0.620</td>
<td>0.678</td>
<td>0.488</td>
<td>48.1</td>
<td>52.5</td>
<td>37.3</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>train_ensemble2</td>
<td>Sojeong2022</td>
<td>0.415</td>
<td>0.467</td>
<td>0.299</td>
<td>0.634</td>
<td>0.698</td>
<td>0.486</td>
<td>48.1</td>
<td>52.8</td>
<td>36.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>0.315</td>
<td>0.360</td>
<td>0.222</td>
<td>0.543</td>
<td>0.591</td>
<td>0.403</td>
<td>37.3</td>
<td>40.8</td>
<td>29.7</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>SCRATCH</td>
<td>Dinkel2022</td>
<td>0.422</td>
<td>0.480</td>
<td>0.298</td>
<td>0.679</td>
<td>0.737</td>
<td>0.528</td>
<td>45.6</td>
<td>49.2</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>SMALL</td>
<td>Dinkel2022</td>
<td>0.373</td>
<td>0.421</td>
<td>0.250</td>
<td>0.613</td>
<td>0.663</td>
<td>0.459</td>
<td>39.3</td>
<td>42.9</td>
<td>29.6</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>0.104</td>
<td>0.119</td>
<td>0.086</td>
<td>0.824</td>
<td>0.855</td>
<td>0.736</td>
<td>14.2</td>
<td>14.9</td>
<td>12.5</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel2022</td>
<td>0.451</td>
<td>0.505</td>
<td>0.325</td>
<td>0.727</td>
<td>0.773</td>
<td>0.605</td>
<td>47.5</td>
<td>51.0</td>
<td>38.3</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>0.078</td>
<td>0.091</td>
<td>0.028</td>
<td>0.723</td>
<td>0.772</td>
<td>0.603</td>
<td>10.8</td>
<td>11.5</td>
<td>9.5</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>0.425</td>
<td>0.475</td>
<td>0.322</td>
<td>0.615</td>
<td>0.669</td>
<td>0.490</td>
<td>47.1</td>
<td>50.9</td>
<td>36.8</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>0.373</td>
<td>0.426</td>
<td>0.249</td>
<td>0.547</td>
<td>0.606</td>
<td>0.400</td>
<td>45.3</td>
<td>48.7</td>
<td>36.4</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.158</td>
<td>0.182</td>
<td>0.126</td>
<td>0.633</td>
<td>0.678</td>
<td>0.521</td>
<td>20.3</td>
<td>21.7</td>
<td>17.1</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.082</td>
<td>0.093</td>
<td>0.033</td>
<td>0.731</td>
<td>0.762</td>
<td>0.645</td>
<td>13.1</td>
<td>13.8</td>
<td>11.7</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.410</td>
<td>0.457</td>
<td>0.310</td>
<td>0.664</td>
<td>0.718</td>
<td>0.531</td>
<td>50.3</td>
<td>54.6</td>
<td>39.4</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.386</td>
<td>0.428</td>
<td>0.305</td>
<td>0.643</td>
<td>0.686</td>
<td>0.531</td>
<td>44.7</td>
<td>48.5</td>
<td>35.0</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>0.432</td>
<td>0.480</td>
<td>0.324</td>
<td>0.649</td>
<td>0.691</td>
<td>0.537</td>
<td>46.5</td>
<td>51.0</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>10-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>0.400</td>
<td>0.447</td>
<td>0.299</td>
<td>0.646</td>
<td>0.694</td>
<td>0.528</td>
<td>45.0</td>
<td>49.3</td>
<td>34.5</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>10-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>0.310</td>
<td>0.350</td>
<td>0.237</td>
<td>0.642</td>
<td>0.689</td>
<td>0.525</td>
<td>37.7</td>
<td>41.5</td>
<td>28.5</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>7-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>0.407</td>
<td>0.454</td>
<td>0.303</td>
<td>0.643</td>
<td>0.686</td>
<td>0.528</td>
<td>46.5</td>
<td>50.6</td>
<td>36.4</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>0.486</td>
<td>0.535</td>
<td>0.378</td>
<td>0.694</td>
<td>0.740</td>
<td>0.589</td>
<td>51.8</td>
<td>55.1</td>
<td>43.8</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>ATST-RCT SED system ATST small</td>
<td>Shao2022</td>
<td>0.476</td>
<td>0.524</td>
<td>0.377</td>
<td>0.666</td>
<td>0.713</td>
<td>0.555</td>
<td>51.6</td>
<td>56.1</td>
<td>41.0</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>ATST-RCT SED system ATST base</td>
<td>Shao2022</td>
<td>0.482</td>
<td>0.533</td>
<td>0.372</td>
<td>0.693</td>
<td>0.740</td>
<td>0.584</td>
<td>51.8</td>
<td>55.1</td>
<td>43.8</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Shao2022</td>
<td>0.368</td>
<td>0.409</td>
<td>0.283</td>
<td>0.594</td>
<td>0.644</td>
<td>0.474</td>
<td>45.0</td>
<td>49.0</td>
<td>35.5</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>0.500</td>
<td>0.551</td>
<td>0.383</td>
<td>0.695</td>
<td>0.738</td>
<td>0.582</td>
<td>55.3</td>
<td>57.6</td>
<td>49.3</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>0.514</td>
<td>0.559</td>
<td>0.406</td>
<td>0.713</td>
<td>0.756</td>
<td>0.598</td>
<td>55.9</td>
<td>59.0</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>0.510</td>
<td>0.555</td>
<td>0.399</td>
<td>0.711</td>
<td>0.752</td>
<td>0.599</td>
<td>55.5</td>
<td>58.8</td>
<td>46.9</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.215</td>
<td>0.239</td>
<td>0.135</td>
<td>0.335</td>
<td>0.358</td>
<td>0.254</td>
<td>31.6</td>
<td>34.7</td>
<td>23.1</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers2022</td>
<td>0.509</td>
<td>0.552</td>
<td>0.413</td>
<td>0.742</td>
<td>0.797</td>
<td>0.626</td>
<td>57.6</td>
<td>61.5</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.047</td>
<td>0.055</td>
<td>0.025</td>
<td>0.824</td>
<td>0.866</td>
<td>0.734</td>
<td>11.8</td>
<td>12.4</td>
<td>10.7</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.552</td>
<td>0.593</td>
<td>0.474</td>
<td>0.786</td>
<td>0.844</td>
<td>0.664</td>
<td>59.8</td>
<td>62.6</td>
<td>53.5</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>tag-conditioned CRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.527</td>
<td>0.568</td>
<td>0.444</td>
<td>0.679</td>
<td>0.729</td>
<td>0.566</td>
<td>65.9</td>
<td>70.1</td>
<td>56.3</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu2022</td>
<td>0.482</td>
<td>0.533</td>
<td>0.354</td>
<td>0.702</td>
<td>0.756</td>
<td>0.582</td>
<td>55.0</td>
<td>58.4</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>PANNs-FDY-CRNN-wrTCL system 1</td>
<td>Xu2022</td>
<td>0.452</td>
<td>0.500</td>
<td>0.338</td>
<td>0.662</td>
<td>0.702</td>
<td>0.552</td>
<td>51.7</td>
<td>54.7</td>
<td>43.5</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.054</td>
<td>0.064</td>
<td>0.022</td>
<td>0.774</td>
<td>0.799</td>
<td>0.699</td>
<td>13.1</td>
<td>14.2</td>
<td>10.7</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.049</td>
<td>0.057</td>
<td>0.018</td>
<td>0.738</td>
<td>0.771</td>
<td>0.637</td>
<td>12.8</td>
<td>13.5</td>
<td>11.4</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam2022</td>
<td>0.409</td>
<td>0.450</td>
<td>0.329</td>
<td>0.656</td>
<td>0.695</td>
<td>0.554</td>
<td>48.9</td>
<td>51.4</td>
<td>42.3</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>0.057</td>
<td>0.068</td>
<td>0.021</td>
<td>0.747</td>
<td>0.770</td>
<td>0.668</td>
<td>12.5</td>
<td>13.6</td>
<td>10.3</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>SED_4</td>
<td>Nam2022</td>
<td>0.055</td>
<td>0.066</td>
<td>0.016</td>
<td>0.747</td>
<td>0.770</td>
<td>0.673</td>
<td>12.7</td>
<td>13.6</td>
<td>10.9</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_1</td>
<td>Nam2022</td>
<td>0.404</td>
<td>0.446</td>
<td>0.317</td>
<td>0.653</td>
<td>0.686</td>
<td>0.558</td>
<td>49.8</td>
<td>52.7</td>
<td>42.4</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.293</td>
<td>0.337</td>
<td>0.200</td>
<td>0.527</td>
<td>0.590</td>
<td>0.391</td>
<td>37.9</td>
<td>41.8</td>
<td>28.3</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>0.365</td>
<td>0.395</td>
<td>0.289</td>
<td>0.584</td>
<td>0.621</td>
<td>0.494</td>
<td>39.5</td>
<td>43.2</td>
<td>30.5</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.069</td>
<td>0.084</td>
<td>0.030</td>
<td>0.728</td>
<td>0.765</td>
<td>0.645</td>
<td>13.9</td>
<td>14.5</td>
<td>12.6</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li2022b</td>
<td>0.480</td>
<td>0.541</td>
<td>0.347</td>
<td>0.713</td>
<td>0.760</td>
<td>0.585</td>
<td>55.1</td>
<td>59.9</td>
<td>42.8</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Mean teacher Pseudo labeling system 4</td>
<td>Li2022b</td>
<td>0.429</td>
<td>0.487</td>
<td>0.305</td>
<td>0.723</td>
<td>0.763</td>
<td>0.614</td>
<td>52.4</td>
<td>56.7</td>
<td>41.5</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>0.451</td>
<td>0.514</td>
<td>0.320</td>
<td>0.740</td>
<td>0.776</td>
<td>0.634</td>
<td>53.8</td>
<td>58.2</td>
<td>42.8</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Mean teacher Pseudo labeling system 3</td>
<td>Li2022b</td>
<td>0.450</td>
<td>0.507</td>
<td>0.329</td>
<td>0.699</td>
<td>0.745</td>
<td>0.576</td>
<td>53.1</td>
<td>58.0</td>
<td>40.7</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.318</td>
<td>0.352</td>
<td>0.244</td>
<td>0.520</td>
<td>0.563</td>
<td>0.406</td>
<td>37.7</td>
<td>40.4</td>
<td>31.3</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>0.053</td>
<td>0.063</td>
<td>0.024</td>
<td>0.810</td>
<td>0.839</td>
<td>0.729</td>
<td>14.3</td>
<td>14.8</td>
<td>13.2</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>DCASE2022 SED mean teacher system 2</td>
<td>He2022</td>
<td>0.503</td>
<td>0.551</td>
<td>0.392</td>
<td>0.749</td>
<td>0.798</td>
<td>0.639</td>
<td>54.5</td>
<td>58.5</td>
<td>44.4</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He2022</td>
<td>0.525</td>
<td>0.578</td>
<td>0.401</td>
<td>0.748</td>
<td>0.795</td>
<td>0.634</td>
<td>55.7</td>
<td>59.7</td>
<td>45.6</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>DCASE2022 SED mean teacher system 1</td>
<td>He2022</td>
<td>0.454</td>
<td>0.503</td>
<td>0.338</td>
<td>0.696</td>
<td>0.744</td>
<td>0.596</td>
<td>53.6</td>
<td>58.0</td>
<td>42.2</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.090</td>
<td>0.095</td>
<td>0.062</td>
<td>0.709</td>
<td>0.747</td>
<td>0.581</td>
<td>9.4</td>
<td>10.3</td>
<td>7.2</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.075</td>
<td>0.085</td>
<td>0.044</td>
<td>0.692</td>
<td>0.731</td>
<td>0.570</td>
<td>9.0</td>
<td>10.3</td>
<td>5.4</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.439</td>
<td>0.486</td>
<td>0.321</td>
<td>0.612</td>
<td>0.649</td>
<td>0.508</td>
<td>29.3</td>
<td>32.0</td>
<td>20.9</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.411</td>
<td>0.453</td>
<td>0.312</td>
<td>0.597</td>
<td>0.635</td>
<td>0.488</td>
<td>34.6</td>
<td>38.5</td>
<td>24.0</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>0.062</td>
<td>0.074</td>
<td>0.021</td>
<td>0.800</td>
<td>0.825</td>
<td>0.719</td>
<td>13.7</td>
<td>14.2</td>
<td>12.6</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>CBAM-T CRNN scratch</td>
<td>Xie2022</td>
<td>0.300</td>
<td>0.335</td>
<td>0.207</td>
<td>0.641</td>
<td>0.695</td>
<td>0.502</td>
<td>38.3</td>
<td>41.8</td>
<td>29.1</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>CBAM-T CRNN 1</td>
<td>Xie2022</td>
<td>0.418</td>
<td>0.463</td>
<td>0.323</td>
<td>0.757</td>
<td>0.815</td>
<td>0.626</td>
<td>52.7</td>
<td>57.3</td>
<td>41.1</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie2022</td>
<td>0.426</td>
<td>0.474</td>
<td>0.333</td>
<td>0.766</td>
<td>0.829</td>
<td>0.630</td>
<td>54.7</td>
<td>58.8</td>
<td>44.3</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>0.345</td>
<td>0.387</td>
<td>0.254</td>
<td>0.540</td>
<td>0.592</td>
<td>0.414</td>
<td>41.1</td>
<td>44.5</td>
<td>32.5</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>0.317</td>
<td>0.361</td>
<td>0.217</td>
<td>0.565</td>
<td>0.619</td>
<td>0.425</td>
<td>42.4</td>
<td>46.5</td>
<td>33.0</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim2022c</td>
<td>0.340</td>
<td>0.388</td>
<td>0.230</td>
<td>0.544</td>
<td>0.604</td>
<td>0.400</td>
<td>41.1</td>
<td>45.5</td>
<td>31.3</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>DCASE2022 SED system3</td>
<td>Kim2022c</td>
<td>0.338</td>
<td>0.381</td>
<td>0.245</td>
<td>0.554</td>
<td>0.603</td>
<td>0.426</td>
<td>42.4</td>
<td>46.4</td>
<td>32.7</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>DCASE2022 SED system 1</td>
<td>Li2022c</td>
<td>0.364</td>
<td>0.411</td>
<td>0.265</td>
<td>0.570</td>
<td>0.623</td>
<td>0.444</td>
<td>44.9</td>
<td>48.7</td>
<td>35.4</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li2022c</td>
<td>0.371</td>
<td>0.408</td>
<td>0.280</td>
<td>0.635</td>
<td>0.688</td>
<td>0.521</td>
<td>47.8</td>
<td>51.9</td>
<td>37.8</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>0.195</td>
<td>0.222</td>
<td>0.158</td>
<td>0.683</td>
<td>0.740</td>
<td>0.537</td>
<td>27.8</td>
<td>29.8</td>
<td>23.3</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>DCASE2022 SED system 2</td>
<td>Li2022c</td>
<td>0.086</td>
<td>0.101</td>
<td>0.060</td>
<td>0.671</td>
<td>0.713</td>
<td>0.561</td>
<td>15.0</td>
<td>15.4</td>
<td>14.6</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Strong and Max-Weak balanced</td>
<td>Castorena2022</td>
<td>0.267</td>
<td>0.299</td>
<td>0.184</td>
<td>0.531</td>
<td>0.577</td>
<td>0.405</td>
<td>32.8</td>
<td>35.7</td>
<td>25.0</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena2022</td>
<td>0.334</td>
<td>0.365</td>
<td>0.256</td>
<td>0.524</td>
<td>0.558</td>
<td>0.420</td>
<td>39.2</td>
<td>43.2</td>
<td>29.0</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>0.072</td>
<td>0.073</td>
<td>0.076</td>
<td>0.559</td>
<td>0.588</td>
<td>0.460</td>
<td>11.2</td>
<td>12.1</td>
<td>9.2</td>
</tr>
</tbody>
</table>
<h2 id="without-external-resources">Without external resources</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS1_all" data-scatter-y="PSDS1_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Development dataset)" data-chartable="true" data-field="PSDS1_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Development dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Development dataset)" data-chartable="true" data-field="PSDS2_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiao_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
<td>0.431</td>
<td>0.645</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
<td>0.437</td>
<td>0.680</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>DCASE2022_CRNN_ADJ</td>
<td>Cheng2022</td>
<td>0.93</td>
<td>0.276</td>
<td>0.543</td>
<td>0.356</td>
<td>0.601</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
<td>0.362</td>
<td>0.635</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
<td>0.037</td>
<td>0.244</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>DCASE2022 Kim system 1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
<td>0.473</td>
<td>0.693</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.695</td>
<td>0.473</td>
<td>0.695</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
<td>0.046</td>
<td>0.165</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>PCO_task4_SED_A</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>0.398</td>
<td>0.571</td>
<td>0.425</td>
<td>0.625</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.421</td>
<td>0.664</td>
<td>0.422</td>
<td>0.667</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>0.423</td>
<td>0.651</td>
<td>0.480</td>
<td>0.726</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>train_ensemble1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>0.390</td>
<td>0.620</td>
<td>0.434</td>
<td>0.675</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>train_ensemble2</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>0.415</td>
<td>0.634</td>
<td>0.494</td>
<td>0.748</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
<td>0.342</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>SCRATCH</td>
<td>Dinkel2022</td>
<td>1.29</td>
<td>0.422</td>
<td>0.679</td>
<td>0.456</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>SMALL</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>0.373</td>
<td>0.613</td>
<td>0.395</td>
<td>0.631</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>0.104</td>
<td>0.824</td>
<td>0.126</td>
<td>0.877</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>0.451</td>
<td>0.727</td>
<td>0.482</td>
<td>0.757</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>0.78</td>
<td>0.078</td>
<td>0.723</td>
<td>0.448</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.24</td>
<td>0.425</td>
<td>0.615</td>
<td>0.448</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.09</td>
<td>0.373</td>
<td>0.547</td>
<td>0.448</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
<td>0.474</td>
<td>0.730</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
<td>0.428</td>
<td>0.655</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>10-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.400</td>
<td>0.646</td>
<td>0.410</td>
<td>0.665</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>10-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>0.310</td>
<td>0.642</td>
<td>0.347</td>
<td>0.663</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>7-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.407</td>
<td>0.643</td>
<td>0.422</td>
<td>0.656</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
<td>0.398</td>
<td>0.611</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>0.500</td>
<td>0.695</td>
<td>0.452</td>
<td>0.682</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
<td>0.458</td>
<td>0.688</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>0.510</td>
<td>0.711</td>
<td>0.456</td>
<td>0.685</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
<td>0.492</td>
<td>0.721</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
<td>0.058</td>
<td>0.813</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>0.409</td>
<td>0.656</td>
<td>0.470</td>
<td>0.700</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.057</td>
<td>0.747</td>
<td>0.061</td>
<td>0.822</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.055</td>
<td>0.747</td>
<td>0.058</td>
<td>0.820</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>0.404</td>
<td>0.653</td>
<td>0.470</td>
<td>0.687</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
<td>0.341</td>
<td>0.596</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Mean teacher Pseudo labeling system 4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>0.429</td>
<td>0.723</td>
<td>0.436</td>
<td>0.778</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Mean teacher Pseudo labeling system 3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>0.450</td>
<td>0.699</td>
<td>0.456</td>
<td>0.726</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
<td>0.356</td>
<td>0.554</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>CBAM-T CRNN scratch</td>
<td>Xie2022</td>
<td>1.06</td>
<td>0.300</td>
<td>0.641</td>
<td>0.360</td>
<td>0.674</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>0.317</td>
<td>0.565</td>
<td>0.372</td>
<td>0.592</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.340</td>
<td>0.544</td>
<td>0.377</td>
<td>0.585</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>DCASE2022 SED system3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.338</td>
<td>0.554</td>
<td>0.373</td>
<td>0.571</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>DCASE2022 SED system 2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>0.086</td>
<td>0.671</td>
<td>0.095</td>
<td>0.754</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>DCASE2022 SED system 1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>0.364</td>
<td>0.570</td>
<td>0.408</td>
<td>0.607</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.75</td>
<td>0.075</td>
<td>0.692</td>
<td>0.099</td>
<td>0.783</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.20</td>
<td>0.411</td>
<td>0.597</td>
<td>0.420</td>
<td>0.618</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Strong and Max-Weak balanced</td>
<td>Castorena2022</td>
<td>0.91</td>
<td>0.267</td>
<td>0.531</td>
<td>0.305</td>
<td>0.587</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena2022</td>
<td>1.01</td>
<td>0.334</td>
<td>0.524</td>
<td>0.343</td>
<td>0.538</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>0.63</td>
<td>0.072</td>
<td>0.559</td>
<td>0.067</td>
<td>0.641</td>
</tr>
</tbody>
</table>
<h2 id="with-external-resources">With external resources</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS1_all" data-scatter-y="PSDS1_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Development dataset)" data-chartable="true" data-field="PSDS1_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Development dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Development dataset)" data-chartable="true" data-field="PSDS2_dev" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>0.484</td>
<td>0.697</td>
<td>0.481</td>
<td>0.694</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>DCASE2022 pretrained system 1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>0.472</td>
<td>0.700</td>
<td>0.475</td>
<td>0.688</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>0.049</td>
<td>0.784</td>
<td>0.051</td>
<td>0.826</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>DCASE2022 PANNs SED 2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>0.000</td>
<td>0.063</td>
<td>0.451</td>
<td>0.734</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>DCASE2022 PANNs SED 3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>0.070</td>
<td>0.194</td>
<td>0.457</td>
<td>0.767</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>0.81</td>
<td>0.062</td>
<td>0.774</td>
<td>0.063</td>
<td>0.814</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>rtzr_strong-real</td>
<td>Suh2022</td>
<td>1.39</td>
<td>0.458</td>
<td>0.721</td>
<td>0.473</td>
<td>0.723</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh2022</td>
<td>1.42</td>
<td>0.478</td>
<td>0.719</td>
<td>0.445</td>
<td>0.704</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>DCASE2022 task4 Pre-Trained 2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>0.129</td>
<td>0.758</td>
<td>0.177</td>
<td>0.801</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>0.051</td>
<td>0.777</td>
<td>0.067</td>
<td>0.827</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu2022</td>
<td>1.25</td>
<td>0.425</td>
<td>0.634</td>
<td>0.443</td>
<td>0.660</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>0.062</td>
<td>0.781</td>
<td>0.068</td>
<td>0.830</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>DCASE2022 Kim system 4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>0.305</td>
<td>0.750</td>
<td>0.354</td>
<td>0.756</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
<td>0.269</td>
<td>0.446</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>SKATTN_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>0.156</td>
<td>0.449</td>
<td>0.161</td>
<td>0.452</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
<td>0.129</td>
<td>0.241</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>0.82</td>
<td>0.062</td>
<td>0.787</td>
<td>0.075</td>
<td>0.852</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi2022</td>
<td>1.26</td>
<td>0.439</td>
<td>0.611</td>
<td>0.449</td>
<td>0.662</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>PCO_task4_SED_C</td>
<td>Mizobuchi2022</td>
<td>0.88</td>
<td>0.197</td>
<td>0.620</td>
<td>0.231</td>
<td>0.714</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>0.104</td>
<td>0.824</td>
<td>0.126</td>
<td>0.877</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>0.451</td>
<td>0.727</td>
<td>0.482</td>
<td>0.757</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>0.158</td>
<td>0.633</td>
<td>0.088</td>
<td>0.837</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>0.082</td>
<td>0.731</td>
<td>0.102</td>
<td>0.840</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>0.410</td>
<td>0.664</td>
<td>0.472</td>
<td>0.721</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
<td>0.477</td>
<td>0.734</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>ATST-RCT SED system ATST small</td>
<td>Shao2022</td>
<td>1.36</td>
<td>0.476</td>
<td>0.666</td>
<td>0.460</td>
<td>0.698</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>ATST-RCT SED system ATST base</td>
<td>Shao2022</td>
<td>1.40</td>
<td>0.482</td>
<td>0.693</td>
<td>0.468</td>
<td>0.702</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
<td>0.459</td>
<td>0.744</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>0.047</td>
<td>0.824</td>
<td>0.080</td>
<td>0.868</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>0.552</td>
<td>0.786</td>
<td>0.512</td>
<td>0.772</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>tag-conditioned CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>0.527</td>
<td>0.679</td>
<td>0.483</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>0.482</td>
<td>0.702</td>
<td>0.485</td>
<td>0.725</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>PANNs-FDY-CRNN-wrTCL system 1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>0.452</td>
<td>0.662</td>
<td>0.481</td>
<td>0.710</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.79</td>
<td>0.054</td>
<td>0.774</td>
<td>0.065</td>
<td>0.835</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>0.365</td>
<td>0.584</td>
<td>0.374</td>
<td>0.583</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>0.069</td>
<td>0.728</td>
<td>0.070</td>
<td>0.794</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>0.480</td>
<td>0.713</td>
<td>0.479</td>
<td>0.735</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>0.451</td>
<td>0.740</td>
<td>0.462</td>
<td>0.785</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>0.82</td>
<td>0.053</td>
<td>0.810</td>
<td>0.071</td>
<td>0.857</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>DCASE2022 SED mean teacher system 2</td>
<td>He2022</td>
<td>1.48</td>
<td>0.503</td>
<td>0.749</td>
<td>0.521</td>
<td>0.771</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He2022</td>
<td>1.52</td>
<td>0.525</td>
<td>0.748</td>
<td>0.533</td>
<td>0.762</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>DCASE2022 SED mean teacher system 1</td>
<td>He2022</td>
<td>1.36</td>
<td>0.454</td>
<td>0.696</td>
<td>0.474</td>
<td>0.692</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.79</td>
<td>0.090</td>
<td>0.709</td>
<td>0.115</td>
<td>0.816</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.26</td>
<td>0.439</td>
<td>0.612</td>
<td>0.449</td>
<td>0.645</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>0.83</td>
<td>0.062</td>
<td>0.800</td>
<td>0.072</td>
<td>0.856</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>CBAM-T CRNN 1</td>
<td>Xie2022</td>
<td>1.36</td>
<td>0.418</td>
<td>0.757</td>
<td>0.460</td>
<td>0.768</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie2022</td>
<td>1.38</td>
<td>0.426</td>
<td>0.766</td>
<td>0.460</td>
<td>0.768</td>
</tr>
<tr class="info">
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
<td>0.342</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>0.195</td>
<td>0.683</td>
<td>0.215</td>
<td>0.735</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>0.371</td>
<td>0.635</td>
<td>0.398</td>
<td>0.640</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best ranking score per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code_1" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS_1_all" data-scatter-y="PSDS_2_all" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code_1" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 1)
            </th>
<th class="sep-right-cell sm-cell" data-field="name_1" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 1)
            </th>
<th data-field="code_2" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 2)
            </th>
<th class="sm-cell" data-field="name_2" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 2)
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>1.49</td>
<td>0.484</td>
<td>0.784</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>1.47</td>
<td>0.478</td>
<td>0.774</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>1.38</td>
<td>0.425</td>
<td>0.777</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>1.42</td>
<td>0.444</td>
<td>0.781</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>1.42</td>
<td>0.439</td>
<td>0.787</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.423</td>
<td>0.664</td>
</tr>
<tr>
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>1.47</td>
<td>0.451</td>
<td>0.824</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.34</td>
<td>0.425</td>
<td>0.723</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.32</td>
<td>0.410</td>
<td>0.731</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.63</td>
<td>0.552</td>
<td>0.824</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>1.47</td>
<td>0.482</td>
<td>0.774</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>1.33</td>
<td>0.409</td>
<td>0.747</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>1.25</td>
<td>0.365</td>
<td>0.728</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.44</td>
<td>0.480</td>
<td>0.740</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>1.57</td>
<td>0.525</td>
<td>0.810</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.35</td>
<td>0.439</td>
<td>0.709</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>1.41</td>
<td>0.426</td>
<td>0.800</td>
</tr>
<tr>
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.06</td>
<td>0.340</td>
<td>0.565</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>1.21</td>
<td>0.371</td>
<td>0.683</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>1.04</td>
<td>0.334</td>
<td>0.559</td>
</tr>
</tbody>
</table>
<h2 id="supplementary-metrics-1">Supplementary metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code_1" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="ranking_score_youtube" data-scatter-y="ranking_score_vimeo" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code_1" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 1)
            </th>
<th class="sep-right-cell sm-cell" data-field="name_1" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 1)
            </th>
<th data-field="code_2" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 2)
            </th>
<th class="sm-cell" data-field="name_2" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 2)
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Evaluation dataset)
            </th>
<th class="text-center" data-axis-label="Ranking score (Public evaluation)" data-chartable="true" data-field="ranking_score_youtube" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Public evaluation)
            </th>
<th class="sep-right-cell text-center" data-axis-label="Ranking score (Vimeo dataset)" data-chartable="true" data-field="ranking_score_vimeo" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Vimeo dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>1.49</td>
<td>1.43</td>
<td>1.69</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>1.26</td>
<td>1.37</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>1.47</td>
<td>1.39</td>
<td>1.71</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>1.01</td>
<td>1.11</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>1.38</td>
<td>1.33</td>
<td>1.57</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>1.42</td>
<td>1.38</td>
<td>1.60</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.82</td>
<td>0.89</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.35</td>
<td>0.27</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>1.42</td>
<td>1.37</td>
<td>1.58</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>1.27</td>
<td>1.34</td>
</tr>
<tr>
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>1.47</td>
<td>1.42</td>
<td>1.64</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.34</td>
<td>1.31</td>
<td>1.47</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.32</td>
<td>1.28</td>
<td>1.49</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>1.25</td>
<td>1.39</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>1.37</td>
<td>1.58</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>1.41</td>
<td>1.65</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.63</td>
<td>1.55</td>
<td>1.97</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>1.47</td>
<td>1.41</td>
<td>1.66</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>1.33</td>
<td>1.27</td>
<td>1.56</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>1.25</td>
<td>1.19</td>
<td>1.45</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.44</td>
<td>1.40</td>
<td>1.56</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.96</td>
<td>1.05</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>1.57</td>
<td>1.51</td>
<td>1.80</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.35</td>
<td>1.30</td>
<td>1.44</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>1.41</td>
<td>1.35</td>
<td>1.64</td>
</tr>
<tr>
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>1.04</td>
<td>1.08</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.06</td>
<td>1.06</td>
<td>1.04</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>1.21</td>
<td>1.19</td>
<td>1.29</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>1.04</td>
<td>1.00</td>
<td>1.14</td>
</tr>
</tbody>
</table>
<h2 id="without-external-resources-1">Without external resources</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code_1" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS_1_all" data-scatter-y="PSDS_2_all" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code_1" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 1)
            </th>
<th class="sep-right-cell sm-cell" data-field="name_1" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 1)
            </th>
<th data-field="code_2" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 2)
            </th>
<th class="sm-cell" data-field="name_2" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 2)
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Xiao_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim_LGE_task4_1</td>
<td>DCASE2022 Kim system 1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>PCO_task4_SED_A</td>
<td>Mizobuchi_PCO_task4_1</td>
<td>PCO_task4_SED_A</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>0.398</td>
<td>0.571</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.423</td>
<td>0.664</td>
</tr>
<tr>
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>1.47</td>
<td>0.451</td>
<td>0.824</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.34</td>
<td>0.425</td>
<td>0.723</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>1.33</td>
<td>0.409</td>
<td>0.747</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Mean teacher Pseudo labeling system 3</td>
<td>Li_USTC_task4_SED_4</td>
<td>Mean teacher Pseudo labeling system 4</td>
<td>Li2022b</td>
<td>1.38</td>
<td>0.450</td>
<td>0.723</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>CBAM-T CRNN scratch</td>
<td>Xie_UESTC_task4_3</td>
<td>CBAM-T CRNN scratch</td>
<td>Xie2022</td>
<td>1.06</td>
<td>0.300</td>
<td>0.641</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.06</td>
<td>0.340</td>
<td>0.565</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>DCASE2022 SED system 1</td>
<td>Li_XJU_task4_2</td>
<td>DCASE2022 SED system 2</td>
<td>Li2022c</td>
<td>1.19</td>
<td>0.364</td>
<td>0.671</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.29</td>
<td>0.411</td>
<td>0.692</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>1.04</td>
<td>0.334</td>
<td>0.559</td>
</tr>
</tbody>
</table>
<h2 id="with-external-resources-1">With external resources</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code_1" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="PSDS_1_all" data-scatter-y="PSDS_2_all" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="ranking_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code_1" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 1)
            </th>
<th class="sep-right-cell sm-cell" data-field="name_1" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 1)
            </th>
<th data-field="code_2" data-sortable="true">
                Submission <br/>code<br/>
                (PSDS 2)
            </th>
<th class="sm-cell" data-field="name_2" data-sortable="true">
                Submission <br/>name<br/>
                (PSDS 2)
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>1.49</td>
<td>0.484</td>
<td>0.784</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>1.47</td>
<td>0.478</td>
<td>0.774</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>1.38</td>
<td>0.425</td>
<td>0.777</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>DCASE2022 Kim system 4</td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>1.20</td>
<td>0.305</td>
<td>0.781</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>1.42</td>
<td>0.439</td>
<td>0.787</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>1.47</td>
<td>0.451</td>
<td>0.824</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.32</td>
<td>0.410</td>
<td>0.731</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.63</td>
<td>0.552</td>
<td>0.824</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>1.47</td>
<td>0.482</td>
<td>0.774</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>1.25</td>
<td>0.365</td>
<td>0.728</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.44</td>
<td>0.480</td>
<td>0.740</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>1.57</td>
<td>0.525</td>
<td>0.810</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.35</td>
<td>0.439</td>
<td>0.709</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>1.41</td>
<td>0.426</td>
<td>0.800</td>
</tr>
<tr>
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>1.21</td>
<td>0.371</td>
<td>0.683</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="Baseline SED" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Baseline SSep_SED" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["Class_f_score_Alarm_bell_ringing", "Class_f_score_Blender", "Class_f_score_Cat", "Class_f_score_Dishes", "Class_f_score_Dog", "Class_f_score_Electric_shaver_toothbrush", "Class_f_score_Frying", "Class_f_score_Running_water", "Class_f_score_Speech", "Class_f_score_Vacuum_cleaner"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="Class_f_score_Frying" data-scatter-y="Class_f_score_Running_water" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission<br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission<br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="Class_f_score_Alarm_bell_ringing" data-sortable="true" data-value-type="float1-percentage">
                Alarm<br/>Bell<br/>Ringing
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Blender" data-sortable="true" data-value-type="float1-percentage">
                Blender
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Cat" data-sortable="true" data-value-type="float1-percentage">
                Cat
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dishes" data-sortable="true" data-value-type="float1-percentage">
                Dishes
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dog" data-sortable="true" data-value-type="float1-percentage">
                Dog
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Electric_shaver_toothbrush" data-sortable="true" data-value-type="float1-percentage">
                Electric<br/>shave<br/>toothbrush
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Frying" data-sortable="true" data-value-type="float1-percentage">
                Frying
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Running_water" data-sortable="true" data-value-type="float1-percentage">
                Running<br/>water
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Speech" data-sortable="true" data-value-type="float1-percentage">
                Speech
            </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Vacuum_cleaner" data-sortable="true" data-value-type="float1-percentage">
                Vacuum<br/>cleaner
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>56.4</td>
<td>66.1</td>
<td>72.2</td>
<td>40.8</td>
<td>45.4</td>
<td>58.2</td>
<td>54.6</td>
<td>39.9</td>
<td>62.4</td>
<td>69.2</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>DCASE2022 pretrained system 1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>58.8</td>
<td>61.9</td>
<td>72.8</td>
<td>42.9</td>
<td>47.2</td>
<td>61.3</td>
<td>49.8</td>
<td>35.9</td>
<td>65.2</td>
<td>66.4</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>45.5</td>
<td>54.5</td>
<td>70.3</td>
<td>39.3</td>
<td>49.8</td>
<td>48.8</td>
<td>43.3</td>
<td>36.4</td>
<td>62.4</td>
<td>62.5</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>6.6</td>
<td>6.1</td>
<td>0.9</td>
<td>0.0</td>
<td>0.3</td>
<td>15.2</td>
<td>52.1</td>
<td>29.8</td>
<td>0.3</td>
<td>38.7</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>DCASE2022 PANNs SED 2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>13.1</td>
<td>1.6</td>
<td>11.0</td>
<td>0.0</td>
<td>29.5</td>
<td></td>
<td></td>
<td>1.0</td>
<td>49.1</td>
<td>0.0</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>DCASE2022 PANNs SED 3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>11.3</td>
<td>1.6</td>
<td>17.5</td>
<td>0.5</td>
<td>14.1</td>
<td>0.0</td>
<td></td>
<td>0.0</td>
<td>38.4</td>
<td>0.0</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>45.1</td>
<td>48.9</td>
<td>65.5</td>
<td>35.8</td>
<td>47.8</td>
<td>54.7</td>
<td>41.3</td>
<td>32.5</td>
<td>70.3</td>
<td>34.3</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>DCASE2022 PANNs SED 4</td>
<td>Liu2022</td>
<td>0.21</td>
<td>8.5</td>
<td>1.6</td>
<td>15.0</td>
<td>0.0</td>
<td>11.2</td>
<td></td>
<td></td>
<td>0.0</td>
<td>37.5</td>
<td>1.5</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh2022</td>
<td>1.22</td>
<td>26.0</td>
<td>53.5</td>
<td>71.7</td>
<td>40.3</td>
<td>45.1</td>
<td>39.8</td>
<td>46.0</td>
<td>33.6</td>
<td>52.6</td>
<td>59.6</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>0.81</td>
<td>5.8</td>
<td>2.8</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>14.7</td>
<td>49.2</td>
<td>18.5</td>
<td>0.2</td>
<td>37.0</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>rtzr_strong-real</td>
<td>Suh2022</td>
<td>1.39</td>
<td>37.8</td>
<td>65.3</td>
<td>77.9</td>
<td>44.5</td>
<td>45.6</td>
<td>53.4</td>
<td>56.4</td>
<td>33.8</td>
<td>56.4</td>
<td>60.1</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh2022</td>
<td>1.42</td>
<td>39.7</td>
<td>62.9</td>
<td>77.8</td>
<td>47.1</td>
<td>46.0</td>
<td>52.5</td>
<td>63.0</td>
<td>32.8</td>
<td>55.1</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>DCASE2022_CRNN_ADJ</td>
<td>Cheng2022</td>
<td>0.93</td>
<td>31.2</td>
<td>39.8</td>
<td>67.4</td>
<td>32.8</td>
<td>32.5</td>
<td>41.2</td>
<td>46.2</td>
<td>29.5</td>
<td>53.8</td>
<td>34.4</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>DCASE2022_CRNN_IMP</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>31.6</td>
<td>48.9</td>
<td>65.6</td>
<td>28.6</td>
<td>24.0</td>
<td>45.6</td>
<td>45.8</td>
<td>32.4</td>
<td>51.0</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>DCASE2022 task4 Pre-Trained 2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>10.9</td>
<td>20.8</td>
<td>2.3</td>
<td>0.7</td>
<td>1.7</td>
<td>19.7</td>
<td>58.3</td>
<td>27.1</td>
<td>4.2</td>
<td>47.8</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>4.8</td>
<td>4.5</td>
<td>0.9</td>
<td>0.0</td>
<td>0.3</td>
<td>15.5</td>
<td>49.2</td>
<td>21.8</td>
<td>0.2</td>
<td>38.9</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu2022</td>
<td>0.24</td>
<td>2.7</td>
<td>2.2</td>
<td>0.0</td>
<td></td>
<td></td>
<td></td>
<td>29.6</td>
<td></td>
<td>0.2</td>
<td>17.7</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu2022</td>
<td>1.25</td>
<td>36.3</td>
<td>58.7</td>
<td>69.8</td>
<td>40.6</td>
<td>48.4</td>
<td>36.4</td>
<td>49.6</td>
<td>27.3</td>
<td>69.2</td>
<td>56.9</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>DCASE2022 Kim system 1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>36.8</td>
<td>52.5</td>
<td>73.3</td>
<td>46.0</td>
<td>45.1</td>
<td>38.6</td>
<td>50.8</td>
<td>30.3</td>
<td>70.2</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>3.3</td>
<td>2.9</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>11.8</td>
<td>50.2</td>
<td>22.5</td>
<td>0.2</td>
<td>36.5</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>DCASE2022 Kim system 4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>12.2</td>
<td>34.1</td>
<td>12.0</td>
<td>8.8</td>
<td>4.3</td>
<td>17.9</td>
<td>50.2</td>
<td>28.1</td>
<td>47.6</td>
<td>58.8</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>36.8</td>
<td>52.8</td>
<td>73.3</td>
<td>46.1</td>
<td>45.1</td>
<td>39.3</td>
<td>50.8</td>
<td>30.2</td>
<td>70.2</td>
<td>66.9</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>23.7</td>
<td>30.1</td>
<td>39.9</td>
<td>1.1</td>
<td>15.1</td>
<td>36.2</td>
<td>46.3</td>
<td>29.1</td>
<td>47.9</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>SKATTN_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>11.3</td>
<td>4.4</td>
<td>18.4</td>
<td>10.1</td>
<td>5.7</td>
<td>16.8</td>
<td>38.9</td>
<td>18.1</td>
<td>36.8</td>
<td>33.0</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>4.5</td>
<td>11.2</td>
<td>6.5</td>
<td>2.5</td>
<td>2.0</td>
<td>8.5</td>
<td>17.4</td>
<td>9.6</td>
<td>3.9</td>
<td>27.9</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>22.9</td>
<td>35.1</td>
<td>29.2</td>
<td>19.5</td>
<td>11.5</td>
<td>21.0</td>
<td>27.9</td>
<td>15.9</td>
<td>45.5</td>
<td>37.0</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>PCO_task4_SED_D</td>
<td>Mizobuchi2022</td>
<td>0.82</td>
<td>3.9</td>
<td>5.5</td>
<td>0.9</td>
<td>0.0</td>
<td>0.3</td>
<td>14.5</td>
<td>47.5</td>
<td>23.9</td>
<td>0.2</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>PCO_task4_SED_B</td>
<td>Mizobuchi2022</td>
<td>1.26</td>
<td>46.5</td>
<td>44.4</td>
<td>71.4</td>
<td>40.8</td>
<td>43.5</td>
<td>44.8</td>
<td>45.4</td>
<td>37.0</td>
<td>64.7</td>
<td>58.5</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>PCO_task4_SED_C</td>
<td>Mizobuchi2022</td>
<td>0.88</td>
<td>11.8</td>
<td>34.8</td>
<td>21.4</td>
<td>1.6</td>
<td>2.5</td>
<td>34.8</td>
<td>27.8</td>
<td>30.4</td>
<td>12.8</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>PCO_task4_SED_A</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>34.6</td>
<td>47.5</td>
<td>69.4</td>
<td>36.5</td>
<td>48.3</td>
<td>40.5</td>
<td>49.4</td>
<td>38.8</td>
<td>61.0</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>43.1</td>
<td>53.5</td>
<td>70.8</td>
<td>33.1</td>
<td>44.3</td>
<td>42.9</td>
<td>50.4</td>
<td>35.3</td>
<td>62.9</td>
<td>59.6</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>42.7</td>
<td>58.2</td>
<td>68.7</td>
<td>31.2</td>
<td>43.1</td>
<td>55.3</td>
<td>48.9</td>
<td>32.6</td>
<td>61.3</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>train_ensemble1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>42.5</td>
<td>53.6</td>
<td>69.6</td>
<td>29.8</td>
<td>44.1</td>
<td>43.2</td>
<td>42.7</td>
<td>37.0</td>
<td>61.2</td>
<td>57.5</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>train_ensemble2</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>39.9</td>
<td>58.5</td>
<td>68.6</td>
<td>32.0</td>
<td>39.9</td>
<td>48.4</td>
<td>49.4</td>
<td>32.0</td>
<td>59.1</td>
<td>53.3</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>32.2</td>
<td>39.0</td>
<td>62.4</td>
<td>28.6</td>
<td>34.5</td>
<td>21.1</td>
<td>37.2</td>
<td>26.4</td>
<td>49.7</td>
<td>42.0</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>SCRATCH</td>
<td>Dinkel2022</td>
<td>1.29</td>
<td>36.7</td>
<td>51.9</td>
<td>61.1</td>
<td>30.9</td>
<td>40.8</td>
<td>47.9</td>
<td>50.4</td>
<td>29.8</td>
<td>60.2</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>SMALL</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>36.8</td>
<td>37.6</td>
<td>57.3</td>
<td>28.2</td>
<td>39.2</td>
<td>29.1</td>
<td>46.1</td>
<td>25.6</td>
<td>58.3</td>
<td>34.4</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>TAG</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>4.4</td>
<td>4.6</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>13.5</td>
<td>53.1</td>
<td>24.2</td>
<td>0.4</td>
<td>41.0</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>PRECISE</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>36.5</td>
<td>55.6</td>
<td>65.0</td>
<td>35.0</td>
<td>41.7</td>
<td>48.2</td>
<td>56.0</td>
<td>33.6</td>
<td>51.4</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>0.78</td>
<td>3.5</td>
<td>4.4</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>15.2</td>
<td>29.6</td>
<td>18.3</td>
<td>0.3</td>
<td>35.8</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.24</td>
<td>49.8</td>
<td>46.2</td>
<td>72.1</td>
<td>28.7</td>
<td>47.4</td>
<td>49.6</td>
<td>25.8</td>
<td>27.6</td>
<td>65.1</td>
<td>58.7</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.09</td>
<td>41.6</td>
<td>46.2</td>
<td>71.6</td>
<td>29.7</td>
<td>45.5</td>
<td>42.9</td>
<td>25.7</td>
<td>27.5</td>
<td>64.5</td>
<td>58.4</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>18.8</td>
<td>14.9</td>
<td>7.4</td>
<td>2.5</td>
<td>2.7</td>
<td>32.0</td>
<td>48.0</td>
<td>23.5</td>
<td>12.9</td>
<td>39.7</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>3.9</td>
<td>3.0</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>14.8</td>
<td>45.2</td>
<td>24.5</td>
<td>0.2</td>
<td>38.6</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>41.5</td>
<td>54.0</td>
<td>69.4</td>
<td>45.8</td>
<td>41.4</td>
<td>47.8</td>
<td>51.0</td>
<td>40.2</td>
<td>51.6</td>
<td>60.5</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>27.7</td>
<td>46.4</td>
<td>67.2</td>
<td>40.9</td>
<td>30.3</td>
<td>39.3</td>
<td>49.1</td>
<td>40.9</td>
<td>48.7</td>
<td>56.1</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>38.6</td>
<td>50.4</td>
<td>65.7</td>
<td>32.6</td>
<td>42.3</td>
<td>45.2</td>
<td>49.8</td>
<td>30.5</td>
<td>51.5</td>
<td>58.2</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>10-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>39.8</td>
<td>55.0</td>
<td>66.5</td>
<td>26.0</td>
<td>34.1</td>
<td>44.6</td>
<td>42.2</td>
<td>34.5</td>
<td>52.4</td>
<td>54.7</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>10-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>40.3</td>
<td>49.3</td>
<td>54.3</td>
<td>3.5</td>
<td>8.1</td>
<td>45.1</td>
<td>44.3</td>
<td>34.1</td>
<td>42.3</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>7-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>39.3</td>
<td>55.9</td>
<td>69.1</td>
<td>29.0</td>
<td>36.5</td>
<td>45.1</td>
<td>47.0</td>
<td>31.6</td>
<td>54.1</td>
<td>57.3</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>39.6</td>
<td>47.5</td>
<td>73.3</td>
<td>43.3</td>
<td>57.0</td>
<td>51.5</td>
<td>47.3</td>
<td>38.3</td>
<td>61.6</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>ATST-RCT SED system ATST small</td>
<td>Shao2022</td>
<td>1.36</td>
<td>45.1</td>
<td>44.9</td>
<td>77.1</td>
<td>47.6</td>
<td>55.3</td>
<td>43.0</td>
<td>60.8</td>
<td>36.2</td>
<td>66.7</td>
<td>39.7</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>ATST-RCT SED system ATST base</td>
<td>Shao2022</td>
<td>1.40</td>
<td>39.6</td>
<td>47.5</td>
<td>73.3</td>
<td>43.3</td>
<td>57.0</td>
<td>51.5</td>
<td>47.3</td>
<td>38.3</td>
<td>61.6</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Shao2022</td>
<td>1.13</td>
<td>33.1</td>
<td>48.3</td>
<td>70.2</td>
<td>32.7</td>
<td>26.1</td>
<td>46.8</td>
<td>44.5</td>
<td>37.6</td>
<td>50.9</td>
<td>60.1</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>48.4</td>
<td>59.0</td>
<td>71.4</td>
<td>30.8</td>
<td>45.2</td>
<td>56.9</td>
<td>64.0</td>
<td>38.6</td>
<td>71.9</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>49.3</td>
<td>58.2</td>
<td>74.0</td>
<td>38.0</td>
<td>46.5</td>
<td>56.1</td>
<td>61.2</td>
<td>39.7</td>
<td>71.5</td>
<td>64.2</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>45.4</td>
<td>61.1</td>
<td>74.1</td>
<td>39.8</td>
<td>46.5</td>
<td>54.3</td>
<td>63.5</td>
<td>38.2</td>
<td>71.7</td>
<td>60.3</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>31.5</td>
<td>22.2</td>
<td>59.5</td>
<td>17.8</td>
<td>30.3</td>
<td>35.4</td>
<td>9.4</td>
<td>19.6</td>
<td>57.8</td>
<td>32.0</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>40.7</td>
<td>61.9</td>
<td>75.5</td>
<td>38.8</td>
<td>53.6</td>
<td>63.1</td>
<td>65.9</td>
<td>41.7</td>
<td>56.7</td>
<td>78.2</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>4.8</td>
<td>4.4</td>
<td>0.9</td>
<td>0.0</td>
<td>0.3</td>
<td>10.3</td>
<td>43.7</td>
<td>17.5</td>
<td>0.2</td>
<td>36.0</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>52.7</td>
<td>64.8</td>
<td>78.1</td>
<td>41.2</td>
<td>51.2</td>
<td>60.6</td>
<td>70.0</td>
<td>40.4</td>
<td>60.3</td>
<td>78.9</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>tag-conditioned CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>55.8</td>
<td>73.2</td>
<td>80.7</td>
<td>48.9</td>
<td>49.9</td>
<td>72.7</td>
<td>72.6</td>
<td>48.5</td>
<td>72.5</td>
<td>84.6</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>45.2</td>
<td>60.8</td>
<td>74.3</td>
<td>46.4</td>
<td>50.7</td>
<td>44.3</td>
<td>53.9</td>
<td>30.3</td>
<td>74.5</td>
<td>69.8</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>PANNs-FDY-CRNN-wrTCL system 1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>43.8</td>
<td>48.1</td>
<td>72.0</td>
<td>43.7</td>
<td>47.7</td>
<td>43.4</td>
<td>56.2</td>
<td>33.3</td>
<td>73.3</td>
<td>55.9</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.79</td>
<td>4.4</td>
<td>4.2</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>13.2</td>
<td>44.8</td>
<td>23.4</td>
<td>0.3</td>
<td>40.2</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.75</td>
<td>3.8</td>
<td>3.4</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>13.0</td>
<td>49.2</td>
<td>20.2</td>
<td>0.2</td>
<td>37.3</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>31.8</td>
<td>58.6</td>
<td>73.1</td>
<td>43.2</td>
<td>41.8</td>
<td>40.2</td>
<td>44.6</td>
<td>31.4</td>
<td>64.9</td>
<td>59.6</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>3.9</td>
<td>3.6</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>13.3</td>
<td>44.4</td>
<td>21.3</td>
<td>0.2</td>
<td>37.7</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>3.9</td>
<td>3.6</td>
<td>0.5</td>
<td>0.0</td>
<td>0.3</td>
<td>14.1</td>
<td>43.9</td>
<td>22.3</td>
<td>0.2</td>
<td>38.5</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>29.1</td>
<td>59.4</td>
<td>71.6</td>
<td>43.9</td>
<td>44.3</td>
<td>45.4</td>
<td>44.3</td>
<td>33.2</td>
<td>64.5</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>33.3</td>
<td>43.4</td>
<td>58.3</td>
<td>18.4</td>
<td>27.8</td>
<td>41.2</td>
<td>44.3</td>
<td>21.7</td>
<td>50.3</td>
<td>40.4</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>29.9</td>
<td>44.8</td>
<td>58.0</td>
<td>29.0</td>
<td>36.4</td>
<td>26.8</td>
<td>42.6</td>
<td>24.3</td>
<td>54.4</td>
<td>48.7</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>5.2</td>
<td>4.4</td>
<td>1.3</td>
<td>0.0</td>
<td>0.8</td>
<td>17.0</td>
<td>47.4</td>
<td>22.1</td>
<td>4.5</td>
<td>36.2</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>43.9</td>
<td>46.5</td>
<td>75.7</td>
<td>37.8</td>
<td>48.2</td>
<td>61.1</td>
<td>61.7</td>
<td>42.9</td>
<td>65.3</td>
<td>68.0</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Mean teacher Pseudo labeling system 4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>33.3</td>
<td>44.9</td>
<td>72.1</td>
<td>36.3</td>
<td>47.6</td>
<td>59.2</td>
<td>60.1</td>
<td>36.1</td>
<td>65.7</td>
<td>68.9</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>41.7</td>
<td>43.2</td>
<td>74.2</td>
<td>36.6</td>
<td>48.6</td>
<td>59.1</td>
<td>60.8</td>
<td>39.4</td>
<td>65.5</td>
<td>69.2</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Mean teacher Pseudo labeling system 3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>33.0</td>
<td>47.2</td>
<td>74.1</td>
<td>38.5</td>
<td>47.9</td>
<td>59.5</td>
<td>58.3</td>
<td>37.2</td>
<td>65.2</td>
<td>70.0</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>DCASE2022 baseline system</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>30.7</td>
<td>45.1</td>
<td>59.8</td>
<td>18.4</td>
<td>38.4</td>
<td>24.6</td>
<td>34.3</td>
<td>21.6</td>
<td>55.3</td>
<td>48.4</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>0.82</td>
<td>6.3</td>
<td>3.8</td>
<td>0.9</td>
<td>0.0</td>
<td>0.3</td>
<td>15.9</td>
<td>48.3</td>
<td>24.4</td>
<td>0.1</td>
<td>42.6</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>DCASE2022 SED mean teacher system 2</td>
<td>He2022</td>
<td>1.48</td>
<td>48.5</td>
<td>62.8</td>
<td>71.5</td>
<td>34.1</td>
<td>43.4</td>
<td>65.0</td>
<td>45.9</td>
<td>36.7</td>
<td>70.0</td>
<td>67.2</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He2022</td>
<td>1.52</td>
<td>55.6</td>
<td>61.9</td>
<td>71.8</td>
<td>42.4</td>
<td>52.0</td>
<td>53.5</td>
<td>47.8</td>
<td>34.7</td>
<td>72.2</td>
<td>65.5</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>DCASE2022 SED mean teacher system 1</td>
<td>He2022</td>
<td>1.36</td>
<td>32.4</td>
<td>64.0</td>
<td>71.1</td>
<td>39.1</td>
<td>44.5</td>
<td>57.9</td>
<td>54.5</td>
<td>39.1</td>
<td>67.4</td>
<td>65.8</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.79</td>
<td>5.0</td>
<td>2.5</td>
<td>0.6</td>
<td>0.0</td>
<td>0.3</td>
<td>5.6</td>
<td>39.8</td>
<td>15.1</td>
<td>0.0</td>
<td>25.4</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.75</td>
<td>5.7</td>
<td>1.3</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>3.5</td>
<td>40.6</td>
<td>12.8</td>
<td>0.0</td>
<td>25.7</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.26</td>
<td>27.2</td>
<td>14.5</td>
<td>38.9</td>
<td>46.0</td>
<td>39.0</td>
<td>28.0</td>
<td>15.2</td>
<td>10.8</td>
<td>48.4</td>
<td>25.0</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.20</td>
<td>31.6</td>
<td>22.1</td>
<td>48.0</td>
<td>44.0</td>
<td>38.6</td>
<td>30.6</td>
<td>45.9</td>
<td>11.0</td>
<td>38.9</td>
<td>35.3</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>CNN14 FC</td>
<td>Xie2022</td>
<td>0.83</td>
<td>6.2</td>
<td>4.7</td>
<td>0.9</td>
<td>0.0</td>
<td>0.3</td>
<td>13.5</td>
<td>48.9</td>
<td>25.3</td>
<td>0.3</td>
<td>36.7</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>CBAM-T CRNN scratch</td>
<td>Xie2022</td>
<td>1.06</td>
<td>33.7</td>
<td>36.6</td>
<td>64.9</td>
<td>19.9</td>
<td>17.0</td>
<td>38.8</td>
<td>35.3</td>
<td>34.0</td>
<td>53.9</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>CBAM-T CRNN 1</td>
<td>Xie2022</td>
<td>1.36</td>
<td>40.5</td>
<td>62.3</td>
<td>71.3</td>
<td>33.3</td>
<td>33.0</td>
<td>59.0</td>
<td>58.6</td>
<td>44.2</td>
<td>58.2</td>
<td>66.4</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>CBAM-T CRNN 2</td>
<td>Xie2022</td>
<td>1.38</td>
<td>43.0</td>
<td>65.3</td>
<td>71.3</td>
<td>32.6</td>
<td>36.9</td>
<td>63.4</td>
<td>60.2</td>
<td>42.8</td>
<td>57.3</td>
<td>74.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>41.3</td>
<td>42.2</td>
<td>60.4</td>
<td>22.3</td>
<td>40.7</td>
<td>25.3</td>
<td>45.6</td>
<td>28.5</td>
<td>56.2</td>
<td>48.5</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>37.1</td>
<td>50.2</td>
<td>64.8</td>
<td>27.5</td>
<td>15.1</td>
<td>44.4</td>
<td>36.6</td>
<td>37.5</td>
<td>49.2</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>39.1</td>
<td>54.5</td>
<td>68.9</td>
<td>36.1</td>
<td>31.0</td>
<td>31.4</td>
<td>27.4</td>
<td>36.4</td>
<td>38.8</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>DCASE2022 SED system3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>31.9</td>
<td>48.7</td>
<td>62.1</td>
<td>31.0</td>
<td>36.6</td>
<td>48.5</td>
<td>30.6</td>
<td>33.2</td>
<td>57.3</td>
<td>44.4</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>DCASE2022 SED system 1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>28.6</td>
<td>48.7</td>
<td>68.6</td>
<td>33.9</td>
<td>38.3</td>
<td>43.3</td>
<td>49.0</td>
<td>34.5</td>
<td>59.9</td>
<td>44.5</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>43.5</td>
<td>50.0</td>
<td>68.7</td>
<td>30.9</td>
<td>32.4</td>
<td>49.8</td>
<td>50.2</td>
<td>39.7</td>
<td>55.8</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>25.3</td>
<td>34.5</td>
<td>7.4</td>
<td>11.4</td>
<td>4.2</td>
<td>38.3</td>
<td>51.6</td>
<td>26.5</td>
<td>43.1</td>
<td>35.8</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>DCASE2022 SED system 2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>6.8</td>
<td>9.6</td>
<td>3.1</td>
<td>0.9</td>
<td>1.4</td>
<td>22.0</td>
<td>44.6</td>
<td>20.3</td>
<td>3.2</td>
<td>38.4</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Strong and Max-Weak balanced</td>
<td>Castorena2022</td>
<td>0.91</td>
<td>35.2</td>
<td>39.3</td>
<td>44.7</td>
<td>21.0</td>
<td>14.9</td>
<td>37.4</td>
<td>40.2</td>
<td>24.9</td>
<td>22.0</td>
<td>48.1</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Max-Weak balanced</td>
<td>Castorena2022</td>
<td>1.01</td>
<td>33.5</td>
<td>41.6</td>
<td>58.7</td>
<td>19.2</td>
<td>37.5</td>
<td>45.7</td>
<td>34.5</td>
<td>26.5</td>
<td>50.6</td>
<td>44.5</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Avg-Weak balanced</td>
<td>Castorena2022</td>
<td>0.63</td>
<td>4.8</td>
<td>3.2</td>
<td>3.4</td>
<td>0.2</td>
<td>1.2</td>
<td>22.3</td>
<td>30.9</td>
<td>17.4</td>
<td>3.4</td>
<td>24.9</td>
</tr>
</tbody>
</table>
<h1 id="energy-consumption">Energy Consumption</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="ranking_score_all" data-scatter-y="energy_training" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Ranking score (Evaluation dataset)" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
<br/>Ranking score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Energy " data-axis-scale="log10_unit" data-chartable="true" data-field="energy_training" data-sortable="true" data-value-type="float3">
<br/>Energy (kWh) <br/>(training)
            </th>
<th class="sep-right-cell text-center" data-axis-label="Energy" data-axis-scale="log10_unit" data-chartable="true" data-field="energy_test" data-sortable="true" data-value-type="float3">
<br/>Energy (kWh) <br/>(Test)
            </th>
<th class="sep-left-cell text-center" data-axis-label="Energy weighted PSDS 1" data-chartable="true" data-field="EW_PSDS_1_all_train" data-sortable="true" data-value-type="float3">
<br/>EW-PSDS 1<br/>(training energy)
        </th>
<th class="sep-right-cell text-center" data-axis-label="Energy weighted PSDS 2" data-chartable="true" data-field="EW_PSDS_2_all_train" data-sortable="true" data-value-type="float3">
<br/>EW-PSDS 2<br/>(training energy)
        </th>
<th class="sep-left-cell text-center" data-axis-label="Energy weighted PSDS 1" data-chartable="true" data-field="EW_PSDS_1_all_test" data-sortable="true" data-value-type="float3">
<br/>EW-PSDS 1 <br/>(test energy)
        </th>
<th class="sep-right-cell text-center" data-axis-label="Energy weighted PSDS 2" data-chartable="true" data-field="EW_PSDS_2_all_test" data-sortable="true" data-value-type="float3">
<br/>EW-PSDS 2 <br/>(test energy)
        </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>DCASE2022 pretrained system 2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>0.484</td>
<td>0.697</td>
<td>4.800</td>
<td>0.060</td>
<td>0.173</td>
<td>0.249</td>
<td>0.242</td>
<td>0.348</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>DCASE2022 pretrained system 1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>0.472</td>
<td>0.700</td>
<td>4.800</td>
<td>0.060</td>
<td>0.169</td>
<td>0.250</td>
<td>0.236</td>
<td>0.350</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>DCASE2022 base system</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
<td>2.700</td>
<td>0.040</td>
<td>0.267</td>
<td>0.381</td>
<td>0.315</td>
<td>0.449</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>DCASE2022 weak_pred system</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>0.049</td>
<td>0.784</td>
<td>2.100</td>
<td>0.032</td>
<td>0.040</td>
<td>0.641</td>
<td>0.046</td>
<td>0.735</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>DCASE2022 PANNs SED 2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>0.000</td>
<td>0.063</td>
<td>1.593</td>
<td>0.002</td>
<td>0.000</td>
<td>0.068</td>
<td>0.003</td>
<td>0.943</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>DCASE2022 PANNs SED 3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>0.070</td>
<td>0.194</td>
<td>7.846</td>
<td>0.004</td>
<td>0.015</td>
<td>0.042</td>
<td>0.525</td>
<td>1.456</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>DCASE2022 KDmt SED</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
<td>9.563</td>
<td>0.008</td>
<td>0.078</td>
<td>0.117</td>
<td>1.629</td>
<td>2.436</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>DCASE2022 PANNs SED 4</td>
<td>Liu2022</td>
<td>0.21</td>
<td>0.046</td>
<td>0.151</td>
<td>6.372</td>
<td>0.006</td>
<td>0.012</td>
<td>0.041</td>
<td>0.231</td>
<td>0.754</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>rtzr_dev-only</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
<td>21.694</td>
<td></td>
<td>0.031</td>
<td>0.051</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>rtzr_weak-SED</td>
<td>Suh2022</td>
<td>0.81</td>
<td>0.062</td>
<td>0.774</td>
<td></td>
<td>0.011</td>
<td></td>
<td></td>
<td>0.169</td>
<td>2.110</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>rtzr_strong-real</td>
<td>Suh2022</td>
<td>1.39</td>
<td>0.458</td>
<td>0.721</td>
<td>22.986</td>
<td>0.010</td>
<td>0.034</td>
<td>0.054</td>
<td>1.379</td>
<td>2.171</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>rtzr_audioset</td>
<td>Suh2022</td>
<td>1.42</td>
<td>0.478</td>
<td>0.719</td>
<td>46.891</td>
<td>0.074</td>
<td>0.017</td>
<td>0.026</td>
<td>0.194</td>
<td>0.292</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>DCASE2022 task4 Pre-Trained 2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>0.129</td>
<td>0.758</td>
<td>6.751</td>
<td>0.004</td>
<td>0.033</td>
<td>0.193</td>
<td>0.871</td>
<td>5.130</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>DCASE2022 task4 Pre-Trained 1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>0.051</td>
<td>0.777</td>
<td>6.751</td>
<td>0.004</td>
<td>0.013</td>
<td>0.198</td>
<td>0.345</td>
<td>5.259</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>DCASE2022 task4 without external data</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
<td>10.012</td>
<td>0.048</td>
<td>0.004</td>
<td>0.038</td>
<td>0.016</td>
<td>0.138</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>DCASE2022 task4 AudioSet strong</td>
<td>Liu2022</td>
<td>1.25</td>
<td>0.425</td>
<td>0.634</td>
<td>0.733</td>
<td>0.004</td>
<td>0.996</td>
<td>1.486</td>
<td>3.275</td>
<td>4.888</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>DCASE2022 Kim system 1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
<td>17.000</td>
<td>0.300</td>
<td>0.045</td>
<td>0.070</td>
<td>0.044</td>
<td>0.070</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>DCASE2022 Kim system 3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>0.062</td>
<td>0.781</td>
<td>17.000</td>
<td>0.300</td>
<td>0.006</td>
<td>0.079</td>
<td>0.006</td>
<td>0.078</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>DCASE2022 Kim system 4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>0.305</td>
<td>0.750</td>
<td>17.000</td>
<td>0.300</td>
<td>0.031</td>
<td>0.076</td>
<td>0.030</td>
<td>0.075</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>DCASE2022 Kim system 2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.695</td>
<td>17.000</td>
<td>0.300</td>
<td>0.045</td>
<td>0.070</td>
<td>0.044</td>
<td>0.069</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>SKATTN_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
<td>29.850</td>
<td>0.040</td>
<td>0.015</td>
<td>0.027</td>
<td>0.193</td>
<td>0.346</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>SKATTN_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>0.156</td>
<td>0.449</td>
<td>18.780</td>
<td>0.040</td>
<td>0.014</td>
<td>0.041</td>
<td>0.117</td>
<td>0.337</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
<td>1.717</td>
<td>0.030</td>
<td>0.029</td>
<td>0.184</td>
<td>0.029</td>
<td>0.184</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Multi-Task Learning using Variational AutoEncoders</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
<td>1.717</td>
<td>0.030</td>
<td>0.104</td>
<td>0.196</td>
<td>0.104</td>
<td>0.196</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>single1</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.421</td>
<td>0.664</td>
<td>1.780</td>
<td>0.010</td>
<td>0.406</td>
<td>0.640</td>
<td>1.264</td>
<td>1.991</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>single2</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>0.423</td>
<td>0.651</td>
<td>1.800</td>
<td>0.004</td>
<td>0.403</td>
<td>0.621</td>
<td>3.172</td>
<td>4.885</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>train_ensemble1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>0.390</td>
<td>0.620</td>
<td>1.910</td>
<td>0.010</td>
<td>0.350</td>
<td>0.557</td>
<td>1.169</td>
<td>1.860</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>train_ensemble2</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>0.415</td>
<td>0.634</td>
<td>1.800</td>
<td>0.005</td>
<td>0.396</td>
<td>0.605</td>
<td>2.492</td>
<td>3.804</td>
</tr>
<tr class="info">
<td></td>
<td>Baseline</td>
<td>DCASE2022 SED baseline system</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
<td>1.717</td>
<td>0.030</td>
<td>0.315</td>
<td>0.543</td>
<td>0.315</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>SMALL</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>0.373</td>
<td>0.613</td>
<td>1.717</td>
<td>0.025</td>
<td>0.373</td>
<td>0.613</td>
<td>0.448</td>
<td>0.736</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>SUBMISSION FOR DCASE2022 TASK4</td>
<td>Hao2022</td>
<td>1.09</td>
<td>0.373</td>
<td>0.547</td>
<td>1.717</td>
<td>0.030</td>
<td>0.373</td>
<td>0.547</td>
<td>0.373</td>
<td>0.547</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>0.158</td>
<td>0.633</td>
<td>1.820</td>
<td>0.005</td>
<td>0.149</td>
<td>0.597</td>
<td>0.968</td>
<td>3.876</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>0.082</td>
<td>0.731</td>
<td>6.100</td>
<td>0.005</td>
<td>0.023</td>
<td>0.206</td>
<td>0.445</td>
<td>3.987</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>0.410</td>
<td>0.664</td>
<td>3.250</td>
<td>0.005</td>
<td>0.217</td>
<td>0.351</td>
<td>2.676</td>
<td>4.332</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>FMSG-NTU DCASE2022 SED Model-1</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
<td>3.630</td>
<td>0.005</td>
<td>0.183</td>
<td>0.304</td>
<td>2.413</td>
<td>4.018</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>7-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
<td>12.872</td>
<td>0.045</td>
<td>0.058</td>
<td>0.087</td>
<td>0.288</td>
<td>0.433</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>10-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.400</td>
<td>0.646</td>
<td>18.162</td>
<td>0.056</td>
<td>0.038</td>
<td>0.061</td>
<td>0.214</td>
<td>0.346</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>10-Resolution CRNN+Conformer with class-wise median filtering</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>0.310</td>
<td>0.642</td>
<td>18.162</td>
<td>0.056</td>
<td>0.029</td>
<td>0.061</td>
<td>0.166</td>
<td>0.344</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>7-Resolution CRNN+Conformer</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.407</td>
<td>0.643</td>
<td>12.872</td>
<td>0.045</td>
<td>0.054</td>
<td>0.086</td>
<td>0.271</td>
<td>0.429</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>ATST-RCT SED system ATST ensemble</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
<td>23.900</td>
<td>1.772</td>
<td>0.035</td>
<td>0.050</td>
<td>0.008</td>
<td>0.012</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>ATST-RCT SED system ATST small</td>
<td>Shao2022</td>
<td>1.36</td>
<td>0.476</td>
<td>0.666</td>
<td>3.500</td>
<td>0.624</td>
<td>0.234</td>
<td>0.327</td>
<td>0.023</td>
<td>0.032</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>ATST-RCT SED system ATST base</td>
<td>Shao2022</td>
<td>1.40</td>
<td>0.482</td>
<td>0.693</td>
<td>4.800</td>
<td>0.626</td>
<td>0.172</td>
<td>0.248</td>
<td>0.023</td>
<td>0.033</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>ATST-RCT SED system CRNN with RCT</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
<td>2.210</td>
<td>0.450</td>
<td>0.286</td>
<td>0.462</td>
<td>0.025</td>
<td>0.040</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>0.500</td>
<td>0.695</td>
<td>151.415</td>
<td>1.190</td>
<td>0.006</td>
<td>0.008</td>
<td>0.013</td>
<td>0.018</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
<td>151.415</td>
<td>1.190</td>
<td>0.006</td>
<td>0.008</td>
<td>0.013</td>
<td>0.018</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>0.510</td>
<td>0.711</td>
<td>151.415</td>
<td>1.190</td>
<td>0.006</td>
<td>0.008</td>
<td>0.013</td>
<td>0.018</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
<td>3.768</td>
<td>0.246</td>
<td>0.098</td>
<td>0.153</td>
<td>0.026</td>
<td>0.041</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>CRNN ensemble w/o external data</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
<td>27.200</td>
<td>0.020</td>
<td>0.032</td>
<td>0.047</td>
<td>0.764</td>
<td>1.113</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>FBCRNN ensemble</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>0.047</td>
<td>0.824</td>
<td>36.000</td>
<td>0.020</td>
<td>0.002</td>
<td>0.039</td>
<td>0.070</td>
<td>1.236</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>0.552</td>
<td>0.786</td>
<td>50.000</td>
<td>0.020</td>
<td>0.019</td>
<td>0.027</td>
<td>0.828</td>
<td>1.179</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>tag-conditioned CRNN ensemble</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>0.527</td>
<td>0.679</td>
<td>50.000</td>
<td>0.020</td>
<td>0.018</td>
<td>0.023</td>
<td>0.791</td>
<td>1.019</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>PANNs-FDY-CRNN-wrTCL system 2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>0.482</td>
<td>0.702</td>
<td>1.823</td>
<td>0.027</td>
<td>0.454</td>
<td>0.662</td>
<td>0.535</td>
<td>0.781</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>PANNs-FDY-CRNN-wrTCL system 1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>0.452</td>
<td>0.662</td>
<td>1.823</td>
<td>0.027</td>
<td>0.426</td>
<td>0.624</td>
<td>0.502</td>
<td>0.736</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>PANNs-FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.79</td>
<td>0.054</td>
<td>0.774</td>
<td>1.514</td>
<td>0.027</td>
<td>0.061</td>
<td>0.878</td>
<td>0.060</td>
<td>0.861</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>FDY-CRNN-weak train</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
<td>1.446</td>
<td>0.027</td>
<td>0.058</td>
<td>0.876</td>
<td>0.054</td>
<td>0.820</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>0.409</td>
<td>0.656</td>
<td>1.327</td>
<td>0.077</td>
<td>0.529</td>
<td>0.849</td>
<td>0.159</td>
<td>0.256</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.057</td>
<td>0.747</td>
<td>1.327</td>
<td>0.077</td>
<td>0.074</td>
<td>0.966</td>
<td>0.022</td>
<td>0.291</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.055</td>
<td>0.747</td>
<td>1.327</td>
<td>0.077</td>
<td>0.071</td>
<td>0.966</td>
<td>0.021</td>
<td>0.291</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>0.404</td>
<td>0.653</td>
<td>1.327</td>
<td>0.077</td>
<td>0.522</td>
<td>0.845</td>
<td>0.157</td>
<td>0.255</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
<td>2.757</td>
<td>0.037</td>
<td>0.182</td>
<td>0.328</td>
<td>0.234</td>
<td>0.422</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>0.365</td>
<td>0.584</td>
<td>2.755</td>
<td>0.037</td>
<td>0.228</td>
<td>0.364</td>
<td>0.300</td>
<td>0.479</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>0.069</td>
<td>0.728</td>
<td>25.295</td>
<td>0.056</td>
<td>0.005</td>
<td>0.049</td>
<td>0.037</td>
<td>0.391</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Mean teacher Pseudo labeling system 1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>0.480</td>
<td>0.713</td>
<td>11.880</td>
<td>0.014</td>
<td>0.069</td>
<td>0.103</td>
<td>1.028</td>
<td>1.528</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Mean teacher Pseudo labeling system 4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>0.429</td>
<td>0.723</td>
<td>3.564</td>
<td>0.009</td>
<td>0.207</td>
<td>0.348</td>
<td>1.445</td>
<td>2.437</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Mean teacher Pseudo labeling system 2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>0.451</td>
<td>0.740</td>
<td>11.880</td>
<td>0.014</td>
<td>0.065</td>
<td>0.107</td>
<td>0.966</td>
<td>1.585</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Mean teacher Pseudo labeling system 3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>0.450</td>
<td>0.699</td>
<td>3.564</td>
<td>0.009</td>
<td>0.217</td>
<td>0.337</td>
<td>1.517</td>
<td>2.355</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>DCASE2022 SED mean teacher system 4</td>
<td>He2022</td>
<td>0.82</td>
<td>0.053</td>
<td>0.810</td>
<td>28.066</td>
<td>0.424</td>
<td>0.003</td>
<td>0.050</td>
<td>0.004</td>
<td>0.057</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>DCASE2022 SED mean teacher system 2</td>
<td>He2022</td>
<td>1.48</td>
<td>0.503</td>
<td>0.749</td>
<td>28.066</td>
<td>0.424</td>
<td>0.031</td>
<td>0.046</td>
<td>0.036</td>
<td>0.053</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>DCASE2022 SED mean teacher system 3</td>
<td>He2022</td>
<td>1.52</td>
<td>0.525</td>
<td>0.748</td>
<td>28.066</td>
<td>0.424</td>
<td>0.032</td>
<td>0.046</td>
<td>0.037</td>
<td>0.053</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>DCASE2022 SED mean teacher system 1</td>
<td>He2022</td>
<td>1.36</td>
<td>0.454</td>
<td>0.696</td>
<td>6.067</td>
<td>0.410</td>
<td>0.129</td>
<td>0.197</td>
<td>0.033</td>
<td>0.051</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.79</td>
<td>0.090</td>
<td>0.709</td>
<td>47.417</td>
<td>0.030</td>
<td>0.003</td>
<td>0.026</td>
<td>0.090</td>
<td>0.709</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>0.75</td>
<td>0.075</td>
<td>0.692</td>
<td>23.850</td>
<td>0.024</td>
<td>0.005</td>
<td>0.050</td>
<td>0.094</td>
<td>0.865</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.26</td>
<td>0.439</td>
<td>0.612</td>
<td>47.417</td>
<td>0.030</td>
<td>0.016</td>
<td>0.022</td>
<td>0.439</td>
<td>0.612</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Hybrid system of SEDT and frame-wise model</td>
<td>Li2022d</td>
<td>1.20</td>
<td>0.411</td>
<td>0.597</td>
<td>23.850</td>
<td>0.024</td>
<td>0.030</td>
<td>0.043</td>
<td>0.514</td>
<td>0.746</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>DCASE2022 SED baseline system (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
<td>2.418</td>
<td>0.027</td>
<td>0.245</td>
<td>0.383</td>
<td>0.383</td>
<td>0.600</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>DCASE2022 SED system1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>0.317</td>
<td>0.565</td>
<td>1.201</td>
<td>0.021</td>
<td>0.453</td>
<td>0.807</td>
<td>0.450</td>
<td>0.803</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>DCASE2022 SED system2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.340</td>
<td>0.544</td>
<td>1.114</td>
<td>0.021</td>
<td>0.525</td>
<td>0.839</td>
<td>0.484</td>
<td>0.774</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>DCASE2022 SED system3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.338</td>
<td>0.554</td>
<td>0.748</td>
<td>0.020</td>
<td>0.776</td>
<td>1.272</td>
<td>0.505</td>
<td>0.827</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>DCASE2022 SED system 1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>0.364</td>
<td>0.570</td>
<td>2.718</td>
<td>0.017</td>
<td>0.230</td>
<td>0.360</td>
<td>0.643</td>
<td>1.007</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>DCASE2022 SED system 3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>0.371</td>
<td>0.635</td>
<td>3.791</td>
<td>0.010</td>
<td>0.168</td>
<td>0.287</td>
<td>1.112</td>
<td>1.904</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>DCASE2022 SED system 4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>0.195</td>
<td>0.683</td>
<td>3.317</td>
<td>0.015</td>
<td>0.101</td>
<td>0.354</td>
<td>0.390</td>
<td>1.367</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>DCASE2022 SED system 2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>0.086</td>
<td>0.671</td>
<td>3.771</td>
<td>0.006</td>
<td>0.039</td>
<td>0.305</td>
<td>0.432</td>
<td>3.353</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score (Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>0.484</td>
<td>0.697</td>
<td>specaugment, mixup, frame_shift, FilterAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>0.472</td>
<td>0.700</td>
<td>specaugment, mixup, frame_shift, FilterAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
<td>specaugment, mixup, frame_shift, FilterAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>0.049</td>
<td>0.784</td>
<td>specaugment, mixup, frame_shift, FilterAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>0.000</td>
<td>0.063</td>
<td>mix-up</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>0.070</td>
<td>0.194</td>
<td>mix-up</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
<td>mixup, frame shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>Liu2022</td>
<td>0.21</td>
<td>0.046</td>
<td>0.151</td>
<td>mix-up</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
<td>time shifting, time masking, Mixup, add noise, FilterAugment, SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>Suh2022</td>
<td>0.81</td>
<td>0.062</td>
<td>0.774</td>
<td>time shifting, time masking, Mixup, add noise, FilterAugment, SpecAugment,</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>Suh2022</td>
<td>1.39</td>
<td>0.458</td>
<td>0.721</td>
<td>time shifting, time masking, Mixup, add noise, FilterAugment, SpecAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>Suh2022</td>
<td>1.42</td>
<td>0.478</td>
<td>0.719</td>
<td>time shifting, time masking, Mixup, add noise, FilterAugment, SpecAugment,</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>Cheng2022</td>
<td>0.93</td>
<td>0.276</td>
<td>0.543</td>
<td>mixup, time shift</td>
<td>MelSpectrogram</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
<td>mixup, FilterAugment algorithm</td>
<td>MelSpectrogram</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>0.129</td>
<td>0.758</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>0.051</td>
<td>0.777</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>Liu2022</td>
<td>1.25</td>
<td>0.425</td>
<td>0.634</td>
<td>frame shift, mixup, spec augment, filter augment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
<td>frame shifting, time masking, frequeny masking, mix-up, filter augment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>0.062</td>
<td>0.781</td>
<td>frame shifting, time masking, frequeny masking, mix-up, filter augment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>0.305</td>
<td>0.750</td>
<td>frame shifting, time masking, frequeny masking, mix-up, filter augment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.695</td>
<td>frame shifting, time masking, frequeny masking, mix-up, filter augment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>0.156</td>
<td>0.449</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>Mizobuchi2022</td>
<td>0.82</td>
<td>0.062</td>
<td>0.787</td>
<td>filter augmentation, MixUp, Frame shift, Time mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>Mizobuchi2022</td>
<td>1.26</td>
<td>0.439</td>
<td>0.611</td>
<td>filter augmentation, MixUp, Frame shift, Time mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>Mizobuchi2022</td>
<td>0.88</td>
<td>0.197</td>
<td>0.620</td>
<td>filter augmentation, MixUp, Frame shift, Time mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>0.398</td>
<td>0.571</td>
<td>filter augmentation, MixUp, Frame shift, Time mask</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.421</td>
<td>0.664</td>
<td>time shifting, mix up, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>0.423</td>
<td>0.651</td>
<td>time shifting, mix up, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>0.390</td>
<td>0.620</td>
<td>time shifting, mix up, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>0.415</td>
<td>0.634</td>
<td>time shifting, mix up, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>Dinkel2022</td>
<td>1.29</td>
<td>0.422</td>
<td>0.679</td>
<td>specaugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>0.373</td>
<td>0.613</td>
<td>specaugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>0.104</td>
<td>0.824</td>
<td>specaugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>0.451</td>
<td>0.727</td>
<td>specaugment, mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>Hao2022</td>
<td>0.78</td>
<td>0.078</td>
<td>0.723</td>
<td>noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>Hao2022</td>
<td>1.24</td>
<td>0.425</td>
<td>0.615</td>
<td>noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>Hao2022</td>
<td>1.09</td>
<td>0.373</td>
<td>0.547</td>
<td>noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>0.158</td>
<td>0.633</td>
<td>time-masking, frame-shifting, mixup, filter-augmentation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>0.082</td>
<td>0.731</td>
<td>time-masking, frame-shifting, mixup, Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>0.410</td>
<td>0.664</td>
<td>time-masking, frame-shifting, mixup, filter-augmentation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
<td>time-masking, frame-shifting, mixup, filter-augmentation, Gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.400</td>
<td>0.646</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>0.310</td>
<td>0.642</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.407</td>
<td>0.643</td>
<td>mixup, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
<td>hard mixup, time masking, filter augmentation, time shifting, frequency masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>Shao2022</td>
<td>1.36</td>
<td>0.476</td>
<td>0.666</td>
<td>hard mixup, time masking, frequency masking, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>Shao2022</td>
<td>1.40</td>
<td>0.482</td>
<td>0.693</td>
<td>hard mixup, time masking, filter augmentation, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
<td>hard mixup, time masking, frequency masking, filter augmentation, time shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>0.500</td>
<td>0.695</td>
<td>mix-up, specaugment, time-frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
<td>mix-up, specaugment, time-frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>0.510</td>
<td>0.711</td>
<td>mix-up, specaugment, time-frequency shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
<td>mixup, time masking, filter augment, gaussian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
<td>time-/frequency warping, time-/frequency-masking, superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>0.047</td>
<td>0.824</td>
<td>time-/frequency warping, time-/frequency-masking, superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>0.552</td>
<td>0.786</td>
<td>time-/frequency warping, time-/frequency-masking, superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>0.527</td>
<td>0.679</td>
<td>time-/frequency warping, time-/frequency-masking, superposition, random noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>0.482</td>
<td>0.702</td>
<td>specaugment, mixup, frame-shift, Filteraugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>0.452</td>
<td>0.662</td>
<td>specaugment, mixup, frame-shift, Filteraugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>Xu2022</td>
<td>0.79</td>
<td>0.054</td>
<td>0.774</td>
<td>specaugment, mixup, frame-shift, Filteraugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
<td>specaugment, mixup, frame-shift, Filteraugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>0.409</td>
<td>0.656</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.057</td>
<td>0.747</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.055</td>
<td>0.747</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>0.404</td>
<td>0.653</td>
<td>time shifiting, mixup, time masking, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
<td>time warping, Brownian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>0.365</td>
<td>0.584</td>
<td>pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>0.069</td>
<td>0.728</td>
<td>time warping, Brownian noise</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>0.480</td>
<td>0.713</td>
<td>spec-augment, time-shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>0.429</td>
<td>0.723</td>
<td>spec-augment, time-shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>0.451</td>
<td>0.740</td>
<td>spec-augment, time-shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>0.450</td>
<td>0.699</td>
<td>spec-augment, time-shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
<td>mixup, time-masking, frequency-masking</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>He2022</td>
<td>0.82</td>
<td>0.053</td>
<td>0.810</td>
<td>time mask, frame shift, mixup, ict, sct, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>He2022</td>
<td>1.48</td>
<td>0.503</td>
<td>0.749</td>
<td>time mask, frame shift, mixup, ict, sct, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>He2022</td>
<td>1.52</td>
<td>0.525</td>
<td>0.748</td>
<td>time mask, frame shift, mixup, ict, sct, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>He2022</td>
<td>1.36</td>
<td>0.454</td>
<td>0.696</td>
<td>time mask, frame shift, mixup, ict, sct, FilterAugment</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Li2022d</td>
<td>0.79</td>
<td>0.090</td>
<td>0.709</td>
<td>mixup, frequency mask (only SEDT), frequency shift (only SEDT), time mask (only SEDT)</td>
<td>log-mel energies (frame-wise model), log-mel spectrogram (SEDT)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Li2022d</td>
<td>0.75</td>
<td>0.075</td>
<td>0.692</td>
<td>mixup, frequency mask (only SEDT), frequency shift (only SEDT), time mask (only SEDT)</td>
<td>log-mel energies (frame-wise model), log-mel spectrogram (SEDT)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Li2022d</td>
<td>1.26</td>
<td>0.439</td>
<td>0.612</td>
<td>mixup, frequency mask (only SEDT), frequency shift (only SEDT), time mask (only SEDT)</td>
<td>log-mel energies (frame-wise model), log-mel spectrogram (SEDT)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Li2022d</td>
<td>1.20</td>
<td>0.411</td>
<td>0.597</td>
<td>mixup, frequency mask (only SEDT), frequency shift (only SEDT), time mask (only SEDT)</td>
<td>log-mel energies (frame-wise model), log-mel spectrogram (SEDT)</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>Xie2022</td>
<td>0.83</td>
<td>0.062</td>
<td>0.800</td>
<td>mixup, SpecAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>Xie2022</td>
<td>1.06</td>
<td>0.300</td>
<td>0.641</td>
<td>mixup, SpecAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>Xie2022</td>
<td>1.36</td>
<td>0.418</td>
<td>0.757</td>
<td>mixup, SpecAug</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>Xie2022</td>
<td>1.38</td>
<td>0.426</td>
<td>0.766</td>
<td>mixup, SpecAug</td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>0.317</td>
<td>0.565</td>
<td>frame shift, mixup, time mask, filter augmentation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.340</td>
<td>0.544</td>
<td>frame shift, mixup, time mask, filter augmentation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.338</td>
<td>0.554</td>
<td>time shift, mixup, time mask, filter augmentation</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>0.364</td>
<td>0.570</td>
<td>mixup,filteraugment,cutout</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>0.371</td>
<td>0.635</td>
<td>mixup,filteraugment,cutout</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>0.195</td>
<td>0.683</td>
<td>mixup,filteraugment,cutout</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>0.086</td>
<td>0.671</td>
<td>mixup,filteraugment,cutout</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Castorena2022</td>
<td>0.91</td>
<td>0.267</td>
<td>0.531</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Castorena2022</td>
<td>1.01</td>
<td>0.334</td>
<td>0.524</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Castorena2022</td>
<td>0.63</td>
<td>0.072</td>
<td>0.559</td>
<td></td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="ranking_score_all" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score (Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-field="machine_learning_semi_supervised" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Semi-supervised approach
            </th>
<th class="text-center narrow-col" data-field="post-processing" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Post-processing
            </th>
<th class="text-center narrow-col" data-field="segmentation_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Segmentation<br/>method
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>0.484</td>
<td>0.697</td>
<td>CRNN,CNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>0.472</td>
<td>0.700</td>
<td>CRNN,CNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
<td>CRNN,CNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>0.049</td>
<td>0.784</td>
<td>CRNN,CNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>0.000</td>
<td>0.063</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>0.070</td>
<td>0.194</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student, knowledge distillation</td>
<td>median filtering (93ms)</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>Liu2022</td>
<td>0.21</td>
<td>0.046</td>
<td>0.151</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>Suh2022</td>
<td>0.81</td>
<td>0.062</td>
<td>0.774</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>weak SED</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>Suh2022</td>
<td>1.39</td>
<td>0.458</td>
<td>0.721</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>Suh2022</td>
<td>1.42</td>
<td>0.478</td>
<td>0.719</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>Cheng2022</td>
<td>0.93</td>
<td>0.276</td>
<td>0.543</td>
<td>CRNN, Multiscale CNN</td>
<td>mean-teacher student</td>
<td>median filtering (0.45s)</td>
<td>attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
<td>CRNN, Multiscale CNN</td>
<td>mean-teacher student</td>
<td>median filtering (0.45s)</td>
<td>attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>0.129</td>
<td>0.758</td>
<td>Transformer, RNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layers</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>0.051</td>
<td>0.777</td>
<td>Transformer, RNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layers</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
<td>CNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layers</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>Liu2022</td>
<td>1.25</td>
<td>0.425</td>
<td>0.634</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
<td>FDY-CRNN</td>
<td>mean-teacher student, ICT, FixMatch</td>
<td>median filtering (329ms)</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>0.062</td>
<td>0.781</td>
<td>FDY-CRNN</td>
<td>mean-teacher student, ICT, FixMatch</td>
<td>median filtering (329ms)</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>0.305</td>
<td>0.750</td>
<td>FDY-CRNN</td>
<td>mean-teacher student, ICT, FixMatch</td>
<td>median filtering (329ms)</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.695</td>
<td>FDY-CRNN</td>
<td>mean-teacher student, ICT, FixMatch</td>
<td>median filtering (329ms)</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
<td>SKATTN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>0.156</td>
<td>0.449</td>
<td>SKATTN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
<td>RNN</td>
<td>multi-task learning</td>
<td>median filtering (456ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
<td>RNN</td>
<td>multi-task learning</td>
<td>median filtering (456ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>Mizobuchi2022</td>
<td>0.82</td>
<td>0.062</td>
<td>0.787</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering, probability correction</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>Mizobuchi2022</td>
<td>1.26</td>
<td>0.439</td>
<td>0.611</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering, probability correction</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>Mizobuchi2022</td>
<td>0.88</td>
<td>0.197</td>
<td>0.620</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering, probability correction</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>0.398</td>
<td>0.571</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering, probability correction</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.421</td>
<td>0.664</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>patch attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>0.423</td>
<td>0.651</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>patch attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>0.390</td>
<td>0.620</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>patch attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>0.415</td>
<td>0.634</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>patch attention layers</td>
<td></td>
</tr>
<tr data-hline="true">
<td></td>
<td>Baseline</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>Dinkel2022</td>
<td>1.29</td>
<td>0.422</td>
<td>0.679</td>
<td>CRNN, RCRNN</td>
<td>uda, mean-teacher student</td>
<td>median filtering (443ms)</td>
<td></td>
<td>avg</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>0.373</td>
<td>0.613</td>
<td>CRNN, RCRNN</td>
<td>mean-teacher student</td>
<td>median filtering (443ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>0.104</td>
<td>0.824</td>
<td>CNN, Transformer</td>
<td>uda, mean-teacher student</td>
<td>median filtering (443ms)</td>
<td></td>
<td>avg</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>0.451</td>
<td>0.727</td>
<td>CRNN, RCRNN, Transformer</td>
<td>uda, mean-teacher student, noisystudent</td>
<td>median filtering (443ms)</td>
<td></td>
<td>avg</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>Hao2022</td>
<td>0.78</td>
<td>0.078</td>
<td>0.723</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering with adaptive window size</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>Hao2022</td>
<td>1.24</td>
<td>0.425</td>
<td>0.615</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering with adaptive window size</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>Hao2022</td>
<td>1.09</td>
<td>0.373</td>
<td>0.547</td>
<td>CRNN</td>
<td>domain adaptation</td>
<td>median filtering with adaptive window size</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>0.158</td>
<td>0.633</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling, interpolation consistency training</td>
<td>class-wise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>0.082</td>
<td>0.731</td>
<td>CRNN</td>
<td>mean-teacher student, interpolation consistency training</td>
<td>class-wise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>0.410</td>
<td>0.664</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling, interpolation consistency training</td>
<td>class-wise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling, interpolation consistency training</td>
<td>class-wise median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
<td>CRNN, conformer</td>
<td>mean-teacher student</td>
<td>median filtering (class dependent)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.400</td>
<td>0.646</td>
<td>CRNN, conformer</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>0.310</td>
<td>0.642</td>
<td>CRNN, conformer</td>
<td>mean-teacher student</td>
<td>median filtering (class dependent)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.407</td>
<td>0.643</td>
<td>CRNN, conformer</td>
<td>mean-teacher student</td>
<td>median filtering (450ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
<td>CRNN, ATST</td>
<td>mean-teacher student, RCT</td>
<td>temperature, median filter</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>Shao2022</td>
<td>1.36</td>
<td>0.476</td>
<td>0.666</td>
<td>CRNN, ATST</td>
<td>mean-teacher student, RCT</td>
<td>temperature, median filter</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>Shao2022</td>
<td>1.40</td>
<td>0.482</td>
<td>0.693</td>
<td>CRNN, ATST</td>
<td>mean-teacher student, RCT</td>
<td>temperature</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
<td>CRNN</td>
<td>mean-teacher student, RCT</td>
<td>median filtering （112ms）</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>0.500</td>
<td>0.695</td>
<td>RCRNN</td>
<td>mean-teacher student, noisy student</td>
<td>classwise median filtering</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
<td>RCRNN</td>
<td>mean-teacher student, noisy student</td>
<td>classwise median filtering</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>0.510</td>
<td>0.711</td>
<td>RCRNN</td>
<td>mean-teacher student, noisy student</td>
<td>classwise median filtering</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
<td>RCRNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
<td>CRNN</td>
<td>self-training</td>
<td>median filtering (event-specific lengths)</td>
<td>MIL</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>0.047</td>
<td>0.824</td>
<td>FBCRNN</td>
<td>self-training</td>
<td>median filtering (event-specific lengths)</td>
<td>MIL</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>0.552</td>
<td>0.786</td>
<td>CRNN</td>
<td>self-training</td>
<td>median filtering (event-specific lengths)</td>
<td>MIL</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>0.527</td>
<td>0.679</td>
<td>CRNN</td>
<td>self-training</td>
<td>median filtering (event-specific lengths)</td>
<td>MIL</td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>0.482</td>
<td>0.702</td>
<td>FDY-CRNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>0.452</td>
<td>0.662</td>
<td>FDY-CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>Xu2022</td>
<td>0.79</td>
<td>0.054</td>
<td>0.774</td>
<td>FDY-CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
<td>FDY-CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>0.409</td>
<td>0.656</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>class-wise median filtering, weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.057</td>
<td>0.747</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>class-wise median filtering, weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.055</td>
<td>0.747</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>class-wise median filtering, weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>0.404</td>
<td>0.653</td>
<td>CRNN, ensemble</td>
<td>mean-teacher student</td>
<td>class-wise median filtering, weak prediction masking</td>
<td></td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (160ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>0.365</td>
<td>0.584</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (160ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>0.069</td>
<td>0.728</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (160ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>0.480</td>
<td>0.713</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>0.429</td>
<td>0.723</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>0.451</td>
<td>0.740</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>0.450</td>
<td>0.699</td>
<td>CRNN</td>
<td>mean-teacher student, pseudo-labelling</td>
<td>median filtering (340ms)</td>
<td></td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>He2022</td>
<td>0.82</td>
<td>0.053</td>
<td>0.810</td>
<td>SK-CRNN, FDY-CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>MIL</td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>He2022</td>
<td>1.48</td>
<td>0.503</td>
<td>0.749</td>
<td>SK-CRNN, FDY-CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>MIL</td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>He2022</td>
<td>1.52</td>
<td>0.525</td>
<td>0.748</td>
<td>SK-CRNN, FDY-CRNN</td>
<td>mean-teacher student</td>
<td>classwise median filtering</td>
<td>MIL</td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>He2022</td>
<td>1.36</td>
<td>0.454</td>
<td>0.696</td>
<td>SK-CRNN, FDY-CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>MIL</td>
<td>averaging</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Li2022d</td>
<td>0.79</td>
<td>0.090</td>
<td>0.709</td>
<td>transformer (SEDT), CNN (frame-wise model), ensemble</td>
<td>mean-teacher student (frame-wise model), pseudo-labelling (SEDT)</td>
<td>median filtering with adaptive window size (only frame-wise model)</td>
<td>attention layers (only frame-wise model)</td>
<td>majority vote (only SEDT), weighted averaging (frame-wise model and ensemble model)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Li2022d</td>
<td>0.75</td>
<td>0.075</td>
<td>0.692</td>
<td>transformer (SEDT), CNN (frame-wise model), ensemble</td>
<td>mean-teacher student (frame-wise model), pseudo-labelling (SEDT)</td>
<td>median filtering with adaptive window size (only frame-wise model)</td>
<td>attention layers (only frame-wise model)</td>
<td>majority vote (only SEDT), weighted averaging (frame-wise model and ensemble model)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Li2022d</td>
<td>1.26</td>
<td>0.439</td>
<td>0.612</td>
<td>transformer (SEDT), CNN (frame-wise model), ensemble</td>
<td>mean-teacher student (frame-wise model), pseudo-labelling (SEDT)</td>
<td>median filtering with adaptive window size (only frame-wise model)</td>
<td>attention layers (only frame-wise model)</td>
<td>majority vote (only SEDT), weighted averaging (frame-wise model and ensemble model)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Li2022d</td>
<td>1.20</td>
<td>0.411</td>
<td>0.597</td>
<td>transformer (SEDT), CNN (frame-wise model), ensemble</td>
<td>mean-teacher student (frame-wise model), pseudo-labelling (SEDT)</td>
<td>median filtering with adaptive window size (only frame-wise model)</td>
<td>attention layers (only frame-wise model)</td>
<td>majority vote (only SEDT), weighted averaging (frame-wise model and ensemble model)</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>Xie2022</td>
<td>0.83</td>
<td>0.062</td>
<td>0.800</td>
<td>CRNN</td>
<td></td>
<td></td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>Xie2022</td>
<td>1.06</td>
<td>0.300</td>
<td>0.641</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (560ms)</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>Xie2022</td>
<td>1.36</td>
<td>0.418</td>
<td>0.757</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (560ms)</td>
<td></td>
<td>average</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>Xie2022</td>
<td>1.38</td>
<td>0.426</td>
<td>0.766</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (560ms)</td>
<td></td>
<td>average</td>
</tr>
<tr data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>0.317</td>
<td>0.565</td>
<td>RCRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.340</td>
<td>0.544</td>
<td>CRNN with cbam attetion</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.338</td>
<td>0.554</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>attention layers</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>0.364</td>
<td>0.570</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>linearsoftmax layer, attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>0.371</td>
<td>0.635</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td>linearsoftmax layer, attention layer</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>0.195</td>
<td>0.683</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>linearsoftmax layer, mean</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>0.086</td>
<td>0.671</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering</td>
<td>linearsoftmax layer, mean</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Castorena2022</td>
<td>0.91</td>
<td>0.267</td>
<td>0.531</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Castorena2022</td>
<td>1.01</td>
<td>0.334</td>
<td>0.524</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Castorena2022</td>
<td>0.63</td>
<td>0.072</td>
<td>0.559</td>
<td>CRNN</td>
<td>mean-teacher student</td>
<td>median filtering (93ms)</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="complexity">Complexity</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="ranking_score_all" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="system_complexity" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="ranking_score_all" data-sortable="true" data-value-type="float2">
                Ranking score (Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-axis-label="PSDS 1 (Evaluation dataset)" data-chartable="true" data-field="PSDS_1_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 1 <br/>(Evaluation dataset)
            </th>
<th class="sep-right-cell text-center" data-axis-label="PSDS 2 (Evaluation dataset)" data-chartable="true" data-field="PSDS_2_all" data-sortable="true" data-value-type="float3">
<br/>PSDS 2 <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_complexity_time" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Training time
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zhang_UCAS_task4_2</td>
<td>Xiao2022</td>
<td>1.41</td>
<td>0.484</td>
<td>0.697</td>
<td>11325746</td>
<td>5</td>
<td>18h (1 Tesla P100 )</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_1</td>
<td>Xiao2022</td>
<td>1.39</td>
<td>0.472</td>
<td>0.700</td>
<td>11325746</td>
<td>5</td>
<td>18h (1 Tesla P100 )</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_3</td>
<td>Xiao2022</td>
<td>1.21</td>
<td>0.420</td>
<td>0.599</td>
<td>4282496</td>
<td>5</td>
<td>12h (1 Tesla P100 )</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task4_4</td>
<td>Xiao2022</td>
<td>0.79</td>
<td>0.049</td>
<td>0.784</td>
<td>2359672</td>
<td>5</td>
<td>8h (1 Tesla P100 )</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_2</td>
<td>Liu2022</td>
<td>0.06</td>
<td>0.000</td>
<td>0.063</td>
<td>3251508</td>
<td></td>
<td>7h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_3</td>
<td>Liu2022</td>
<td>0.29</td>
<td>0.070</td>
<td>0.194</td>
<td>16257540</td>
<td>5</td>
<td>35h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Huang_NSYSU_task4_1</td>
<td>Huang2022</td>
<td>1.28</td>
<td>0.434</td>
<td>0.650</td>
<td>14973876</td>
<td>6</td>
<td>10h * 6 (3060ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_NSYSU_task4_4</td>
<td>Liu2022</td>
<td>0.21</td>
<td>0.046</td>
<td>0.151</td>
<td>13006032</td>
<td>4</td>
<td>28h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_1</td>
<td>Suh2022</td>
<td>1.22</td>
<td>0.393</td>
<td>0.650</td>
<td>116400000</td>
<td>12</td>
<td>10h 8m 25s (1 NVIDIA A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_4</td>
<td>Suh2022</td>
<td>0.81</td>
<td>0.062</td>
<td>0.774</td>
<td>116400000</td>
<td>12</td>
<td>(1 NVIDIA A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_2</td>
<td>Suh2022</td>
<td>1.39</td>
<td>0.458</td>
<td>0.721</td>
<td>116400000</td>
<td>12</td>
<td>10h 52m 16s(1 NVIDIA A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Suh_ReturnZero_task4_3</td>
<td>Suh2022</td>
<td>1.42</td>
<td>0.478</td>
<td>0.719</td>
<td>116400000</td>
<td>12</td>
<td>13h 46m 27s (1 NVIDIA A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_2</td>
<td>Cheng2022</td>
<td>0.93</td>
<td>0.276</td>
<td>0.543</td>
<td>4721326</td>
<td></td>
<td>18h (nvidia A100)</td>
</tr>
<tr>
<td></td>
<td>Cheng_CHT_task4_1</td>
<td>Cheng2022</td>
<td>1.03</td>
<td>0.314</td>
<td>0.582</td>
<td>4729921</td>
<td></td>
<td>18h (nvidia A100)</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_2</td>
<td>Liu2022</td>
<td>0.90</td>
<td>0.129</td>
<td>0.758</td>
<td>89500000</td>
<td></td>
<td>36h (1 NVIDIA A100 40Gb)</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_1</td>
<td>Liu2022</td>
<td>0.79</td>
<td>0.051</td>
<td>0.777</td>
<td>89500000</td>
<td></td>
<td>36h (1 NVIDIA A100 40Gb)</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_4</td>
<td>Liu2022</td>
<td>0.24</td>
<td>0.025</td>
<td>0.219</td>
<td>79700000</td>
<td></td>
<td>11h (1 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Liu_SRCN_task4_3</td>
<td>Liu2022</td>
<td>1.25</td>
<td>0.425</td>
<td>0.634</td>
<td>11061000</td>
<td></td>
<td>6h (1 NVIDIA A100 40Gb)</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_1</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.697</td>
<td>11061000</td>
<td></td>
<td>8h (1 RTX A5000)</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_3</td>
<td>Kim2022a</td>
<td>0.81</td>
<td>0.062</td>
<td>0.781</td>
<td>11061000</td>
<td></td>
<td>8h (1 RTX A5000)</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_4</td>
<td>Kim2022a</td>
<td>1.17</td>
<td>0.305</td>
<td>0.750</td>
<td>11061000</td>
<td></td>
<td>8h (1 RTX A5000)</td>
</tr>
<tr>
<td></td>
<td>Kim_LGE_task4_2</td>
<td>Kim2022a</td>
<td>1.34</td>
<td>0.444</td>
<td>0.695</td>
<td>11061000</td>
<td></td>
<td>8h (1 RTX A5000)</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_1</td>
<td>Ryu2022</td>
<td>0.83</td>
<td>0.257</td>
<td>0.461</td>
<td>625K</td>
<td></td>
<td>25h (4 A100 GPUs)</td>
</tr>
<tr>
<td></td>
<td>Ryu_Deeply_task4_2</td>
<td>Ryu2022</td>
<td>0.66</td>
<td>0.156</td>
<td>0.449</td>
<td>625K</td>
<td></td>
<td>16.9h (4 A100 GPUs)</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_2</td>
<td>Giannakopoulos2022</td>
<td>0.21</td>
<td>0.029</td>
<td>0.184</td>
<td>4213258</td>
<td></td>
<td>6h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Giannakopoulos_UNIPI_task4_1</td>
<td>Giannakopoulos2022</td>
<td>0.35</td>
<td>0.104</td>
<td>0.196</td>
<td>4213258</td>
<td></td>
<td>6h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_4</td>
<td>Mizobuchi2022</td>
<td>0.82</td>
<td>0.062</td>
<td>0.787</td>
<td>52793884</td>
<td>11</td>
<td>77h (1 NVIDIA Tesla V100 SXM2)</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_2</td>
<td>Mizobuchi2022</td>
<td>1.26</td>
<td>0.439</td>
<td>0.611</td>
<td>70847296</td>
<td>16</td>
<td>48h (1 NVIDIA Tesla V100 SXM2)</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_3</td>
<td>Mizobuchi2022</td>
<td>0.88</td>
<td>0.197</td>
<td>0.620</td>
<td>44279560</td>
<td>10</td>
<td>30h (1 NVIDIA Tesla V100 SXM2)</td>
</tr>
<tr>
<td></td>
<td>Mizobuchi_PCO_task4_1</td>
<td>Mizobuchi2022</td>
<td>1.15</td>
<td>0.398</td>
<td>0.571</td>
<td>35423648</td>
<td>8</td>
<td>24h (1 NVIDIA Tesla V100 SXM2)</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_2</td>
<td>Sojeong2022</td>
<td>1.28</td>
<td>0.421</td>
<td>0.664</td>
<td>1112420</td>
<td>5</td>
<td>6h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_4</td>
<td>Sojeong2022</td>
<td>1.27</td>
<td>0.423</td>
<td>0.651</td>
<td>1112420</td>
<td>5</td>
<td>6h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_1</td>
<td>Sojeong2022</td>
<td>1.19</td>
<td>0.390</td>
<td>0.620</td>
<td>1112420</td>
<td>3</td>
<td>6h (1 GTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>KIM_HYU_task4_3</td>
<td>Sojeong2022</td>
<td>1.24</td>
<td>0.415</td>
<td>0.634</td>
<td>1112420</td>
<td>2</td>
<td>4h (1 GTX 3090 Ti)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>Turpault2022</td>
<td>1.00</td>
<td>0.315</td>
<td>0.543</td>
<td>2200000</td>
<td></td>
<td>6h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_1</td>
<td>Dinkel2022</td>
<td>1.29</td>
<td>0.422</td>
<td>0.679</td>
<td>8430844</td>
<td>9</td>
<td>3 h</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_2</td>
<td>Dinkel2022</td>
<td>1.15</td>
<td>0.373</td>
<td>0.613</td>
<td>148852</td>
<td></td>
<td>3 h</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_4</td>
<td>Dinkel2022</td>
<td>0.92</td>
<td>0.104</td>
<td>0.824</td>
<td>27992026</td>
<td>6</td>
<td>24 h</td>
</tr>
<tr>
<td></td>
<td>Dinkel_XiaoRice_task4_3</td>
<td>Dinkel2022</td>
<td>1.38</td>
<td>0.451</td>
<td>0.727</td>
<td>37451786</td>
<td>11</td>
<td>24 h</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_2</td>
<td>Hao2022</td>
<td>0.78</td>
<td>0.078</td>
<td>0.723</td>
<td>4590228</td>
<td>3</td>
<td>36h (1 RTX 6000)</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_1</td>
<td>Hao2022</td>
<td>1.24</td>
<td>0.425</td>
<td>0.615</td>
<td>4590228</td>
<td>3</td>
<td>36h (1 RTX 6000)</td>
</tr>
<tr>
<td></td>
<td>Hao_UNISOC_task4_3</td>
<td>Hao2022</td>
<td>1.09</td>
<td>0.373</td>
<td>0.547</td>
<td>4590228</td>
<td></td>
<td>36h (1 RTX 6000)</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_1</td>
<td>Khandelwal2022</td>
<td>0.83</td>
<td>0.158</td>
<td>0.633</td>
<td>2770884</td>
<td></td>
<td>20h (1 NVIDIA Quadro RTX 5000)</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_2</td>
<td>Khandelwal2022</td>
<td>0.80</td>
<td>0.082</td>
<td>0.731</td>
<td>118567907</td>
<td></td>
<td>24h (1 NVIDIA Quadro RTX 5000)</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_3</td>
<td>Khandelwal2022</td>
<td>1.26</td>
<td>0.410</td>
<td>0.664</td>
<td>2770884</td>
<td></td>
<td>20h (1 NVIDIA Quadro RTX 5000)</td>
</tr>
<tr>
<td></td>
<td>Khandelwal_FMSG-NTU_task4_4</td>
<td>Khandelwal2022</td>
<td>1.20</td>
<td>0.386</td>
<td>0.643</td>
<td>2770884</td>
<td></td>
<td>20h (1 NVIDIA Quadro RTX 5000)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_4</td>
<td>deBenito2022</td>
<td>1.28</td>
<td>0.432</td>
<td>0.649</td>
<td>10659182</td>
<td>7</td>
<td>77h (1 GeForce RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_1</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.400</td>
<td>0.646</td>
<td>15911270</td>
<td>10</td>
<td>111h (1 GeForce RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_2</td>
<td>deBenito2022</td>
<td>1.08</td>
<td>0.310</td>
<td>0.642</td>
<td>15911270</td>
<td>10</td>
<td>111h (1 GeForce RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>deBenito_AUDIAS_task4_3</td>
<td>deBenito2022</td>
<td>1.23</td>
<td>0.407</td>
<td>0.643</td>
<td>10659182</td>
<td>7</td>
<td>77h (1 GeForce RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_4</td>
<td>Shao2022</td>
<td>1.41</td>
<td>0.486</td>
<td>0.694</td>
<td>475547380</td>
<td>5</td>
<td>8.3h (1 A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_2</td>
<td>Shao2022</td>
<td>1.36</td>
<td>0.476</td>
<td>0.666</td>
<td>29986148</td>
<td></td>
<td>6.6h (1 A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_3</td>
<td>Shao2022</td>
<td>1.40</td>
<td>0.482</td>
<td>0.693</td>
<td>95109476</td>
<td></td>
<td>8.3h (1 A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Li_WU_task4_1</td>
<td>Shao2022</td>
<td>1.13</td>
<td>0.368</td>
<td>0.594</td>
<td>1112420</td>
<td></td>
<td>4h (1 A100-SXM4-80GB)</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_3</td>
<td>Kim2022b</td>
<td>1.43</td>
<td>0.500</td>
<td>0.695</td>
<td>1691694</td>
<td>10</td>
<td>74h (5 RTX 2080ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_1</td>
<td>Kim2022b</td>
<td>1.47</td>
<td>0.514</td>
<td>0.713</td>
<td>1691694</td>
<td>10</td>
<td>74h (5 RTX 2080ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_2</td>
<td>Kim2022b</td>
<td>1.46</td>
<td>0.510</td>
<td>0.711</td>
<td>1691694</td>
<td>10</td>
<td>74h (5 RTX 2080ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_task4_4</td>
<td>Kim2022b</td>
<td>0.65</td>
<td>0.215</td>
<td>0.335</td>
<td>792228</td>
<td></td>
<td>18h (1 RTX A6000)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_4</td>
<td>Ebbers2022</td>
<td>1.49</td>
<td>0.509</td>
<td>0.742</td>
<td>134119060</td>
<td>30</td>
<td>2d (10 A100)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_2</td>
<td>Ebbers2022</td>
<td>0.83</td>
<td>0.047</td>
<td>0.824</td>
<td>499812480</td>
<td>40</td>
<td>5d (10 A100)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_1</td>
<td>Ebbers2022</td>
<td>1.59</td>
<td>0.552</td>
<td>0.786</td>
<td>779623240</td>
<td>60</td>
<td>5d (10 A100)</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task4_3</td>
<td>Ebbers2022</td>
<td>1.46</td>
<td>0.527</td>
<td>0.679</td>
<td>780237640</td>
<td>60</td>
<td>5d (10 A100)</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_2</td>
<td>Xu2022</td>
<td>1.41</td>
<td>0.482</td>
<td>0.702</td>
<td>11066748</td>
<td>10</td>
<td>4h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_1</td>
<td>Xu2022</td>
<td>1.32</td>
<td>0.452</td>
<td>0.662</td>
<td>11066748</td>
<td>5</td>
<td>4h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_3</td>
<td>Xu2022</td>
<td>0.79</td>
<td>0.054</td>
<td>0.774</td>
<td>11117798</td>
<td>5</td>
<td>4h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Xu_SRCB-BIT_task4_4</td>
<td>Xu2022</td>
<td>0.75</td>
<td>0.049</td>
<td>0.738</td>
<td>11081958</td>
<td>2</td>
<td>4h (1 RTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_2</td>
<td>Nam2022</td>
<td>1.25</td>
<td>0.409</td>
<td>0.656</td>
<td>11061468</td>
<td>12</td>
<td>6h (1 RTX Titan)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_3</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.057</td>
<td>0.747</td>
<td>11061468</td>
<td>53</td>
<td>6h (1 RTX Titan)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_4</td>
<td>Nam2022</td>
<td>0.77</td>
<td>0.055</td>
<td>0.747</td>
<td>11061468</td>
<td>150</td>
<td>6h (1 RTX Titan)</td>
</tr>
<tr>
<td></td>
<td>Nam_KAIST_task4_SED_1</td>
<td>Nam2022</td>
<td>1.24</td>
<td>0.404</td>
<td>0.653</td>
<td>11061468</td>
<td>31</td>
<td>6h (1 RTX Titan)</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_3</td>
<td>Blakala2022</td>
<td>0.95</td>
<td>0.293</td>
<td>0.527</td>
<td>1.2M</td>
<td></td>
<td>4.5h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_1</td>
<td>Blakala2022</td>
<td>1.11</td>
<td>0.365</td>
<td>0.584</td>
<td>1177663</td>
<td></td>
<td>8h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>Blakala_SRPOL_task4_2</td>
<td>Blakala2022</td>
<td>0.78</td>
<td>0.069</td>
<td>0.728</td>
<td>5.3M</td>
<td></td>
<td>29h (1 RTX 2080)</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_1</td>
<td>Li2022b</td>
<td>1.41</td>
<td>0.480</td>
<td>0.713</td>
<td>26842020</td>
<td>10</td>
<td>20h (2 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_4</td>
<td>Li2022b</td>
<td>1.34</td>
<td>0.429</td>
<td>0.723</td>
<td>8052606</td>
<td>10</td>
<td>6h (2 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_2</td>
<td>Li2022b</td>
<td>1.39</td>
<td>0.451</td>
<td>0.740</td>
<td>26842020</td>
<td>10</td>
<td>20h (2 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Li_USTC_task4_SED_3</td>
<td>Li2022b</td>
<td>1.35</td>
<td>0.450</td>
<td>0.699</td>
<td>8052606</td>
<td>10</td>
<td>6h (2 GTX 3090)</td>
</tr>
<tr>
<td></td>
<td>Bertola_UPF_task4_1</td>
<td>Bertola2022</td>
<td>0.98</td>
<td>0.318</td>
<td>0.520</td>
<td>1112420</td>
<td></td>
<td>3h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_4</td>
<td>He2022</td>
<td>0.82</td>
<td>0.053</td>
<td>0.810</td>
<td>15919068</td>
<td>40</td>
<td>8h (1 A100)</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_2</td>
<td>He2022</td>
<td>1.48</td>
<td>0.503</td>
<td>0.749</td>
<td>15919068</td>
<td>40</td>
<td>8h (1 A100)</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_3</td>
<td>He2022</td>
<td>1.52</td>
<td>0.525</td>
<td>0.748</td>
<td>15919068</td>
<td>16</td>
<td>8h (1 A100)</td>
</tr>
<tr>
<td></td>
<td>He_BYTEDANCE_task4_1</td>
<td>He2022</td>
<td>1.36</td>
<td>0.454</td>
<td>0.696</td>
<td>11061468</td>
<td>8</td>
<td>3h (1 A100)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_2</td>
<td>Li2022d</td>
<td>0.79</td>
<td>0.090</td>
<td>0.709</td>
<td>224997445</td>
<td>10 (5 SEDT, 5 frame-wise model)</td>
<td>186 h (1 RTX A4000) + 35 h (3 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_4</td>
<td>Li2022d</td>
<td>0.75</td>
<td>0.075</td>
<td>0.692</td>
<td>188469803</td>
<td>9 (4 SEDT, 5 frame-wise model)</td>
<td>53h (1 RTX A4000) + 30 h (3 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_1</td>
<td>Li2022d</td>
<td>1.26</td>
<td>0.439</td>
<td>0.612</td>
<td>224997445</td>
<td>10 (5 SEDT, 5 frame-wise model)</td>
<td>186 h (1 RTX A4000) + 35 h (3 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Li_ICT-TOSHIBA_task4_3</td>
<td>Li2022d</td>
<td>1.20</td>
<td>0.411</td>
<td>0.597</td>
<td>188469803</td>
<td>9 (4 SEDT, 5 frame-wise model)</td>
<td>53h (1 RTX A4000) + 30 h (3 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_2</td>
<td>Xie2022</td>
<td>0.83</td>
<td>0.062</td>
<td>0.800</td>
<td>166283314</td>
<td>8</td>
<td>20min (1 GTX 3080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_3</td>
<td>Xie2022</td>
<td>1.06</td>
<td>0.300</td>
<td>0.641</td>
<td>25054503</td>
<td>2</td>
<td>3h (1 GTX 3080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_1</td>
<td>Xie2022</td>
<td>1.36</td>
<td>0.418</td>
<td>0.757</td>
<td>225490527</td>
<td>8</td>
<td>2h (1 GTX 3080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Xie_UESTC_task4_4</td>
<td>Xie2022</td>
<td>1.38</td>
<td>0.426</td>
<td>0.766</td>
<td>225490527</td>
<td>8</td>
<td>2h (1 GTX 3080 Ti)</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline (AudioSet)</td>
<td>Ronchini2022</td>
<td>1.04</td>
<td>0.345</td>
<td>0.540</td>
<td>2200000</td>
<td></td>
<td>6h (1 GTX 1080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_1</td>
<td>Kim2022c</td>
<td>1.02</td>
<td>0.317</td>
<td>0.565</td>
<td>Trainable 1.7 M non-Trainable 1.7M</td>
<td></td>
<td>10h (1 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_2</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.340</td>
<td>0.544</td>
<td>Trainable 1.1 M non-Trainable 1.1M</td>
<td></td>
<td>9h (1 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Kim_CAUET_task4_3</td>
<td>Kim2022c</td>
<td>1.04</td>
<td>0.338</td>
<td>0.554</td>
<td>Trainable 1.1 M non-Trainable 1.1M</td>
<td></td>
<td>9h (1 RTX 2080 Ti)</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_1</td>
<td>Li2022c</td>
<td>1.10</td>
<td>0.364</td>
<td>0.570</td>
<td>4.2MB</td>
<td></td>
<td>7h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_3</td>
<td>Li2022c</td>
<td>1.17</td>
<td>0.371</td>
<td>0.635</td>
<td>4.2MB</td>
<td></td>
<td>7h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_4</td>
<td>Li2022c</td>
<td>0.93</td>
<td>0.195</td>
<td>0.683</td>
<td>4.2MB</td>
<td></td>
<td>7h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Li_XJU_task4_2</td>
<td>Li2022c</td>
<td>0.75</td>
<td>0.086</td>
<td>0.671</td>
<td>4.2MB</td>
<td></td>
<td>7h (1 Titan RTX)</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_3</td>
<td>Castorena2022</td>
<td>0.91</td>
<td>0.267</td>
<td>0.531</td>
<td>1100000</td>
<td></td>
<td>4h (1 GTX 3060 Ti)</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_1</td>
<td>Castorena2022</td>
<td>1.01</td>
<td>0.334</td>
<td>0.524</td>
<td>1100000</td>
<td></td>
<td>4h (1 GTX 3060 Ti)</td>
</tr>
<tr>
<td></td>
<td>Castorena_UV_task4_2</td>
<td>Castorena2022</td>
<td>0.63</td>
<td>0.072</td>
<td>0.559</td>
<td>1100000</td>
<td></td>
<td>4h (1 GTX 3060 Ti)</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2022/technical_reports_task4.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Bertola2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Bertola2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Data Augmentation Methods Exploration For Sound Event Detection
       </h4>
<p style="text-align:left">
        Bertola, Marco
       </p>
<p style="text-align:left">
<em>
         Universitat Pompeu Fabra, Barcelona, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bertola_UPF_task4_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bertola2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bertola2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bertola2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Bertola_88_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bertola2022" class="panel-collapse collapse" id="collapse-Bertola2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Data Augmentation Methods Exploration For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Bertola, Marco
       </small>
<br/>
<small>
<em>
         Universitat Pompeu Fabra, Barcelona, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report is describe the submission of a system for DCASE2022 Task4: Sound Event Detection in Domestic Environments 2022 [1]. Sound Event Detection (SED) systems have gained great attention in the past few years, motivated by emerging applications in several different fields such as smart homes, autonomous cars, and healthcare. Their performances can heavily depend on the availability of a large amount of strongly labeled data. Generating or retrieving this data is often difficult and costly. The aim of this work is to explore, combine and compare different data augmentation techniques to balance out the lack of strongly labeled data. As conclusion, the best result is submitted to DCASE 2022 Task4 challenge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bertola2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Bertola_88_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bertola2022label" class="modal fade" id="bibtex-Bertola2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBertola2022label">
        Data Augmentation Methods Exploration For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bertola2022,
    Author = "Bertola, Marco",
    title = "Data Augmentation Methods Exploration For Sound Event Detection",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report is describe the submission of a system for DCASE2022 Task4: Sound Event Detection in Domestic Environments 2022 [1]. Sound Event Detection (SED) systems have gained great attention in the past few years, motivated by emerging applications in several different fields such as smart homes, autonomous cars, and healthcare. Their performances can heavily depend on the availability of a large amount of strongly labeled data. Generating or retrieving this data is often difficult and costly. The aim of this work is to explore, combine and compare different data augmentation techniques to balance out the lack of strongly labeled data. As conclusion, the best result is submitted to DCASE 2022 Task4 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Blakala2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Blakala2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Dcase 2022 Task 4 Technical Report
       </h4>
<p style="text-align:left">
        Kornel, Błakała and Sikorski, Olaf
       </p>
<p style="text-align:left">
<em>
         Samsung R&amp;D Intsitute Poland, Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Blakala_SRPOL_task4_1</span> <span class="label label-primary">Blakala_SRPOL_task4_2</span> <span class="label label-primary">Blakala_SRPOL_task4_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Blakala2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Blakala2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Blakala2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Blakala_79_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Blakala2022" class="panel-collapse collapse" id="collapse-Blakala2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Dcase 2022 Task 4 Technical Report
      </h4>
<p style="text-align:left">
<small>
        Kornel, Błakała and Sikorski, Olaf
       </small>
<br/>
<small>
<em>
         Samsung R&amp;D Intsitute Poland, Warsaw, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes our solution for Task 4 of the 2022 edition of the Detection and Classification of Acoustic Scenes and Events competition. Our solution practically consists of two specialised systems that excel in either of the two scenarios in the challenge. Both utilise the CRNN model architecture and mean-teacher training setup proposed in the baseline solution. The modifications that they share are the replacement of the CNN extractor with a ResNet-18 architecture and the reduction of the FFT window from 2048 to 1024 samples. The systems diverge in four aspects: the set of augmentations selected and whether they use any additional techniques during training. For Scenario 1 we observed improvement when using pitch shift, while all other data augmentation methods resulted in lower PSDS. On the other hand, Scenario 2 benefited greatly from spectrogram time warping and adding brown noise. Further improvement on Scenario 2 was achieved by replacing attention with mean aggregation for weak predictions, incorporating per-frame embeddings from Audio Spectrogram Transformer (AST) and injecting Gaussian noise between teacher and student during consistency loss calculation. Curiously, these modifications diminished performance on Scenario 1. The system specialising in Scenario 1 scored [0.3743, 0.5826] and the system specialising in Scenario 2 scored [0.0701, 0.7938] in [P SDS1, P SDS2] respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Blakala2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Blakala_79_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Blakala2022label" class="modal fade" id="bibtex-Blakala2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBlakala2022label">
        Dcase 2022 Task 4 Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Blakala2022,
    Author = "Kornel, Błakała and Sikorski, Olaf",
    title = "Dcase 2022 Task 4 Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This paper describes our solution for Task 4 of the 2022 edition of the Detection and Classification of Acoustic Scenes and Events competition. Our solution practically consists of two specialised systems that excel in either of the two scenarios in the challenge. Both utilise the CRNN model architecture and mean-teacher training setup proposed in the baseline solution. The modifications that they share are the replacement of the CNN extractor with a ResNet-18 architecture and the reduction of the FFT window from 2048 to 1024 samples. The systems diverge in four aspects: the set of augmentations selected and whether they use any additional techniques during training. For Scenario 1 we observed improvement when using pitch shift, while all other data augmentation methods resulted in lower PSDS. On the other hand, Scenario 2 benefited greatly from spectrogram time warping and adding brown noise. Further improvement on Scenario 2 was achieved by replacing attention with mean aggregation for weak predictions, incorporating per-frame embeddings from Audio Spectrogram Transformer (AST) and injecting Gaussian noise between teacher and student during consistency loss calculation. Curiously, these modifications diminished performance on Scenario 1. The system specialising in Scenario 1 scored [0.3743, 0.5826] and the system specialising in Scenario 2 scored [0.0701, 0.7938] in [P SDS1, P SDS2] respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cheng2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Cheng2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For Dcase 2022 Task 4
       </h4>
<p style="text-align:left">
        Cheng, Yu-Han and Lu, Chung-Li and Chan, Bo-Cheng and Chuang, Hsiang-Feng
       </p>
<p style="text-align:left">
<em>
         Chunghwa Telecom Laboratories, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cheng_CHT_task4_1</span> <span class="label label-primary">Cheng_CHT_task4_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cheng2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cheng2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cheng2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Cheng_38_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cheng2022" class="panel-collapse collapse" id="collapse-Cheng2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For Dcase 2022 Task 4
      </h4>
<p style="text-align:left">
<small>
        Cheng, Yu-Han and Lu, Chung-Li and Chan, Bo-Cheng and Chuang, Hsiang-Feng
       </small>
<br/>
<small>
<em>
         Chunghwa Telecom Laboratories, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE 2022 Task4: sound event detection and separation in domestic environments. The proposed system is based on mean-teacher framework of semi-supervised learning and neural networks of CRNN. We employ consistency training of interpolation (ICT), shift (SCT), and clip-level (CCT) to enhance the generalization and representation. A multiscale CNN block is applied to extract various features to mitigate the influence of the event length diversity for the network. An efficient channel attention network (ECA-Net) and attention pooling enable the model to obtain definite sound event predictions. To further improve the performance, we use data augmentation including mixup, time shift, and filter augmentation. Our best system achieves the PSDS-scenario1 of 36.20% and PSDS-scenario2 of 63.45% on the validation set, significantly outperforming that of the baseline score of 32.93% and 53.22%, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cheng2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Cheng_38_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cheng2022label" class="modal fade" id="bibtex-Cheng2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCheng2022label">
        Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For Dcase 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cheng2022,
    Author = "Cheng, Yu-Han and Lu, Chung-Li and Chan, Bo-Cheng and Chuang, Hsiang-Feng",
    title = "Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For Dcase 2022 Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE 2022 Task4: sound event detection and separation in domestic environments. The proposed system is based on mean-teacher framework of semi-supervised learning and neural networks of CRNN. We employ consistency training of interpolation (ICT), shift (SCT), and clip-level (CCT) to enhance the generalization and representation. A multiscale CNN block is applied to extract various features to mitigate the influence of the event length diversity for the network. An efficient channel attention network (ECA-Net) and attention pooling enable the model to obtain definite sound event predictions. To further improve the performance, we use data augmentation including mixup, time shift, and filter augmentation. Our best system achieves the PSDS-scenario1 of 36.20\% and PSDS-scenario2 of 63.45\% on the validation set, significantly outperforming that of the baseline score of 32.93\% and 53.22\%, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="deBenito2022" style="box-shadow: none">
<div class="panel-heading" id="heading-deBenito2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Resolution Combination Of CRNN And Conformers For Dcase 2022 Task 4
       </h4>
<p style="text-align:left">
        de Benito-Gorron, Diego and Barahona, Sara and Segovia, Sergio and Ramos, Daniel and Toledano Doroteo
       </p>
<p style="text-align:left">
<em>
         AUDIAS Research Group, Universidad Autónoma de Madrid, Madrid, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">deBenito_AUDIAS_task4_1</span> <span class="label label-primary">deBenito_AUDIAS_task4_2</span> <span class="label label-primary">deBenito_AUDIAS_task4_3</span> <span class="label label-primary">deBenito_AUDIAS_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-deBenito2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-deBenito2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-deBenito2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_deBenito_61_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-deBenito2022" class="panel-collapse collapse" id="collapse-deBenito2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Resolution Combination Of CRNN And Conformers For Dcase 2022 Task 4
      </h4>
<p style="text-align:left">
<small>
        de Benito-Gorron, Diego and Barahona, Sara and Segovia, Sergio and Ramos, Daniel and Toledano Doroteo
       </small>
<br/>
<small>
<em>
         AUDIAS Research Group, Universidad Autónoma de Madrid, Madrid, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our submission to DCASE 2022 Task 4: Sound event detection in domestic environments. We follow a multi-resolution approach consisting on a late fusion of systems that are trained with different feature extraction parameters, aiming to leverage the characteristics of different event categories in time and frequency. Our systems are built upon the Convolutional-Recurrent Neural Network (CRNN) proposed by the baseline system and the Conformer structure proposed by the winners of the 2020 challenge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-deBenito2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_deBenito_61_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-deBenito2022label" class="modal fade" id="bibtex-deBenito2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexdeBenito2022label">
        Multi-Resolution Combination Of CRNN And Conformers For Dcase 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{deBenito2022,
    Author = "de Benito-Gorron, Diego and Barahona, Sara and Segovia, Sergio and Ramos, Daniel and Doroteo, Toledano",
    title = "Multi-Resolution Combination Of {CRNN} And Conformers For Dcase 2022 Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes our submission to DCASE 2022 Task 4: Sound event detection in domestic environments. We follow a multi-resolution approach consisting on a late fusion of systems that are trained with different feature extraction parameters, aiming to leverage the characteristics of different event categories in time and frequency. Our systems are built upon the Convolutional-Recurrent Neural Network (CRNN) proposed by the baseline system and the Conformer structure proposed by the winners of the 2020 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dinkel2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Dinkel2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Large Multi-Modal Ensemble For Sound Event Detection
       </h4>
<p style="text-align:left">
        Dinkel, Heinrich and Yan, Zhiyong and Wang, Yongqing and Song, Meixu and Zhang, Junbo and Wang, Wang
       </p>
<p style="text-align:left">
<em>
         Xiaomi Corporation, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dinkel_XiaoRice_task4_1</span> <span class="label label-primary">Dinkel_XiaoRice_task4_2</span> <span class="label label-primary">Dinkel_XiaoRice_task4_3</span> <span class="label label-primary">Dinkel_XiaoRice_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dinkel2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dinkel2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dinkel2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Dinkel_52_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dinkel2022" class="panel-collapse collapse" id="collapse-Dinkel2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Large Multi-Modal Ensemble For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Dinkel, Heinrich and Yan, Zhiyong and Wang, Yongqing and Song, Meixu and Zhang, Junbo and Wang, Wang
       </small>
<br/>
<small>
<em>
         Xiaomi Corporation, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper is a system description of the XiaoRice team submission to the DCASE 2022 Task 4 challenge. Our method focuses on merging commonly used convolutional neural networks (CNNs) with transformer-based methods and recurrent-neural networks (RNNs). We deliberately divide our efforts into optimizing the two evaluation metrics for the challenge: the onset and offset sensitive PSDS-1 score and the clip-level PSDS-2 score. This work shows that a large ensemble of differently trained architectures and frameworks can lead to significant gains. Our PSDS-1 optimized system consists of an 11-way convolutional recurrent neural network (CRNN), Vision transformer (ViT) fusion, and achieves a PSDS-1 score of 48.19. Further, our PSDS-2 system comprised of a 6-way CNN and ViT fusion achieved a PSDS-2 score of 87.70 on the development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dinkel2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Dinkel_52_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dinkel2022label" class="modal fade" id="bibtex-Dinkel2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDinkel2022label">
        A Large Multi-Modal Ensemble For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dinkel2022,
    Author = "Dinkel, Heinrich and Yan, Zhiyong and Wang, Yongqing and Song, Meixu and Zhang, Junbo and Wang, Wang",
    title = "A Large Multi-Modal Ensemble For Sound Event Detection",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This paper is a system description of the XiaoRice team submission to the DCASE 2022 Task 4 challenge. Our method focuses on merging commonly used convolutional neural networks (CNNs) with transformer-based methods and recurrent-neural networks (RNNs). We deliberately divide our efforts into optimizing the two evaluation metrics for the challenge: the onset and offset sensitive PSDS-1 score and the clip-level PSDS-2 score. This work shows that a large ensemble of differently trained architectures and frameworks can lead to significant gains. Our PSDS-1 optimized system consists of an 11-way convolutional recurrent neural network (CRNN), Vision transformer (ViT) fusion, and achieves a PSDS-1 score of 48.19. Further, our PSDS-2 system comprised of a 6-way CNN and ViT fusion achieved a PSDS-2 score of 87.70 on the development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ebbers2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Ebbers2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Pre-Training And Self-Training For Sound Event Detection In Domestic Environments
       </h4>
<p style="text-align:left">
        Ebbers, Janek and Haeb-Umbach, Reinhold
       </p>
<p style="text-align:left">
<em>
         Paderborn University, Paderborn, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ebbers_UPB_task4_1</span> <span class="label label-primary">Ebbers_UPB_task4_2</span> <span class="label label-primary">Ebbers_UPB_task4_3</span> <span class="label label-primary">Ebbers_UPB_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ebbers2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ebbers2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ebbers2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Ebbers_125_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Ebbers2022').collapse('show');window.location.hash='#Ebbers2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ebbers2022" class="panel-collapse collapse" id="collapse-Ebbers2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Pre-Training And Self-Training For Sound Event Detection In Domestic Environments
      </h4>
<p style="text-align:left">
<small>
        Ebbers, Janek and Haeb-Umbach, Reinhold
       </small>
<br/>
<small>
<em>
         Paderborn University, Paderborn, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report we present our system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge Task 4: Sound Event Detection in Domestic Environments 1 . As in previous editions of the Challenge, we use forward-backward convolutional recurrent neural networks (FBCRNNs) [1, 2] for weakly labeled and semi-supervised sound event detection (SED) and eventually generate strong pseudo labels for weakly labeled and unlabeled data. Then, (tag-conditioned) bidirectional CRNNs (Bi-CRNNs) [1, 2] are trained in a strongly supervised manner as our final SED models. In each of the training stages we use multiple iterations of self-training. Compared to previous editions, we improved our system performance by 1) some tweaks regarding data augmentation, pseudo labeling and inference 2) using weakly labeled AudioSet data [3] for pretraining larger networks and 3) augmenting the DESED data [4] with strongly labeled AudioSet data [5] for finetuning of the networks. Source code is publicly available at https://github.com/fgnt/pb_sed.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ebbers2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Ebbers_125_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/fgnt/pb_sed" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ebbers2022label" class="modal fade" id="bibtex-Ebbers2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEbbers2022label">
        Pre-Training And Self-Training For Sound Event Detection In Domestic Environments
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ebbers2022,
    Author = "Ebbers, Janek and Haeb-Umbach, Reinhold",
    title = "Pre-Training And Self-Training For Sound Event Detection In Domestic Environments",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this report we present our system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge Task 4: Sound Event Detection in Domestic Environments 1 . As in previous editions of the Challenge, we use forward-backward convolutional recurrent neural networks (FBCRNNs) [1, 2] for weakly labeled and semi-supervised sound event detection (SED) and eventually generate strong pseudo labels for weakly labeled and unlabeled data. Then, (tag-conditioned) bidirectional CRNNs (Bi-CRNNs) [1, 2] are trained in a strongly supervised manner as our final SED models. In each of the training stages we use multiple iterations of self-training. Compared to previous editions, we improved our system performance by 1) some tweaks regarding data augmentation, pseudo labeling and inference 2) using weakly labeled AudioSet data [3] for pretraining larger networks and 3) augmenting the DESED data [4] with strongly labeled AudioSet data [5] for finetuning of the networks. Source code is publicly available at https://github.com/fgnt/pb\_sed."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gan2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Gan2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised Sound Event Detection Based On Mean Teacher With Selective Kernel Multiscale Convolution And Resident Cam Clastering
       </h4>
<p style="text-align:left">
        Qiao, Ziling and Gan, Yanggang and Wu, Juan and Cai, Xichang and Wu, Menglong and Dong, Hongxia and Zhang, Lin Zhang and Liu, Zihan
       </p>
<p style="text-align:left">
<em>
         North China University of Technology, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Gan_NCUT_task4_1</span> <span class="label label-primary">Gan_NCUT_task4_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Gan2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gan2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gan2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Gan_14_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gan2022" class="panel-collapse collapse" id="collapse-Gan2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised Sound Event Detection Based On Mean Teacher With Selective Kernel Multiscale Convolution And Resident Cam Clastering
      </h4>
<p style="text-align:left">
<small>
        Qiao, Ziling and Gan, Yanggang and Wu, Juan and Cai, Xichang and Wu, Menglong and Dong, Hongxia and Zhang, Lin Zhang and Liu, Zihan
       </small>
<br/>
<small>
<em>
         North China University of Technology, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our submission system for DCASE 2022 Task4: sound event detection in domestic environments. The proposed system is based on mean teacher framework of semi-supervised learning and Selective Kernel Convolution Network. We use Multi-scale convolution to extract more abundant features of sound events. In order to improve the localization ability of the system, we use a dynamically selected attention mechanism called SK unit in CNN, which allows each neuron to adaptively adjust the size of its receptive field according to multiple scales of input information. Our system finally achieves the PSDS-scenario1 of 39.0% and PSDS-scenario2 of 58.50% on the validation set. In terms of innovative methods, this technical report will provide a technical description of system 2 submitted by the NCUT team. In system 2, the team selected the audio event monitoring method based on grad CAM clustering. This method attempts to use PANNs based migration learning network to generate grad CAM class activation diagram to locate the time point of the event. Finally, the adaptability of several different network models is evaluated, and the models with higher scores and better adaptability are probability fused to obtain the reasoning of events. Finally, the system 2 based on CAM clustering achieved 9.963% PSDS-scenario1 and 69.877% PSDS-scenario2 scores in the development data set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gan2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Gan_14_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gan2022label" class="modal fade" id="bibtex-Gan2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGan2022label">
        Semi-Supervised Sound Event Detection Based On Mean Teacher With Selective Kernel Multiscale Convolution And Resident Cam Clastering
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Gan2022,
    Author = "Qiao, Ziling and Gan, Yanggang and Wu, Juan and Cai, Xichang and Wu, Menglong and Dong, Hongxia and Zhang, Lin Zhang and Liu, Zihan",
    title = "Semi-Supervised Sound Event Detection Based On Mean Teacher With Selective Kernel Multiscale Convolution And Resident Cam Clastering",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we present our submission system for DCASE 2022 Task4: sound event detection in domestic environments. The proposed system is based on mean teacher framework of semi-supervised learning and Selective Kernel Convolution Network. We use Multi-scale convolution to extract more abundant features of sound events. In order to improve the localization ability of the system, we use a dynamically selected attention mechanism called SK unit in CNN, which allows each neuron to adaptively adjust the size of its receptive field according to multiple scales of input information. Our system finally achieves the PSDS-scenario1 of 39.0\% and PSDS-scenario2 of 58.50\% on the validation set. In terms of innovative methods, this technical report will provide a technical description of system 2 submitted by the NCUT team. In system 2, the team selected the audio event monitoring method based on grad CAM clustering. This method attempts to use PANNs based migration learning network to generate grad CAM class activation diagram to locate the time point of the event. Finally, the adaptability of several different network models is evaluated, and the models with higher scores and better adaptability are probability fused to obtain the reasoning of events. Finally, the system 2 based on CAM clustering achieved 9.963\% PSDS-scenario1 and 69.877\% PSDS-scenario2 scores in the development data set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Giannakopoulos2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Giannakopoulos2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Task Learning For Sound Event Detection Using Variational Autoencoders
       </h4>
<p style="text-align:left">
        Giannakopoulos, Petros<sup>1</sup> and Pikrakis, Aggelos<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>National and Kapodistrian University of Athens, Athens, Greece <sup>2</sup>University of Piraeus, Piraeus, Greece
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Giannakopoulos_UNIPI_task4_1</span> <span class="label label-primary">Giannakopoulos_UNIPI_task4_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Giannakopoulos2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Giannakopoulos2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Giannakopoulos2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Giannakopoulos_44_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Giannakopoulos2022" class="panel-collapse collapse" id="collapse-Giannakopoulos2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Task Learning For Sound Event Detection Using Variational Autoencoders
      </h4>
<p style="text-align:left">
<small>
        Giannakopoulos, Petros<sup>1</sup> and Pikrakis, Aggelos<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>National and Kapodistrian University of Athens, Athens, Greece <sup>2</sup>University of Piraeus, Piraeus, Greece
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents a multi-task learning model based on recurrent variational autoencoders (VAEs). The proposed method employs recurrent VAEs with shared parameters to simultaneously learn the tasks of strong labeling, weak labeling and feature sequence reconstruction. During the training stage, the model receives as input strongly labeled, weakly labeled data and unlabeled data and it simultaneously optimizes frame-based and file-based cross-entropy losses for strongly labeled and weakly labeled data, respectively, as well as the reconstruction loss for the unlabeled data. Using a shared posterior among all task branches, the model projects the input data for each task into a common latent space. The decoding of latents sampled from this common latent space, in combination with the shared parameters among task branches act jointly as a regularizer that prevents the model from overfitting to the individual tasks. The proposed method is evaluated on the DCASE-2022 Task4 dataset on which it achieves an event-based macro F1 score of 32.5% on the validation set and 31.8% on the public evaluation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Giannakopoulos2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Giannakopoulos_44_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Giannakopoulos2022label" class="modal fade" id="bibtex-Giannakopoulos2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGiannakopoulos2022label">
        Multi-Task Learning For Sound Event Detection Using Variational Autoencoders
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Giannakopoulos2022,
    Author = "Giannakopoulos, Petros and Pikrakis, Aggelos",
    title = "Multi-Task Learning For Sound Event Detection Using Variational Autoencoders",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report presents a multi-task learning model based on recurrent variational autoencoders (VAEs). The proposed method employs recurrent VAEs with shared parameters to simultaneously learn the tasks of strong labeling, weak labeling and feature sequence reconstruction. During the training stage, the model receives as input strongly labeled, weakly labeled data and unlabeled data and it simultaneously optimizes frame-based and file-based cross-entropy losses for strongly labeled and weakly labeled data, respectively, as well as the reconstruction loss for the unlabeled data. Using a shared posterior among all task branches, the model projects the input data for each task into a common latent space. The decoding of latents sampled from this common latent space, in combination with the shared parameters among task branches act jointly as a regularizer that prevents the model from overfitting to the individual tasks. The proposed method is evaluated on the DCASE-2022 Task4 dataset on which it achieves an event-based macro F1 score of 32.5\% on the validation set and 31.8\% on the public evaluation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hao2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Hao2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Dcase 2022 Task4 Challenge Technical Report
       </h4>
<p style="text-align:left">
        Hao, Junyong and Ye, Shunzhou and Lu, Cheng and Dong, Fei and Liu, Jingang
       </p>
<p style="text-align:left">
<em>
         UNISOC, Chongqing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hao_UNISOC_task4_1</span> <span class="label label-primary">Hao_UNISOC_task4_2</span><span class="label label-primary">Hao_UNISOC_task4_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hao2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hao2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Hao_95_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hao2022" class="panel-collapse collapse" id="collapse-Hao2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Dcase 2022 Task4 Challenge Technical Report
      </h4>
<p style="text-align:left">
<small>
        Hao, Junyong and Ye, Shunzhou and Lu, Cheng and Dong, Fei and Liu, Jingang
       </small>
<br/>
<small>
<em>
         UNISOC, Chongqing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report proposes a polyphonic sound event detection (SED) method for the DCASE 2022 Challenge Task 4-Sound Event Detection in Domestic Environments. We use the dataset of DESED to train our model, contains strongly labeled synthetic data, large unlabeled data, weakly labeled data and strongly labeled real data. To perform this task, we propose a DACRNN network for joint learning of SED and domain adaptation (DA).We consider the impact of the distribution within a single sound on the generalization performance of the model by mitigating the impact of complex background noise on event detection and the self-correlation consistency regularization of clip-level sound event classification, these make the intra-domain of a single sound smoother; for cross-domain adaptation, adversarial learning through feature extraction network with weighted frame-level domain discriminator. Experiments on the DCASE 2022 task4 validation dataset and public-evaluation dataset demonstrate the effectiveness of the techniques used in our system. Specifically, PSDS1 scores of 0.448 and PSDS2 scores of 0.853 are achieved for validation dataset, PSDS1 scores of 0.553 and PSDS2 scores of 0.836 are achieved for public-evaluation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Hao_95_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hao2022label" class="modal fade" id="bibtex-Hao2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHao2022label">
        Dcase 2022 Task4 Challenge Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hao2022,
    Author = "Hao, Junyong and Ye, Shunzhou and Lu, Cheng and Dong, Fei and Liu, Jingang",
    title = "Dcase 2022 Task4 Challenge Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This report proposes a polyphonic sound event detection (SED) method for the DCASE 2022 Challenge Task 4-Sound Event Detection in Domestic Environments. We use the dataset of DESED to train our model, contains strongly labeled synthetic data, large unlabeled data, weakly labeled data and strongly labeled real data. To perform this task, we propose a DACRNN network for joint learning of SED and domain adaptation (DA).We consider the impact of the distribution within a single sound on the generalization performance of the model by mitigating the impact of complex background noise on event detection and the self-correlation consistency regularization of clip-level sound event classification, these make the intra-domain of a single sound smoother; for cross-domain adaptation, adversarial learning through feature extraction network with weighted frame-level domain discriminator. Experiments on the DCASE 2022 task4 validation dataset and public-evaluation dataset demonstrate the effectiveness of the techniques used in our system. Specifically, PSDS1 scores of 0.448 and PSDS2 scores of 0.853 are achieved for validation dataset, PSDS1 scores of 0.553 and PSDS2 scores of 0.836 are achieved for public-evaluation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="He2022" style="box-shadow: none">
<div class="panel-heading" id="heading-He2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised Sound Event Detection System For Dcase 2022 Task 4
       </h4>
<p style="text-align:left">
        He, Kexin and Shu, Xin and Jia, Shaoyong and He, Yi
       </p>
<p style="text-align:left">
<em>
         Bytedance AI Lab, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">He_BYTEDANCE_task4_1</span> <span class="label label-primary">He_BYTEDANCE_task4_2</span><span class="label label-primary">He_BYTEDANCE_task4_3</span><span class="label label-primary">He_BYTEDANCE_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-He2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-He2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-He2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_He_40_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-He2022" class="panel-collapse collapse" id="collapse-He2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised Sound Event Detection System For Dcase 2022 Task 4
      </h4>
<p style="text-align:left">
<small>
        He, Kexin and Shu, Xin and Jia, Shaoyong and He, Yi
       </small>
<br/>
<small>
<em>
         Bytedance AI Lab, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we describe our submissions for the task 4 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge: Sound Event Detection in Domestic Environments. Our methods are mainly based on two types of deep learning models: Convolutional Recurrent Neural Network with selective kernel convolution (SK-CRNN) and frequency dynamic convolution (FDY-CRNN). In order to prevent overfitting, we adopt data augmentation using mixup strategy, FilterAugment, Interpolation Consistency Training (ICT) and Shift Consistency Training (SCT). Besides, we utilize external data and pretrained model to further improve performance, and try an ensemble of multiple subsystems to enhance the generalization capability of our system. Our final systems achieve a PSDS1/PSDS2 score of 0.5331/0.8569 on development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-He2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_He_40_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-He2022label" class="modal fade" id="bibtex-He2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHe2022label">
        Semi-Supervised Sound Event Detection System For Dcase 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{He2022,
    Author = "He, Kexin and Shu, Xin and Jia, Shaoyong and He, Yi",
    title = "Semi-Supervised Sound Event Detection System For Dcase 2022 Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this report, we describe our submissions for the task 4 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge: Sound Event Detection in Domestic Environments. Our methods are mainly based on two types of deep learning models: Convolutional Recurrent Neural Network with selective kernel convolution (SK-CRNN) and frequency dynamic convolution (FDY-CRNN). In order to prevent overfitting, we adopt data augmentation using mixup strategy, FilterAugment, Interpolation Consistency Training (ICT) and Shift Consistency Training (SCT). Besides, we utilize external data and pretrained model to further improve performance, and try an ensemble of multiple subsystems to enhance the generalization capability of our system. Our final systems achieve a PSDS1/PSDS2 score of 0.5331/0.8569 on development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Cht+Nsysu Sound Event Detection System With Different Kinds Of Pretrained Models For Dcase 2022 Task 4
       </h4>
<p style="text-align:left">
        Huang, Sung-Jen<sup>1</sup> and Liu, Chia-Chuan<sup>1</sup> and Chen, Chia-Ping<sup>1</sup> and Lu, Chung-Li<sup>2</sup> and Chan, Bo-Cheng<sup>2</sup> and Cheng, Yu-Han<sup>2</sup> and Chuang, Hsiang-Feng<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>National Sun Yat-Sen University, Taiwan <sup>2</sup>Chunghwa Telecom Laboratories, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Huang_NSYSU_task4_1</span> <span class="label label-primary">Liu_NSYSU_task4_2</span> <span class="label label-primary">Liu_NSYSU_task4_3</span><span class="label label-primary">Liu_NSYSU_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Huang_41_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2022" class="panel-collapse collapse" id="collapse-Huang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Cht+Nsysu Sound Event Detection System With Different Kinds Of Pretrained Models For Dcase 2022 Task 4
      </h4>
<p style="text-align:left">
<small>
        Huang, Sung-Jen<sup>1</sup> and Liu, Chia-Chuan<sup>1</sup> and Chen, Chia-Ping<sup>1</sup> and Lu, Chung-Li<sup>2</sup> and Chan, Bo-Cheng<sup>2</sup> and Cheng, Yu-Han<sup>2</sup> and Chuang, Hsiang-Feng<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>National Sun Yat-Sen University, Taiwan <sup>2</sup>Chunghwa Telecom Laboratories, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission system for DCASE 2022 Task4: sound event detection in domestic environments. We proposed two kinds of systems. One is trained by combining the mean teacher framework and knowledge distillation (one student model and two teacher models) without external data. While training this system, we first trained a mean teacher model to be a pretrained model. Our next step is to select the better one, the teacher or student model, to be the trained model for knowledge distillation. Afterword, we trained another mean teacher model with a different architecture using knowledge distillation. Finally, we repeat the select model step and knowledge distillation several times. The mean teacher model in the final round is composed of a VGG block, selective kernels and a clip level consistency branch. Comparing to the PSDS-scenario1 of 35.1% and PSDS-scenario2 of 55.2% of the baseline system trained without external data, the ensemble of this kind of system can achieve 43.7% and 68.0%, respectively. The other system can be separated into two parts. The first part is the top three layers of pretrained PANNs, while the second part is a similar system to baseline with only three convolution blocks. Then we trained the whole system (included PANNs) with DESED data. Ensembleing this system, the PSDS-scenario1 and 2 of 46.5% and 76.7% outperforms the baseline system (trained with AST embedding) of 31.3% and 72.2%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Huang_41_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2022label" class="modal fade" id="bibtex-Huang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2022label">
        Cht+Nsysu Sound Event Detection System With Different Kinds Of Pretrained Models For Dcase 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang2022,
    Author = "Huang, Sung-Jen and Liu, Chia-Chuan and Chen, Chia-Ping and Lu, Chung-Li and Chan, Bo-Cheng and Cheng, Yu-Han and Chuang, Hsiang-Feng",
    title = "Cht+Nsysu Sound Event Detection System With Different Kinds Of Pretrained Models For Dcase 2022 Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our submission system for DCASE 2022 Task4: sound event detection in domestic environments. We proposed two kinds of systems. One is trained by combining the mean teacher framework and knowledge distillation (one student model and two teacher models) without external data. While training this system, we first trained a mean teacher model to be a pretrained model. Our next step is to select the better one, the teacher or student model, to be the trained model for knowledge distillation. Afterword, we trained another mean teacher model with a different architecture using knowledge distillation. Finally, we repeat the select model step and knowledge distillation several times. The mean teacher model in the final round is composed of a VGG block, selective kernels and a clip level consistency branch. Comparing to the PSDS-scenario1 of 35.1\% and PSDS-scenario2 of 55.2\% of the baseline system trained without external data, the ensemble of this kind of system can achieve 43.7\% and 68.0\%, respectively. The other system can be separated into two parts. The first part is the top three layers of pretrained PANNs, while the second part is a similar system to baseline with only three convolution blocks. Then we trained the whole system (included PANNs) with DESED data. Ensembleing this system, the PSDS-scenario1 and 2 of 46.5\% and 76.7\% outperforms the baseline system (trained with AST embedding) of 31.3\% and 72.2\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Khandelwal2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Khandelwal2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Fmsg-Ntu Submission For Dcase 2022 Task 4 On Sound Event Detection In Domestic Environments
       </h4>
<p style="text-align:left">
        Khandelwal, Tanmay<sup>1,2</sup> and Das, Rohan Kumar<sup>1</sup> and Koh, Andrew<sup>2</sup> and Chng, Eng Siong<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Fortemedia Singapore, Singapore <sup>2</sup>Nanyang Technological University (NTU), Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Khandelwal_FMSG-NTU_task4_1</span> <span class="label label-primary">Khandelwal_FMSG-NTU_task4_2</span> <span class="label label-primary">Khandelwal_FMSG-NTU_task4_3</span><span class="label label-primary">Khandelwal_FMSG-NTU_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Khandelwal2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Khandelwal2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Khandelwal2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Khandelwal_42_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Khandelwal2022" class="panel-collapse collapse" id="collapse-Khandelwal2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Fmsg-Ntu Submission For Dcase 2022 Task 4 On Sound Event Detection In Domestic Environments
      </h4>
<p style="text-align:left">
<small>
        Khandelwal, Tanmay<sup>1,2</sup> and Das, Rohan Kumar<sup>1</sup> and Koh, Andrew<sup>2</sup> and Chng, Eng Siong<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Fortemedia Singapore, Singapore <sup>2</sup>Nanyang Technological University (NTU), Singapore
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this work, we describe the jointly submitted systems by Fortemedia Singapore (FMSG) and Nanyang Technological University (NTU) for DCASE 2022 Task 4: sound event detection in domestic environments. The proposed framework is divided into two stages: Stage-1 focuses on the audio-tagging system, which assists the sound event detection system in Stage-2. We train the Stage-1 utilizing a strongly labeled set converted into weak predictions, a weakly labeled set, and an unlabeled set to develop an effective audio-tagging system. This audio-tagging system is then used to infer on the unlabeled set to generate reliable pseudo-weak labels, which are used together with the strongly labeled set and weakly labeled set to train the sound event detection system at Stage-2. In Stage-1, we used two different networks, which are frequency dynamic (FDY)-convolutional recurrent neural network (CRNN) and convolutional neural network (CNN)-14 based pretrained audio neural networks (PANNs) for our developed systems. While the system at Stage-2 is based on FDY-CRNN for all the systems submitted to the challenge. It is noted that the systems at both stages employ data augmentation to reduce the risk of overfitting, and apply adaptive post-processing techniques to further enhance the performance. On the DESED real validation dataset, we obtain the highest PSDS1 and PSDS2 of 0.474 and 0.840, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Khandelwal2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Khandelwal_42_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Khandelwal2022label" class="modal fade" id="bibtex-Khandelwal2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKhandelwal2022label">
        Fmsg-Ntu Submission For Dcase 2022 Task 4 On Sound Event Detection In Domestic Environments
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Khandelwal2022,
    Author = "Khandelwal, Tanmay and Das, Rohan Kumar and Koh, Andrew and Chng, Eng Siong",
    title = "Fmsg-Ntu Submission For Dcase 2022 Task 4 On Sound Event Detection In Domestic Environments",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this work, we describe the jointly submitted systems by Fortemedia Singapore (FMSG) and Nanyang Technological University (NTU) for DCASE 2022 Task 4: sound event detection in domestic environments. The proposed framework is divided into two stages: Stage-1 focuses on the audio-tagging system, which assists the sound event detection system in Stage-2. We train the Stage-1 utilizing a strongly labeled set converted into weak predictions, a weakly labeled set, and an unlabeled set to develop an effective audio-tagging system. This audio-tagging system is then used to infer on the unlabeled set to generate reliable pseudo-weak labels, which are used together with the strongly labeled set and weakly labeled set to train the sound event detection system at Stage-2. In Stage-1, we used two different networks, which are frequency dynamic (FDY)-convolutional recurrent neural network (CRNN) and convolutional neural network (CNN)-14 based pretrained audio neural networks (PANNs) for our developed systems. While the system at Stage-2 is based on FDY-CRNN for all the systems submitted to the challenge. It is noted that the systems at both stages employ data augmentation to reduce the risk of overfitting, and apply adaptive post-processing techniques to further enhance the performance. On the DESED real validation dataset, we obtain the highest PSDS1 and PSDS2 of 0.474 and 0.840, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2022a" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2022a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection System Using Fixmatch For Dcase 2022 Challenge Task 4
       </h4>
<p style="text-align:left">
        Kim, Changmin and Yang, Siyoung
       </p>
<p style="text-align:left">
<em>
         LG Electronics, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_LGE_task4_1</span> <span class="label label-primary">Kim_LGE_task4_2</span><span class="label label-primary">Kim_LGE_task4_3</span><span class="label label-primary">Kim_LGE_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2022a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2022a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2022a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kim_94_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2022a" class="panel-collapse collapse" id="collapse-Kim2022a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection System Using Fixmatch For Dcase 2022 Challenge Task 4
      </h4>
<p style="text-align:left">
<small>
        Kim, Changmin and Yang, Siyoung
       </small>
<br/>
<small>
<em>
         LG Electronics, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report proposes a sound event detection (SED) system in domestic environments for DCASE 2022 challenge task 4. In this system, the training method consists of two stages. In the stage 1, mean teacher (MT) and interpolation consistency training (ICT) are used. In the stage 2, FixMatch is additionally applied. We adopted the frequency dynamic convolution recurrent neural network (FDY-CRNN) structure as our model. In order to further improve the performance of polyphonic sound detection score (PSDS) scenario 2, three techniques were used. First, we applied a temperature parameter to the sigmoid function to obtain soft confidence value. Second, we used a weak SED that is a method that uses only weak predictions and sets the timestamp equal to the total duration of the audio clip. Third, the FSD50K dataset was added to the weakly labeled dataset, which helped the PSDS scenario 2. As a result, we obtained the best PSDS scenario 1 of 0.473, and best PSDS scenario 2 of 0.695 on the domestic environment SED real validation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2022a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kim_94_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2022alabel" class="modal fade" id="bibtex-Kim2022a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2022alabel">
        Sound Event Detection System Using Fixmatch For Dcase 2022 Challenge Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2022a,
    Author = "Kim, Changmin and Yang, Siyoung",
    title = "Sound Event Detection System Using Fixmatch For Dcase 2022 Challenge Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report proposes a sound event detection (SED) system in domestic environments for DCASE 2022 challenge task 4. In this system, the training method consists of two stages. In the stage 1, mean teacher (MT) and interpolation consistency training (ICT) are used. In the stage 2, FixMatch is additionally applied. We adopted the frequency dynamic convolution recurrent neural network (FDY-CRNN) structure as our model. In order to further improve the performance of polyphonic sound detection score (PSDS) scenario 2, three techniques were used. First, we applied a temperature parameter to the sigmoid function to obtain soft confidence value. Second, we used a weak SED that is a method that uses only weak predictions and sets the timestamp equal to the total duration of the audio clip. Third, the FSD50K dataset was added to the weakly labeled dataset, which helped the PSDS scenario 2. As a result, we obtained the best PSDS scenario 1 of 0.473, and best PSDS scenario 2 of 0.695 on the domestic environment SED real validation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2022b" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2022b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised Learning-Based Sound Event Detection Using Frequency-Channel-Wise Selective Kernel For Dcase Challenge 2022 Task 4
       </h4>
<p style="text-align:left">
        Kim, Ji Won Kim<sup>1</sup> and Lee, Geon Woo<sup>1</sup> and Kim, Hong Kook<sup>1,2</sup> and Seo, Yeon Sik<sup>3</sup> and Song, Il Hoon<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>AI Graduate School, Gwangju, Korea <sup>2</sup>Gwangju Institude of Science and Technology, Gwangju, Korea <sup>3</sup>I Lab., R&amp;D Center, Hanwha Techwin, Gyeonggi-do, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_GIST_task4_1</span> <span class="label label-primary">Kim_GIST_task4_2</span> <span class="label label-primary">Kim_GIST_task4_3</span><span class="label label-primary">Kim_GIST_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2022b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2022b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2022b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kim_103_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2022b" class="panel-collapse collapse" id="collapse-Kim2022b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised Learning-Based Sound Event Detection Using Frequency-Channel-Wise Selective Kernel For Dcase Challenge 2022 Task 4
      </h4>
<p style="text-align:left">
<small>
        Kim, Ji Won Kim<sup>1</sup> and Lee, Geon Woo<sup>1</sup> and Kim, Hong Kook<sup>1,2</sup> and Seo, Yeon Sik<sup>3</sup> and Song, Il Hoon<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>AI Graduate School, Gwangju, Korea <sup>2</sup>Gwangju Institude of Science and Technology, Gwangju, Korea <sup>3</sup>I Lab., R&amp;D Center, Hanwha Techwin, Gyeonggi-do, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we propose a mean-teacher model-based sound event detection (SED) model that uses semi-supervised learning to the labeled data deficiency problem for the DCASE 2022 Challenge Task 4. The mean-teacher model of the proposed SED model is based on a residual convolutional recurrent neural network (RCRNN) architecture, and the residual convolutional blocks in the RCRNN are modified to include the frequency-wise and/or channel-wise selective kernel attention (SKA), which is hereafter referred to as SKA-RCRNN. This enables the RCRNN to have an adaptive receptive field for different lengths of audio. In particular, the proposed SKA-RCRNN-based SED model is first trained on the training dataset, during which it generated pseudo-labeled data for weakly labeled and unlabeled data. Next, the noisy student model, which is also based on SKA-RCRNN, in the second stage is optimized via semi-supervised learning by using strongly labeled and pseudo-labeled data. Finally, several ensemble models are obtained from fivefold cross-validation SED models with various hyper-parameters, and some of them are selected as the submitted models that show higher F1 and polyphonic sound detection scores on the validation dataset of the DCASE 2022 Challenge Task 4 are selected for submission.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2022b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kim_103_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2022blabel" class="modal fade" id="bibtex-Kim2022b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2022blabel">
        Semi-Supervised Learning-Based Sound Event Detection Using Frequency-Channel-Wise Selective Kernel For Dcase Challenge 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2022b,
    Author = "Kim, Ji Won and Lee, Geon Woo and Kim, Hong Kook and Seo, Yeon Sik and Song, Il Hoon",
    title = "Semi-Supervised Learning-Based Sound Event Detection Using Frequency-Channel-Wise Selective Kernel For Dcase Challenge 2022 Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this report, we propose a mean-teacher model-based sound event detection (SED) model that uses semi-supervised learning to the labeled data deficiency problem for the DCASE 2022 Challenge Task 4. The mean-teacher model of the proposed SED model is based on a residual convolutional recurrent neural network (RCRNN) architecture, and the residual convolutional blocks in the RCRNN are modified to include the frequency-wise and/or channel-wise selective kernel attention (SKA), which is hereafter referred to as SKA-RCRNN. This enables the RCRNN to have an adaptive receptive field for different lengths of audio. In particular, the proposed SKA-RCRNN-based SED model is first trained on the training dataset, during which it generated pseudo-labeled data for weakly labeled and unlabeled data. Next, the noisy student model, which is also based on SKA-RCRNN, in the second stage is optimized via semi-supervised learning by using strongly labeled and pseudo-labeled data. Finally, several ensemble models are obtained from fivefold cross-validation SED models with various hyper-parameters, and some of them are selected as the submitted models that show higher F1 and polyphonic sound detection scores on the validation dataset of the DCASE 2022 Challenge Task 4 are selected for submission."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2022c" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2022c" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The Cau-Et For Dcase 2022 Challenge Technical Reports
       </h4>
<p style="text-align:left">
        Kim, Narin and Lee, Sumi Lee and Kwak, Il youp
       </p>
<p style="text-align:left">
<em>
         Chung-Ang University, Department of Applied Statistics, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_CAUET_task4_1</span> <span class="label label-primary">Kim_CAUET_task4_2</span><span class="label label-primary">Kim_CAUET_task4_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2022c" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2022c" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2022c" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kim_108_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2022c" class="panel-collapse collapse" id="collapse-Kim2022c" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The Cau-Et For Dcase 2022 Challenge Technical Reports
      </h4>
<p style="text-align:left">
<small>
        Kim, Narin and Lee, Sumi Lee and Kwak, Il youp
       </small>
<br/>
<small>
<em>
         Chung-Ang University, Department of Applied Statistics, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, We present a semi-supervised learning method using RCRNN for DCASE 2022 challenge Task 4. We applied three main methods to improve the performance of sound event detection(SED). The first is semi-supervised network using RCRNN based on mean teacher model. The CNN part consists of residual convolution block with a CBAM[1] self-attention module which is stacked 5-layers, and the classification was performed with the RNN part. The second is the application of different data augmentation to features with different types of labels. Mix up, frame shift, time shift, time masking, and filter augmentation were applied to features, mix up was applied differently to the strong label and the weak label, and time masking was applied only to the strong labeled data. The third is to feed features that give different noise to student models and teacher models through data augmentation.The weight of the student model was shared with the teacher model by injecting different feature noise so that it could converge to the global optical faster through consistency loss.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2022c" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Kim_108_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2022clabel" class="modal fade" id="bibtex-Kim2022c" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2022clabel">
        The Cau-Et For Dcase 2022 Challenge Technical Reports
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2022c,
    Author = "Kim, Narin and Lee, Sumi Lee and Kwak, Il youp",
    title = "The Cau-Et For Dcase 2022 Challenge Technical Reports",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, We present a semi-supervised learning method using RCRNN for DCASE 2022 challenge Task 4. We applied three main methods to improve the performance of sound event detection(SED). The first is semi-supervised network using RCRNN based on mean teacher model. The CNN part consists of residual convolution block with a CBAM[1] self-attention module which is stacked 5-layers, and the classification was performed with the RNN part. The second is the application of different data augmentation to features with different types of labels. Mix up, frame shift, time shift, time masking, and filter augmentation were applied to features, mix up was applied differently to the strong label and the weak label, and time masking was applied only to the strong labeled data. The third is to feed features that give different noise to student models and teacher models through data augmentation.The weight of the student model was shared with the teacher model by injecting different feature noise so that it could converge to the global optical faster through consistency loss."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022b" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2022b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Two-Stage Training Method For Dcase 2022 Challenge Task4
       </h4>
<p style="text-align:left">
        Li, Kang and Zheng, Xu and Song, Yan
       </p>
<p style="text-align:left">
<em>
         University of Science and Technology of China, Hefei, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_USTC_task4_SED_1</span> <span class="label label-primary">Li_USTC_task4_SED_2</span><span class="label label-primary">Li_USTC_task4_SED_3</span><span class="label label-primary">Li_USTC_task4_SED_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2022b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_56_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022b" class="panel-collapse collapse" id="collapse-Li2022b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Two-Stage Training Method For Dcase 2022 Challenge Task4
      </h4>
<p style="text-align:left">
<small>
        Li, Kang and Zheng, Xu and Song, Yan
       </small>
<br/>
<small>
<em>
         University of Science and Technology of China, Hefei, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The goal of DCASE 2022 CHALLENGE TASK4 is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled, simulated data that is strongly labeled and external data. In this technical report, we present a two-stage learning strategy based method to explore synthetic strong data and real strong data (from AudioSet). Specifically, a CRNN model is used as the baseline SED system for this year’s challenge. According to different supervisory signals from weakly-labeled and strongly-labeled data, the frame-level and clip-level tasks (i.e. SED and Audio Tagging (AT)) are designed. In the first stage, the model is trained on weakly labeled, unlabeled and synthetic data with strong labels under the semi-supervised learning framework, i.e. Mean Teacher (MT). There are two types of MT, including frame-level MT and clip-level MT, corresponding to the subsets with different supervisory signals. In the second stage, a new model is trained using pseudo-labeling scheme, in which the pre-trained teacher model is utilized to provide the pseudo-label of the real weakly and unlabeled data. Furthermore, we explore the strongly labeled real data as external one in both stages. Results on the DCASE2022 Task4 validation set verify the effectiveness of our proposed method with PSDS1 and PSDS2 of 0.479 and 0.785, outperforming the baseline results of 0.351 and 0.552 respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_56_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022blabel" class="modal fade" id="bibtex-Li2022b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022blabel">
        A Two-Stage Training Method For Dcase 2022 Challenge Task4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2022b,
    Author = "Li, Kang and Zheng, Xu and Song, Yan",
    title = "A Two-Stage Training Method For Dcase 2022 Challenge Task4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "The goal of DCASE 2022 CHALLENGE TASK4 is to evaluate systems for the detection of sound events using real data either weakly labeled or unlabeled, simulated data that is strongly labeled and external data. In this technical report, we present a two-stage learning strategy based method to explore synthetic strong data and real strong data (from AudioSet). Specifically, a CRNN model is used as the baseline SED system for this year’s challenge. According to different supervisory signals from weakly-labeled and strongly-labeled data, the frame-level and clip-level tasks (i.e. SED and Audio Tagging (AT)) are designed. In the first stage, the model is trained on weakly labeled, unlabeled and synthetic data with strong labels under the semi-supervised learning framework, i.e. Mean Teacher (MT). There are two types of MT, including frame-level MT and clip-level MT, corresponding to the subsets with different supervisory signals. In the second stage, a new model is trained using pseudo-labeling scheme, in which the pre-trained teacher model is utilized to provide the pseudo-label of the real weakly and unlabeled data. Furthermore, we explore the strongly labeled real data as external one in both stages. Results on the DCASE2022 Task4 validation set verify the effectiveness of our proposed method with PSDS1 and PSDS2 of 0.479 and 0.785, outperforming the baseline results of 0.351 and 0.552 respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022c" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2022c" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        An Effective Consistency Regularization Training Based Mean Teacher Method For Sound Event Detection
       </h4>
<p style="text-align:left">
        Li, Yunlong<sup>1,2</sup> and Hu, Ying<sup>1,2</sup> and Zhu, Xiujuan<sup>1,2</sup> and Xie, Yin<sup>1,2</sup> and Hou, Shijing<sup>1,2</sup> and Wang, Liusong<sup>1,2</sup> and Chen, Zihao<sup>1,2</sup> Wang, Mingyu<sup>1,2</sup> and Fang, Wenjie<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Xinjiang University, Urumqi, China<sup>2</sup>Key Laboratory of Signal Detection and Processing in Xinjiang, Urumqi, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_XJU_task4_1</span> <span class="label label-primary">Li_XJU_task4_2</span><span class="label label-primary">Li_XJU_task4_3</span><span class="label label-primary">Li_XJU_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2022c" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022c" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022c" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_80_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022c" class="panel-collapse collapse" id="collapse-Li2022c" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       An Effective Consistency Regularization Training Based Mean Teacher Method For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Li, Yunlong<sup>1,2</sup> and Hu, Ying<sup>1,2</sup> and Zhu, Xiujuan<sup>1,2</sup> and Xie, Yin<sup>1,2</sup> and Hou, Shijing<sup>1,2</sup> and Wang, Liusong<sup>1,2</sup> and Chen, Zihao<sup>1,2</sup> Wang, Mingyu<sup>1,2</sup> and Fang, Wenjie<sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Xinjiang University, Urumqi, China<sup>2</sup>Key Laboratory of Signal Detection and Processing in Xinjiang, Urumqi, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the system we submitted to DCASE2021 Task4: Sound Event Detection in Domestic Environments. Specifically, we apply three main techniques to improve the performance of the official baseline system. Firstly, to improve the detection and classification ability of the CRNN model, we propose to add an auxiliary branch to the CRNN network. Consistency loss of mean teacher method is improved by auxiliary branch. Secondly, we propose to add an MDTC module to the CRNN network so that the receptive fields of the network can be adjusted according to the short-term and long-term correlation. Thirdly, several data-augmentation strategies are adopted to improve the generalization capability of the network. Experiments on the DCASE2022 Task4 validation dataset demonstrate the effectiveness of the techniques used in our system. As a result, the best PSDS1 is 0.408 and the best PSDS2 is 0.754.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022c" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_80_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022clabel" class="modal fade" id="bibtex-Li2022c" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022clabel">
        An Effective Consistency Regularization Training Based Mean Teacher Method For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2022c,
    Author = "Li, Yunlong and Hu, Ying and Zhu, Xiujuan and Xie, Yin and Hou, Shijing and Wang, Liusong and Chen, Zihao Wang, Mingyu and Fang, Wenjie",
    title = "An Effective Consistency Regularization Training Based Mean Teacher Method For Sound Event Detection",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes the system we submitted to DCASE2021 Task4: Sound Event Detection in Domestic Environments. Specifically, we apply three main techniques to improve the performance of the official baseline system. Firstly, to improve the detection and classification ability of the CRNN model, we propose to add an auxiliary branch to the CRNN network. Consistency loss of mean teacher method is improved by auxiliary branch. Secondly, we propose to add an MDTC module to the CRNN network so that the receptive fields of the network can be adjusted according to the short-term and long-term correlation. Thirdly, several data-augmentation strategies are adopted to improve the generalization capability of the network. Experiments on the DCASE2022 Task4 validation dataset demonstrate the effectiveness of the techniques used in our system. As a result, the best PSDS1 is 0.408 and the best PSDS2 is 0.754."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022d" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2022d" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Hybrid System Of Sound Event Detection Transformer And Frame-Wise Model For Dcase 2022 Task 4
       </h4>
<p style="text-align:left">
        Li, Yiming<sup>1,2</sup> and Guo, Zhifang<sup>1,2</sup> and Ye, Zhirong<sup>1,2</sup> and Wang , Xiangdong<sup>1,2</sup> and Liu, Hong<sup>1</sup> and Qian, Yueliang<sup>1</sup> and Tao, Rui<sup>3</sup> and Yan, Long<sup>3</sup> and Ouchi, Kazushige<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_ICT-TOSHIBA_task4_1</span> <span class="label label-primary">Li_ICT-TOSHIBA_task4_2</span><span class="label label-primary">Li_ICT-TOSHIBA_task4_3</span><span class="label label-primary">Li_ICT-TOSHIBA_task4_4</span>
</p>
<p style="text-align:left">
<span class="label label-success">
         Judges’ award
        </span>
</p>
<button aria-controls="collapse-Li2022d" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022d" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022d" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_98_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Li2022d').collapse('show');window.location.hash='#Li2022d';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022d" class="panel-collapse collapse" id="collapse-Li2022d" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Hybrid System Of Sound Event Detection Transformer And Frame-Wise Model For Dcase 2022 Task 4
      </h4>
<p style="text-align:left">
<small>
        Li, Yiming<sup>1,2</sup> and Guo, Zhifang<sup>1,2</sup> and Ye, Zhirong<sup>1,2</sup> and Wang , Xiangdong<sup>1,2</sup> and Liu, Hong<sup>1</sup> and Qian, Yueliang<sup>1</sup> and Tao, Rui<sup>3</sup> and Yan, Long<sup>3</sup> and Ouchi, Kazushige<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe in detail our system for DCASE 2022 Task4. The system combines two considerably different models: an end-to-end Sound Event Detection Transformer (SEDT) and a frame-wise model (MLFL-CNN). The former is an event-wise model which learns event-level representations and predicts sound event categories and boundaries directly, while the latter is based on the widely-adopted frame-classification scheme, under which each frame is classified into event categories and event boundaries are obtained by post-processing such as thresholding and smoothing. For SEDT, self-supervised pre-training using unlabeled data is applied, and semi-supervised learning is adopted by using an online teacher, which is updated from the student model using the EMA strategy and generates reliable pseudo labels for weakly-labeled and unlabeled data. For the frame-wise model, the ICT-TOSHIBA system of DCASE 2021 Task 4 is used, which incorporates techniques such as focal loss and metric learning into a CNN model to form the MLFL-CNN model, adopts mean-teacher for semi-supervised learning, and uses a tag-condition CNN model to predict final results using the output of MLFL-CNN. Experimental results show that the hybrid system considerably outperforms either individual model, and achieves psds1 of 0.420 and psds2 of 0.783 on the validation set without external data. The code is available at https://github.com/965694547/Hybrid-system-of-frame-wise-model-and-SEDT.
      </p>
<p>
<strong>
        Awards:
       </strong>
       Judges’ award
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022d" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Li_98_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/965694547/Hybrid-system-of-frame-wise-model-and-SEDT" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022dlabel" class="modal fade" id="bibtex-Li2022d" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022dlabel">
        A Hybrid System Of Sound Event Detection Transformer And Frame-Wise Model For Dcase 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2022d,
    Author = "Li, Yiming and Guo, Zhifang and Ye, Zhirong and Wang, Xiangdong and Liu, Hong and Qian, Yueliang and Tao, Rui and Yan, Long and Ouchi, Kazushige",
    title = "A Hybrid System Of Sound Event Detection Transformer And Frame-Wise Model For Dcase 2022 Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe in detail our system for DCASE 2022 Task4. The system combines two considerably different models: an end-to-end Sound Event Detection Transformer (SEDT) and a frame-wise model (MLFL-CNN). The former is an event-wise model which learns event-level representations and predicts sound event categories and boundaries directly, while the latter is based on the widely-adopted frame-classification scheme, under which each frame is classified into event categories and event boundaries are obtained by post-processing such as thresholding and smoothing. For SEDT, self-supervised pre-training using unlabeled data is applied, and semi-supervised learning is adopted by using an online teacher, which is updated from the student model using the EMA strategy and generates reliable pseudo labels for weakly-labeled and unlabeled data. For the frame-wise model, the ICT-TOSHIBA system of DCASE 2021 Task 4 is used, which incorporates techniques such as focal loss and metric learning into a CNN model to form the MLFL-CNN model, adopts mean-teacher for semi-supervised learning, and uses a tag-condition CNN model to predict final results using the output of MLFL-CNN. Experimental results show that the hybrid system considerably outperforms either individual model, and achieves psds1 of 0.420 and psds2 of 0.783 on the validation set without external data. The code is available at https://github.com/965694547/Hybrid-system-of-frame-wise-model-and-SEDT."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Dcase 2022 Challenge Task4 Technical Report
       </h4>
<p style="text-align:left">
        Chen, Minjun<sup>1</sup> and Wang, Tian<sup>1</sup> and Shao, Jun<sup>1</sup> and Tang, Yiqi<sup>1</sup> and Liu, Yangyang<sup>1</sup> and Peng, Bo<sup>1</sup> and Chen, Jie<sup>1</sup> and Shao, Xi<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Samsung Research China-Nanjing, Nanjing, China <sup>2</sup>College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_SRCN_task4_1</span> <span class="label label-primary">Liu_SRCN_task4_2</span><span class="label label-primary">Liu_SRCN_task4_3</span><span class="label label-primary">Liu_SRCN_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Liu_112_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2022" class="panel-collapse collapse" id="collapse-Liu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Dcase 2022 Challenge Task4 Technical Report
      </h4>
<p style="text-align:left">
<small>
        Chen, Minjun<sup>1</sup> and Wang, Tian<sup>1</sup> and Shao, Jun<sup>1</sup> and Tang, Yiqi<sup>1</sup> and Liu, Yangyang<sup>1</sup> and Peng, Bo<sup>1</sup> and Chen, Jie<sup>1</sup> and Shao, Xi<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Samsung Research China-Nanjing, Nanjing, China <sup>2</sup>College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We describe our submitted systems for DCASE2022 Task4 in this technical report: Sound Event Detection in Domestic Environments. We propose three models to solve this problem. In the first model, we try to utilize all the training data provided. To be specific, firstly, we employ a joint model both for event classification and location based on strongly labeled data and weakly labeled data to propagate the clip level annotations on the unlabeled dataset, which is so called pseudo-label dataset. In order to link frame level strongly annotations with the weakly annotations, we introduce weighted average pooling scheme. Finally, the joint model trained on strongly labeled data, weakly labeled data and pseudo-label data are employed to solve the Task 4 problem. To utilize the external dataset and pre-trained model, we proposal a system which use pre-trained model to extract embedding, and to train a RNN decode to generate prediction finally. And the third system with some data augmentation methods based on the baseline CRNN. Our proposed systems achieve poly-phonic sound event detection scores (PSDS-scores) of 0.4428 (PSDS1) and 0.8266 (PSDS-scenario2) respectively on development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Liu_112_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2022label" class="modal fade" id="bibtex-Liu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2022label">
        Dcase 2022 Challenge Task4 Technical Report
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2022,
    Author = "Chen, Minjun and Wang, Tian and Shao, Jun and Tang, Yiqi and Liu, Yangyang and Peng, Bo and Chen, Jie and Shao, Xi",
    title = "Dcase 2022 Challenge Task4 Technical Report",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "We describe our submitted systems for DCASE2022 Task4 in this technical report: Sound Event Detection in Domestic Environments. We propose three models to solve this problem. In the first model, we try to utilize all the training data provided. To be specific, firstly, we employ a joint model both for event classification and location based on strongly labeled data and weakly labeled data to propagate the clip level annotations on the unlabeled dataset, which is so called pseudo-label dataset. In order to link frame level strongly annotations with the weakly annotations, we introduce weighted average pooling scheme. Finally, the joint model trained on strongly labeled data, weakly labeled data and pseudo-label data are employed to solve the Task 4 problem. To utilize the external dataset and pre-trained model, we proposal a system which use pre-trained model to extract embedding, and to train a RNN decode to generate prediction finally. And the third system with some data augmentation methods based on the baseline CRNN. Our proposed systems achieve poly-phonic sound event detection scores (PSDS-scores) of 0.4428 (PSDS1) and 0.8266 (PSDS-scenario2) respectively on development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mizobuchi2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Mizobuchi2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Mizobuchi Pco Team’s Submission For Dcase2022 Task4 -- Sound Event Detection Using External Resources
       </h4>
<p style="text-align:left">
        Mizobuchi, Shohei and Ohashi, Hiromasa and Izumi, Akitoshi and Kodama, Nobutaka
       </p>
<p style="text-align:left">
<em>
         Advanced Research Lab., R&amp;D Division, Panasonic Connect Co., Ltd., Fukuoka, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mizobuchi_PCO_task4_1</span> <span class="label label-primary">Mizobuchi_PCO_task4_2</span><span class="label label-primary">Mizobuchi_PCO_task4_3</span><span class="label label-primary">Mizobuchi_PCO_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mizobuchi2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mizobuchi2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mizobuchi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Mizobuchi_105_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mizobuchi2022" class="panel-collapse collapse" id="collapse-Mizobuchi2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Mizobuchi Pco Team’s Submission For Dcase2022 Task4 -- Sound Event Detection Using External Resources
      </h4>
<p style="text-align:left">
<small>
        Mizobuchi, Shohei and Ohashi, Hiromasa and Izumi, Akitoshi and Kodama, Nobutaka
       </small>
<br/>
<small>
<em>
         Advanced Research Lab., R&amp;D Division, Panasonic Connect Co., Ltd., Fukuoka, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this Technical report, we describe an overview and performance of the system we submitted for DCASE 2022 Task 4. We submitted the following 4 systems. System 1 is aimed to improve the performance of PSDS1 under the condition that external resources are not used. System 2 uses AudioSet as additional training dataset on System 1. System 3 uses System 1 with additional training dataset including not only AudioSet dataset but also synthetic dataset generated by ourselves, and changes the training conditions to improve the performance of PSDS2. System 4 adds PANNs pretrained model to System 3. The highest performance evaluated using “ development dataset ” in these systems is 0.4489 for PSDS1 and 0.8519 for PSDS2. Details will be described below.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mizobuchi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Mizobuchi_105_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mizobuchi2022label" class="modal fade" id="bibtex-Mizobuchi2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMizobuchi2022label">
        Mizobuchi Pco Team’s Submission For Dcase2022 Task4 -- Sound Event Detection Using External Resources
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mizobuchi2022,
    Author = "Mizobuchi, Shohei and Ohashi, Hiromasa and Izumi, Akitoshi and Kodama, Nobutaka",
    title = "Mizobuchi Pco Team’s Submission For Dcase2022 Task4 -- Sound Event Detection Using External Resources",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this Technical report, we describe an overview and performance of the system we submitted for DCASE 2022 Task 4. We submitted the following 4 systems. System 1 is aimed to improve the performance of PSDS1 under the condition that external resources are not used. System 2 uses AudioSet as additional training dataset on System 1. System 3 uses System 1 with additional training dataset including not only AudioSet dataset but also synthetic dataset generated by ourselves, and changes the training conditions to improve the performance of PSDS2. System 4 adds PANNs pretrained model to System 3. The highest performance evaluated using “ development dataset ” in these systems is 0.4489 for PSDS1 and 0.8519 for PSDS2. Details will be described below."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nam2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Nam2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Frequency Dependent Sound Event Detection For Dcase 2022 Challenge Task 4
       </h4>
<p style="text-align:left">
        Nam, Hyeonuk and Kim, Seong-Hu and Min, Deokki and Ko, Byeong-Yun and Choi, Seung-Deok and Park, Yong-Hwa
       </p>
<p style="text-align:left">
<em>
         Korea Advanced Institute of Science and Technology, Daejeon, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nam_KAIST_task4_1</span> <span class="label label-primary">Nam_KAIST_task4_2</span><span class="label label-primary">Nam_KAIST_task4_3</span><span class="label label-primary">Nam_KAIST_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nam2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nam2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nam2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Nam_67_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Nam2022').collapse('show');window.location.hash='#Nam2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nam2022" class="panel-collapse collapse" id="collapse-Nam2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Frequency Dependent Sound Event Detection For Dcase 2022 Challenge Task 4
      </h4>
<p style="text-align:left">
<small>
        Nam, Hyeonuk and Kim, Seong-Hu and Min, Deokki and Ko, Byeong-Yun and Choi, Seung-Deok and Park, Yong-Hwa
       </small>
<br/>
<small>
<em>
         Korea Advanced Institute of Science and Technology, Daejeon, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       While many deep learning methods on other domains have been applied to sound event detection (SED), differences between original domains of the methods and SED have not been appropriately considered so far. As SED uses audio data with two dimensions (time and frequency) for input, thorough comprehension on these two dimensions is essential for application of methods from other domains on SED. Previous works proved that methods those address on frequency dimension are especially powerful in SED. By applying FilterAugment and frequency dynamic convolution those are frequency dependent methods proposed to enhance SED performance, our submitted models achieved best PSDS 1 of 0.4704 and best PSDS 2 of 0.8224.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nam2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Nam_67_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/frednam93/FDY-SED" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nam2022label" class="modal fade" id="bibtex-Nam2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNam2022label">
        Frequency Dependent Sound Event Detection For Dcase 2022 Challenge Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nam2022,
    Author = "Nam, Hyeonuk and Kim, Seong-Hu and Min, Deokki and Ko, Byeong-Yun and Choi, Seung-Deok and Park, Yong-Hwa",
    title = "Frequency Dependent Sound Event Detection For Dcase 2022 Challenge Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "While many deep learning methods on other domains have been applied to sound event detection (SED), differences between original domains of the methods and SED have not been appropriately considered so far. As SED uses audio data with two dimensions (time and frequency) for input, thorough comprehension on these two dimensions is essential for application of methods from other domains on SED. Previous works proved that methods those address on frequency dimension are especially powerful in SED. By applying FilterAugment and frequency dynamic convolution those are frequency dependent methods proposed to enhance SED performance, our submitted models achieved best PSDS 1 of 0.4704 and best PSDS 2 of 0.8224."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ryu2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Ryu2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SKATTN team’s submission for DCASE 2022 Task 4 -- Sound Event Detection in Domestic Environments
       </h4>
<p style="text-align:left">
        Ryu, Myeonghoon and Byun, Jeunghyun and Oh, Hongseok and Lee, Suji and Park, Han
       </p>
<p style="text-align:left">
<em>
         Deeply Inc. Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">skattn_task4_1</span> <span class="label label-primary">skattn_task4_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ryu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ryu2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ryu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Ryu_121_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ryu2022" class="panel-collapse collapse" id="collapse-Ryu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SKATTN team’s submission for DCASE 2022 Task 4 -- Sound Event Detection in Domestic Environments
      </h4>
<p style="text-align:left">
<small>
        Ryu, Myeonghoon and Byun, Jeunghyun and Oh, Hongseok and Lee, Suji and Park, Han
       </small>
<br/>
<small>
<em>
         Deeply Inc. Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our submitted system for DCASE 2022 Task4: Sound Event Detection in Domestic Environments. There are two main aspects we considered to improve the performance of the official baseline system: (1) use of external datasets (2) designing a novel model SKATTN. Our newly proposed SKATNN model combines Selective Kernel Network (SKNet) with the self-attention blocks from the Transformer model. Motivated from the SKNet’s successful applications in Computer Vision and Audio domains, we adopted SKNet as a feature extractor for processing the input mel-spectrogram. We used self-attention blocks to process the spectro-temporal features since they are flexible in modeling short and long-range dependencies while being less susceptible to vanishing gradients which commonly occur in RNNs. Experiments on DCASE2022 task 4 validation dataset demonstrate that our system achieves PSDS1 + PSDS2 = 1.372 on the validation dataset, outperforming 0.872 of the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ryu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Ryu_121_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ryu2022label" class="modal fade" id="bibtex-Ryu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRyu2022label">
        SKATTN team’s submission for DCASE 2022 Task 4 -- Sound Event Detection in Domestic Environments
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ryu2022,
    Author = "Ryu, Myeonghoon and Byun, Jeunghyun and Oh, Hongseok and Lee, Suji and Park, Han",
    title = "SKATTN team’s submission for DCASE 2022 Task 4 -- Sound Event Detection in Domestic Environments",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we present our submitted system for DCASE 2022 Task4: Sound Event Detection in Domestic Environments. There are two main aspects we considered to improve the performance of the official baseline system: (1) use of external datasets (2) designing a novel model SKATTN. Our newly proposed SKATNN model combines Selective Kernel Network (SKNet) with the self-attention blocks from the Transformer model. Motivated from the SKNet’s successful applications in Computer Vision and Audio domains, we adopted SKNet as a feature extractor for processing the input mel-spectrogram. We used self-attention blocks to process the spectro-temporal features since they are flexible in modeling short and long-range dependencies while being less susceptible to vanishing gradients which commonly occur in RNNs. Experiments on DCASE2022 task 4 validation dataset demonstrate that our system achieves PSDS1 + PSDS2 = 1.372 on the validation dataset, outperforming 0.872 of the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Shao2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Shao2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Atst Self-Supervised Plus Rct Semi-Supervised Sound Event Detection: Submission To Dcase 2022 Challenge Task 4
       </h4>
<p style="text-align:left">
        Shao, Nian and Li, Xian and Li, Xiaofei
       </p>
<p style="text-align:left">
<em>
         Westlake University &amp; Westlake Institute for Advanced Study, Hangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">RCT-ATST_Westlake_task4_1</span> <span class="label label-primary">RCT-ATST_Westlake_task4_2</span><span class="label label-primary">RCT-ATST_Westlake_task4_3</span><span class="label label-primary">RCT-ATST_Westlake_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Shao2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Shao2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Shao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Shao_35_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Shao2022').collapse('show');window.location.hash='#Shao2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Shao2022" class="panel-collapse collapse" id="collapse-Shao2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Atst Self-Supervised Plus Rct Semi-Supervised Sound Event Detection: Submission To Dcase 2022 Challenge Task 4
      </h4>
<p style="text-align:left">
<small>
        Shao, Nian and Li, Xian and Li, Xiaofei
       </small>
<br/>
<small>
<em>
         Westlake University &amp; Westlake Institute for Advanced Study, Hangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our methods proposed for participating the Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge Task 4: Sound Event Detection in Domestic Environments. The proposed methods integrate a semi-supervised sound event detection model (called random consistency training, RCT) trained with the relatively small official dataset of the challenge, and a self-supervised model (called audio teacher-student transformer, ATST) trained with the very large AudioSet. RCT uses the baseline convolutional recurrent neural network (CRNN) of the challenge, and adopts a newly proposed semi-supervised learning scheme based on random data augmentation and a self-consistency loss. To integrate ATST into RCT, the feature extracted by ATST is concatenated with the feature extracted by the convolutional layers of RCT, and then fed to the RNN layers of RCT. It is found that these two types of feature are complementary and the performance can be largely improved by combining them. In development, RCT individually achieves 39.80% and 61.12% of P SDS 1 and P SDS 2 , respectively, which are improved to 45.99% and 70.65% by integrating the ATST feature, and further to 47.71% and 73.44% by ensembling five models with different training configurations.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Shao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Shao_35_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/SaoYear/RCT-ATST" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Shao2022label" class="modal fade" id="bibtex-Shao2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexShao2022label">
        Atst Self-Supervised Plus Rct Semi-Supervised Sound Event Detection: Submission To Dcase 2022 Challenge Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Shao2022,
    Author = "Shao, Nian and Li, Xian and Li, Xiaofei",
    title = "Atst Self-Supervised Plus Rct Semi-Supervised Sound Event Detection: Submission To Dcase 2022 Challenge Task 4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this report, we present our methods proposed for participating the Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge Task 4: Sound Event Detection in Domestic Environments. The proposed methods integrate a semi-supervised sound event detection model (called random consistency training, RCT) trained with the relatively small official dataset of the challenge, and a self-supervised model (called audio teacher-student transformer, ATST) trained with the very large AudioSet. RCT uses the baseline convolutional recurrent neural network (CRNN) of the challenge, and adopts a newly proposed semi-supervised learning scheme based on random data augmentation and a self-consistency loss. To integrate ATST into RCT, the feature extracted by ATST is concatenated with the feature extracted by the convolutional layers of RCT, and then fed to the RNN layers of RCT. It is found that these two types of feature are complementary and the performance can be largely improved by combining them. In development, RCT individually achieves 39.80\% and 61.12\% of P SDS 1 and P SDS 2 , respectively, which are improved to 45.99\% and 70.65\% by integrating the ATST feature, and further to 47.71\% and 73.44\% by ensembling five models with different training configurations."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sojeong2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Sojeong2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Hyu Submission For Dcase 2022 Task 4 -- Pa-Net: Patch-Based Attention For Sound Event Detection
       </h4>
<p style="text-align:left">
        Kim, Sojeong
       </p>
<p style="text-align:left">
<em>
         Hanyang University, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">KIM_HYU_task4_1</span> <span class="label label-primary">KIM_HYU_task4_2</span><span class="label label-primary">KIM_HYU_task4_3</span><span class="label label-primary">KIM_HYU_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sojeong2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sojeong2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sojeong2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Sojeong_133_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sojeong2022" class="panel-collapse collapse" id="collapse-Sojeong2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Hyu Submission For Dcase 2022 Task 4 -- Pa-Net: Patch-Based Attention For Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Kim, Sojeong
       </small>
<br/>
<small>
<em>
         Hanyang University, Seoul, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we describe details about submitted systems for DCASE 2022 challenge task 4: sound event detection in domestic environments. We focus on how to effectively use a spectrogram as input for SED model since it has different time-frequency characteristics. Frequencies have various characteristics for some reasons like recording devices and type of sound event. Specifically, each time frame has different features from each other due to uncertainty on whether any sound event may happen or not in an audio clip and what type of sound event. Therefore, we propose a patch attention(PA) mechanism capturing patch-range dependencies across input sequences so that the model can learn by training with important local information. We use PA with efficient channel attention for learning important channels in feature maps. In addition, we adopt a strategy called subspectral normalization (SSN), which split the input frequencies into multiple sub-groups and normalizes each group to stand out specific features. Experiments result on the DESED 2022 validation dataset show that our proposed model outperforms the baseline system. Particularly, our model demonstrates improvement in performance on PSDS scores of 0.4438 and 0.683 on scenario1 and scenario2 respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sojeong2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Sojeong_133_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sojeong2022label" class="modal fade" id="bibtex-Sojeong2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSojeong2022label">
        Hyu Submission For Dcase 2022 Task 4 -- Pa-Net: Patch-Based Attention For Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sojeong2022,
    Author = "Kim, Sojeong",
    title = "Hyu Submission For Dcase 2022 Task 4 -- Pa-Net: Patch-Based Attention For Sound Event Detection",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this paper, we describe details about submitted systems for DCASE 2022 challenge task 4: sound event detection in domestic environments. We focus on how to effectively use a spectrogram as input for SED model since it has different time-frequency characteristics. Frequencies have various characteristics for some reasons like recording devices and type of sound event. Specifically, each time frame has different features from each other due to uncertainty on whether any sound event may happen or not in an audio clip and what type of sound event. Therefore, we propose a patch attention(PA) mechanism capturing patch-range dependencies across input sequences so that the model can learn by training with important local information. We use PA with efficient channel attention for learning important channels in feature maps. In addition, we adopt a strategy called subspectral normalization (SSN), which split the input frequencies into multiple sub-groups and normalizes each group to stand out specific features. Experiments result on the DESED 2022 validation dataset show that our proposed model outperforms the baseline system. Particularly, our model demonstrates improvement in performance on PSDS scores of 0.4438 and 0.683 on scenario1 and scenario2 respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Suh2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Suh2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Data Engineering For Noisy Student Model In Sound Event Detection
       </h4>
<p style="text-align:left">
        Suh, Sangwon and Lee, Dong Youn
       </p>
<p style="text-align:left">
<em>
         ReturnZero, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Suh_ReturnZero_task4_1</span> <span class="label label-primary">Suh_ReturnZero_task4_2</span><span class="label label-primary">Suh_ReturnZero_task4_3</span> <span class="label label-primary">Suh_ReturnZero_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Suh2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Suh2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Suh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Suh_120_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Suh2022" class="panel-collapse collapse" id="collapse-Suh2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Data Engineering For Noisy Student Model In Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Suh, Sangwon and Lee, Dong Youn
       </small>
<br/>
<small>
<em>
         ReturnZero, Seoul, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes the Sound Event Detection (SED) system for DCASE2022 Task4. We focused on combining data augmentation techniques for the SED mean-teacher system and selecting trainable samples from AudioSet. The neural architecture follows the baseline CRNN model, but a frequency dynamic convolution replaces each convolution layer except the first one. The cost function was also constructed identically to the baseline, but an asymmetric focal loss was used instead of binary cross-entropy for training the AudioSet. The best metrics in the validation set of our experiments were 0.473, 0.723 for PSDS 1 and 2, and 56.9% for color-based F1 scores.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Suh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Suh_120_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Suh2022label" class="modal fade" id="bibtex-Suh2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSuh2022label">
        Data Engineering For Noisy Student Model In Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Suh2022,
    Author = "Suh, Sangwon and Lee, Dong Youn",
    title = "Data Engineering For Noisy Student Model In Sound Event Detection",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This report describes the Sound Event Detection (SED) system for DCASE2022 Task4. We focused on combining data augmentation techniques for the SED mean-teacher system and selecting trainable samples from AudioSet. The neural architecture follows the baseline CRNN model, but a frequency dynamic convolution replaces each convolution layer except the first one. The cost function was also constructed identically to the baseline, but an asymmetric focal loss was used instead of binary cross-entropy for training the AudioSet. The best metrics in the validation set of our experiments were 0.473, 0.723 for PSDS 1 and 2, and 56.9\% for color-based F1 scores."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xiao2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Xiao2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Pretrained Models In Sound Event Detection For Dcase 2022 Challenge Task4
       </h4>
<p style="text-align:left">
        Xiao, Shengchang
       </p>
<p style="text-align:left">
<em>
         University of Chinese Academy of Sciences, department of Electronic Engineering, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xiao_UCAS_task4_1</span> <span class="label label-primary">Xiao_UCAS_task4_2</span><span class="label label-primary">Xiao_UCAS_task4_3</span> <span class="label label-primary">Xiao_UCAS_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xiao2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xiao2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xiao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Xiao_109_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xiao2022" class="panel-collapse collapse" id="collapse-Xiao2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Pretrained Models In Sound Event Detection For Dcase 2022 Challenge Task4
      </h4>
<p style="text-align:left">
<small>
        Xiao, Shengchang
       </small>
<br/>
<small>
<em>
         University of Chinese Academy of Sciences, department of Electronic Engineering, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submitted systems for dcase 2022 Challenge Task4: Sound Event Detection in Domestic Environments. Specifically, we submit two different systems respectively for PSDS1 and PSDS2. As PSDS2 focuses on avoiding confusion between classes rather than the localization of sound events, we only predict weak labels of clips to improve PSDS2. Moreover, we apply the pretrained neural networks including PANNs and SSAST in our systems to improve the generalization and robustness of our models. These pretrained models trained on large-scale datasets such as audioset can effectively alleviate the problems of lack of real training data. We fuse multiple pretrained models to make full use of the information of external data, which significantly improve the performance of our systems. In addition, we use various data augmentation techniques to expand provided data. According to the character of each sound event, we use the classwise median filter and further classify some confusing events. As a result, we achieve the best PSDS1 of of 0.481 and best PSDS2 of 0.826 on the DESED real validation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xiao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Xiao_109_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xiao2022label" class="modal fade" id="bibtex-Xiao2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXiao2022label">
        Pretrained Models In Sound Event Detection For Dcase 2022 Challenge Task4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xiao2022,
    Author = "Xiao, Shengchang",
    title = "Pretrained Models In Sound Event Detection For Dcase 2022 Challenge Task4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe our submitted systems for dcase 2022 Challenge Task4: Sound Event Detection in Domestic Environments. Specifically, we submit two different systems respectively for PSDS1 and PSDS2. As PSDS2 focuses on avoiding confusion between classes rather than the localization of sound events, we only predict weak labels of clips to improve PSDS2. Moreover, we apply the pretrained neural networks including PANNs and SSAST in our systems to improve the generalization and robustness of our models. These pretrained models trained on large-scale datasets such as audioset can effectively alleviate the problems of lack of real training data. We fuse multiple pretrained models to make full use of the information of external data, which significantly improve the performance of our systems. In addition, we use various data augmentation techniques to expand provided data. According to the character of each sound event, we use the classwise median filter and further classify some confusing events. As a result, we achieve the best PSDS1 of of 0.481 and best PSDS2 of 0.826 on the DESED real validation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xie2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Xie2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-Supervised Sound Event Detection Using Pretrained Model
       </h4>
<p style="text-align:left">
        Xie, Rong and Shi, Chuang and Zhang, Le and Li, Huiyong
       </p>
<p style="text-align:left">
<em>
         School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">RongXie_UESTC_task4_1</span> <span class="label label-primary">RongXie_UESTC_task4_2</span><span class="label label-primary">RongXie_UESTC_task4_3</span> <span class="label label-primary">RongXie_UESTC_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xie2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xie2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xie2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Xie_19_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xie2022" class="panel-collapse collapse" id="collapse-Xie2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-Supervised Sound Event Detection Using Pretrained Model
      </h4>
<p style="text-align:left">
<small>
        Xie, Rong and Shi, Chuang and Zhang, Le and Li, Huiyong
       </small>
<br/>
<small>
<em>
         School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, submitted systems for DCASE 2022 Task4 are described. Early output embeddings of CNN14 in PANNs with a CRNN is designed to achieve a good performance on PSDS-scenario1. The fully connected (FC) layer of CNN14 is replaced by output 10 categories for PSDS-scenario 2. Submitted systms achieve an overall PSDS-scores of 1.31 (0.460 for PSDS scenario 1 and 0.856 for PSDS scenario 2) on test set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xie2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Xie_19_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xie2022label" class="modal fade" id="bibtex-Xie2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXie2022label">
        Semi-Supervised Sound Event Detection Using Pretrained Model
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xie2022,
    Author = "Xie, Rong and Shi, Chuang and Zhang, Le and Li, Huiyong",
    title = "Semi-Supervised Sound Event Detection Using Pretrained Model",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, submitted systems for DCASE 2022 Task4 are described. Early output embeddings of CNN14 in PANNs with a CRNN is designed to achieve a good performance on PSDS-scenario1. The fully connected (FC) layer of CNN14 is replaced by output 10 categories for PSDS-scenario 2. Submitted systms achieve an overall PSDS-scores of 1.31 (0.460 for PSDS scenario 1 and 0.856 for PSDS scenario 2) on test set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Xu2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Srcb-Bit Team’s Submission For Dcase2022 Task4
       </h4>
<p style="text-align:left">
        Xu, Liang<sup>1,2</sup> and Wang, Lizhong<sup>2</sup> and Bi, Sijun<sup>1</sup> and Liu, Hanyue<sup>1</sup> and Wang, Jing<sup>1</sup> and Zhao, Shenghui<sup>1</sup> and Zheng, Yuxing<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Information and Electronics, Beijing Institute of Technology, Beijing, China <sup>2Samsung Research China-Beijing (SRC-B), Beijing, China
        </sup></em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xu_SRCB-BIT_task4_1</span> <span class="label label-primary">Xu_SRCB-BIT_task4_2</span><span class="label label-primary">Xu_SRCB-BIT_task4_3</span> <span class="label label-primary">Xu_SRCB-BIT_task4_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Xu_13_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2022" class="panel-collapse collapse" id="collapse-Xu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Srcb-Bit Team’s Submission For Dcase2022 Task4
      </h4>
<p style="text-align:left">
<small>
        Xu, Liang<sup>1,2</sup> and Wang, Lizhong<sup>2</sup> and Bi, Sijun<sup>1</sup> and Liu, Hanyue<sup>1</sup> and Wang, Jing<sup>1</sup> and Zhao, Shenghui<sup>1</sup> and Zheng, Yuxing<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>School of Information and Electronics, Beijing Institute of Technology, Beijing, China <sup>2Samsung Research China-Beijing (SRC-B), Beijing, China
        </sup></em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our submitted system for DCASE2022 Task4: Sound Event Detection in Domestic Environments. We propose three main ways to improve the performance of the network. First, we use the frequency dynamic convolution (FDY) which applies kernel that adapts to frequency components of input to improve physical inconsistency in 2D convolution on sound event detection (SED). Then, we propose a weight raised temporal contrastive loss based coherence learning to improve the continuity of event prediction and the switching efficiency of event boundaries. Third, we use pre-trained model PANNS in this task and propose two methods to fuse the features from PANNs and our model which improve the PSDS1 and PSDS2 score respectively. The system we submitted is based on the mean-teacher architecture, and the PSDS1 and PSDS2 score on the development dataset can reach 0.482 and 0.835 respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Xu_13_t4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2022label" class="modal fade" id="bibtex-Xu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2022label">
        Srcb-Bit Team’s Submission For Dcase2022 Task4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xu2022,
    Author = "Xu, Liang and Wang, Lizhong and Bi, Sijun and Liu, Hanyue and Wang, Jing and Zhao, Shenghui and Zheng, Yuxing",
    title = "Srcb-Bit Team’s Submission For Dcase2022 Task4",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we present our submitted system for DCASE2022 Task4: Sound Event Detection in Domestic Environments. We propose three main ways to improve the performance of the network. First, we use the frequency dynamic convolution (FDY) which applies kernel that adapts to frequency components of input to improve physical inconsistency in 2D convolution on sound event detection (SED). Then, we propose a weight raised temporal contrastive loss based coherence learning to improve the continuity of event prediction and the switching efficiency of event boundaries. Third, we use pre-trained model PANNS in this task and propose two methods to fuse the features from PANNs and our model which improve the PSDS1 and PSDS2 score respectively. The system we submitted is based on the mean-teacher architecture, and the PSDS1 and PSDS2 score on the development dataset can reach 0.482 and 0.835 respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>