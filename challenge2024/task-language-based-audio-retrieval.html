<!DOCTYPE html><html lang="en">
<head>
    <title>Language-Based Audio Retrieval - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2024/task-language-based-audio-retrieval">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Audio retrieval with human written captions. Challenge has ended. Full results for this task can be found in the Results page. If you are interested in the task, you can join us on the dedicated slack channel Description This subtask is concerned with retrieving audio signals using their sound content …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right navbar-tighter" id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2024</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2024/"><i class="fa fa-home"></i>&nbsp;Intro</a>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-t1"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-t2"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-t3"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-t4"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-t5"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-automated-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-t6"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-scene-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-t7"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2024/task-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-retrieval text-t8"></i>&nbsp;Task8&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2024/task-language-based-audio-retrieval"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-queried-audio-source-separation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-separation text-t9"></i>&nbsp;Task9&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-traffic text-t10"></i>&nbsp;Task10&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2024/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2024/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wave-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-t8"></i><i class="fa dc-retrieval fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Retrieval</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 8</span></span><img src="../images/logos/dcase/dcase2024_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Language-Based Audio Retrieval</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Huang Xie" class="img img-circle" src="/images/person/default.png" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Huang Xie</strong>
<a class="icon" href="mailto:huang.xie@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Tuomas Virtanen" class="img img-circle" src="/images/person/tuomas_virtanen.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Tuomas Virtanen</strong>
<a class="icon" href="mailto:tuomas.virtanen@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Romain Serizel" class="img img-circle" src="/images/person/romain_serizel.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Romain Serizel</strong>
<a class="icon" href="mailto:romain.serizel@loria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.loria.fr/en/">
                                University of Lorraine
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Etienne Labbé" class="img img-circle" src="/images/person/etienne_labbe.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Etienne Labbé</strong>
<a class="icon" href="mailto:etienne.labbe@irit.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.univ-tlse3.fr/home/about-us">Université Toulouse III – Paul Sabatier</a><br/>
<a class="text" href="https://www.irit.fr/en/home/">Institut de Recherche en Informatique de Toulouse</a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Thomas Pellegrini" class="img img-circle" src="/images/person/thomas_pellegrini.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Thomas Pellegrini</strong>
<a class="icon" href="mailto:thomas.pellegrini@irit.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.univ-tlse3.fr/home/about-us">Université Toulouse III – Paul Sabatier</a><br/>
<a class="text" href="https://www.irit.fr/en/home/">Institut de Recherche en Informatique de Toulouse</a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a></li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a>
<ul>
<li><a href="#excluded-data">Excluded data</a></li>
</ul>
</li>
<li><a href="#submission">Submission</a>
<ul>
<li><a href="#system-output-file">System output file</a></li>
<li><a href="#metadata-file">Metadata file</a></li>
<li><a href="#open-and-reproducible-research">Open and reproducible research</a></li>
</ul>
</li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#repository">Repository</a></li>
<li><a href="#results-for-the-development-dataset">Results for the development dataset</a></li>
</ul>
</li>
<li><a href="#citations">Citations</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">Audio retrieval with human written captions.</p>
<p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found in the <a class="btn btn-default btn-xs" href="/challenge2024/task-language-based-audio-retrieval-results">Results <i class="fa fa-caret-right"></i></a> page.
</p>
<div class="alert alert-info">
    If you are interested in the task, you can join us on the <strong><a href="https://dcase.slack.com/archives/C01PU39CB2N">dedicated slack channel</a></strong>
</div>
<h1 id="description">Description</h1>
<p>This subtask is concerned with retrieving audio signals using their sound content textual descriptions (i.e., audio captions).
Human written audio captions will be used as text queries.
For each text query, the goal of this task is to retrieve 10 audio files from a given dataset and sort them based their match with the query.
Through this subtask, we aim to inspire further research into language-based audio retrieval with unconstrained textual descriptions.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2024/task8_audio_retrieval.png"/>
<figcaption>Figure 1: Overview of Language-based Audio Retrieval.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>This subtask will <strong>allow</strong> using <strong>pre-trained models</strong> and <strong>external data</strong> for training models.
This includes <strong>pre-trained models</strong> for embedding extraction from audio and/or captions, and <strong>pre-optimized methods</strong> for natural language processing like <em>part-of-speech (POS) tagging</em>.
Additionally, the participants can use <strong>external audio and/or textual data</strong>, e.g., external text corpus for learning a language model or additional audio data like <em>AudioSet</em>, <em>ESC-50</em>.</p>
<h1 id="audio-dataset">Audio dataset</h1>
<p>The development dataset for this subtask is <strong>Clotho v2</strong>, the same as in the <a href="https://dcase.community/challenge2023/task-automated-audio-captioning-and-language-based-audio-retrieval">DCASE 2023 Challenge (task 6b)</a>.
Specifically, <strong>each caption will be treated as a text query</strong>, and the corresponding audio file as the relevant item in retrieval.</p>
<p>The Clotho v2 dataset consists of audio samples of 15 to 30 seconds duration, with each audio sample having five captions of eight to 20 words length.
There is a total of 6974 audio samples, with 34870 captions (i.e. 6974 audio samples * 5 captions per each sample).
All audio samples are from the <a href="https://freesound.org/">Freesound platform</a>, and captions are crowd-sourced using <a href="https://www.mturk.com/">Amazon Mechanical Turk</a> and <a href="https://arxiv.org/abs/1907.09238">a three-step framework</a>.
For complete details on the data recording and processing see</p>
<div class="btex-item" data-item="Drossos_2020_icassp" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Drossos_2020_icassp"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen.
<em>Clotho: an audio captioning dataset.</em>
In Proc. IEEE Int. Conf. Acoustic., Speech and Signal Process. (ICASSP), 736–740. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1910.09387.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" class="panel-collapse collapse" id="collapseDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" role="tabpanel">
<h4>Clotho: an Audio Captioning Dataset</h4>
<h5>Abstract</h5>
<p class="text-justify">Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online (https://zenodo.org/record/3490684).</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1910.09387.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23label" class="modal fade" id="bibtexDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDrossos_2020_icassp2655b7e75a754a23823850e0916c2f23label">Clotho: an Audio Captioning Dataset</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Drossos_2020_icassp,
    author = "Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas",
    title = "Clotho: an Audio Captioning Dataset",
    booktitle = "Proc. IEEE Int. Conf. Acoustic., Speech and Signal Process. (ICASSP)",
    year = "2020",
    pages = "736-740",
    abstract = "Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online (https://zenodo.org/record/3490684)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>The data collection of Clotho received funding from the European Research Council, grant agreement 637422 EVERYSOUND.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="../images/sponsors/erc.jpg" title="ERC"/></a></p>
<h1 id="task-setup">Task setup</h1>
<h2 id="development-dataset">Development dataset</h2>
<p>The Clotho v2 dataset is currently divided into a development split of 3839 audio clips with 19195 captions, a validation split of 1045 audio clips with 5225 captions, and an evaluation split of 1045 audio clips with 5225 captions.
These splits are created by first constructing the sets of unique words of the captions of each audio clip.
These sets of words are combined to form the bag of words of the whole dataset, from which the frequency of a given word can be derived.
With the unique words of audio files as classes, <a href="https://github.com/trent-b/iterative-stratification">multi-label stratification</a> is applied.
More information on the splits of Clotho v2 can be found <a href="https://arxiv.org/pdf/1910.09387.pdf">here</a>.</p>
<p><strong>Please note that</strong> the name of the splits for Clotho v2 differ from the DCASE terminology.
To avoid confusion for participants, the correspondence of splits between Clotho v2 and DCASE challenge is:</p>
<table class="table table-responsive table-hover table-striped">
<thead>
<tr class="active">
<td><strong>Clotho naming of splits</strong></td>
<td colspan="2"><strong>DCASE Challenge naming of splits</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td class="success">development</td>
<td class="danger"><strong>training</strong></td>
<td class="danger" rowspan="3" style="vertical-align: middle;"><strong>development</strong></td>
</tr>
<tr>
<td class="success">validation</td>
<td class="danger"><strong>validation</strong></td>
</tr>
<tr>
<td class="success">evaluation</td>
<td class="danger"><strong>testing</strong></td>
</tr>
</tbody></table>
<p>For the rest of this text, the DCASE challenge terminology will be used.
For differentiating between Clotho development and evaluation, the terms development-training, development-validation, and development-testing will be used, wherever necessary.
<strong>Development-training</strong> refers to Clotho development split, <strong>development-validation</strong> refers to Clotho validation split, and <strong>development-testing</strong> refers to Clotho evaluation split.</p>
<p>The development data can be found at the online Zenodo repository.
<strong>Make sure that you download Clotho v2.1</strong>, as there were some minor fixes in the dataset (fixing of file naming and some corrupted files).</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/4783391" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-database fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/4783391" target="_blank">
<span style="font-size:20px;">Clotho v2.1 audio captioning dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(7.1 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.4783391">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.4783391.svg"/>
</a>
<span class="text-muted">
                
                version 2.1
                
                
                </span>
</div>
</div>
<p><br/></p>
<p>Development-training data are:</p>
<ul>
<li><code>clotho_audio_development.7z</code>: The development-training audio clips.</li>
<li><code>clotho_captions_development.csv</code>: The captions of the development-training audio clips.</li>
<li><code>clotho_metadata_development.csv</code>: The meta-data of the development-training audio clips.</li>
</ul>
<p>Development-validation data are:</p>
<ul>
<li><code>clotho_audio_validation.7z</code>: The development-validation audio clips.</li>
<li><code>clotho_captions_validation.csv</code>: The captions of the development-validation audio clips.</li>
<li><code>clotho_metadata_validation.csv</code>: The meta-data of the development-validation audio clips.</li>
</ul>
<p>Development-testing data are:</p>
<ul>
<li><code>clotho_audio_evaluation.7z</code>: The development-testing audio clips.</li>
<li><code>clotho_captions_evaluation.csv</code>: The captions of the development-testing audio clips.</li>
<li><code>clotho_metadata_evaluation.csv</code>: The meta-data of the development-testing audio clips.</li>
</ul>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>The evaluation dataset, the same as in the <a href="https://dcase.community/challenge2023/task-language-based-audio-retrieval">DCASE 2023 Challenge (task 6b)</a>, is reused for evaluation this year.
It consists of 1000 audio samples, with one caption per each.
The data are collected following the same procedure as <a href="https://arxiv.org/pdf/1910.09387.pdf">Clotho v2</a>.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/6590983" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-database fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/6590983" target="_blank">
<span style="font-size:20px;">Language-based audio retrieval DCASE 2024 evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(1.1 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.6590983">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.6590983.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="task-rules">Task rules</h1>
<p><strong>Participants are allowed to:</strong></p>
<ul>
<li>Use external data (e.g. audio files, text, annotations), except if Freesound data is involved (<a href="#excluded-data">see below</a>).</li>
<li>Use pre-trained models (e.g. text models like Word2Vec, audio tagging models, sound event detection models).</li>
<li>Augment the development dataset (i.e. development-training and development-testing) <strong>with or without</strong> the use of external data.</li>
<li>Use all the available metadata provided, but they <strong>must explicitly state it and indicate if they use the available metadata</strong>. This will not affect the rating of their method.</li>
</ul>
<p><strong>Participants are NOT allowed to:</strong></p>
<ul>
<li>Use Freesound data for training or validation, if these data overlap with the development-testing and the evaluation subsets of Clotho (<a href="#excluded-data">see below</a>).</li>
<li>Make subjective judgments of the evaluation (testing) data, nor to annotate it.</li>
<li>Use additional information of the evaluation (testing) data for their method, apart from the provided audio files and captions from the evaluation data.</li>
</ul>
<h2 id="excluded-data">Excluded data</h2>
<p>Since the Clotho dataset is extracted from <a href="https://freesound.org/">Freesound website</a>, any dataset crowdsourced from this website may have an overlap with the Clotho evaluation data.
To solve this issue, <strong>we published a CSV file containing the forbidden sound ids of Freesound</strong> (see also <strong>Task 6</strong>).
So if you use any data from Freesound (e.g., through WavCaps or FSD50K), you have to exclude them from your pretraining, training and validation data.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2024/dcase2024_task6_excluded_freesound_ids.csv" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2024/dcase2024_task6_excluded_freesound_ids.csv" target="_blank">
<span style="font-size:20px;">DCASE2024 Task 6 Excluded Freesound IDs <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(16 KB)</span>
<br/>
<span class="text-muted">
                
                version 1.0
                
                
                </span>
</div>
</div>
<p><br/></p>
<h1 id="submission">Submission</h1>
<p>All participants should submit:</p>
<ul>
<li>the output of their audio retrieval with the evaluation data (<code>*.csv</code> file),</li>
<li>metadata for their submission (<code>*.yaml</code> file), and</li>
<li>a technical report for their submission (<code>*.pdf</code> file).</li>
</ul>
<p>We allow up to 4 system output submissions per participant/team.
For each system, metadata should be provided in a separate file, containing the task specific information.
All files should be packaged into a zip file for submission.
Please make a clear connection between the system name in the submitted metadata (the <code>.yaml</code> file), submitted system output (the <code>.csv</code> file), and the technical report (the <code>.pdf</code> file)!
For indicating the connection of your files, you can consider using the following naming convention:</p>
<div class="highlight"><pre><span></span><code>&lt;author&gt;_&lt;institute&gt;_task8_&lt;submission_index&gt;_&lt;output or meta or technical_report&gt;.&lt;csv or yaml or pdf&gt;
</code></pre></div>
<p>For example:</p>
<div class="highlight"><pre><span></span><code>Xie_TAU_task8_1.output.csv
Xie_TAU_task8_1.meta.yaml
Xie_TAU_task8_1.technical_report.pdf
</code></pre></div>
<p>The field <code>&lt;submission_index&gt;</code> is to differentiate your submissions in case that you have multiple submissions.</p>
<h2 id="system-output-file">System output file</h2>
<p>The system output file should be a <code>*.csv</code> file, and should have the following 11 columns:</p>
<ol>
<li><code>caption</code>: query caption.</li>
<li><code>fname_1</code>: file name of the audio file that is <strong>most relevant</strong> to the query caption in <code>caption</code>.</li>
<li><code>fname_2</code>: file name of the audio file that is <strong>2nd</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_3</code>: file name of the audio file that is <strong>3rd</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_4</code>: file name of the audio file that is <strong>4th</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_5</code>: file name of the audio file that is <strong>5th</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_6</code>: file name of the audio file that is <strong>6th</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_7</code>: file name of the audio file that is <strong>7th</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_8</code>: file name of the audio file that is <strong>8th</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_9</code>: file name of the audio file that is <strong>9th</strong> most relevant to the query caption in <code>caption</code>.</li>
<li><code>fname_10</code>: file name of the audio file that is <strong>10th</strong> most relevant to the query caption in <code>caption</code>.</li>
</ol>
<p>Example output:</p>
<pre class="tab18">caption,				fname_1,	fname_2,	fname_3,	fname_4,	fname_5,	fname_6,	fname_7,	fname_8,	fname_9,	fname_10
The person is rummaging through the pans while looking for something,	fn1.wav,	fn2.wav,	fn3.wav,	fn4.wav,	fn5.wav,	fn6.wav,	fn7.wav,	fn8.wav,	fn9.wav,	fn10.wav</pre>
<p>In the example output, "<em>fn2.wav</em>" is the file name of the audio file that is <em>2nd</em> most relevant to the query caption "<em>The person is rummaging through the pans while looking for something</em>".</p>
<h2 id="metadata-file">Metadata file</h2>
<p>For each system, metadata should be provided in a separate file.
The file format should be as indicated below.</p>
<div aria-multiselectable="true" class="panel-group" id="metadata-A-accordion" role="tablist">
<div class="panel panel-default">
<div class="panel-heading" id="task8-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#metadata-A-accordion" data-toggle="collapse" href="#task8-example-collapse" role="button">
                   Metadata
                </a>
</h4>
</div>
<div aria-labelledby="task8-example-header" class="panel-collapse collapse" id="task8-example-collapse" role="tabpanel">
<div class="panel-body" style="padding: 0px">
<pre class="font110" style="padding:0;border:0;border-radius:0;"><code class="yaml"># Submission information for task 8
submission:
    # Submission label
    # Label is used to index submissions.
    # Generate your label following way to avoid overlapping codes among submissions:
    # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
    label: Xie_TAU_task8_1
    #
    # Submission name
    # This name will be used in the results tables when space permits
    name: DCASE2024 baseline system
    #
    # Submission name abbreviated
    # This abbreviated name will be used in the result table when space is tight.
    # Use maximum 10 characters.
    abbreviation: Baseline

    # Authors of the submitted system.
    # Mark authors in the order you want them to appear in submission lists.
    # One of the authors has to be marked as corresponding author,
    # this will be listed next to the submission in the results tables.
    authors:
        # First author
        -   lastname: Xie
            firstname: Huang
            email: huang.xie@tuni.fi                    # Contact email address
            corresponding: true                         # Mark true for one of the authors

            # Affiliation information for the author
            affiliation:
                abbreviation: TAU
                institute: Tampere University
                department: Computing Sciences
                location: Tampere, Finland

        # Second author
        -   lastname: Virtanen
            firstname: Tuomas
            email: tuomas.virtanen@tuni.fi

            affiliation:
                abbreviation: TAU
                institute: Tampere University
                department: Computing Sciences
                location: Tampere, Finland

# System information
system:
    # System description, meta-data provided here will be used to do meta analysis of the submitted system.
    # Use general level tags, when possible use the tags provided in comments.
    # If information field is not applicable to the system, use "!!null".
    description:

        # Audio input / sampling rate, e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
        input_sampling_rate: 44.1kHz

        # Acoustic representation
        # Here you should indicate what can or audio representation you used.
        # If your system used hand-crafted features (e.g. mel band energies), then you can do:
        #
        # `acoustic_features: mel energies`
        #
        # Else, if you used some pre-trained audio feature extractor, you can indicate the name of the system, for example:
        #
        # `acoustic_features: audioset`
        acoustic_features: log-mel energies

        # Text embeddings
        # Here you can indicate how you treated text embeddings.
        # If your method learned its own text embeddings (i.e. you did not use any pre-trained or fine-tuned NLP embeddings),
        # then you can do:
        #
        # `text_embeddings: learned`
        #
        # Else, specify the pre-trained or fine-tuned NLP embeddings that you used, for example:
        #
        # `text_embeddings: Sentece-BERT`
        text_embeddings: Sentece-BERT

        # Data augmentation methods for audio
        # e.g. mixup, time stretching, block mixing, pitch shifting, ...
        audio_augmentation: !!null

          # Data augmentation methods for text
        # e.g. random swapping, synonym replacement, ...
        text_augmentation: !!null

          # Learning scheme
          # Here you should indicate the learning scheme.
        # For example, you could specify either supervised, self-supervised, or even reinforcement learning.
        learning_scheme: self-supervised

        # Ensemble
        # Here you should indicate if you used ensemble of systems or not.
        ensemble: No

        # Audio modelling
        # Here you should indicate the type of system used for audio modelling.
        # For example, if you used some stacked CNNs, then you could do:
        #
        # audio_modelling: cnn
        #
        # If you used some pre-trained system for audio modelling, then you should indicate the system used,
        # for example, PANNs-CNN14, PANNs-ResNet38.
        audio_modelling: PANNs-CNN14

        # Text modelling
        # Similarly, here you should indicate the type of system used for text modelling.
        # For example, if you used some RNNs, then you could do:
        #
        # text_modelling: rnn
        #
        # If you used some pre-trained system for text modelling,
        # then you should indicate the system used (e.g. BERT).
        text_modelling: Sentece-BERT

        # Loss function
        # Here you should indicate the loss function that you employed.
        loss_function: InfoNCE

        # Optimizer
        # Here you should indicate the name of the optimizer that you used.
        optimizer: adam

        # Learning rate
        # Here you should indicate the learning rate of the optimizer that you used.
        learning_rate: 1e-3

        # Metric monitored
        # Here you should report the monitored metric for optimizing your method.
        # For example, did you monitor the loss on the validation data (i.e. validation loss)?
        # Or you monitored the training mAP?
        metric_monitored: validation_loss

    # System complexity, meta-data provided here will be used to evaluate
    # submitted systems from the computational load perspective.
    complexity:
        # Total amount of parameters used in the acoustic model.
        # For neural networks, this information is usually given before training process in the network summary.
        # For other than neural networks, if parameter count information is not directly
        # available, try estimating the count as accurately as possible.
        # In case of ensemble approaches, add up parameters for all subsystems.
        # In case embeddings are used, add up parameter count of the embedding
        # extraction networks and classification network
        # Use numerical value (do not use comma for thousands-separator).
        total_parameters: 732354

    # List of datasets used for the system (e.g., pre-training, fine-tuning, training).
    # Development-training data is used here only as example.
    training_datasets:
        -   name: Clotho-development
            purpose: training                           # Used for training system
            url: https://doi.org/10.5281/zenodo.4783391
            data_types: audio, caption                  # Contained data types, e.g., audio, caption, label.
            data_instances:
                audio: 3839                             # Number of contained audio instances
                caption: 19195                          # Number of contained caption instances
            data_volume:
                audio: 86353                            # Total amount durations (in seconds) of audio instances
                caption: 6453                           # Total word types in caption instances

        # More datasets
        #-   name:
        #    purpose: pre-training
        #    url:
        #    data_types: A, B, C
        #    data_instances:
        #        A: xxx
        #        B: xxx
        #        C: xxx
        #    data_volume:
        #        A: xxx
        #        B: xxx
        #        C: xxx

    # List of datasets used for validating the system, for example, optimizing hyperparameter.
    # Development-validation data is used here only as example.
    validation_datasets:
        -   name: Clotho-validation
            url: https://doi.org/10.5281/zenodo.4783391
            data_types: audio, caption
            data_instances:
                audio: 1045
                caption: 5225
            data_volume:
                audio: 23636
                caption: 2763

        # More datasets
        #-   name:
        #    url:
        #    data_types: A, B, C
        #    data_instances:
        #        A: xxx
        #        B: xxx
        #        C: xxx
        #    data_volume:
        #        A: xxx
        #        B: xxx
        #        C: xxx

    # URL to the source code of the system [optional]
    source_code: https://github.com/xieh97/dcase2023-audio-retrieval

# System results
results:
    development_testing:
        # System results for the development-testing split.
        # Full results are not mandatory, however, they are highly recommended as they are needed for through analysis of the challenge submissions.
        # If you are unable to provide all results, also incomplete results can be reported.
        R@1: 0.130
        R@5: 0.343
        R@10: 0.480
        mAP@10: 0.222
</code></pre>
</div>
</div>
</div>
</div>
<h2 id="open-and-reproducible-research">Open and reproducible research</h2>
<p>Finally, for <strong>supporting open and reproducible research</strong>, we kindly ask from each participant/team to consider making available the code of their method (e.g. in GitHub) and pre-trained models, <em>after the challenge is over</em>.</p>
<h1 id="evaluation">Evaluation</h1>
<p>The submitted systems will be evaluated according to their performance, i.e., <em>recall at K</em> (R@K) and <em>mean average precision at K</em> (mAP@K), on the withheld the evaluation dataset.
An explanation of these metrics can be found <a href="http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html">here</a>.
Specifically, the following metrics will be reported for every submitted method:</p>
<ol>
<li><code>R@1</code>: Recall score among the top-1 retrieved result, averaged across all caption queries.</li>
<li><code>R@5</code>: Recall score among the top-5 retrieved results, averaged across all caption queries.</li>
<li><code>R@10</code>: Recall score among the top-10 retrieved results, averaged across all caption queries.</li>
<li><code>mAP@10</code>: Average precision among the top-10 retrieved results, averaged across all caption queries.</li>
</ol>
<p>The R@K is a rank-unaware evaluation metric, which measures the proportion of relevant audio files among the top-K retrieved results to all relevant audio files in the evaluation dataset for the caption query.
The mAP@K is a rank-aware metric, which gives the mean of the averaged precision of relevant audio files among the top-K retrieved results for all caption queries.
Submitted methods will be ranked by the <strong>mAP@10</strong> metric.</p>
<p><br/></p>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="label" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="false" data-sort-name="test_mAP10" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true" rowspan="2">Official<br/>rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="1"></th>
</tr>
<tr>
<th class="sm-cell" data-field="label" data-sortable="false">Code</th>
<th class="sep-left-cell" data-field="author" data-sortable="false">Author</th>
<th class="sm-cell" data-field="affiliation" data-sortable="false">Affiliation</th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">Technical<br/>Report</th>
<th class="sep-left-cell text-center" data-axis-label="Rank Score" data-chartable="true" data-field="test_mAP10" data-sortable="true" data-value-type="float3">Rank Score</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Primus_CP-JKU_8_1</td>
<td>Paul Primus</td>
<td>Computational Perception, Johannes Kepler University, Linz, Austria</td>
<td>task-language-based-audio-retrieval-results#Primus2024_t8</td>
<td>0.416</td>
</tr>
<tr>
<td></td>
<td>Kulik_SRPOL_task8_4</td>
<td>Jan Kulik</td>
<td>Artificial Intelligence Team, Samsung R&amp;D Institute Poland, Warsaw, Poland</td>
<td>task-language-based-audio-retrieval-results#Kulik2024_t8</td>
<td>0.403</td>
</tr>
<tr>
<td></td>
<td>Chen_SRCN_task8_1</td>
<td>Minjun Chen</td>
<td>AI SW Team, Samsung Research China-Nanjing, Nanjing, China</td>
<td>task-language-based-audio-retrieval-results#Chen2024_t8</td>
<td>0.396</td>
</tr>
<tr>
<td></td>
<td>Munakata_LYVA_1</td>
<td>Hokuto Munakata</td>
<td>Video Analysis, LY Corporation, Tokyo, Japan</td>
<td>task-language-based-audio-retrieval-results#Munakata2024_t8</td>
<td>0.388</td>
</tr>
<tr>
<td></td>
<td>Kim_MAUM_task8_2</td>
<td>Jaeyeon Kim</td>
<td>Seoul National Unversity, Seoul, Republic of Korea</td>
<td>task-language-based-audio-retrieval-results#Kim2024_t8</td>
<td>0.363</td>
</tr>
<tr>
<td></td>
<td>Cai_NCUT_task8_2</td>
<td>Xichang Cai</td>
<td>School of Information, North China University of Technology, Beijing, China</td>
<td>task-language-based-audio-retrieval-results#Cai2024_t8</td>
<td>0.259</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Xie_tau_task8_1</td>
<td>Huang Xie</td>
<td>Computing Sciences, Tampere University, Tampere, Finland</td>
<td>task-language-based-audio-retrieval-results#Xie2024_t8</td>
<td>0.211</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2024/task-language-based-audio-retrieval-results">results page</a></p>
<h1 id="baseline-system">Baseline system</h1>
<p>The baseline system, the same as in the <a href="https://dcase.community/challenge2023/task-language-based-audio-retrieval">DCASE 2023 Challenge (task 6b)</a>, is reused this year.
The baseline system employs a bi-encoder architecture with a pretrained CNN14 (see <a href="https://arxiv.org/abs/1912.10211">PANNs</a>) being the audio encoder and the <a href="https://arxiv.org/abs/1908.10084">Sentence-BERT</a> (i.e., "all-mpnet-base-v2") being the text encoder.
The pretrained CNN14 is fine-tuned and the Sentence-BERT is frozen during training.
The relevant score between an audio signal and a textual description is calculated by the dot product of their audio embedding and text embedding.
The <a href="https://arxiv.org/abs/1807.03748">InfoNCE loss</a> is used to optimize the baseline system.</p>
<p>For details on PANNs, please see</p>
<div class="btex-item" data-item="Kong2020PANNs" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Kong2020PANNs"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley.
<em>PANNs: large-scale pretrained audio neural networks for audio pattern recognition.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, pages 2880–2894, 2020.
<a href="https://doi.org/10.1109/TASLP.2020.3030497">doi:10.1109/TASLP.2020.3030497</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexKong2020PANNs5dc24a5a836f466a854e8f15a398d300" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1912.10211.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseKong2020PANNs5dc24a5a836f466a854e8f15a398d300" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseKong2020PANNs5dc24a5a836f466a854e8f15a398d300" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingKong2020PANNs5dc24a5a836f466a854e8f15a398d300" class="panel-collapse collapse" id="collapseKong2020PANNs5dc24a5a836f466a854e8f15a398d300" role="tabpanel">
<h4>PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexKong2020PANNs5dc24a5a836f466a854e8f15a398d300" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1912.10211.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexKong2020PANNs5dc24a5a836f466a854e8f15a398d300label" class="modal fade" id="bibtexKong2020PANNs5dc24a5a836f466a854e8f15a398d300" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexKong2020PANNs5dc24a5a836f466a854e8f15a398d300label">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</h4>
</div>
<div class="modal-body">
<pre>@article{Kong2020PANNs,
    author = "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.",
    title = "{PANN}s: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    year = "2020",
    pages = "2880-2894",
    doi = "10.1109/TASLP.2020.3030497"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>For details on Sentence-BERT, please see</p>
<div class="btex-item" data-item="Reimers2019Sentence" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Reimers2019Sentence"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nils Reimers and Iryna Gurevych.
<em>Sentence-BERT: sentence embeddings using siamese BERT-networks.</em>
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. 2019.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1908.10084.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" class="panel-collapse collapse" id="collapseReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" role="tabpanel">
<h4>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1908.10084.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96blabel" class="modal fade" id="bibtexReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexReimers2019Sentence345b73d81fd24ca791dc2b2d0985c96blabel">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Reimers2019Sentence,
    author = "Reimers, Nils and Gurevych, Iryna",
    title = "Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    year = "2019"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>For details on InfoNCE loss, please see</p>
<div class="btex-item" data-item="Oord2018Representation" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Oord2018Representation"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
<em>Representation learning with contrastive predictive coding.</em>
2018.
URL: <a href="https://arxiv.org/abs/1807.03748">https://arxiv.org/abs/1807.03748</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexOord2018Representation57453870e3ea47cea145e48f6ae680b7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1807.03748.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseOord2018Representation57453870e3ea47cea145e48f6ae680b7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseOord2018Representation57453870e3ea47cea145e48f6ae680b7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingOord2018Representation57453870e3ea47cea145e48f6ae680b7" class="panel-collapse collapse" id="collapseOord2018Representation57453870e3ea47cea145e48f6ae680b7" role="tabpanel">
<h4>Representation Learning with Contrastive Predictive Coding</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexOord2018Representation57453870e3ea47cea145e48f6ae680b7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1807.03748.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexOord2018Representation57453870e3ea47cea145e48f6ae680b7label" class="modal fade" id="bibtexOord2018Representation57453870e3ea47cea145e48f6ae680b7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexOord2018Representation57453870e3ea47cea145e48f6ae680b7label">Representation Learning with Contrastive Predictive Coding</h4>
</div>
<div class="modal-body">
<pre>@misc{Oord2018Representation,
    author = "van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol",
    title = "Representation Learning with Contrastive Predictive Coding",
    year = "2018",
    url = "https://arxiv.org/abs/1807.03748"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>For information about submitted systems last year, please see</p>
<div class="btex-item" data-item="Xie2022Language" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Xie2022Language"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Huang Xie, Samuel Lipping, and Tuomas Virtanen.
<em>Language-based audio retrieval task in dcase 2022 challenge.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (<span class="bibtex-protected">DCASE</span>), 216–220. 2022.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexXie2022Language0754b1f131e3481a95e2be274cacb923" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2206.06108.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseXie2022Language0754b1f131e3481a95e2be274cacb923" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseXie2022Language0754b1f131e3481a95e2be274cacb923" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingXie2022Language0754b1f131e3481a95e2be274cacb923" class="panel-collapse collapse" id="collapseXie2022Language0754b1f131e3481a95e2be274cacb923" role="tabpanel">
<h4>Language-based Audio Retrieval Task in DCASE 2022 Challenge</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexXie2022Language0754b1f131e3481a95e2be274cacb923" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2206.06108.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexXie2022Language0754b1f131e3481a95e2be274cacb923label" class="modal fade" id="bibtexXie2022Language0754b1f131e3481a95e2be274cacb923" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexXie2022Language0754b1f131e3481a95e2be274cacb923label">Language-based Audio Retrieval Task in DCASE 2022 Challenge</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xie2022Language,
    author = "Xie, Huang and Lipping, Samuel and Virtanen, Tuomas",
    title = "Language-based Audio Retrieval Task in DCASE 2022 Challenge",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop ({DCASE})",
    year = "2022",
    pages = "216-220"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="repository">Repository</h2>
<p>The PyTorch implementation of the baseline system is freely available online, and can be found at GitHub.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/xieh97/dcase2023-audio-retrieval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/xieh97/dcase2023-audio-retrieval" target="_blank">
<span style="font-size:20px;">Language-based audio retrieval baseline system <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="results-for-the-development-dataset">Results for the development dataset</h2>
<p>The results of the baseline system on the development-evaluation split are shown below.</p>
<table class="table table-responsive table-hover table-striped table-sm">
<thead>
<tr class="active">
<td><strong>Metric</strong></td>
<td style="vertical-align:middle"><strong>Value</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>R1</td>
<td style="vertical-align:middle">0.130</td>
</tr>
<tr>
<td>R5</td>
<td style="vertical-align:middle">0.343</td>
</tr>
<tr>
<td>R10</td>
<td style="vertical-align:middle">0.480</td>
</tr>
<tr class="danger">
<td><strong>mAP10</strong></td>
<td style="vertical-align:middle"><strong>0.222</strong></td>
</tr>
</tbody></table>
<h1 id="citations">Citations</h1>
<p>If you participate in this task, you might want to check the following papers.
If you find a paper that need to be cited here, please contact us and report it to us.</p>
<ul>
<li>The Clotho dataset:</li>
</ul>
<div class="btex-item" data-item="Drossos_2020_icassp" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Drossos_2020_icassp"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen.
<em>Clotho: an audio captioning dataset.</em>
In Proc. IEEE Int. Conf. Acoustic., Speech and Signal Process. (ICASSP), 736–740. 2020.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1910.09387.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" class="panel-collapse collapse" id="collapseDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" role="tabpanel">
<h4>Clotho: an Audio Captioning Dataset</h4>
<h5>Abstract</h5>
<p class="text-justify">Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online (https://zenodo.org/record/3490684).</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1910.09387.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3label" class="modal fade" id="bibtexDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDrossos_2020_icassp02cf343636cb4ebe961058eb48a96ba3label">Clotho: an Audio Captioning Dataset</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Drossos_2020_icassp,
    author = "Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas",
    title = "Clotho: an Audio Captioning Dataset",
    booktitle = "Proc. IEEE Int. Conf. Acoustic., Speech and Signal Process. (ICASSP)",
    year = "2020",
    pages = "736-740",
    abstract = "Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online (https://zenodo.org/record/3490684)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>The CNN14 audio encoder, used for the baseline system:</li>
</ul>
<div class="btex-item" data-item="Kong2020PANNs" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Kong2020PANNs"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley.
<em>PANNs: large-scale pretrained audio neural networks for audio pattern recognition.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, pages 2880–2894, 2020.
<a href="https://doi.org/10.1109/TASLP.2020.3030497">doi:10.1109/TASLP.2020.3030497</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1912.10211.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" class="panel-collapse collapse" id="collapseKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" role="tabpanel">
<h4>PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1912.10211.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fclabel" class="modal fade" id="bibtexKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fc" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexKong2020PANNsbdc80f3dc40b4f9387e793176fb3e0fclabel">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</h4>
</div>
<div class="modal-body">
<pre>@article{Kong2020PANNs,
    author = "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.",
    title = "{PANN}s: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    year = "2020",
    pages = "2880-2894",
    doi = "10.1109/TASLP.2020.3030497"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>The Sentence-BERT text encoder, used for the baseline system:</li>
</ul>
<div class="btex-item" data-item="Reimers2019Sentence" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Reimers2019Sentence"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Nils Reimers and Iryna Gurevych.
<em>Sentence-BERT: sentence embeddings using siamese BERT-networks.</em>
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. 2019.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1908.10084.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" class="panel-collapse collapse" id="collapseReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" role="tabpanel">
<h4>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1908.10084.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1label" class="modal fade" id="bibtexReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexReimers2019Sentenceaf19acbc50fd487bb57e1ce5b9d50fa1label">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Reimers2019Sentence,
    author = "Reimers, Nils and Gurevych, Iryna",
    title = "Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    year = "2019"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>The InfoNCE loss, used for the baseline system:</li>
</ul>
<div class="btex-item" data-item="Oord2018Representation" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Oord2018Representation"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
<em>Representation learning with contrastive predictive coding.</em>
2018.
URL: <a href="https://arxiv.org/abs/1807.03748">https://arxiv.org/abs/1807.03748</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1807.03748.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" class="panel-collapse collapse" id="collapseOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" role="tabpanel">
<h4>Representation Learning with Contrastive Predictive Coding</h4>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1807.03748.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexOord2018Representation06adf4dbcb0c4441aafeadf1f605834dlabel" class="modal fade" id="bibtexOord2018Representation06adf4dbcb0c4441aafeadf1f605834d" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexOord2018Representation06adf4dbcb0c4441aafeadf1f605834dlabel">Representation Learning with Contrastive Predictive Coding</h4>
</div>
<div class="modal-body">
<pre>@misc{Oord2018Representation,
    author = "van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol",
    title = "Representation Learning with Contrastive Predictive Coding",
    year = "2018",
    url = "https://arxiv.org/abs/1807.03748"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/>
<br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>