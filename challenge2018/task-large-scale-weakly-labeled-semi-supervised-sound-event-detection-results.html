<!DOCTYPE html><html lang="en">
<head>
    <title>Large-scale weakly labeled semi-supervised sound event detection in domestic environments - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2018/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-general-purpose-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-bird-audio-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-monitoring-domestic-activities" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-home text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2018/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2018/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/dunes-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><i class="fa dc-large-scale fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Large-scale</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 4</span></span><img src="../images/logos/dcase/dcase2018_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Large-scale weakly labeled semi-supervised sound event detection in domestic environments</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without timestamps). The target of the systems is to provide <strong>not only the event class but also the event time boundaries</strong> given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to <strong>exploit a large amount of unbalanced and unlabeled training data</strong> together with a small weakly annotated training set to improve system performance. <strong>The labels in the annotated subset are verified and can be considered as reliable.</strong></p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_dev" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_1</td>
<td>PPF_system</td>
<td>Avdveeva2018</td>
<td>20.1</td>
<td>28.1</td>
</tr>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_2</td>
<td>PPF_system</td>
<td>Avdveeva2018</td>
<td>19.5</td>
<td>28.1</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.4</td>
<td>22.1</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.6</td>
<td>22.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.0</td>
<td>20.5</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.2</td>
<td>20.1</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_1</td>
<td>SJTU-ASR-GRU</td>
<td>Dinkel2018</td>
<td>10.4</td>
<td>13.4</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_2</td>
<td>SJTU-ASR-CRNN</td>
<td>Dinkel2018</td>
<td>10.7</td>
<td>13.7</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_3</td>
<td>SJTU-ASR-GAUSS</td>
<td>Dinkel2018</td>
<td>13.4</td>
<td>19.4</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_4</td>
<td>SJTU-CRNN</td>
<td>Dinkel2018</td>
<td>11.2</td>
<td>14.9</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_1</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>21.3</td>
<td>29.2</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_2</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>20.6</td>
<td>29.2</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_3</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>19.1</td>
<td>29.2</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_4</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>19.0</td>
<td>29.2</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_1</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>19.4</td>
<td>34.6</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_2</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>15.7</td>
<td>34.6</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_3</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>21.6</td>
<td>34.6</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_1</td>
<td>Hou_BUPT_1</td>
<td>Hou2018</td>
<td>19.6</td>
<td>32.7</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_2</td>
<td>Hou_BUPT_2</td>
<td>Hou2018</td>
<td>18.9</td>
<td>30.8</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_3</td>
<td>Hou_BUPT_3</td>
<td>Hou2018</td>
<td>20.9</td>
<td>33.0</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_4</td>
<td>Hou_BUPT_4</td>
<td>Hou2018</td>
<td>21.1</td>
<td>31.5</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_1</td>
<td>IRIT_WGRU_GRU_fusion</td>
<td>Cances2018</td>
<td>8.4</td>
<td>16.3</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_2</td>
<td>IRIT_MIL</td>
<td>Cances2018</td>
<td>16.6</td>
<td>24.6</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>20.6</td>
<td>29.3</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>29.8</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>24.5</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>22.4</td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_1</td>
<td>JKU_rcnn_threshold</td>
<td>Koutini2018</td>
<td>21.5</td>
<td>40.9</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_2</td>
<td>JKU_rcnn_prec</td>
<td>Koutini2018</td>
<td>21.1</td>
<td>40.2</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_3</td>
<td>JKU_rcnn_prec2</td>
<td>Koutini2018</td>
<td>20.6</td>
<td>40.2</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_4</td>
<td>JKU_rcnn_uth</td>
<td>Koutini2018</td>
<td>18.8</td>
<td>35.6</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_1</td>
<td>USTC_NEL1</td>
<td>Liu2018</td>
<td>27.3</td>
<td>42.4</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_2</td>
<td>USTC_NEL2</td>
<td>Liu2018</td>
<td>28.8</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_3</td>
<td>USTC_NEL3</td>
<td>Liu2018</td>
<td>28.1</td>
<td>50.3</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_4</td>
<td>USTC_NEL4</td>
<td>Liu2018</td>
<td>29.9</td>
<td>51.6</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_1</td>
<td>LJK_PSH_task4_1</td>
<td>Lu2018</td>
<td>24.1</td>
<td>28.6</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_2</td>
<td>LJK_PSH_task4_2</td>
<td>Lu2018</td>
<td>26.3</td>
<td>26.4</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_3</td>
<td>LJK_PSH_task4_3</td>
<td>Lu2018</td>
<td>29.5</td>
<td>27.2</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_4</td>
<td>LJK_PSH_task4_4</td>
<td>Lu2018</td>
<td>32.4</td>
<td>25.9</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_1</td>
<td>Yonsei_str_1</td>
<td>Moon2018</td>
<td>15.9</td>
<td>21.6</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_2</td>
<td>Yonsei_str_2</td>
<td>Moon2018</td>
<td>14.3</td>
<td>24.3</td>
</tr>
<tr>
<td></td>
<td>Raj_IITKGP_task4_1</td>
<td>Raj_IIT_KGP_Task4_1</td>
<td>Raj2018</td>
<td>9.4</td>
<td>21.9</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim_task4_1</td>
<td>Lim2018</td>
<td>17.1</td>
<td>21.9</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim_task4_2</td>
<td>Lim2018</td>
<td>18.0</td>
<td>23.1</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim_task4_3</td>
<td>Lim2018</td>
<td>19.6</td>
<td>28.4</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2018</td>
<td>20.4</td>
<td>29.3</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task4_2</td>
<td>BUPT_Attention</td>
<td>WangJ2018</td>
<td>17.9</td>
<td>27.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Serizel2018</td>
<td>10.8</td>
<td>14.1</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_1</td>
<td>SurreyCNN8</td>
<td>Kong2018</td>
<td>18.6</td>
<td>20.8</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_2</td>
<td>SurreyCNN4</td>
<td>Kong2018</td>
<td>16.7</td>
<td>20.8</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_3</td>
<td>SurreyFuse</td>
<td>Kong2018</td>
<td>24.0</td>
<td>26.7</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Event-based F-score (Evaluation dataset)" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_dev" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_1</td>
<td>PPF_system</td>
<td>Avdveeva2018</td>
<td>20.1</td>
<td>28.1</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.6</td>
<td>22.0</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_3</td>
<td>SJTU-ASR-GAUSS</td>
<td>Dinkel2018</td>
<td>13.4</td>
<td>19.4</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_1</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>21.3</td>
<td>29.2</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_3</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>21.6</td>
<td>34.6</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_4</td>
<td>Hou_BUPT_4</td>
<td>Hou2018</td>
<td>21.1</td>
<td>31.5</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_2</td>
<td>IRIT_MIL</td>
<td>Cances2018</td>
<td>16.6</td>
<td>24.6</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>22.4</td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_1</td>
<td>JKU_rcnn_threshold</td>
<td>Koutini2018</td>
<td>21.5</td>
<td>40.9</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_4</td>
<td>USTC_NEL4</td>
<td>Liu2018</td>
<td>29.9</td>
<td>51.6</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_4</td>
<td>LJK_PSH_task4_4</td>
<td>Lu2018</td>
<td>32.4</td>
<td>25.9</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_1</td>
<td>Yonsei_str_1</td>
<td>Moon2018</td>
<td>15.9</td>
<td>21.6</td>
</tr>
<tr>
<td></td>
<td>Raj_IITKGP_task4_1</td>
<td>Raj_IIT_KGP_Task4_1</td>
<td>Raj2018</td>
<td>9.4</td>
<td>21.9</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2018</td>
<td>20.4</td>
<td>29.3</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task4_2</td>
<td>BUPT_Attention</td>
<td>WangJ2018</td>
<td>17.9</td>
<td>27.0</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Serizel2018</td>
<td>10.8</td>
<td>14.1</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_3</td>
<td>SurreyFuse</td>
<td>Kong2018</td>
<td>24.0</td>
<td>26.7</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2018 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Liu_USTC_task4_1" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["Class_f_score_Alarm_bell_ringing", "Class_f_score_Blender", "Class_f_score_Cat", "Class_f_score_Dishes", "Class_f_score_Dog", "Class_f_score_Electric_shaver_toothbrush", "Class_f_score_Frying", "Class_f_score_Running_water", "Class_f_score_Speech", "Class_f_score_Vacuum_cleaner"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="f_score_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
            Submission<br/>code
        </th>
<th class="sm-cell" data-field="name" data-sortable="true">
            Submission<br/>name
        </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
            Technical<br/>Report
        </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
            Event-based<br/>F-score <br/>(Evaluation dataset)
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="Class_f_score_Alarm_bell_ringing" data-sortable="true" data-value-type="float1-percentage">
            Alarm<br/>Bell<br/>Ringing
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Blender" data-sortable="true" data-value-type="float1-percentage">
            Blender
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Cat" data-sortable="true" data-value-type="float1-percentage">
            Cat
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dishes" data-sortable="true" data-value-type="float1-percentage">
            Dishes
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Dog" data-sortable="true" data-value-type="float1-percentage">
            Dog
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Electric_shaver_toothbrush" data-sortable="true" data-value-type="float1-percentage">
            Electric<br/>shave<br/>toothbrush
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Frying" data-sortable="true" data-value-type="float1-percentage">
            Frying
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Running_water" data-sortable="true" data-value-type="float1-percentage">
            Running<br/>water
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Speech" data-sortable="true" data-value-type="float1-percentage">
            Speech
        </th>
<th class="text-center" data-chartable="true" data-field="Class_f_score_Vacuum_cleaner" data-sortable="true" data-value-type="float1-percentage">
            Vacuum<br/>cleaner
        </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_1</td>
<td>PPF_system</td>
<td>Avdveeva2018</td>
<td>20.1</td>
<td>33.3</td>
<td>15.2</td>
<td>14.9</td>
<td>6.3</td>
<td>16.3</td>
<td>15.8</td>
<td>24.6</td>
<td>13.3</td>
<td>27.2</td>
<td>34.8</td>
</tr>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_2</td>
<td>PPF_system</td>
<td>Avdveeva2018</td>
<td>19.5</td>
<td>33.3</td>
<td>11.8</td>
<td>14.9</td>
<td>6.3</td>
<td>16.3</td>
<td>13.1</td>
<td>24.6</td>
<td>13.3</td>
<td>27.2</td>
<td>34.7</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.4</td>
<td>6.8</td>
<td>14.1</td>
<td>2.6</td>
<td>0.8</td>
<td>2.7</td>
<td>29.3</td>
<td>20.2</td>
<td>11.2</td>
<td>1.3</td>
<td>35.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.6</td>
<td>6.7</td>
<td>14.4</td>
<td>2.5</td>
<td>1.1</td>
<td>2.6</td>
<td>29.7</td>
<td>22.0</td>
<td>11.1</td>
<td>1.3</td>
<td>34.0</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.0</td>
<td>7.2</td>
<td>17.8</td>
<td>4.2</td>
<td>2.3</td>
<td>3.0</td>
<td>26.2</td>
<td>13.7</td>
<td>10.0</td>
<td>2.7</td>
<td>32.5</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>NUDT-System</td>
<td>WangD2018</td>
<td>12.2</td>
<td>7.0</td>
<td>18.2</td>
<td>3.6</td>
<td>2.7</td>
<td>3.1</td>
<td>27.2</td>
<td>13.9</td>
<td>10.1</td>
<td>2.8</td>
<td>33.1</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_1</td>
<td>SJTU-ASR-GRU</td>
<td>Dinkel2018</td>
<td>10.4</td>
<td>12.2</td>
<td>17.1</td>
<td>2.0</td>
<td>2.7</td>
<td>5.4</td>
<td>12.2</td>
<td>0.0</td>
<td>6.0</td>
<td>23.7</td>
<td>22.6</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_2</td>
<td>SJTU-ASR-CRNN</td>
<td>Dinkel2018</td>
<td>10.7</td>
<td>12.9</td>
<td>15.9</td>
<td>0.6</td>
<td>4.4</td>
<td>5.3</td>
<td>7.5</td>
<td>0.0</td>
<td>9.9</td>
<td>30.6</td>
<td>20.0</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_3</td>
<td>SJTU-ASR-GAUSS</td>
<td>Dinkel2018</td>
<td>13.4</td>
<td>20.2</td>
<td>19.0</td>
<td>0.0</td>
<td>14.1</td>
<td>11.3</td>
<td>9.7</td>
<td>0.0</td>
<td>3.9</td>
<td>39.7</td>
<td>16.0</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_4</td>
<td>SJTU-CRNN</td>
<td>Dinkel2018</td>
<td>11.2</td>
<td>12.7</td>
<td>22.6</td>
<td>0.0</td>
<td>6.1</td>
<td>5.1</td>
<td>11.3</td>
<td>0.0</td>
<td>3.3</td>
<td>31.1</td>
<td>19.6</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_1</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>21.3</td>
<td>35.3</td>
<td>31.8</td>
<td>7.8</td>
<td>4.0</td>
<td>9.9</td>
<td>17.4</td>
<td>32.7</td>
<td>18.3</td>
<td>31.0</td>
<td>24.8</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_2</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>20.6</td>
<td>35.3</td>
<td>19.9</td>
<td>6.6</td>
<td>4.4</td>
<td>10.6</td>
<td>13.6</td>
<td>36.8</td>
<td>13.5</td>
<td>35.4</td>
<td>29.4</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_3</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>19.1</td>
<td>16.7</td>
<td>12.7</td>
<td>6.0</td>
<td>10.7</td>
<td>14.1</td>
<td>12.8</td>
<td>22.1</td>
<td>19.2</td>
<td>36.2</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_4</td>
<td>THU_multiCRNN</td>
<td>Guo2018</td>
<td>19.0</td>
<td>16.5</td>
<td>11.8</td>
<td>7.0</td>
<td>11.3</td>
<td>15.1</td>
<td>14.2</td>
<td>19.9</td>
<td>16.8</td>
<td>37.9</td>
<td>39.2</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_1</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>19.4</td>
<td>21.6</td>
<td>23.7</td>
<td>6.6</td>
<td>0.4</td>
<td>4.8</td>
<td>26.4</td>
<td>34.8</td>
<td>18.1</td>
<td>33.0</td>
<td>25.0</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_2</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>15.7</td>
<td>14.6</td>
<td>20.0</td>
<td>7.2</td>
<td>15.0</td>
<td>10.0</td>
<td>9.1</td>
<td>14.8</td>
<td>13.5</td>
<td>33.7</td>
<td>19.2</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_3</td>
<td>Harb_TUG</td>
<td>Harb2018</td>
<td>21.6</td>
<td>15.4</td>
<td>30.0</td>
<td>8.1</td>
<td>17.5</td>
<td>9.7</td>
<td>21.0</td>
<td>34.7</td>
<td>17.3</td>
<td>31.1</td>
<td>31.5</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_1</td>
<td>Hou_BUPT_1</td>
<td>Hou2018</td>
<td>19.6</td>
<td>38.6</td>
<td>18.4</td>
<td>3.5</td>
<td>22.2</td>
<td>20.4</td>
<td>31.5</td>
<td>1.4</td>
<td>14.4</td>
<td>37.6</td>
<td>8.5</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_2</td>
<td>Hou_BUPT_2</td>
<td>Hou2018</td>
<td>18.9</td>
<td>38.9</td>
<td>15.0</td>
<td>5.7</td>
<td>16.5</td>
<td>16.5</td>
<td>35.1</td>
<td>2.0</td>
<td>15.5</td>
<td>35.4</td>
<td>8.7</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_3</td>
<td>Hou_BUPT_3</td>
<td>Hou2018</td>
<td>20.9</td>
<td>43.8</td>
<td>12.2</td>
<td>10.0</td>
<td>23.4</td>
<td>18.3</td>
<td>9.2</td>
<td>10.9</td>
<td>15.6</td>
<td>37.3</td>
<td>28.4</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_4</td>
<td>Hou_BUPT_4</td>
<td>Hou2018</td>
<td>21.1</td>
<td>41.4</td>
<td>16.4</td>
<td>6.4</td>
<td>23.5</td>
<td>20.2</td>
<td>9.8</td>
<td>6.2</td>
<td>14.0</td>
<td>40.6</td>
<td>32.3</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_1</td>
<td>IRIT_WGRU_GRU_fusion</td>
<td>Cances2018</td>
<td>8.4</td>
<td>2.5</td>
<td>5.9</td>
<td>0.5</td>
<td>0.3</td>
<td>1.8</td>
<td>17.7</td>
<td>20.9</td>
<td>8.6</td>
<td>4.0</td>
<td>21.6</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_2</td>
<td>IRIT_MIL</td>
<td>Cances2018</td>
<td>16.6</td>
<td>23.8</td>
<td>5.1</td>
<td>25.3</td>
<td>0.7</td>
<td>4.1</td>
<td>6.5</td>
<td>18.3</td>
<td>15.0</td>
<td>22.3</td>
<td>44.9</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>20.6</td>
<td>36.0</td>
<td>13.0</td>
<td>20.0</td>
<td>13.1</td>
<td>24.4</td>
<td>22.0</td>
<td>0.0</td>
<td>10.4</td>
<td>34.5</td>
<td>32.7</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>32.5</td>
<td>21.7</td>
<td>18.6</td>
<td>13.4</td>
<td>25.4</td>
<td>24.7</td>
<td>0.0</td>
<td>7.8</td>
<td>34.2</td>
<td>31.3</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>37.2</td>
<td>20.4</td>
<td>17.8</td>
<td>12.4</td>
<td>24.5</td>
<td>16.9</td>
<td>0.0</td>
<td>10.4</td>
<td>34.0</td>
<td>35.1</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>JHU_T4</td>
<td>Kothinti2018</td>
<td>22.4</td>
<td>36.7</td>
<td>22.0</td>
<td>20.5</td>
<td>12.8</td>
<td>26.5</td>
<td>24.3</td>
<td>0.0</td>
<td>9.6</td>
<td>34.3</td>
<td>37.0</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_1</td>
<td>JKU_rcnn_threshold</td>
<td>Koutini2018</td>
<td>21.5</td>
<td>30.0</td>
<td>16.4</td>
<td>13.1</td>
<td>9.5</td>
<td>8.4</td>
<td>23.5</td>
<td>18.1</td>
<td>12.6</td>
<td>42.9</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_2</td>
<td>JKU_rcnn_prec</td>
<td>Koutini2018</td>
<td>21.1</td>
<td>30.0</td>
<td>15.8</td>
<td>13.1</td>
<td>9.5</td>
<td>8.4</td>
<td>23.5</td>
<td>17.6</td>
<td>12.1</td>
<td>42.0</td>
<td>39.2</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_3</td>
<td>JKU_rcnn_prec2</td>
<td>Koutini2018</td>
<td>20.6</td>
<td>30.0</td>
<td>15.8</td>
<td>12.9</td>
<td>9.3</td>
<td>8.5</td>
<td>22.7</td>
<td>16.1</td>
<td>12.3</td>
<td>40.9</td>
<td>37.6</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_4</td>
<td>JKU_rcnn_uth</td>
<td>Koutini2018</td>
<td>18.8</td>
<td>29.2</td>
<td>15.1</td>
<td>12.6</td>
<td>9.5</td>
<td>9.4</td>
<td>22.1</td>
<td>15.2</td>
<td>12.2</td>
<td>41.1</td>
<td>21.4</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_1</td>
<td>USTC_NEL1</td>
<td>Liu2018</td>
<td>27.3</td>
<td>44.2</td>
<td>20.7</td>
<td>23.1</td>
<td>15.2</td>
<td>18.1</td>
<td>30.6</td>
<td>8.7</td>
<td>20.8</td>
<td>43.3</td>
<td>48.8</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_2</td>
<td>USTC_NEL2</td>
<td>Liu2018</td>
<td>28.8</td>
<td>46.0</td>
<td>27.1</td>
<td>21.6</td>
<td>10.8</td>
<td>26.5</td>
<td>42.0</td>
<td>11.0</td>
<td>20.9</td>
<td>33.5</td>
<td>48.6</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_3</td>
<td>USTC_NEL3</td>
<td>Liu2018</td>
<td>28.1</td>
<td>41.7</td>
<td>28.4</td>
<td>22.9</td>
<td>9.2</td>
<td>26.7</td>
<td>33.3</td>
<td>10.3</td>
<td>21.6</td>
<td>43.1</td>
<td>43.9</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_4</td>
<td>USTC_NEL4</td>
<td>Liu2018</td>
<td>29.9</td>
<td>46.0</td>
<td>27.1</td>
<td>20.3</td>
<td>13.0</td>
<td>26.5</td>
<td>37.6</td>
<td>10.9</td>
<td>23.9</td>
<td>43.1</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_1</td>
<td>LJK_PSH_task4_1</td>
<td>Lu2018</td>
<td>24.1</td>
<td>23.1</td>
<td>32.6</td>
<td>1.2</td>
<td>0.0</td>
<td>5.0</td>
<td>51.4</td>
<td>36.0</td>
<td>30.4</td>
<td>14.0</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_2</td>
<td>LJK_PSH_task4_2</td>
<td>Lu2018</td>
<td>26.3</td>
<td>25.1</td>
<td>36.1</td>
<td>1.9</td>
<td>0.4</td>
<td>3.1</td>
<td>52.1</td>
<td>42.4</td>
<td>36.2</td>
<td>16.7</td>
<td>49.1</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_3</td>
<td>LJK_PSH_task4_3</td>
<td>Lu2018</td>
<td>29.5</td>
<td>48.0</td>
<td>30.4</td>
<td>2.3</td>
<td>3.7</td>
<td>20.1</td>
<td>46.8</td>
<td>29.4</td>
<td>27.9</td>
<td>41.4</td>
<td>44.6</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_4</td>
<td>LJK_PSH_task4_4</td>
<td>Lu2018</td>
<td>32.4</td>
<td>49.9</td>
<td>38.2</td>
<td>3.6</td>
<td>3.2</td>
<td>18.1</td>
<td>48.7</td>
<td>35.4</td>
<td>31.2</td>
<td>46.8</td>
<td>48.3</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_1</td>
<td>Yonsei_str_1</td>
<td>Moon2018</td>
<td>15.9</td>
<td>26.3</td>
<td>14.0</td>
<td>9.8</td>
<td>6.3</td>
<td>15.7</td>
<td>10.4</td>
<td>8.7</td>
<td>11.0</td>
<td>29.6</td>
<td>27.5</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_2</td>
<td>Yonsei_str_2</td>
<td>Moon2018</td>
<td>14.3</td>
<td>17.8</td>
<td>14.9</td>
<td>8.1</td>
<td>2.0</td>
<td>10.3</td>
<td>14.6</td>
<td>13.7</td>
<td>12.7</td>
<td>17.3</td>
<td>31.7</td>
</tr>
<tr>
<td></td>
<td>Raj_IITKGP_task4_1</td>
<td>Raj_IIT_KGP_Task4_1</td>
<td>Raj2018</td>
<td>9.4</td>
<td>5.1</td>
<td>7.2</td>
<td>1.0</td>
<td>0.3</td>
<td>2.3</td>
<td>15.9</td>
<td>20.4</td>
<td>6.6</td>
<td>0.3</td>
<td>34.9</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim_task4_1</td>
<td>Lim2018</td>
<td>17.1</td>
<td>10.0</td>
<td>20.8</td>
<td>4.8</td>
<td>0.6</td>
<td>6.2</td>
<td>29.1</td>
<td>18.3</td>
<td>16.4</td>
<td>11.2</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim_task4_2</td>
<td>Lim2018</td>
<td>18.0</td>
<td>12.9</td>
<td>22.5</td>
<td>4.9</td>
<td>0.6</td>
<td>7.0</td>
<td>30.5</td>
<td>19.7</td>
<td>16.5</td>
<td>11.9</td>
<td>53.2</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim_task4_3</td>
<td>Lim2018</td>
<td>19.6</td>
<td>10.2</td>
<td>20.5</td>
<td>6.8</td>
<td>5.9</td>
<td>16.9</td>
<td>25.4</td>
<td>13.5</td>
<td>13.2</td>
<td>20.2</td>
<td>63.3</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim_task4_4</td>
<td>Lim2018</td>
<td>20.4</td>
<td>11.6</td>
<td>21.6</td>
<td>7.9</td>
<td>5.9</td>
<td>17.4</td>
<td>27.8</td>
<td>14.9</td>
<td>15.5</td>
<td>21.0</td>
<td>60.0</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task4_2</td>
<td>BUPT_Attention</td>
<td>WangJ2018</td>
<td>17.9</td>
<td>40.3</td>
<td>14.5</td>
<td>19.0</td>
<td>6.1</td>
<td>4.6</td>
<td>18.6</td>
<td>20.4</td>
<td>18.3</td>
<td>26.0</td>
<td>11.3</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Serizel2018</td>
<td>10.8</td>
<td>4.8</td>
<td>12.7</td>
<td>2.9</td>
<td>0.4</td>
<td>2.4</td>
<td>20.0</td>
<td>24.5</td>
<td>10.1</td>
<td>0.1</td>
<td>30.2</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_1</td>
<td>SurreyCNN8</td>
<td>Kong2018</td>
<td>18.6</td>
<td>6.0</td>
<td>18.9</td>
<td>2.4</td>
<td>0.0</td>
<td>3.6</td>
<td>46.4</td>
<td>43.6</td>
<td>15.2</td>
<td>0.0</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_2</td>
<td>SurreyCNN4</td>
<td>Kong2018</td>
<td>16.7</td>
<td>5.5</td>
<td>16.3</td>
<td>2.5</td>
<td>0.0</td>
<td>4.0</td>
<td>44.1</td>
<td>42.5</td>
<td>13.5</td>
<td>0.0</td>
<td>38.8</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_3</td>
<td>SurreyFuse</td>
<td>Kong2018</td>
<td>24.0</td>
<td>24.5</td>
<td>18.9</td>
<td>7.8</td>
<td>7.7</td>
<td>5.6</td>
<td>46.4</td>
<td>43.6</td>
<td>15.2</td>
<td>19.9</td>
<td>50.0</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_input" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Input
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_1</td>
<td>Avdveeva2018</td>
<td>20.1</td>
<td>mono</td>
<td>16kHz</td>
<td>time stretching, pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_2</td>
<td>Avdveeva2018</td>
<td>19.5</td>
<td>mono</td>
<td>16kHz</td>
<td>time stretching, pitch shifting</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>WangD2018</td>
<td>12.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>WangD2018</td>
<td>12.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>WangD2018</td>
<td>12.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>WangD2018</td>
<td>12.2</td>
<td>mono</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies, delta features</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_1</td>
<td>Dinkel2018</td>
<td>10.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_2</td>
<td>Dinkel2018</td>
<td>10.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_3</td>
<td>Dinkel2018</td>
<td>13.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_4</td>
<td>Dinkel2018</td>
<td>11.2</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_1</td>
<td>Guo2018</td>
<td>21.3</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_2</td>
<td>Guo2018</td>
<td>20.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_3</td>
<td>Guo2018</td>
<td>19.1</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_4</td>
<td>Guo2018</td>
<td>19.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_1</td>
<td>Harb2018</td>
<td>19.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_2</td>
<td>Harb2018</td>
<td>15.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_3</td>
<td>Harb2018</td>
<td>21.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_1</td>
<td>Hou2018</td>
<td>19.6</td>
<td>mono</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_2</td>
<td>Hou2018</td>
<td>18.9</td>
<td>mono</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_3</td>
<td>Hou2018</td>
<td>20.9</td>
<td>mono</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_4</td>
<td>Hou2018</td>
<td>21.1</td>
<td>mono</td>
<td>16kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_1</td>
<td>Cances2018</td>
<td>8.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_2</td>
<td>Cances2018</td>
<td>16.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>Kothinti2018</td>
<td>20.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>Kothinti2018</td>
<td>22.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, auditory spectrogram</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_1</td>
<td>Koutini2018</td>
<td>21.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_2</td>
<td>Koutini2018</td>
<td>21.1</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_3</td>
<td>Koutini2018</td>
<td>20.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_4</td>
<td>Koutini2018</td>
<td>18.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_1</td>
<td>Liu2018</td>
<td>27.3</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_2</td>
<td>Liu2018</td>
<td>28.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_3</td>
<td>Liu2018</td>
<td>28.1</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_4</td>
<td>Liu2018</td>
<td>29.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_1</td>
<td>Lu2018</td>
<td>24.1</td>
<td>mono</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_2</td>
<td>Lu2018</td>
<td>26.3</td>
<td>mono</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_3</td>
<td>Lu2018</td>
<td>29.5</td>
<td>mono</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_4</td>
<td>Lu2018</td>
<td>32.4</td>
<td>mono</td>
<td>22.05kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_1</td>
<td>Moon2018</td>
<td>15.9</td>
<td>mono</td>
<td>22.05kHz</td>
<td>time stretching, pitch shifting, block mixing, DRC</td>
<td>raw waveforms</td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_2</td>
<td>Moon2018</td>
<td>14.3</td>
<td>mono</td>
<td>22.05kHz</td>
<td>time stretching, pitch shifting, block mixing, DRC</td>
<td>raw waveforms</td>
</tr>
<tr>
<td></td>
<td>Raj_IITKGP_task4_1</td>
<td>Raj2018</td>
<td>9.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim2018</td>
<td>17.1</td>
<td>mono</td>
<td>16kHz</td>
<td>time stretching, pitch shifting, reversing</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim2018</td>
<td>18.0</td>
<td>mono</td>
<td>16kHz</td>
<td>time stretching, pitch shifting, reversing</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim2018</td>
<td>19.6</td>
<td>mono</td>
<td>16kHz</td>
<td>time stretching, pitch shifting, reversing</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim2018</td>
<td>20.4</td>
<td>mono</td>
<td>16kHz</td>
<td>time stretching, pitch shifting, reversing</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task4_2</td>
<td>WangJ2018</td>
<td>17.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Serizel2018</td>
<td>10.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_1</td>
<td>Kong2018</td>
<td>18.6</td>
<td>mono</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_2</td>
<td>Kong2018</td>
<td>16.7</td>
<td>mono</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_3</td>
<td>Kong2018</td>
<td>24.0</td>
<td>mono</td>
<td>32kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_score_eval" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="f_score_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_score_eval" data-sortable="true" data-value-type="float1-percentage">
                Event-based<br/>F-score <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_1</td>
<td>Avdveeva2018</td>
<td>20.1</td>
<td>200242</td>
<td>CRNN, CNN</td>
<td>2</td>
<td>hierarchical</td>
</tr>
<tr>
<td></td>
<td>Avdeeva_ITMO_task4_2</td>
<td>Avdveeva2018</td>
<td>19.5</td>
<td>200242</td>
<td>CRNN, CNN</td>
<td>2</td>
<td>hierarchical</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_1</td>
<td>WangD2018</td>
<td>12.4</td>
<td>24210492</td>
<td>CRNN</td>
<td>3</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_2</td>
<td>WangD2018</td>
<td>12.6</td>
<td>24210492</td>
<td>CRNN</td>
<td>3</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_3</td>
<td>WangD2018</td>
<td>12.0</td>
<td>24210492</td>
<td>CRNN</td>
<td>3</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Wang_NUDT_task4_4</td>
<td>WangD2018</td>
<td>12.2</td>
<td>24210492</td>
<td>CRNN</td>
<td>3</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_1</td>
<td>Dinkel2018</td>
<td>10.4</td>
<td>1781259</td>
<td>HMM-GMM, GRU</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_2</td>
<td>Dinkel2018</td>
<td>10.7</td>
<td>126219</td>
<td>HMM-GMM, CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_3</td>
<td>Dinkel2018</td>
<td>13.4</td>
<td>126219</td>
<td>HMM-GMM, CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dinkel_SJTU_task4_4</td>
<td>Dinkel2018</td>
<td>11.2</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_1</td>
<td>Guo2018</td>
<td>21.3</td>
<td>970644</td>
<td>multi-scale CRNN</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_2</td>
<td>Guo2018</td>
<td>20.6</td>
<td>970644</td>
<td>multi-scale CRNN</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_3</td>
<td>Guo2018</td>
<td>19.1</td>
<td>970644</td>
<td>multi-scale CRNN</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Guo_THU_task4_4</td>
<td>Guo2018</td>
<td>19.0</td>
<td>970644</td>
<td>multi-scale CRNN</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_1</td>
<td>Harb2018</td>
<td>19.4</td>
<td>497428</td>
<td>CRNN, VAT</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_2</td>
<td>Harb2018</td>
<td>15.7</td>
<td>497428</td>
<td>CRNN, VAT</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Harb_TUG_task4_3</td>
<td>Harb2018</td>
<td>21.6</td>
<td>497428</td>
<td>CRNN, VAT</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_1</td>
<td>Hou2018</td>
<td>19.6</td>
<td>1166484</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_2</td>
<td>Hou2018</td>
<td>18.9</td>
<td>1166484</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_3</td>
<td>Hou2018</td>
<td>20.9</td>
<td>1166484</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task4_4</td>
<td>Hou2018</td>
<td>21.1</td>
<td>1166484</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>CANCES_IRIT_task4_1</td>
<td>Cances2018</td>
<td>8.4</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>PELLEGRINI_IRIT_task4_2</td>
<td>Cances2018</td>
<td>16.6</td>
<td>1040724</td>
<td>CNN, CRNN with Multi-Instance Learning</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_1</td>
<td>Kothinti2018</td>
<td>20.6</td>
<td>1540854</td>
<td>CRNN, RBM, cRBM, PCA</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_2</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>1540854</td>
<td>CRNN, RBM, cRBM, PCA</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_3</td>
<td>Kothinti2018</td>
<td>20.9</td>
<td>1189290</td>
<td>CRNN, RBM, cRBM, PCA</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kothinti_JHU_task4_4</td>
<td>Kothinti2018</td>
<td>22.4</td>
<td>1540854</td>
<td>CRNN, RBM, cRBM, PCA</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_1</td>
<td>Koutini2018</td>
<td>21.5</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_2</td>
<td>Koutini2018</td>
<td>21.1</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_3</td>
<td>Koutini2018</td>
<td>20.6</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Koutini_JKU_task4_4</td>
<td>Koutini2018</td>
<td>18.8</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_1</td>
<td>Liu2018</td>
<td>27.3</td>
<td>3478026</td>
<td>Capsule-RNN, ensemble</td>
<td>8</td>
<td>dynamic threshold</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_2</td>
<td>Liu2018</td>
<td>28.8</td>
<td>534460</td>
<td>Capsule-RNN, ensemble</td>
<td>2</td>
<td>dynamic threshold</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_3</td>
<td>Liu2018</td>
<td>28.1</td>
<td>4012486</td>
<td>Capsule-RNN, CRNN, ensemble</td>
<td>9</td>
<td>dynamic threshold</td>
</tr>
<tr>
<td></td>
<td>Liu_USTC_task4_4</td>
<td>Liu2018</td>
<td>29.9</td>
<td>4012486</td>
<td>Capsule-RNN, CRNN, ensemble</td>
<td>10</td>
<td>dynamic threshold</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_1</td>
<td>Lu2018</td>
<td>24.1</td>
<td>1382246</td>
<td>CRNN</td>
<td>4</td>
<td>mean probabilities</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_2</td>
<td>Lu2018</td>
<td>26.3</td>
<td>1382246</td>
<td>CRNN</td>
<td>2</td>
<td>mean probabilities</td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_3</td>
<td>Lu2018</td>
<td>29.5</td>
<td>1382246</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>LJK_PSH_task4_4</td>
<td>Lu2018</td>
<td>32.4</td>
<td>1382246</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_1</td>
<td>Moon2018</td>
<td>15.9</td>
<td>10902218</td>
<td>GLU, Bi-RNN, ResNet, SENet, Multi-level</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Moon_YONSEI_task4_2</td>
<td>Moon2018</td>
<td>14.3</td>
<td>10902218</td>
<td>GLU, Bi-RNN, ResNet, SENet, Multi-level</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Raj_IITKGP_task4_1</td>
<td>Raj2018</td>
<td>9.4</td>
<td>215890</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_1</td>
<td>Lim2018</td>
<td>17.1</td>
<td>239338</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_2</td>
<td>Lim2018</td>
<td>18.0</td>
<td>239338</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_3</td>
<td>Lim2018</td>
<td>19.6</td>
<td>239338</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lim_ETRI_task4_4</td>
<td>Lim2018</td>
<td>20.4</td>
<td>239338</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task4_2</td>
<td>WangJ2018</td>
<td>17.9</td>
<td>1263508</td>
<td>RNN,BGRU,self-attention</td>
<td></td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Serizel2018</td>
<td>10.8</td>
<td>126090</td>
<td>CRNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_1</td>
<td>Kong2018</td>
<td>18.6</td>
<td>4691274</td>
<td>VGGish 8 layer CNN with global max pooling</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_2</td>
<td>Kong2018</td>
<td>16.7</td>
<td>4309450</td>
<td>AlexNetish 4 layer CNN with global max pooling</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task4_3</td>
<td>Kong2018</td>
<td>24.0</td>
<td>4691274</td>
<td>VGGish 8 layer CNN with global max pooling, fuse SED and non-SED</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2018/technical_reports_task4.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Avdveeva2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Avdveeva2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sound Event Detection Using Weakly Labeled Dataset with Convolutional Recurrent Neural Network
       </h4>
<p style="text-align:left">
        Avdeeva, Anastasia and Agafonov, Iurii
       </p>
<p style="text-align:left">
<em>
         Speech Information Systems Department, University of Information Technology Mechanics and Optics, Saint-Petersburg, Russia
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Avdeeva_ITMO_task4_1</span> <span class="label label-primary">Avdeeva_ITMO_task4_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Avdveeva2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Avdveeva2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Avdveeva2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Avdeeva_79.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Avdveeva2018" class="panel-collapse collapse" id="collapse-Avdveeva2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sound Event Detection Using Weakly Labeled Dataset with Convolutional Recurrent Neural Network
      </h4>
<p style="text-align:left">
<small>
        Avdeeva, Anastasia and Agafonov, Iurii
       </small>
<br/>
<small>
<em>
         Speech Information Systems Department, University of Information Technology Mechanics and Optics, Saint-Petersburg, Russia
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, a sound event detection system is proposed. This system uses fusion of CNN classifier and CRNN segmentator.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         hierarchical
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Avdveeva2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Avdeeva_79.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Avdveeva2018label" class="modal fade" id="bibtex-Avdveeva2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAvdveeva2018label">
        Sound Event Detection Using Weakly Labeled Dataset with Convolutional Recurrent Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Avdveeva2018,
    Author = "Avdeeva, Anastasia and Agafonov, Iurii",
    title = "Sound Event Detection Using Weakly Labeled Dataset with Convolutional Recurrent Neural Network",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, a sound event detection system is proposed. This system uses fusion of CNN classifier and CRNN segmentator."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cances2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Cances2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT DETECTION FROM WEAK ANNOTATIONS: WEIGHTED GRU VERSUS MULTI-INSTANCE LEARNING
       </h4>
<p style="text-align:left">
        Cances, leo and Pellegrini, Thomas and Guyot, Patrice
       </p>
<p style="text-align:left">
<em>
         Institut de Recherche en Informatique de Toulouse, Université Paul Sabatier Toulouse III, Toulouse, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">CANCES_IRIT_task4_1</span> <span class="label label-primary">PELLEGRINI_IRIT_task4_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cances2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cances2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cances2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Cances_94.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cances2018" class="panel-collapse collapse" id="collapse-Cances2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT DETECTION FROM WEAK ANNOTATIONS: WEIGHTED GRU VERSUS MULTI-INSTANCE LEARNING
      </h4>
<p style="text-align:left">
<small>
        Cances, leo and Pellegrini, Thomas and Guyot, Patrice
       </small>
<br/>
<small>
<em>
         Institut de Recherche en Informatique de Toulouse, Université Paul Sabatier Toulouse III, Toulouse, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we address the detection of audio events in domestic environments in the case where a weakly annotated dataset is available for training. The weak annotations provide tags from audio events but do not provide temporal boundaries. We report experiments in the framework of the task four of the DCASE 2018 challenge. The objective is twofold: detect audio events (multi-categorical classification at recording level), localize the events precisely within the recordings. We explored two approaches: 1) a ”weighted-GRU” (WGRU), in which we train a Convolutional Recurrent Neural Network (CRNN) for classification and then exploit its frame-based predictions at the output of the time-distributed dense layer to perform localization. We propose to lower the influence of the hidden states to avoid predicting a same score throughout a recording. 2) An approach inspired by Multi-Instance Learning (MIL), in which we train a CRNN to give predictions at frame-level, using a custom loss function based on the weak label and statistics of the frame-based predictions. Both approaches outperform the baseline of 14.06% in F-measure by a large margin, with values of respectively 16.77% and 24.58% for combined WGRUs and MIL, on a test set comprised of 288 recordings.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cances2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Cances_94.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cances2018label" class="modal fade" id="bibtex-Cances2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCances2018label">
        SOUND EVENT DETECTION FROM WEAK ANNOTATIONS: WEIGHTED GRU VERSUS MULTI-INSTANCE LEARNING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cances2018,
    Author = "Cances, leo and Pellegrini, Thomas and Guyot, Patrice",
    title = "SOUND EVENT DETECTION FROM WEAK ANNOTATIONS: WEIGHTED GRU VERSUS MULTI-INSTANCE LEARNING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, we address the detection of audio events in domestic environments in the case where a weakly annotated dataset is available for training. The weak annotations provide tags from audio events but do not provide temporal boundaries. We report experiments in the framework of the task four of the DCASE 2018 challenge. The objective is twofold: detect audio events (multi-categorical classification at recording level), localize the events precisely within the recordings. We explored two approaches: 1) a ”weighted-GRU” (WGRU), in which we train a Convolutional Recurrent Neural Network (CRNN) for classification and then exploit its frame-based predictions at the output of the time-distributed dense layer to perform localization. We propose to lower the influence of the hidden states to avoid predicting a same score throughout a recording. 2) An approach inspired by Multi-Instance Learning (MIL), in which we train a CRNN to give predictions at frame-level, using a custom loss function based on the weak label and statistics of the frame-based predictions. Both approaches outperform the baseline of 14.06\% in F-measure by a large margin, with values of respectively 16.77\% and 24.58\% for combined WGRUs and MIL, on a test set comprised of 288 recordings."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dinkel2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Dinkel2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A HYBRID ASR MODEL APPROACH ON WEAKLY LABELED SCENE CLASSIFICATION
       </h4>
<p style="text-align:left">
        Dinkel, Heinrich and Qiand, Yanmin and Yu, Kai
       </p>
<p style="text-align:left">
<em>
         Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dinkel_SJTU_task4_1</span> <span class="label label-primary">Dinkel_SJTU_task4_2</span> <span class="label label-primary">Dinkel_SJTU_task4_3</span> <span class="label label-primary">Dinkel_SJTU_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dinkel2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dinkel2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dinkel2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dinkel_38.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dinkel2018" class="panel-collapse collapse" id="collapse-Dinkel2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A HYBRID ASR MODEL APPROACH ON WEAKLY LABELED SCENE CLASSIFICATION
      </h4>
<p style="text-align:left">
<small>
        Dinkel, Heinrich and Qiand, Yanmin and Yu, Kai
       </small>
<br/>
<small>
<em>
         Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents our submission to task 4 of the DCASE 2018 challenge. Our approach focuses on refining the training labels by using a HMM-GMM to obtain a frame-wise alignment from the clip-wise labels. Then we train a convolutional recurrent neural network (CRNN), as well as a single gated recurrent neural network on those labels in standard cross-entropy fashion. Our approach utilizes a ”blank” state which is treated as a junk collector for all uninteresting events. Moreover, Gaussian posterior filtering is introduced in order to enhance the connectivity between segments. Compared to the baseline result, the proposed framework significantly enhances the models capability to detect short, impulsively occurring events such as speech, dog, dishes and alarm. Our best submission on the test set is a CRNN model with Gaussian posterior filtering, resulting in a 19.37% macro average, as well as 24.41% micro average F-score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         HMM-GMM, GRU, CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dinkel2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dinkel_38.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dinkel2018label" class="modal fade" id="bibtex-Dinkel2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDinkel2018label">
        A HYBRID ASR MODEL APPROACH ON WEAKLY LABELED SCENE CLASSIFICATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dinkel2018,
    Author = "Dinkel, Heinrich and Qiand, Yanmin and Yu, Kai",
    title = "A HYBRID ASR MODEL APPROACH ON WEAKLY LABELED SCENE CLASSIFICATION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper presents our submission to task 4 of the DCASE 2018 challenge. Our approach focuses on refining the training labels by using a HMM-GMM to obtain a frame-wise alignment from the clip-wise labels. Then we train a convolutional recurrent neural network (CRNN), as well as a single gated recurrent neural network on those labels in standard cross-entropy fashion. Our approach utilizes a ”blank” state which is treated as a junk collector for all uninteresting events. Moreover, Gaussian posterior filtering is introduced in order to enhance the connectivity between segments. Compared to the baseline result, the proposed framework significantly enhances the models capability to detect short, impulsively occurring events such as speech, dog, dishes and alarm. Our best submission on the test set is a CRNN model with Gaussian posterior filtering, resulting in a 19.37\% macro average, as well as 24.41\% micro average F-score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Guo2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Guo2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTI-SCALE CONVOLUTIONAL RECURRENT NEURAL NETWORK WITH ENSEMBLE METHOD FOR WEAKLY LABELED SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Guo, Yingmei and Xu, Mingxing and Wu, Jianming and Wang, Yanan and Hoashi, Keiichiro
       </p>
<p style="text-align:left">
<em>
         Department of Computer Science and Technology, Tsinghua University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Guo_THU_task4_1</span> <span class="label label-primary">Guo_THU_task4_2</span> <span class="label label-primary">Guo_THU_task4_3</span> <span class="label label-primary">Guo_THU_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Guo2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Guo2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Guo2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Guo_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Guo2018" class="panel-collapse collapse" id="collapse-Guo2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTI-SCALE CONVOLUTIONAL RECURRENT NEURAL NETWORK WITH ENSEMBLE METHOD FOR WEAKLY LABELED SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Guo, Yingmei and Xu, Mingxing and Wu, Jianming and Wang, Yanan and Hoashi, Keiichiro
       </small>
<br/>
<small>
<em>
         Department of Computer Science and Technology, Tsinghua University, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we describe our contributions to the challenge of detection and classification of acoustic scenes and events 2018 (DCASE2018). We propose multi-scale convolutional recurrent neural network (Multi-scale CRNN), a novel weakly-supervised learning framework for sound event detection. By integrating information from different time resolutions, the multi-scale method can capture both the fine-grained and coarse-grained features of sound events and model the temporal dependencies including fine-grained dependencies and long-term dependencies. CRNN using learnable gated linear units(GLUs) can also help to select the most related features corresponding to the audio labels. Furthermore, the ensemble method proposed in the paper can help to correct the frame-level prediction errors with classification results as identifying the sound events occurred in the audio is much easier than providing the event time boundaries. The proposed method achieves 29.2% in the event-based F1-score and 1.40 in event-based error rate in development set of DCASE2018 task4 compared to the baseline of 14.1% F-value and 1.54 error rate.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         multi-scale CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Guo2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Guo_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Guo2018label" class="modal fade" id="bibtex-Guo2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGuo2018label">
        MULTI-SCALE CONVOLUTIONAL RECURRENT NEURAL NETWORK WITH ENSEMBLE METHOD FOR WEAKLY LABELED SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Guo2018,
    Author = "Guo, Yingmei and Xu, Mingxing and Wu, Jianming and Wang, Yanan and Hoashi, Keiichiro",
    title = "MULTI-SCALE CONVOLUTIONAL RECURRENT NEURAL NETWORK WITH ENSEMBLE METHOD FOR WEAKLY LABELED SOUND EVENT DETECTION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, we describe our contributions to the challenge of detection and classification of acoustic scenes and events 2018 (DCASE2018). We propose multi-scale convolutional recurrent neural network (Multi-scale CRNN), a novel weakly-supervised learning framework for sound event detection. By integrating information from different time resolutions, the multi-scale method can capture both the fine-grained and coarse-grained features of sound events and model the temporal dependencies including fine-grained dependencies and long-term dependencies. CRNN using learnable gated linear units(GLUs) can also help to select the most related features corresponding to the audio labels. Furthermore, the ensemble method proposed in the paper can help to correct the frame-level prediction errors with classification results as identifying the sound events occurred in the audio is much easier than providing the event time boundaries. The proposed method achieves 29.2\% in the event-based F1-score and 1.40 in event-based error rate in development set of DCASE2018 task4 compared to the baseline of 14.1\% F-value and 1.54 error rate."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Harb2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Harb2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT DETECTION USING WEAKLY LABELED SEMI-SUPERVISED DATA WITH GCRNNS, VAT AND SELF-ADAPTIVE LABEL REFINEMENT
       </h4>
<p style="text-align:left">
        Harb, Robert and Pernkopf, Franz
       </p>
<p style="text-align:left">
<em>
         Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Harb_TUG_task4_1</span> <span class="label label-primary">Harb_TUG_task4_2</span> <span class="label label-primary">Harb_TUG_task4_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Harb2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Harb2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Harb2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Harb_108.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Harb2018" class="panel-collapse collapse" id="collapse-Harb2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT DETECTION USING WEAKLY LABELED SEMI-SUPERVISED DATA WITH GCRNNS, VAT AND SELF-ADAPTIVE LABEL REFINEMENT
      </h4>
<p style="text-align:left">
<small>
        Harb, Robert and Pernkopf, Franz
       </small>
<br/>
<small>
<em>
         Signal Processing and Speech Communication Laboratory, Graz University of Technology, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present a gated convolutional recurrent neural network based approach to solve task 4, large-scale weakly labeled semi-supervised sound event detection in domestic environments, of the DCASE 2018 challenge. Gated linear units and a temporal attention layer are used to predict the onset and offset of sound events in 10s long audio clips. Whereby for training only weakly-labeled data is used. Virtual adverserial training is used for regularization, utilizing both labelled and unlabelled data. Furthermore, we introduce self-adaptive label refinement, a method which allows unsupervised adaption of our trained system to increase the quality of frame-level class predictions. The proposed system reaches an overall macro averaged event-based F-score of 34.6%, resulting in a relative improvement of 20.5% over the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, VAT
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Harb2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Harb_108.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Harb2018label" class="modal fade" id="bibtex-Harb2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHarb2018label">
        SOUND EVENT DETECTION USING WEAKLY LABELED SEMI-SUPERVISED DATA WITH GCRNNS, VAT AND SELF-ADAPTIVE LABEL REFINEMENT
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Harb2018,
    Author = "Harb, Robert and Pernkopf, Franz",
    title = "SOUND EVENT DETECTION USING WEAKLY LABELED SEMI-SUPERVISED DATA WITH GCRNNS, VAT AND SELF-ADAPTIVE LABEL REFINEMENT",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, we present a gated convolutional recurrent neural network based approach to solve task 4, large-scale weakly labeled semi-supervised sound event detection in domestic environments, of the DCASE 2018 challenge. Gated linear units and a temporal attention layer are used to predict the onset and offset of sound events in 10s long audio clips. Whereby for training only weakly-labeled data is used. Virtual adverserial training is used for regularization, utilizing both labelled and unlabelled data. Furthermore, we introduce self-adaptive label refinement, a method which allows unsupervised adaption of our trained system to increase the quality of frame-level class predictions. The proposed system reaches an overall macro averaged event-based F-score of 34.6\%, resulting in a relative improvement of 20.5\% over the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hou2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Hou2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semi-supervised sound event detection with convolutional recurrent neural network using weakly labelled data
       </h4>
<p style="text-align:left">
        Hou, Yuanbo and Li, Shengchen
       </p>
<p style="text-align:left">
<em>
         Institute of Information Photonics and Optical, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hou_BUPT_task4_1</span> <span class="label label-primary">Hou_BUPT_task4_2</span> <span class="label label-primary">Hou_BUPT_task4_3</span> <span class="label label-primary">Hou_BUPT_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hou2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hou2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hou2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Hou_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hou2018" class="panel-collapse collapse" id="collapse-Hou2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semi-supervised sound event detection with convolutional recurrent neural network using weakly labelled data
      </h4>
<p style="text-align:left">
<small>
        Hou, Yuanbo and Li, Shengchen
       </small>
<br/>
<small>
<em>
         Institute of Information Photonics and Optical, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technique report, we present a polyphonic sound event detection (SED) system based on a convolutional recurrent neural network for the task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE2018) challenge. Convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) are adopted as our framework. We used a learnable gating activation function for selecting informative local features. In a summary, we get 32.95% F-value and 1.34 error rate (ER) for SED on the development set. While the baseline just obtained 14.06% F-value and 1.54 for SED.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hou2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Hou_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hou2018label" class="modal fade" id="bibtex-Hou2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHou2018label">
        Semi-supervised sound event detection with convolutional recurrent neural network using weakly labelled data
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hou2018,
    Author = "Hou, Yuanbo and Li, Shengchen",
    title = "Semi-supervised sound event detection with convolutional recurrent neural network using weakly labelled data",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technique report, we present a polyphonic sound event detection (SED) system based on a convolutional recurrent neural network for the task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE2018) challenge. Convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) are adopted as our framework. We used a learnable gating activation function for selecting informative local features. In a summary, we get 32.95\% F-value and 1.34 error rate (ER) for SED on the development set. While the baseline just obtained 14.06\% F-value and 1.54 for SED."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2018 Challenge Baseline with Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Iqbal Turab, Xu Yong, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Baseline_Surrey_task4_1</span> <span class="label label-primary">Baseline_Surrey_task4_2</span> <span class="label label-primary">Baseline_Surrey_task4_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Baseline_87.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2018').collapse('show');window.location.hash='#Kong2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2018" class="panel-collapse collapse" id="collapse-Kong2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2018 Challenge Baseline with Convolutional Neural Networks
      </h4>
<p style="text-align:left">
<small>
        Qiuqiang Kong, Iqbal Turab, Xu Yong, Wenwu Wang and Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Detection and classification of acoustic scenes and events (DCASE) 2018 challenge is a well known IEEE AASP challenge consists of several audio classification and sound event detection tasks. DCASE 2018 challenge includes five tasks: 1) Acoustic scene classification, 2) Audio tagging of Freesound, 3) Bird audio detection, 4) Weakly labeled semi-supervised sound event detection and 5) Multi-channel audio tagging. In this paper we open source the python code of all of Task 1 - 5 of DCASE 2018 challenge. The baseline source code contains the implementation of the convolutioanl neural networks (CNNs) including the AlexNetish and the VGGish from the image processing area. We researched how the performance varies from task to task when the configuration of the neural networks are the same. The experiment shows deeper VGGish network performs better than AlexNetish on Task 2 - 5 except Task 1 where VGGish and AlexNetish network perform similar. With the VGGish network, we achieve an accuracy of 0.680 on Task 1, a mean average precision (mAP) of 0.928 on Task 2, an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8% on Task 4 and a F1 score of 87.75% on Task 5.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         VGGish 8 layer CNN with global max pooling; AlexNetish 4 layer CNN with global max pooling
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Baseline_87.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2018label" class="modal fade" id="bibtex-Kong2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2018label">
        DCASE 2018 Challenge Baseline with Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2018,
    Author = "Kong, Qiuqiang and Turab, Iqbal and Yong, Xu and Wang, Wenwu and Plumbley, Mark D.",
    title = "{DCASE} 2018 Challenge Baseline with Convolutional Neural Networks",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Detection and classification of acoustic scenes and events (DCASE) 2018 challenge is a well known IEEE AASP challenge consists of several audio classification and sound event detection tasks. DCASE 2018 challenge includes five tasks: 1) Acoustic scene classification, 2) Audio tagging of Freesound, 3) Bird audio detection, 4) Weakly labeled semi-supervised sound event detection and 5) Multi-channel audio tagging. In this paper we open source the python code of all of Task 1 - 5 of DCASE 2018 challenge. The baseline source code contains the implementation of the convolutioanl neural networks (CNNs) including the AlexNetish and the VGGish from the image processing area. We researched how the performance varies from task to task when the configuration of the neural networks are the same. The experiment shows deeper VGGish network performs better than AlexNetish on Task 2 - 5 except Task 1 where VGGish and AlexNetish network perform similar. With the VGGish network, we achieve an accuracy of 0.680 on Task 1, a mean average precision (mAP) of 0.928 on Task 2, an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8\% on Task 4 and a F1 score of 87.75\% on Task 5."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kothinti2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Kothinti2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        JOINT ACOUSTIC AND CLASS INFERENCE FOR WEAKLY SUPERVISED SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Kothinti, Sandeep and Imoto, Keisuke and Chakrabarty, Debmalya and Gregory, Sell and Watanabe, Shinji and Elhilali, Mounya
       </p>
<p style="text-align:left">
<em>
         Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kothinti_JHU_task4_1</span> <span class="label label-primary">Kothinti_JHU_task4_2</span> <span class="label label-primary">Kothinti_JHU_task4_3</span> <span class="label label-primary">Kothinti_JHU_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kothinti2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kothinti2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kothinti2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Kothinti_90.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kothinti2018" class="panel-collapse collapse" id="collapse-Kothinti2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       JOINT ACOUSTIC AND CLASS INFERENCE FOR WEAKLY SUPERVISED SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Kothinti, Sandeep and Imoto, Keisuke and Chakrabarty, Debmalya and Gregory, Sell and Watanabe, Shinji and Elhilali, Mounya
       </small>
<br/>
<small>
<em>
         Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event detection is a challenging task, especially for scenes with simultaneous presence of multiple events. Task4 of the 2018 DCASE challenge presents an event detection task that requires accuracy in both segmentation and recognition of events. Supervised methods produce accurate event labels but are limited in event segmentation when training data lacks event time stamps. On the other hand, unsupervised methods that model acoustic properties of the audio can produce accurate event boundaries but are not guided by the characteristics of event classes and sound categories. In this report, we present a hybrid approach that combines an acoustic-driven event boundary detection and a supervised label inference using a deep neural network. This framework leverages benefits of both unsupervised and supervised methodologies and takes advantage of large amounts of unlabeled data, making it ideal for large-scale weakly labeled event detection. Compared to a baseline system, the proposed approach delivers a 15% absolute improvement in F1-score, demonstrating the benefits of the hybrid bottom-up, top-down approach.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, auditory spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN, RBM, cRBM, PCA
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kothinti2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Kothinti_90.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kothinti2018label" class="modal fade" id="bibtex-Kothinti2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKothinti2018label">
        JOINT ACOUSTIC AND CLASS INFERENCE FOR WEAKLY SUPERVISED SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kothinti2018,
    Author = "Kothinti, Sandeep and Imoto, Keisuke and Chakrabarty, Debmalya and Gregory, Sell and Watanabe, Shinji and Elhilali, Mounya",
    title = "JOINT ACOUSTIC AND CLASS INFERENCE FOR WEAKLY SUPERVISED SOUND EVENT DETECTION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Sound event detection is a challenging task, especially for scenes with simultaneous presence of multiple events. Task4 of the 2018 DCASE challenge presents an event detection task that requires accuracy in both segmentation and recognition of events. Supervised methods produce accurate event labels but are limited in event segmentation when training data lacks event time stamps. On the other hand, unsupervised methods that model acoustic properties of the audio can produce accurate event boundaries but are not guided by the characteristics of event classes and sound categories. In this report, we present a hybrid approach that combines an acoustic-driven event boundary detection and a supervised label inference using a deep neural network. This framework leverages benefits of both unsupervised and supervised methodologies and takes advantage of large amounts of unlabeled data, making it ideal for large-scale weakly labeled event detection. Compared to a baseline system, the proposed approach delivers a 15\% absolute improvement in F1-score, demonstrating the benefits of the hybrid bottom-up, top-down approach."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koutini2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Koutini2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        ITERATIVE KNOWLEDGE DISTILLATION IN R-CNNS FOR WEAKLY-LABELED SEMI-SUPERVISED SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception, Johannes Kepler University, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Koutini_JKU_task4_1</span> <span class="label label-primary">Koutini_JKU_task4_2</span> <span class="label label-primary">Koutini_JKU_task4_3</span> <span class="label label-primary">Koutini_JKU_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Koutini2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koutini2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koutini2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Koutini_71.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koutini2018" class="panel-collapse collapse" id="collapse-Koutini2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       ITERATIVE KNOWLEDGE DISTILLATION IN R-CNNS FOR WEAKLY-LABELED SEMI-SUPERVISED SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception, Johannes Kepler University, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present our approach used for the CP-JKU submission in Task 4 of the DCASE-2018 Challenge. We propose a novel iterative knowledge distillation technique for weakly-labeled semi-supervised event detection using neural networks, specifically Recurrent Convolutional Neural Networks (R-CNNs). R-CNNs are used to tag the unlabeled data and predict strong labels. Further, we use the R-CNN strong pseudo-labels on the training datasets and train new models after applying label-smoothing techniques on the strong pseudo-labels. Our proposed approach significantly improved the performance of the baseline, achieving the event-based f-measure of 40.86% compared to 15.11% event-based f-measure of the baseline in the provided test set from the development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koutini2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Koutini_71.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koutini2018label" class="modal fade" id="bibtex-Koutini2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoutini2018label">
        ITERATIVE KNOWLEDGE DISTILLATION IN R-CNNS FOR WEAKLY-LABELED SEMI-SUPERVISED SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Koutini2018,
    Author = "Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard",
    title = "ITERATIVE KNOWLEDGE DISTILLATION IN R-CNNS FOR WEAKLY-LABELED SEMI-SUPERVISED SOUND EVENT DETECTION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technical report, we present our approach used for the CP-JKU submission in Task 4 of the DCASE-2018 Challenge. We propose a novel iterative knowledge distillation technique for weakly-labeled semi-supervised event detection using neural networks, specifically Recurrent Convolutional Neural Networks (R-CNNs). R-CNNs are used to tag the unlabeled data and predict strong labels. Further, we use the R-CNN strong pseudo-labels on the training datasets and train new models after applying label-smoothing techniques on the strong pseudo-labels. Our proposed approach significantly improved the performance of the baseline, achieving the event-based f-measure of 40.86\% compared to 15.11\% event-based f-measure of the baseline in the provided test set from the development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lim2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Lim2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION USING CRNN WITH INCEPTION MODULE
       </h4>
<p style="text-align:left">
        Lim, Wootaek and Suh, Sangwon and Jeong, Youngho
       </p>
<p style="text-align:left">
<em>
         Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lim_task4_1</span> <span class="label label-primary">Lim_task4_2</span> <span class="label label-primary">Lim_task4_3</span> <span class="label label-primary">Lim_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lim2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lim2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Lim_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lim2018" class="panel-collapse collapse" id="collapse-Lim2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION USING CRNN WITH INCEPTION MODULE
      </h4>
<p style="text-align:left">
<small>
        Lim, Wootaek and Suh, Sangwon and Jeong, Youngho
       </small>
<br/>
<small>
<em>
         Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present a method for large-scale detection of sound events using small weakly labeled data proposed in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 challenge Task 4. To perform this task, we adopted the convolutional neural network (CNN) and gated recurrent unit (GRU) based bidirectional recurrent neural network (RNN) as our proposed system. In addition, we proposed the Inception module for handling various receptive fields at once in each CNN layer. We also applied the data augmentation method to solve the labeled data shortage problem and applied the event activity detection method for strong label learning. By applying the proposed method to a weakly labeled semi-supervised sound event detection, it was verified that the proposed system provides better detection performance compared to the DCASE 2018 baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, pitch shifting, reversing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Lim_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lim2018label" class="modal fade" id="bibtex-Lim2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLim2018label">
        WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION USING CRNN WITH INCEPTION MODULE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lim2018,
    Author = "Lim, Wootaek and Suh, Sangwon and Jeong, Youngho",
    title = "WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION USING CRNN WITH INCEPTION MODULE",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, we present a method for large-scale detection of sound events using small weakly labeled data proposed in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 challenge Task 4. To perform this task, we adopted the convolutional neural network (CNN) and gated recurrent unit (GRU) based bidirectional recurrent neural network (RNN) as our proposed system. In addition, we proposed the Inception module for handling various receptive fields at once in each CNN layer. We also applied the data augmentation method to solve the labeled data shortage problem and applied the event activity detection method for strong label learning. By applying the proposed method to a weakly labeled semi-supervised sound event detection, it was verified that the proposed system provides better detection performance compared to the DCASE 2018 baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        USTC-NELSLIP SYSTEM FOR DCASE 2018 CHALLENGE TASK 4
       </h4>
<p style="text-align:left">
        Liu, Yaming Liu and Yan, Jie and Song, Yan and Du, Jun
       </p>
<p style="text-align:left">
<em>
         National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_USTC_task4_1</span> <span class="label label-primary">Liu_USTC_task4_2</span> <span class="label label-primary">Liu_USTC_task4_3</span> <span class="label label-primary">Liu_USTC_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Liu_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2018" class="panel-collapse collapse" id="collapse-Liu2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       USTC-NELSLIP SYSTEM FOR DCASE 2018 CHALLENGE TASK 4
      </h4>
<p style="text-align:left">
<small>
        Liu, Yaming Liu and Yan, Jie and Song, Yan and Du, Jun
       </small>
<br/>
<small>
<em>
         National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we present a group of methods for the task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE 2018). This task aims to detect sound events in domestic environments using weakly labeled training data in a semi-supervised way. In this report, firstly, an event activity detection technique is performed to transform weak labels to strong labels before training. Then a capsule based method and a gated convolutional neural networks (CNN) are performed to estimate event activity probabilities respectively. At last, event activity probabilities of two systems are combined to obtain the final sound event detection (SED) estimation. On the other hand, a tagging model based on proposed CNN is used to tag the unlabeled in domain training set. Data with high confidence are added to the training data to get a further promotion of performance. Experiments on the validation dataset show that the proposed approach obtains an F1-score of 51.60% and an error rate of 0.93, outperforming the baseline of 14.06% and 1.54.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Capsule-RNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         dynamic threshold
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Liu_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2018label" class="modal fade" id="bibtex-Liu2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2018label">
        USTC-NELSLIP SYSTEM FOR DCASE 2018 CHALLENGE TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2018,
    Author = "Liu, Yaming Liu and Yan, Jie and Song, Yan and Du, Jun",
    title = "USTC-NELSLIP SYSTEM FOR DCASE 2018 CHALLENGE TASK 4",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technical report, we present a group of methods for the task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE 2018). This task aims to detect sound events in domestic environments using weakly labeled training data in a semi-supervised way. In this report, firstly, an event activity detection technique is performed to transform weak labels to strong labels before training. Then a capsule based method and a gated convolutional neural networks (CNN) are performed to estimate event activity probabilities respectively. At last, event activity probabilities of two systems are combined to obtain the final sound event detection (SED) estimation. On the other hand, a tagging model based on proposed CNN is used to tag the unlabeled in domain training set. Data with high confidence are added to the training data to get a further promotion of performance. Experiments on the validation dataset show that the proposed approach obtains an F1-score of 51.60\% and an error rate of 0.93, outperforming the baseline of 14.06\% and 1.54."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lu2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Lu2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MEAN TEACHER CONVOLUTION SYSTEM FOR DCASE 2018 TASK 4
       </h4>
<p style="text-align:left">
        JiaKai, Lu
       </p>
<p style="text-align:left">
<em>
         1T5K, PFU SHANGHAI Co., LTD, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">LJK_PSH_task4_1</span> <span class="label label-primary">LJK_PSH_task4_2</span> <span class="label label-primary">LJK_PSH_task4_3</span> <span class="label label-primary">LJK_PSH_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lu2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lu2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Lu_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lu2018" class="panel-collapse collapse" id="collapse-Lu2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MEAN TEACHER CONVOLUTION SYSTEM FOR DCASE 2018 TASK 4
      </h4>
<p style="text-align:left">
<small>
        JiaKai, Lu
       </small>
<br/>
<small>
<em>
         1T5K, PFU SHANGHAI Co., LTD, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we present our neural network for the DCASE 2018 challenge’s Task 4 (Large-scale weakly labeled semi-supervised sound event detection in domestic environments). This task evaluates systems for the large-scale detection of sound events using weakly labeled data, and explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance to doing audio tagging and sound event detection. We propose a mean-teacher model with context-gating convolutional neural network (CNN) and recurrent neural network (RNN) to maximize the use of unlabeled in-domain dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probabilities
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Lu_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lu2018label" class="modal fade" id="bibtex-Lu2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLu2018label">
        MEAN TEACHER CONVOLUTION SYSTEM FOR DCASE 2018 TASK 4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lu2018,
    Author = "JiaKai, Lu",
    title = "MEAN TEACHER CONVOLUTION SYSTEM FOR DCASE 2018 TASK 4",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, we present our neural network for the DCASE 2018 challenge’s Task 4 (Large-scale weakly labeled semi-supervised sound event detection in domestic environments). This task evaluates systems for the large-scale detection of sound events using weakly labeled data, and explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance to doing audio tagging and sound event detection. We propose a mean-teacher model with context-gating convolutional neural network (CNN) and recurrent neural network (RNN) to maximize the use of unlabeled in-domain dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Moon2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Moon2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        End-to-end CRNN Architectures for Weakly Supervised Sound Event Detection
       </h4>
<p style="text-align:left">
        Hyeongi, Moon and Joon, Byun and Bum-Jun, Kim and Shin-hyuk, Jeon and Youngho, Jeong and Young-cheol, Park and Sung-wook, Park
       </p>
<p style="text-align:left">
<em>
         School of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yonsei_str_1</span> <span class="label label-primary">Yonsei_str_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Moon2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Moon2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Moon2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Moon_110.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Moon2018" class="panel-collapse collapse" id="collapse-Moon2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       End-to-end CRNN Architectures for Weakly Supervised Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Hyeongi, Moon and Joon, Byun and Bum-Jun, Kim and Shin-hyuk, Jeon and Youngho, Jeong and Young-cheol, Park and Sung-wook, Park
       </small>
<br/>
<small>
<em>
         School of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This presentation describes our approach for large-scale weakly labeled semi-supervise sound event detection in domestic environments (TASK4) of the DCASE 2018. Our structure is based on Convolutional Recurrent Neural Network (CRNN) using raw waveform. The conventional Convolutional Neural Network (CNN) is modified to adopt Gated Linear Unit (GLU), ResNet, and Squeeze-and-Excitation (SE) network. Then three Recurrent Neural Networks (RNNs) follow. Each RNN receives features from different layers, respectively, and the outputs of RNNs are concatenate for final classification by Feed-forward connection (FC) layers. Simple data augmentation method is also applied to augment small amount of labeled data. With this approach F1 score of 5.5% improvement is achieved.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, pitch shifting, block mixing, DRC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         raw waveforms
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GLU, Bi-RNN, ResNet, SENet, Multi-level
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Moon2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Moon_110.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Moon2018label" class="modal fade" id="bibtex-Moon2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMoon2018label">
        End-to-end CRNN Architectures for Weakly Supervised Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Moon2018,
    Author = "Hyeongi, Moon and Joon, Byun and Bum-Jun, Kim and Shin-hyuk, Jeon and Youngho, Jeong and Young-cheol, Park and Sung-wook, Park",
    title = "End-to-end CRNN Architectures for Weakly Supervised Sound Event Detection",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This presentation describes our approach for large-scale weakly labeled semi-supervise sound event detection in domestic environments (TASK4) of the DCASE 2018. Our structure is based on Convolutional Recurrent Neural Network (CRNN) using raw waveform. The conventional Convolutional Neural Network (CNN) is modified to adopt Gated Linear Unit (GLU), ResNet, and Squeeze-and-Excitation (SE) network. Then three Recurrent Neural Networks (RNNs) follow. Each RNN receives features from different layers, respectively, and the outputs of RNNs are concatenate for final classification by Feed-forward connection (FC) layers. Simple data augmentation method is also applied to augment small amount of labeled data. With this approach F1 score of 5.5\% improvement is achieved."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Raj2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Raj2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        LARGE-SCALE WEAKLY LABELLED SEMI-SUPERVISED CQT BASED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
       </h4>
<p style="text-align:left">
        Raj, Rojin and Waldekar, Shefali and Saha, Goutam
       </p>
<p style="text-align:left">
<em>
         Electronics and Electrical Communication Engineering Department, Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Raj_IITKGP_task4_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Raj2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Raj2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Raj2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Raj_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Raj2018" class="panel-collapse collapse" id="collapse-Raj2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       LARGE-SCALE WEAKLY LABELLED SEMI-SUPERVISED CQT BASED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
      </h4>
<p style="text-align:left">
<small>
        Raj, Rojin and Waldekar, Shefali and Saha, Goutam
       </small>
<br/>
<small>
<em>
         Electronics and Electrical Communication Engineering Department, Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper proposes a constant quality transform based input feature for baseline architecture to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). This is achieved using constant quality transform coefficients as input feature for convolutional recurrent neural network. The proposed method is a contribution to the challenge of detection and classification of acoustic scenes and events (DCASE 2018, Task 4) and evaluated on a publicly available dataset from youtube with 10 sound event classes. The method achieves the best error rate of 1.48 and F-score of 14.55 %.Based on the results obtained using a CPU based system there is a decrease of 7.5 % in case of error rate and increase of 11.5 % in case of F-score as compared to baseline results.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Raj2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Raj_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Raj2018label" class="modal fade" id="bibtex-Raj2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRaj2018label">
        LARGE-SCALE WEAKLY LABELLED SEMI-SUPERVISED CQT BASED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Raj2018,
    Author = "Raj, Rojin and Waldekar, Shefali and Saha, Goutam",
    title = "LARGE-SCALE WEAKLY LABELLED SEMI-SUPERVISED CQT BASED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper proposes a constant quality transform based input feature for baseline architecture to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). This is achieved using constant quality transform coefficients as input feature for convolutional recurrent neural network. The proposed method is a contribution to the challenge of detection and classification of acoustic scenes and events (DCASE 2018, Task 4) and evaluated on a publicly available dataset from youtube with 10 sound event classes. The method achieves the best error rate of 1.48 and F-score of 14.55 \%.Based on the results obtained using a CPU based system there is a decrease of 7.5 \% in case of error rate and increase of 11.5 \% in case of F-score as compared to baseline results."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Serizel2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Serizel2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        LARGE-SCALE WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
       </h4>
<p style="text-align:left">
        Serizel, Romain and Turpault, Nicolas and Eghbal-Zadeh, Hamid and Shah, Ankit Parag
       </p>
<p style="text-align:left">
<em>
         Department of Natural Language Processing &amp; Knowledge Discovery, University of Lorraine, Loria, Nancy, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Task 4 baseline</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Serizel2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Serizel2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Serizel2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-01850270v1" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Serizel2018').collapse('show');window.location.hash='#Serizel2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Serizel2018" class="panel-collapse collapse" id="collapse-Serizel2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       LARGE-SCALE WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
      </h4>
<p style="text-align:left">
<small>
        Serizel, Romain and Turpault, Nicolas and Eghbal-Zadeh, Hamid and Shah, Ankit Parag
       </small>
<br/>
<small>
<em>
         Department of Natural Language Processing &amp; Knowledge Discovery, University of Lorraine, Loria, Nancy, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events... ) and potential industrial applications.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Serizel2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://hal.inria.fr/hal-01850270v1" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Serizel2018label" class="modal fade" id="bibtex-Serizel2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSerizel2018label">
        LARGE-SCALE WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Serizel2018,
    Author = "Serizel, Romain and Turpault, Nicolas and Eghbal-Zadeh, Hamid and Shah, Ankit Parag",
    title = "LARGE-SCALE WEAKLY LABELED SEMI-SUPERVISED SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events... ) and potential industrial applications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="WangD2018" style="box-shadow: none">
<div class="panel-heading" id="heading-WangD2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A CRNN-BASED SYSTEM WITH MIXUP TECHNIQUE FOR LARGE-SCALE WEAKLY LABELED SOUND EVENT DETECTION
       </h4>
<p style="text-align:left">
        Wang, Dezhi and Xu, Kele and Zhu, Boqing and Zhang, Lilun and Peng, Yuxing and Wang, Huaimin
       </p>
<p style="text-align:left">
<em>
         School of Computer, National University of Defense Technology, Changsha, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wang_NUDT_task4_1</span> <span class="label label-primary">Wang_NUDT_task4_2</span> <span class="label label-primary">Wang_NUDT_task4_3</span> <span class="label label-primary">Wang_NUDT_task4_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-WangD2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-WangD2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-WangD2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wang_77.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-WangD2018" class="panel-collapse collapse" id="collapse-WangD2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A CRNN-BASED SYSTEM WITH MIXUP TECHNIQUE FOR LARGE-SCALE WEAKLY LABELED SOUND EVENT DETECTION
      </h4>
<p style="text-align:left">
<small>
        Wang, Dezhi and Xu, Kele and Zhu, Boqing and Zhang, Lilun and Peng, Yuxing and Wang, Huaimin
       </small>
<br/>
<small>
<em>
         School of Computer, National University of Defense Technology, Changsha, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The details of our method submitted to the task 4 of DCASE challenge 2018 are described in this technical report. This task evaluates systems for the detection of sound events in domestic environments using large-scale weakly labeled data. In particular, an architecture based on the framework of convolutional recurrent neural network (CRNN) is utilized to detect the timestamps of all the events in given audio clips where the training audio files have only clip-level labels. In order to take advantage of the large-scale unlabeled in-domain training data, a deep residual network based model (ResNeXt) is first employed to make predictions for weak labels of the unlabeled data. In addition, a mixup technique is applied in model training process, which is believed to have some benefits on the data augmentation and the model generalization capability. Finally, the system achieves 22.05% F1-value in class-wise average metrics for the sound event detection on the provided testing dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, delta features
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-WangD2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wang_77.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-WangD2018label" class="modal fade" id="bibtex-WangD2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWangD2018label">
        A CRNN-BASED SYSTEM WITH MIXUP TECHNIQUE FOR LARGE-SCALE WEAKLY LABELED SOUND EVENT DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{WangD2018,
    Author = "Wang, Dezhi and Xu, Kele and Zhu, Boqing and Zhang, Lilun and Peng, Yuxing and Wang, Huaimin",
    title = "A CRNN-BASED SYSTEM WITH MIXUP TECHNIQUE FOR LARGE-SCALE WEAKLY LABELED SOUND EVENT DETECTION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "The details of our method submitted to the task 4 of DCASE challenge 2018 are described in this technical report. This task evaluates systems for the detection of sound events in domestic environments using large-scale weakly labeled data. In particular, an architecture based on the framework of convolutional recurrent neural network (CRNN) is utilized to detect the timestamps of all the events in given audio clips where the training audio files have only clip-level labels. In order to take advantage of the large-scale unlabeled in-domain training data, a deep residual network based model (ResNeXt) is first employed to make predictions for weak labels of the unlabeled data. In addition, a mixup technique is applied in model training process, which is believed to have some benefits on the data augmentation and the model generalization capability. Finally, the system achieves 22.05\% F1-value in class-wise average metrics for the sound event detection on the provided testing dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="WangJ2018" style="box-shadow: none">
<div class="panel-heading" id="heading-WangJ2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SELF-ATTENTION MECHANISM BASED SYSTEM FOR DCASE2018 CHALLENGE TASK1 AND TASK4
       </h4>
<p style="text-align:left">
        Jun, Wang and Shengchen, Li
       </p>
<p style="text-align:left">
<em>
         Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">WangJun_BUPT_task4_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-WangJ2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-WangJ2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-WangJ2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wangjun_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-WangJ2018" class="panel-collapse collapse" id="collapse-WangJ2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SELF-ATTENTION MECHANISM BASED SYSTEM FOR DCASE2018 CHALLENGE TASK1 AND TASK4
      </h4>
<p style="text-align:left">
<small>
        Jun, Wang and Shengchen, Li
       </small>
<br/>
<small>
<em>
         Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technique report, we provide self-attention mechanism for the Task1 and Task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE2017) challenge. We take convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) as our basic systems in Task 1 and Task 4. In this convolutional recurrent neural network (CRNN), gated linear units (GLUs) is used for non-linearity which implement a gating mechanism over the output of the network for selecting informative local features. Self-attention mechanism called intra-attention is used for modeling relationship between different positions of a single sequence over the output of the CRNN. Attention-based pooling scheme is used for localizing the specific events in Task 4 and for obtaining the final labels in Task 1. In a summary, we get 70.81% accuracy subtask 1 of Task 1. In the subtask 2 of Task 1, we get 70.1% accuracy for device a, 59.4% accuracy for device b, and 55.6 accuracy for device c. For Task 1, we get 26.98% F1 value for sound event detection in old test data of developmemt data.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, pitch shifting, reversing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-WangJ2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wangjun_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-WangJ2018label" class="modal fade" id="bibtex-WangJ2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWangJ2018label">
        SELF-ATTENTION MECHANISM BASED SYSTEM FOR DCASE2018 CHALLENGE TASK1 AND TASK4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{WangJ2018,
    Author = "Jun, Wang and Shengchen, Li",
    title = "SELF-ATTENTION MECHANISM BASED SYSTEM FOR DCASE2018 CHALLENGE TASK1 AND TASK4",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technique report, we provide self-attention mechanism for the Task1 and Task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE2017) challenge. We take convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) as our basic systems in Task 1 and Task 4. In this convolutional recurrent neural network (CRNN), gated linear units (GLUs) is used for non-linearity which implement a gating mechanism over the output of the network for selecting informative local features. Self-attention mechanism called intra-attention is used for modeling relationship between different positions of a single sequence over the output of the CRNN. Attention-based pooling scheme is used for localizing the specific events in Task 4 and for obtaining the final labels in Task 1. In a summary, we get 70.81\% accuracy subtask 1 of Task 1. In the subtask 2 of Task 1, we get 70.1\% accuracy for device a, 59.4\% accuracy for device b, and 55.6 accuracy for device c. For Task 1, we get 26.98\% F1 value for sound event detection in old test data of developmemt data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>