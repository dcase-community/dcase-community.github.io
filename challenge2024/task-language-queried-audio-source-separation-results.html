<!DOCTYPE html><html lang="en">
<head>
    <title>Language-Queried Audio Source Separation - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2024/task-language-queried-audio-source-separation-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description Language-queried audio source separation (LASS) is the task of separating arbitrary sound sources using textual descriptions of the desired source. LASS provides a useful tool for future source separation systems, allowing users to extract audio sources via natural language instructions. Submissions will be evaluated by signal-to-distortion ratio (SDR â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right navbar-tighter" id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2024</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2024/"><i class="fa fa-home"></i>&nbsp;Intro</a>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-t1"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-t2"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-t3"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-t4"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-t5"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-automated-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-t6"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-scene-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-t7"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-retrieval text-t8"></i>&nbsp;Task8&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2024/task-language-queried-audio-source-separation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-separation text-t9"></i>&nbsp;Task9&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2024/task-language-queried-audio-source-separation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-traffic text-t10"></i>&nbsp;Task10&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2024/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2024/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wave-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-t9"></i><i class="fa dc-separation fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Separation</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 9</span></span><img src="../images/logos/dcase/dcase2024_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Language-Queried Audio Source Separation</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#subjective-evaluation-score">Subjective Evaluation Score</a></li>
<li><a href="#objective-evaluation-score">Objective Evaluation Score</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a></li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>Language-queried audio source separation (LASS) is the task of separating arbitrary sound sources using textual descriptions of the desired source. LASS provides a useful tool for future source separation systems, allowing users to extract audio sources via natural language instructions. Submissions will be evaluated by signal-to-distortion ratio (SDR), followed by a subjective test. Final rankings are determined by subjective listening tests.</p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2024/task-language-queried-audio-source-separation" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<h2 id="subjective-evaluation-score">Subjective Evaluation Score</h2>
<p>If multiple systems were submitted by one team, only the system with the highest SDR score was subjectively evaluated. The weighted average of the two ratings was calculated using a <strong>1:1</strong> ratio of REL (relevance between the target audio and the language query) and OVL (overall audio quality of the separated signal).</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="label" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="evaluation_subjective_avg" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="2">Submission Information</th>
<th class="sep-left-cell" colspan="4">Subjective Evaluation Score</th>
</tr>
<tr>
<th data-field="label" data-sortable="true">
                Submission Code
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="team_rank" data-sortable="true" data-value-type="int">
                Official<br/>Rank
            </th>
<th class="text-center" data-chartable="true" data-field="evaluation_subjective_avg" data-sortable="true" data-value-type="float3">
                Average Score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="evaluation_ovl" data-sortable="true" data-value-type="float3">
                OVL Score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="evaluation_rel" data-sortable="true" data-value-type="float3">
                REL Score
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Kim_GIST-AunionAI_task9_4</td>
<td>Lee2024_t9</td>
<td>1</td>
<td>3.310</td>
<td>3.430</td>
<td>3.188</td>
</tr>
<tr>
<td></td>
<td>Guan_HEU_task9_2</td>
<td>Xiao2024_t9</td>
<td>2</td>
<td>3.288</td>
<td>3.416</td>
<td>3.159</td>
</tr>
<tr>
<td></td>
<td>HanYin_NWPU-JLESS_task9_4</td>
<td>Yin2024_t9</td>
<td>3</td>
<td>3.266</td>
<td>3.400</td>
<td>3.133</td>
</tr>
<tr>
<td></td>
<td>Romaniuk_SRPOL_task9_2</td>
<td>Romaniuk2024_t9</td>
<td>4</td>
<td>3.260</td>
<td>3.386</td>
<td>3.134</td>
</tr>
<tr>
<td></td>
<td>Chung_KT_task9_1</td>
<td>Chung2024_t9</td>
<td>5</td>
<td>3.240</td>
<td>3.378</td>
<td>3.102</td>
</tr>
</tbody>
</table>
<h2 id="objective-evaluation-score">Objective Evaluation Score</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="label" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="evaluation_sdr" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="2">Submission Information</th>
<th class="sep-left-cell" colspan="4">Evaluation Set</th>
<th class="sep-left-cell" colspan="4">Validation (Development) Set</th>
</tr>
<tr>
<th data-field="label" data-sortable="true">
                Submission Code
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="evaluation_sdr_rank" data-sortable="true" data-value-type="int">
                SDR<br/>Rank
            </th>
<th class="text-center" data-chartable="true" data-field="evaluation_sdr" data-sortable="true" data-value-type="float3">
                SDR Score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="evaluation_sdri" data-sortable="true" data-value-type="float3">
                SDRi Score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="evaluation_sisdr" data-sortable="true" data-value-type="float3">
                SI-SDR Score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="validation_sdr" data-sortable="true" data-value-type="float3">
                SDR Score
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="validation_sdri" data-sortable="true" data-value-type="float3">
                SDRi Score
            </th>
<th class="text-center" data-chartable="true" data-field="validation_sisdr" data-sortable="true" data-value-type="float3">
                SI-SDR Score
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Kim_GIST-AunionAI_task9_4</td>
<td>Lee2024_t9</td>
<td>1</td>
<td>8.869</td>
<td>8.763</td>
<td>7.764</td>
<td>8.610</td>
<td>8.575</td>
<td>7.493</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST-AunionAI_task9_3</td>
<td>Lee2024_t9</td>
<td>2</td>
<td>8.864</td>
<td>8.757</td>
<td>7.780</td>
<td>8.599</td>
<td>8.564</td>
<td>7.497</td>
</tr>
<tr>
<td></td>
<td>HanYin_NWPU-JLESS_task9_4</td>
<td>Yin2024_t9</td>
<td>3</td>
<td>8.842</td>
<td>8.736</td>
<td>7.820</td>
<td>8.467</td>
<td>8.432</td>
<td>7.403</td>
</tr>
<tr>
<td></td>
<td>HanYin_NWPU-JLESS_task9_3</td>
<td>Yin2024_t9</td>
<td>4</td>
<td>8.764</td>
<td>8.658</td>
<td>7.394</td>
<td>8.191</td>
<td>8.156</td>
<td>6.794</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST-AunionAI_task9_2</td>
<td>Lee2024_t9</td>
<td>5</td>
<td>8.671</td>
<td>8.564</td>
<td>7.217</td>
<td>8.459</td>
<td>8.424</td>
<td>7.072</td>
</tr>
<tr>
<td></td>
<td>Guan_HEU_task9_2</td>
<td>Xiao2024_t9</td>
<td>6</td>
<td>8.368</td>
<td>8.262</td>
<td>6.800</td>
<td>8.192</td>
<td>8.157</td>
<td>6.680</td>
</tr>
<tr>
<td></td>
<td>HanYin_NWPU-JLESS_task9_2</td>
<td>Yin2024_t9</td>
<td>7</td>
<td>8.186</td>
<td>8.080</td>
<td>6.499</td>
<td>8.007</td>
<td>7.972</td>
<td>6.459</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST-AunionAI_task9_1</td>
<td>Lee2024_t9</td>
<td>8</td>
<td>8.059</td>
<td>7.953</td>
<td>6.510</td>
<td>7.750</td>
<td>7.715</td>
<td>6.161</td>
</tr>
<tr>
<td></td>
<td>Romaniuk_SRPOL_task9_2</td>
<td>Romaniuk2024_t9</td>
<td>9</td>
<td>7.572</td>
<td>7.466</td>
<td>5.455</td>
<td>7.398</td>
<td>7.363</td>
<td>5.551</td>
</tr>
<tr>
<td></td>
<td>HanYin_NWPU-JLESS_task9_1</td>
<td>Yin2024_t9</td>
<td>10</td>
<td>7.306</td>
<td>7.200</td>
<td>5.481</td>
<td>7.087</td>
<td>7.052</td>
<td>5.413</td>
</tr>
<tr>
<td></td>
<td>Chung_KT_task9_1</td>
<td>Chung2024_t9</td>
<td>11</td>
<td>7.302</td>
<td>7.195</td>
<td>5.628</td>
<td>7.030</td>
<td>6.995</td>
<td>5.368</td>
</tr>
<tr>
<td></td>
<td>Romaniuk_SRPOL_task9_1</td>
<td>Romaniuk2024_t9</td>
<td>12</td>
<td>7.245</td>
<td>7.138</td>
<td>5.294</td>
<td>7.021</td>
<td>6.986</td>
<td>5.291</td>
</tr>
<tr>
<td></td>
<td>Chung_KT_task9_2</td>
<td>Chung2024_t9</td>
<td>13</td>
<td>7.186</td>
<td>7.080</td>
<td>5.526</td>
<td>7.124</td>
<td>7.089</td>
<td>5.593</td>
</tr>
<tr>
<td></td>
<td>Chung_KT_task9_3</td>
<td>Chung2024_t9</td>
<td>14</td>
<td>7.118</td>
<td>7.012</td>
<td>5.301</td>
<td>7.139</td>
<td>7.104</td>
<td>5.504</td>
</tr>
<tr>
<td></td>
<td>Romaniuk_SRPOL_task9_4</td>
<td>Romaniuk2024_t9</td>
<td>15</td>
<td>6.478</td>
<td>6.372</td>
<td>4.513</td>
<td>6.282</td>
<td>6.247</td>
<td>4.620</td>
</tr>
<tr>
<td></td>
<td>Romaniuk_SRPOL_task9_3</td>
<td>Romaniuk2024_t9</td>
<td>16</td>
<td>6.153</td>
<td>6.046</td>
<td>3.811</td>
<td>6.181</td>
<td>6.146</td>
<td>4.188</td>
</tr>
<tr>
<td></td>
<td>Guan_HEU_task9_1</td>
<td>Xiao2024_t9</td>
<td>17</td>
<td>6.022</td>
<td>5.916</td>
<td>4.115</td>
<td>5.937</td>
<td>5.902</td>
<td>4.191</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline</td>
<td>Liu2024_t9</td>
<td>18</td>
<td>5.799</td>
<td>5.693</td>
<td>3.873</td>
<td>5.708</td>
<td>5.673</td>
<td>3.862</td>
</tr>
<tr>
<td></td>
<td>Guan_HEU_task9_3</td>
<td>Xiao2024_t9</td>
<td>19</td>
<td>-5.417</td>
<td>-5.523</td>
<td>-39.983</td>
<td>-4.747</td>
<td>-4.792</td>
<td>-42.346</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<p>Summary of the submitted system characteristics. </p>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="system_rank_trackA" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sm-cell" data-field="label" data-sortable="true">
                Submission<br/>Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="input_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Input<br/>SR
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data<br/>augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                ML<br/>method
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="loss_function" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Loss<br/>function
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="ensemble_num_systems" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true" data-value-type="int">
                Ensemble<br/>systems
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-field="total_parameters" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-value-type="numeric-unit">
                Total<br/>parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="training_datasets" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Training<br/>datasets
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="used_pretrained_models" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Used<br/>pre-trained models
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>Kim_GIST-AunionAI_task9_4</td>
<td>Lee2024_t9</td>
<td>16kHz</td>
<td>caption augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>5</td>
<td>467M</td>
<td>Clotho, FSD50K, WavCaps</td>
<td>CLAP, AudioSep, Phi-2.0</td>
</tr>
<tr>
<td>Kim_GIST-AunionAI_task9_3</td>
<td>Lee2024_t9</td>
<td>16kHz</td>
<td>caption augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>4</td>
<td>467M</td>
<td>Clotho, FSD50K, WavCaps</td>
<td>CLAP, AudioSep, Phi-2.0</td>
</tr>
<tr>
<td>HanYin_NWPU-JLESS_task9_4</td>
<td>Yin2024_t9</td>
<td>32kHz, 16kHz</td>
<td>volume augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking, DPRNN</td>
<td>waveform l1 loss</td>
<td>3</td>
<td>267M</td>
<td>Clotho, FSD50K, Audiocaps, Auto-ACD, WavCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>HanYin_NWPU-JLESS_task9_3</td>
<td>Yin2024_t9</td>
<td>32kHz</td>
<td>volume augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking, DPRNN</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>267M</td>
<td>Clotho, FSD50K, Audiocaps, Auto-ACD, WavCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>Kim_GIST-AunionAI_task9_2</td>
<td>Lee2024_t9</td>
<td>32kHz</td>
<td>caption augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238M</td>
<td>Clotho, FSD50K, WavCaps</td>
<td>CLAP, AudioSep, Phi-2.0</td>
</tr>
<tr>
<td>Guan_HEU_task9_2</td>
<td>Xiao2024_t9</td>
<td>32kHz</td>
<td>N/A</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238.60M</td>
<td>Clotho, FSD50K</td>
<td>CLAP, AudioSep</td>
</tr>
<tr>
<td>HanYin_NWPU-JLESS_task9_2</td>
<td>Yin2024_t9</td>
<td>16kHz</td>
<td>volume augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking, DPRNN</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>267M</td>
<td>Clotho, FSD50K, Audiocaps, Auto-ACD, WavCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>Kim_GIST-AunionAI_task9_1</td>
<td>Lee2024_t9</td>
<td>16kHz</td>
<td>caption augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>229M</td>
<td>Clotho, FSD50K, WavCaps</td>
<td>CLAP, Phi-2.0</td>
</tr>
<tr>
<td>Romaniuk_SRPOL_task9_2</td>
<td>Romaniuk2024_t9</td>
<td>16kHz</td>
<td>random crop for long audio clips</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>229M</td>
<td>Clotho, FSD50K, AudioCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>HanYin_NWPU-JLESS_task9_1</td>
<td>Yin2024_t9</td>
<td>16kHz</td>
<td>volume augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238.60M</td>
<td>Clotho, FSD50K, Audiocaps, Auto-ACD, WavCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>Chung_KT_task9_1</td>
<td>Chung2024_t9</td>
<td>16kHz</td>
<td>N/A</td>
<td>FLAN-T5, ResUNet-based separation model, CLAP</td>
<td>waveform l1 loss, multi-scale mel-spectrogram loss, contrastive loss, loss balancer</td>
<td>1</td>
<td>372.73M</td>
<td>AudioCaps, Clotho, WavCaps, FSD50K</td>
<td>FLAN-T5, CLAP</td>
</tr>
<tr>
<td>Romaniuk_SRPOL_task9_1</td>
<td>Romaniuk2024_t9</td>
<td>16kHz</td>
<td>random crop for long audio clips</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking, separate masks for real and imaginary components</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>229M</td>
<td>Clotho, FSD50K, AudioCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>Chung_KT_task9_2</td>
<td>Chung2024_t9</td>
<td>16kHz</td>
<td>N/A</td>
<td>FLAN-T5, ResUNet-based separation model, CLAP</td>
<td>waveform l1 loss, multi-scale mel-spectrogram loss, contrastive loss, loss balancer</td>
<td>1</td>
<td>372.73M</td>
<td>AudioCaps, Clotho, WavCaps, FSD50K</td>
<td>FLAN-T5, CLAP</td>
</tr>
<tr>
<td>Chung_KT_task9_3</td>
<td>Chung2024_t9</td>
<td>16kHz</td>
<td>N/A</td>
<td>FLAN-T5, ResUNet-based separation model, CLAP</td>
<td>waveform l1 loss, multi-scale mel-spectrogram loss, contrastive loss, loss balancer</td>
<td>1</td>
<td>372.73M</td>
<td>AudioCaps, Clotho, WavCaps, FSD50K</td>
<td>FLAN-T5, CLAP</td>
</tr>
<tr>
<td>Romaniuk_SRPOL_task9_4</td>
<td>Romaniuk2024_t9</td>
<td>16kHz</td>
<td>random crop for long audio clips</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238.60M</td>
<td>Clotho, FSD50K, AudioCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>Romaniuk_SRPOL_task9_3</td>
<td>Romaniuk2024_t9</td>
<td>16kHz</td>
<td>random crop for long audio clips</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238.60M</td>
<td>Clotho, FSD50K, AudioCaps</td>
<td>CLAP</td>
</tr>
<tr>
<td>Guan_HEU_task9_1</td>
<td>Xiao2024_t9</td>
<td>16kHz</td>
<td>GPT-based text augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238.60M</td>
<td>Clotho, FSD50K</td>
<td>CLAP</td>
</tr>
<tr class="info" data-hline="true">
<td>Baseline</td>
<td>Liu2024_t9</td>
<td>16kHz</td>
<td>volume augmentation</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking</td>
<td>waveform l1 loss</td>
<td>1</td>
<td>238.60M</td>
<td>Clotho, FSD50K</td>
<td>CLAP</td>
</tr>
<tr>
<td>Guan_HEU_task9_3</td>
<td>Xiao2024_t9</td>
<td>16kHz</td>
<td>N/A</td>
<td>CLAP, ResUNet-based separation model, time-frequency masking, Latent Diffusion Model</td>
<td>waveform l1 loss, Latent Diffusion Model loss</td>
<td>1</td>
<td>671M</td>
<td>Clotho, FSD50K</td>
<td>CLAP, AudioLDM</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2024/technical_reports_task9.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Chung2024_t9" style="box-shadow: none">
<div class="panel-heading" id="heading-Chung2024_t9" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        LANGUAGE-QUERIED AUDIO SOURCE SEPARATION ENHANCED BY EXPANDED LANGUAGE-AUDIO CONTRASTIVE LOSS
       </h4>
<p style="text-align:left">
        Hae Chun Chung, Jae Hoon Jung
       </p>
<p style="text-align:left">
<em>
         AI Tech Lab, KT Corporation
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chung_KT_task9_1</span> <span class="label label-primary">Chung_KT_task9_2</span> <span class="label label-primary">Chung_KT_task9_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chung2024_t9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chung2024_t9" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chung2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Chung_67_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chung2024_t9" class="panel-collapse collapse" id="collapse-Chung2024_t9" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       LANGUAGE-QUERIED AUDIO SOURCE SEPARATION ENHANCED BY EXPANDED LANGUAGE-AUDIO CONTRASTIVE LOSS
      </h4>
<p style="text-align:left">
<small>
        Hae Chun Chung, Jae Hoon Jung
       </small>
<br/>
<small>
<em>
         AI Tech Lab, KT Corporation
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report outlines the efforts of KT Corporation's Acoustic Processing Project for addressing language-queried audio source separation (LASS), DCASE 2024 Challenge Task 9. The objective of this work is to separate arbitrary sound sources using a text description of the desired source. We propose three systems, each with the same model architecture but different training methods. These systems use the FLAN-T5 model as the text encoder and the ResUNet model as the separator. To train these systems, we introduced three loss functions: L1 loss in the time domain, multi-scale mel-spectrogram loss in the frequency domain, and contrastive loss, with a loss balancer to stabilize the training. Utilizing the Contrastive Language-Audio Pre-training (CLAP) model, we designed three contrastive losses: audio-to-text (A2T-CL), audio-to-audio (A2A-CL), and audio-to-multi (A2M-CL). The first system was trained with A2T-CL, the second with both A2A-CL and A2T-CL, and the third with A2M-CL. These systems achieved signal-to-distortion ratio (SDR) of 7.030, 7.124, and 7.136, respectively, showing nearly a 30\% improvement over the baseline SDR of 5.708 provided by the challenge.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chung2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Chung_67_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chung2024_t9label" class="modal fade" id="bibtex-Chung2024_t9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChung2024_t9label">
        LANGUAGE-QUERIED AUDIO SOURCE SEPARATION ENHANCED BY EXPANDED LANGUAGE-AUDIO CONTRASTIVE LOSS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chung2024_t9,
    Author = "Chung, Hae Chun and Jung, Jae Hoon",
    title = "LANGUAGE-QUERIED AUDIO SOURCE SEPARATION ENHANCED BY EXPANDED LANGUAGE-AUDIO CONTRASTIVE LOSS",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "June",
    abstract = "This technical report outlines the efforts of KT Corporation's Acoustic Processing Project for addressing language-queried audio source separation (LASS), DCASE 2024 Challenge Task 9. The objective of this work is to separate arbitrary sound sources using a text description of the desired source. We propose three systems, each with the same model architecture but different training methods. These systems use the FLAN-T5 model as the text encoder and the ResUNet model as the separator. To train these systems, we introduced three loss functions: L1 loss in the time domain, multi-scale mel-spectrogram loss in the frequency domain, and contrastive loss, with a loss balancer to stabilize the training. Utilizing the Contrastive Language-Audio Pre-training (CLAP) model, we designed three contrastive losses: audio-to-text (A2T-CL), audio-to-audio (A2A-CL), and audio-to-multi (A2M-CL). The first system was trained with A2T-CL, the second with both A2A-CL and A2T-CL, and the third with A2M-CL. These systems achieved signal-to-distortion ratio (SDR) of 7.030, 7.124, and 7.136, respectively, showing nearly a 30\\% improvement over the baseline SDR of 5.708 provided by the challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2024_t9" style="box-shadow: none">
<div class="panel-heading" id="heading-Lee2024_t9" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        PERFORMANCE IMPROVEMENT OF LANGUAGE-QUERIED AUDIO SOURCE SEPARATION BASED ON CAPTION AUGMENTATION FROM LARGE LANGUAGE MODELS FOR DCASE CHALLENGE 2024 TASK 9
       </h4>
<p style="text-align:left">
        Do Hyun Lee<sup>1</sup>, Yoonah Song<sup>1</sup>, Hong Kook Kim<sup>1,2,3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>AI Graduate School, Gwangju Institute of Science and Technology, Republic of Korea, <sup>2</sup>School of EECS, Gwangju Institute of Science and Technology, Republic of Korea, <sup>3</sup>Aunion AI, Co. Ltd, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_GIST-AunionAI_task9_1</span> <span class="label label-primary">Kim_GIST-AunionAI_task9_2</span> <span class="label label-primary">Kim_GIST-AunionAI_task9_3</span> <span class="label label-primary">Kim_GIST-AunionAI_task9_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lee2024_t9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2024_t9" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kim_80_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2024_t9" class="panel-collapse collapse" id="collapse-Lee2024_t9" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       PERFORMANCE IMPROVEMENT OF LANGUAGE-QUERIED AUDIO SOURCE SEPARATION BASED ON CAPTION AUGMENTATION FROM LARGE LANGUAGE MODELS FOR DCASE CHALLENGE 2024 TASK 9
      </h4>
<p style="text-align:left">
<small>
        Do Hyun Lee<sup>1</sup>, Yoonah Song<sup>1</sup>, Hong Kook Kim<sup>1,2,3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>AI Graduate School, Gwangju Institute of Science and Technology, Republic of Korea, <sup>2</sup>School of EECS, Gwangju Institute of Science and Technology, Republic of Korea, <sup>3</sup>Aunion AI, Co. Ltd, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We present a prompt-engineering-based text-augmentation approach applied to a language-queried audio source separation (LASS) task. To enhance the performance of LASS, the proposed approach utilizes large language models (LLMs) to generate multiple captions corresponding to each sentence of the training dataset. To this end, we first perform experiments to identify the most effective prompts for caption augmentation with a smaller number of captions. A LASS model trained with these augmented captions demonstrates improved performance on the DCASE 2024 Task 9 validation set compared to that trained without augmentation. This study highlights the effectiveness of LLM-based caption augmentation in advancing language-queried audio source separation.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kim_80_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2024_t9label" class="modal fade" id="bibtex-Lee2024_t9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2024_t9label">
        PERFORMANCE IMPROVEMENT OF LANGUAGE-QUERIED AUDIO SOURCE SEPARATION BASED ON CAPTION AUGMENTATION FROM LARGE LANGUAGE MODELS FOR DCASE CHALLENGE 2024 TASK 9
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lee2024_t9,
    Author = "Lee, Do Hyun and Song, Yoonah and Kim, Hong Kook",
    title = "PERFORMANCE IMPROVEMENT OF LANGUAGE-QUERIED AUDIO SOURCE SEPARATION BASED ON CAPTION AUGMENTATION FROM LARGE LANGUAGE MODELS FOR DCASE CHALLENGE 2024 TASK 9",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "June",
    abstract = "We present a prompt-engineering-based text-augmentation approach applied to a language-queried audio source separation (LASS) task. To enhance the performance of LASS, the proposed approach utilizes large language models (LLMs) to generate multiple captions corresponding to each sentence of the training dataset. To this end, we first perform experiments to identify the most effective prompts for caption augmentation with a smaller number of captions. A LASS model trained with these augmented captions demonstrates improved performance on the DCASE 2024 Task 9 validation set compared to that trained without augmentation. This study highlights the effectiveness of LLM-based caption augmentation in advancing language-queried audio source separation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Romaniuk2024_t9" style="box-shadow: none">
<div class="panel-heading" id="heading-Romaniuk2024_t9" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SRPOL submission to DCASE 2024 challenge task 9: Modeling real and imaginary components, Mixit and SDR based loss
       </h4>
<p style="text-align:left">
        Michal Romaniuk, Justyna Krzywdziak
       </p>
<p style="text-align:left">
<em>
         Samsung R&amp;D Institute Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Romaniuk_SRPOL_task9_1</span> <span class="label label-primary">Romaniuk_SRPOL_task9_2</span> <span class="label label-primary">Romaniuk_SRPOL_task9_3</span> <span class="label label-primary">Romaniuk_SRPOL_task9_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Romaniuk2024_t9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Romaniuk2024_t9" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Romaniuk2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Romaniuk_87_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Romaniuk2024_t9" class="panel-collapse collapse" id="collapse-Romaniuk2024_t9" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SRPOL submission to DCASE 2024 challenge task 9: Modeling real and imaginary components, Mixit and SDR based loss
      </h4>
<p style="text-align:left">
<small>
        Michal Romaniuk, Justyna Krzywdziak
       </small>
<br/>
<small>
<em>
         Samsung R&amp;D Institute Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We present our solution to the DCASE 2024 challenge task 9 (Language-Queried Audio Source Separation). Our solution is based on the official baseline, with training dataset including FSD50k, Clotho and additionally extended with AudioCaps. We show that the additional data improve results throughout the training process. We explore changing the ratio masking method from spectrogram amplitude and phase to individual masks for real and imaginary components. We also investigate how different losses, such as Mixit loss and SDR based loss, affect the training process.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Romaniuk2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Romaniuk_87_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Romaniuk2024_t9label" class="modal fade" id="bibtex-Romaniuk2024_t9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRomaniuk2024_t9label">
        SRPOL submission to DCASE 2024 challenge task 9: Modeling real and imaginary components, Mixit and SDR based loss
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Romaniuk2024_t9,
    Author = "Romaniuk, Michal and Krzywdziak, Justyna",
    title = "SRPOL submission to DCASE 2024 challenge task 9: Modeling real and imaginary components, Mixit and SDR based loss",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "June",
    abstract = "We present our solution to the DCASE 2024 challenge task 9 (Language-Queried Audio Source Separation). Our solution is based on the official baseline, with training dataset including FSD50k, Clotho and additionally extended with AudioCaps. We show that the additional data improve results throughout the training process. We explore changing the ratio masking method from spectrogram amplitude and phase to individual masks for real and imaginary components. We also investigate how different losses, such as Mixit loss and SDR based loss, affect the training process."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xiao2024_t9" style="box-shadow: none">
<div class="panel-heading" id="heading-Xiao2024_t9" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        LANGUAGE-QUERIED AUDIO SOURCE SEPARATION WITH GPT-BASED TEXT AUGMENTATION AND IDEAL RATIO MASKING
       </h4>
<p style="text-align:left">
        Feiyang Xiao<sup>1</sup>, Wenbo Wang<sup>2</sup>, Dongli Xu<sup>3</sup>, Shuhan Qi<sup>4</sup>, Kejia Zhang<sup>1</sup>, Qiaoxi Zhu<sup>5</sup>, Jian Guan<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Harbin Engineering University, Harbin, China, <sup>2</sup>Harbin Institute of Technology, Harbin, China, <sup>3</sup>Independent Researcher, China, <sup>4</sup>Harbin Institute of Technology, Shenzhen, China, <sup>5</sup>University of Technology Sydney, Ultimo, Australia
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Guan_HEU_task9_1</span> <span class="label label-primary">Guan_HEU_task9_2</span> <span class="label label-primary">Guan_HEU_task9_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xiao2024_t9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xiao2024_t9" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xiao2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Guan_56_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xiao2024_t9" class="panel-collapse collapse" id="collapse-Xiao2024_t9" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       LANGUAGE-QUERIED AUDIO SOURCE SEPARATION WITH GPT-BASED TEXT AUGMENTATION AND IDEAL RATIO MASKING
      </h4>
<p style="text-align:left">
<small>
        Feiyang Xiao<sup>1</sup>, Wenbo Wang<sup>2</sup>, Dongli Xu<sup>3</sup>, Shuhan Qi<sup>4</sup>, Kejia Zhang<sup>1</sup>, Qiaoxi Zhu<sup>5</sup>, Jian Guan<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Harbin Engineering University, Harbin, China, <sup>2</sup>Harbin Institute of Technology, Harbin, China, <sup>3</sup>Independent Researcher, China, <sup>4</sup>Harbin Institute of Technology, Shenzhen, China, <sup>5</sup>University of Technology Sydney, Ultimo, Australia
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report details our submission systems for Task 9 (language-queried audio source separation) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge. Our four proposed systems utilize the large language model GPT-4 for data augmentation and apply the ideal ratio masking strategy in the latent feature space of the text-to-audio generation model, AudioLDM. Additionally, our systems incorporate the pre-trained language-queried audio source separation model, AudioSep-32K, which leverages extensive pre-training on large-scale data to separate audio sources based on text queries. Experimental results demonstrate that our systems achieve better separation performance compared to the official baseline method on objective metrics. Furthermore, we introduce a novel evaluation metric, audio-text similarity (ATS), which measures the semantic similarity between the separated audio and the text query without requiring a reference target audio signal.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xiao2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Guan_56_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xiao2024_t9label" class="modal fade" id="bibtex-Xiao2024_t9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXiao2024_t9label">
        LANGUAGE-QUERIED AUDIO SOURCE SEPARATION WITH GPT-BASED TEXT AUGMENTATION AND IDEAL RATIO MASKING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xiao2024_t9,
    Author = "Xiao, Feiyang and Wang, Wenbo and Xu, Dongli and Qi, Shuhan and Zhang, Kejia and Zhu, Qiaoxi and Guan, Jian",
    title = "LANGUAGE-QUERIED AUDIO SOURCE SEPARATION WITH GPT-BASED TEXT AUGMENTATION AND IDEAL RATIO MASKING",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "June",
    abstract = "This technical report details our submission systems for Task 9 (language-queried audio source separation) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge. Our four proposed systems utilize the large language model GPT-4 for data augmentation and apply the ideal ratio masking strategy in the latent feature space of the text-to-audio generation model, AudioLDM. Additionally, our systems incorporate the pre-trained language-queried audio source separation model, AudioSep-32K, which leverages extensive pre-training on large-scale data to separate audio sources based on text queries. Experimental results demonstrate that our systems achieve better separation performance compared to the official baseline method on objective metrics. Furthermore, we introduce a novel evaluation metric, audio-text similarity (ATS), which measures the semantic similarity between the separated audio and the text query without requiring a reference target audio signal."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yin2024_t9" style="box-shadow: none">
<div class="panel-heading" id="heading-Yin2024_t9" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        LANGUAGE-QUERIED AUDIO SOURCE SEPARATION VIA RESUNET WITH DPRNN
       </h4>
<p style="text-align:left">
        Han Yin<sup>1</sup>, Jisheng Bai<sup>1,3,4</sup>, Mou Wang<sup>2</sup>, Jianfeng Chen<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Northwestern Polytechnical University, Xiâ€™an, China, <sup>2</sup>Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Nanyang Technological University, Singapore, <sup>4</sup>LianFeng Acoustic Technologies Co., Ltd. Xiâ€™an China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">HanYin_NWPU-JLESS_task9_1</span> <span class="label label-primary">HanYin_NWPU-JLESS_task9_2</span> <span class="label label-primary">HanYin_NWPU-JLESS_task9_3</span> <span class="label label-primary">HanYin_NWPU-JLESS_task9_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yin2024_t9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yin2024_t9" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yin2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Yin_13_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yin2024_t9" class="panel-collapse collapse" id="collapse-Yin2024_t9" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       LANGUAGE-QUERIED AUDIO SOURCE SEPARATION VIA RESUNET WITH DPRNN
      </h4>
<p style="text-align:left">
<small>
        Han Yin<sup>1</sup>, Jisheng Bai<sup>1,3,4</sup>, Mou Wang<sup>2</sup>, Jianfeng Chen<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Northwestern Polytechnical University, Xiâ€™an, China, <sup>2</sup>Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Nanyang Technological University, Singapore, <sup>4</sup>LianFeng Acoustic Technologies Co., Ltd. Xiâ€™an China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report presents our submitted systems for the task 9 of DCASE challenge: language-queried audio source separation (LASS). LASS is the task of separating arbitrary sound sources using textual descriptions of the desired source, also known as ''separate what you describe''. Specifically, we first incorporate a dual-path recurrent neural network (DPRNN) block into ResUNet, which is significantly beneficial for improving the separation performance. Then, we trained the proposed model using a large number of public datasets, including Clotho, FSD50K, Audiocaps, Auto-ACD, and Wavcaps. We trained the proposed model at 16 kHz and 32 kHz respectively, and the 32 kHz model achieved the best separation performance with an SDR of 8.191 dB on the validation set, which is 2.483 dB higher than the challenge baseline.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yin2024_t9" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Yin_13_t9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yin2024_t9label" class="modal fade" id="bibtex-Yin2024_t9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYin2024_t9label">
        LANGUAGE-QUERIED AUDIO SOURCE SEPARATION VIA RESUNET WITH DPRNN
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yin2024_t9,
    Author = "Yin, Han and Bai, Jisheng and Wang, Mou and Chen, Jianfeng",
    title = "LANGUAGE-QUERIED AUDIO SOURCE SEPARATION VIA RESUNET WITH DPRNN",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "June",
    abstract = "This report presents our submitted systems for the task 9 of DCASE challenge: language-queried audio source separation (LASS). LASS is the task of separating arbitrary sound sources using textual descriptions of the desired source, also known as ''separate what you describe''. Specifically, we first incorporate a dual-path recurrent neural network (DPRNN) block into ResUNet, which is significantly beneficial for improving the separation performance. Then, we trained the proposed model using a large number of public datasets, including Clotho, FSD50K, Audiocaps, Auto-ACD, and Wavcaps. We trained the proposed model at 16 kHz and 32 kHz respectively, and the 32 kHz model achieved the best separation performance with an SDR of 8.191 dB on the validation set, which is 2.483 dB higher than the challenge baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>