<!DOCTYPE html><html lang="en">
<head>
    <title>General-purpose audio tagging of Freesound content with AudioSet labels - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2018/task-general-purpose-audio-tagging">
        <meta name="author" content="DCASE" />
        <meta name="description" content="This task evaluates systems for general-purpose audio tagging with an increased number of categories and using data with annotations of varying reliability. This task will provide insight towards the development of broadly-applicable sound event classifiers that consider an increased and diverse amount of categories. Description This task addresses the problem …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2018/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2018/task-general-purpose-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2018/task-general-purpose-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-bird-audio-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-monitoring-domestic-activities" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-home text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2018/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2018/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/marina-bay-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-success"></i><i class="fa dc-tags fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Tags</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 2</span></span><img src="../images/logos/dcase/dcase2018_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">General-purpose audio tagging of Freesound content with AudioSet labels</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Frederic Font Corbera" class="img img-circle" src="/images/person/frederic_font.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Frederic Font Corbera</strong>
<a class="icon" href="mailto:frederic.font@upf.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.mtg.upf.edu/">
                                Universitat Pompeu Fabra
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Eduardo Fonseca" class="img img-circle" src="/images/person/eduardo_fonseca.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Eduardo Fonseca</strong>
<a class="icon" href="mailto:eduardo.fonseca@upf.edu"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.mtg.upf.edu/">
                                Universitat Pompeu Fabra
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Daniel P. W. Ellis" class="img img-circle" src="/images/person/dan_ellis.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Daniel P. W. Ellis</strong>
<a class="icon" href="mailto:dpwe@google.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.google.com/">
                                Google, Inc.
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Manoj Plakal" class="img img-circle" src="/images/person/manoj_plakal.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Manoj Plakal</strong>
<a class="icon" href="mailto:plakal@google.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://research.google.com/">
                                Google, Inc.
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#recording-and-annotation-procedure">Recording and annotation procedure</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#train-set">Train set</a></li>
<li><a href="#test-set">Test set</a></li>
</ul>
</li>
<li><a href="#submission-and-evaluation">Submission and evaluation</a></li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#repository">Repository</a></li>
<li><a href="#system-description">System description</a></li>
<li><a href="#system-performance">System performance</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">
This task evaluates systems for general-purpose audio tagging with an increased number of categories and using data with annotations of varying reliability. This task will provide insight towards the development of broadly-applicable sound event classifiers that consider an increased and diverse amount of categories.
</p>
<h1 id="description">Description</h1>
<p>This task addresses the problem of general-purpose automatic audio tagging and poses two main challenges. The first one is to build models that can recognize an increased number of sound events of very diverse nature, including musical instruments, human sounds, domestic sounds, animals, etc. The second challenge consists of leveraging subsets of training data featuring annotations of varying reliability, as a reflection of the expensiveness of having high quality annotations. This task will provide insight towards the development of broadly-applicable sound event classifiers that consider an increased and diverse amount of categories. These models can be used, for example, in automatic description of multimedia or acoustic monitoring applications.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-6 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2018/task2_freesound_audio_tagging.png"/>
<figcaption>Figure 1: Overview of a single-tag tagging system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>This task is hosted in <strong>Kaggle</strong> - a platform that hosts machine learning competitions with a vibrant community of participants. Hence, <strong>the resources associated to this task (datasets download, leaderboard and submission) will be provided by Kaggle</strong>. Please visit the 
<a href="https://www.kaggle.com/c/freesound-audio-tagging" target="_blank"><em>Freesound General-Purpose Audio Tagging Challenge</em> Kaggle competition page</a> for detailed information about how to participate and all other relevant aspects of the challenge. What follows in this page is a summary of the most important aspects of the challenge.</p>
<p><a class="btn btn-primary" href="https://www.kaggle.com/c/freesound-audio-tagging" style="" target="_blank">Kaggle competition page</a></p>
<h1 id="audio-dataset">Audio dataset</h1>
<p>This task uses the <strong>FSDKaggle2018</strong> dataset, consisting of audio samples from <a href="https://freesound.org/" target="_blank">Freesound</a> annotated using a vocabulary of 41 labels from Google’s <a href="https://research.google.com/audioset/" target="_blank">AudioSet</a> Ontology:</p>
<table style="width: 100%;padding:0px;margin:0px;">
<tr>
<td>
<ul>
<li>Tearing</li>
<li>Bus</li>
<li>Shatter</li>
<li>Gunshot, gunfire</li>
<li>Fireworks</li>
<li>Writing</li>
<li>Computer keyboard</li>
<li>Scissors</li>
<li>Microwave oven</li>
<li>Keys jangling</li>
<li>Drawer open or close</li>
<li>Squeak</li>
<li>Knock</li>
<li>Telephone</li>
</ul>
</td>
<td>
<ul>
<li>Saxophone</li>
<li>Oboe</li>
<li>Flute</li>
<li>Clarinet</li>
<li>Acoustic guitar</li>
<li>Tambourine</li>
<li>Glockenspiel</li>
<li>Gong</li>
<li>Snare drum</li>
<li>Bass drum</li>
<li>Hi-hat</li>
<li>Electric piano</li>
<li>Harmonica</li>
<li>Trumpet</li>
</ul>
</td>
<td>
<ul>
<li>Violin, fiddle</li>
<li>Double bass</li>
<li>Cello</li>
<li>Chime</li>
<li>Cough</li>
<li>Laughter</li>
<li>Applause</li>
<li>Finger snapping</li>
<li>Fart</li>
<li>Burping, eructation</li>
<li>Cowbell</li>
<li>Bark</li>
<li>Meow</li>
</ul>
</td>
</tr>
</table>
<p>The <strong>FSDKaggle2018</strong> dataset provided for this task is a reduced subset of <strong>FSD</strong>: a work-in-progress, large-scale, general-purpose audio dataset composed of <a href="https://freesound.org/" target="_blank">Freesound</a> content annotated with labels from the <a href="https://research.google.com/audioset/" target="_blank">AudioSet</a> Ontology. FSD is being collected through the <a href="https://datasets.freesound.org/" target="_blank">Freesound Datasets platform</a>, which is a platform for the collaborative creation of open audio collections. We encourage participants of the DCASE challenge to check out the Freesound Datasets platform and, why not, contribute with some annotations for the FSD. More information about the Freesound Datasets platform and the creation of FSD is available in:</p>
<div class="btex-item" data-item="Fonseca2017freesound" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Fonseca2017freesound"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Eduardo Fonseca, Jordi Pons, Xavier Favory, Frederic Font, Dmitry Bogdanov, Andr<span class="bibtex-protected"><span class="bibtex-protected">é</span></span>s Ferraro, Sergio Oramas, Alastair Porter, and Xavier Serra.
<em>Freesound datasets: a platform for the creation of open audio datasets.</em>
In Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR 2017), 486–493. Suzhou, China, 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://repositori.upf.edu/bitstream/handle/10230/33299/fonseca_ismir17_freesound.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" class="panel-collapse collapse" id="collapseFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" role="tabpanel">
<h4>Freesound Datasets: a platform for the creation of open audio datasets</h4>
<h5>Abstract</h5>
<p class="text-justify">Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://repositori.upf.edu/bitstream/handle/10230/33299/fonseca_ismir17_freesound.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2label" class="modal fade" id="bibtexFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexFonseca2017freesound45e33f69614140aa9e8e6fe3f584c9a2label">Freesound Datasets: a platform for the creation of open audio datasets</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Fonseca2017freesound,
    Author = "Fonseca, Eduardo and Pons, Jordi and Favory, Xavier and Font, Frederic and Bogdanov, Dmitry and Ferraro, Andr{\'{e}}s and Oramas, Sergio and Porter, Alastair and Serra, Xavier",
    title = "Freesound Datasets: a platform for the creation of open audio datasets",
    booktitle = "Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR 2017)",
    year = "2017",
    address = "Suzhou, China",
    pages = "486-493",
    abstract = "Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="recording-and-annotation-procedure">Recording and annotation procedure</h2>
<p>This task employs data drawn from content uploaded by the Freesound user community, encompassing sounds in a wide range of real-world environments. Recording scenarios and techniques can be very different as sounds are uploaded by users across the globe. All audio samples in this dataset are provided as uncompressed PCM 16 bit, 44.1 kHz, mono audio files.</p>
<p>The data labeling process started from a manual mapping between Freesound tags and AudioSet Ontology categories. Using this mapping, a number of Freesound audio samples were <strong>automatically annotated</strong>. These annotations can be understood as weak labels since they express the presence of a sound category in an audio sample. Then, a <strong>data validation process</strong> was carried out in which a number of participants did listen to the annotated sounds and manually assesed the presence/absence of an automatically assigned sound category, according to the AudioSet category description. More details about the annotation procedure can be found in <em>Fonseca et al. (2017)</em>. More information about AudioSet can be found in:</p>
<div class="btex-item" data-item="Gemmeke2017" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Gemmeke2017"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter.
<em>Audio set: an ontology and human-labeled dataset for audio events.</em>
In Proc. IEEE ICASSP 2017. New Orleans, LA, 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexGemmeke20171f8fb96c8c364857a99fe1a06b719a58" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://research.google.com/pubs/archive/45857.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseGemmeke20171f8fb96c8c364857a99fe1a06b719a58" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseGemmeke20171f8fb96c8c364857a99fe1a06b719a58" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingGemmeke20171f8fb96c8c364857a99fe1a06b719a58" class="panel-collapse collapse" id="collapseGemmeke20171f8fb96c8c364857a99fe1a06b719a58" role="tabpanel">
<h4>Audio Set: An ontology and human-labeled dataset for audio events</h4>
<h5>Abstract</h5>
<p class="text-justify">Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets -- principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 635 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexGemmeke20171f8fb96c8c364857a99fe1a06b719a58" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://research.google.com/pubs/archive/45857.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexGemmeke20171f8fb96c8c364857a99fe1a06b719a58label" class="modal fade" id="bibtexGemmeke20171f8fb96c8c364857a99fe1a06b719a58" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexGemmeke20171f8fb96c8c364857a99fe1a06b719a58label">Audio Set: An ontology and human-labeled dataset for audio events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gemmeke2017,
    Author = "Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin",
    title = "Audio Set: An ontology and human-labeled dataset for audio events",
    booktitle = "Proc. IEEE ICASSP 2017",
    year = "2017",
    address = "New Orleans, LA",
    abstract = "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets -- principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 635 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>In the provided <strong>FSDKaggle2018</strong> dataset, a number of the ground truth labels have been <strong>manually verified</strong> (some of the labels have inter-annotator agreement but not all of them) while the <strong>the rest has not been manually verified</strong> and therefore some of them could be inaccurate. All audio samples in this dataset have a single label (i.e., they are only annotated with one label).</p>
<h2 id="download">Download</h2>
<p>The <strong>FSDKaggle2018</strong> dataset can be downloaded from the <a href="https://www.kaggle.com/c/freesound-audio-tagging/data" target="_blank"><em>Freesound General-Purpose Audio Tagging Challenge</em> Kaggle competition page</a>. Details about usage restrictions and sound licenses are provided there.</p>
<h1 id="task-setup">Task setup</h1>
<p>The task consists of predicting the sound category to which every audio sample in the test set belongs to. A single label should be assigned to each file in the test set (i.e., single-tag tagging), although up to three labels can be predicted for each file as we evaluate with Mean Average Precision @ 3. The predictions are to be done at the audio sample level, i.e., no start/end timestamps for the events are required. The dataset for this task is split into a <strong>train set</strong> and a <strong>test set</strong>.</p>
<!-- Participants are required to report performance of their system using this train/test setup in order to allow comparison of systems on the development set. -->
<h2 id="train-set">Train set</h2>
<p>The <strong>train set</strong> is meant to be for system development and includes <strong>~9.5k samples unequally distributed among 41 categories</strong>. The minimum number of audio samples per category in the train set is 94, and the maximum 300. The duration of the audio samples ranges from 300ms to 30s due to the diversity of the sound categories and the preferences of Freesound users when recording sounds.</p>
<p>The train set is composed of ~3.7k manually-verified annotations and ~5.8k non-verified annotations. The quality of the non-verified annotations has been roughly estimated to be at least 65-70% in each sound category. A flag for each annotation is provided which indicates whether or not that annotation has been manually verified. Participants can use this information during the development of their systems.</p>
<h2 id="test-set">Test set</h2>
<p>The <strong>test set</strong> is composed of ~1.6k manually-verified annotations with a similar category distribution than that of the train set. These annotations are complemented with ~7.8k <em>padding</em> annotations which are also included in the test set but that won't be used for evaluating the systems.</p>
<h1 id="submission-and-evaluation">Submission and evaluation</h1>
<p>Submissions will be done through the Kaggle platform and will be evaluated with the Mean Average Precision @ 3 metric. Please visit the <a href="https://www.kaggle.com/c/freesound-audio-tagging/data" target="_blank"><em>Freesound General-Purpose Audio Tagging Challenge</em> Kaggle competition page</a> for detailed information about submission and evaluation.</p>
<h1 id="task-rules">Task rules</h1>
<p>A detailed description of the task rules can be found in the <a href="https://www.kaggle.com/c/freesound-audio-tagging/data" target="_blank"><em>Freesound General-Purpose Audio Tagging Challenge</em> Kaggle competition page</a>. This is a summary of the most important points:</p>
<ul>
<li>Participants <strong>are allowed</strong> to use external data for system development, but external data can not be sourced from Freesound (including any of the original sound's metadata or other sounds in Freesound).</li>
<li>Participants <strong>are not allowed</strong> to make subjective judgements of the evaluation data, nor to annotate it (this includes the use of statistics about the evaluation dataset in the decision making). The evaluation dataset cannot be used to train the submitted system.</li>
<li>The top-3 winning teams are required to publish their systems under an <strong>open-source license</strong> in order to be considered <em>winners</em>.</li>
</ul>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="10" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_map3" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="1"></th>
</tr>
<tr>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3 (Public leaderboard)" data-chartable="true" data-field="eval_map3" data-sortable="true" data-value-type="float4">
                mAP@3 <br/>(Private leaderboard)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Eduardo Fonseca</td>
<td>Music Technology Group (UPF), Universitat Pompeu Fabra, Barcelona, Barcelona, Spain.</td>
<td>task-general-purpose-audio-tagging-results#Fonseca2018</td>
<td>0.6943</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_1</td>
<td>Il-Young Jeong</td>
<td>COCAI, Cochlear.ai, Seoul, South Korea.</td>
<td>task-general-purpose-audio-tagging-results#Jeong2018</td>
<td>0.9538</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_2</td>
<td>Il-Young Jeong</td>
<td>COCAI, Cochlear.ai, Seoul, South Korea.</td>
<td>task-general-purpose-audio-tagging-results#Jeong2018</td>
<td>0.9506</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_3</td>
<td>Il-Young Jeong</td>
<td>COCAI, Cochlear.ai, Seoul, South Korea.</td>
<td>task-general-purpose-audio-tagging-results#Jeong2018</td>
<td>0.9405</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_1</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Electrical and Electronic Engineering (NTU), Nanyang Technological University, Singapore.</td>
<td>task-general-purpose-audio-tagging-results#Nguyen2018</td>
<td>0.9496</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_2</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Electrical and Electronic Engineering (NTU), Nanyang Technological University, Singapore.</td>
<td>task-general-purpose-audio-tagging-results#Nguyen2018</td>
<td>0.9251</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_3</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Electrical and Electronic Engineering (NTU), Nanyang Technological University, Singapore.</td>
<td>task-general-purpose-audio-tagging-results#Nguyen2018</td>
<td>0.9213</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_4</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Electrical and Electronic Engineering (NTU), Nanyang Technological University, Singapore.</td>
<td>task-general-purpose-audio-tagging-results#Nguyen2018</td>
<td>0.9478</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_1</td>
<td>Benjamin Wilhelm</td>
<td>Computer and Information Science (UKON), University of Konstanz, Constance, Germany.</td>
<td>task-general-purpose-audio-tagging-results#Wilhelm2018</td>
<td>0.9435</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_2</td>
<td>Benjamin Wilhelm</td>
<td>Computer and Information Science (UKON), University of Konstanz, Constance, Germany.</td>
<td>task-general-purpose-audio-tagging-results#Wilhelm2018</td>
<td>0.9416</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_1</td>
<td>Nam Kyun Kim</td>
<td>School of Electrical Engineering and Computer Science (GIST), Gwangju Institute of Science and Technology, Gwangju, Korea.</td>
<td>task-general-purpose-audio-tagging-results#Kim2018</td>
<td>0.9151</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_2</td>
<td>Nam Kyun Kim</td>
<td>School of Electrical Engineering and Computer Science (GIST), Gwangju Institute of Science and Technology, Gwangju, Korea.</td>
<td>task-general-purpose-audio-tagging-results#Kim2018</td>
<td>0.9133</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_3</td>
<td>Nam Kyun Kim</td>
<td>School of Electrical Engineering and Computer Science (GIST), Gwangju Institute of Science and Technology, Gwangju, Korea.</td>
<td>task-general-purpose-audio-tagging-results#Kim2018</td>
<td>0.9139</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_4</td>
<td>Nam Kyun Kim</td>
<td>School of Electrical Engineering and Computer Science (GIST), Gwangju Institute of Science and Technology, Gwangju, Korea.</td>
<td>task-general-purpose-audio-tagging-results#Kim2018</td>
<td>0.9174</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_1</td>
<td>Zhicun Xu</td>
<td>Department of Signal Processing and Acoustics (Aalto), Aalto University, Finland, Espoo, Finland.</td>
<td>task-general-purpose-audio-tagging-results#Xu2018</td>
<td>0.9065</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_2</td>
<td>Zhicun Xu</td>
<td>Department of Signal Processing and Acoustics (Aalto), Aalto University, Finland, Espoo, Finland.</td>
<td>task-general-purpose-audio-tagging-results#Xu2018</td>
<td>0.9081</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_1</td>
<td>Ria Chakraborty</td>
<td>Cognitive Business Decision Services (IBM), International Business Machines, India, Kolkata, India.</td>
<td>task-general-purpose-audio-tagging-results#Chakraborty2018</td>
<td>0.9328</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_2</td>
<td>Ria Chakraborty</td>
<td>Cognitive Business Decision Services (IBM), International Business Machines, India, Kolkata, India.</td>
<td>task-general-purpose-audio-tagging-results#Chakraborty2018</td>
<td>0.9320</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_judges_award</td>
<td>Ria Chakraborty</td>
<td>Cognitive Business Decision Services (IBM), International Business Machines, India, Kolkata, India.</td>
<td>task-general-purpose-audio-tagging-results#Chakraborty2018</td>
<td>0.9079</td>
</tr>
<tr>
<td></td>
<td>Han_NPU_task2_1</td>
<td>Xueyu Han</td>
<td>Center of Intelligence Acoustics and Immersive Communication (NPU), Northwestern Polytechnical University, Xi'an, China.</td>
<td>task-general-purpose-audio-tagging-results#Han2018</td>
<td>0.8723</td>
</tr>
<tr>
<td></td>
<td>Zhesong_PKU_task2_1</td>
<td>Zhesong Yu</td>
<td>Institute of computer science &amp; technology (PKU), Peking University, Beijing, China.</td>
<td>task-general-purpose-audio-tagging-results#Yu2018</td>
<td>0.8807</td>
</tr>
<tr>
<td></td>
<td>Hanyu_BUPT_task2</td>
<td>Zhang Hanyu</td>
<td>Embedded Artificial Intelligence Group (BUPT), University of Posts and Telecommunications, Beijing, Beijing, China.</td>
<td>task-general-purpose-audio-tagging-results#Hanyu2018</td>
<td>0.7877</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_1</td>
<td>Qingkai WEI</td>
<td>Kuaiyu, Beijing Kuaiyu Electronics Co., Ltd, Beijing, PRC.</td>
<td>task-general-purpose-audio-tagging-results#WEI2018</td>
<td>0.9409</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_2</td>
<td>Qingkai WEI</td>
<td>Kuaiyu, Beijing Kuaiyu Electronics Co., Ltd, Beijing, PRC.</td>
<td>task-general-purpose-audio-tagging-results#WEI2018</td>
<td>0.9423</td>
</tr>
<tr>
<td></td>
<td>Colangelo_RM3_task2_1</td>
<td>Federico Colangelo</td>
<td>Department of Engineering (RM3), Universita degli studi Roma Tre, Rome, Italy.</td>
<td>task-general-purpose-audio-tagging-results#Colangelo2018</td>
<td>0.6978</td>
</tr>
<tr>
<td></td>
<td>Shan_DBSonics_task2_1</td>
<td>Yi Ren</td>
<td>DB Sonics, Beijing, China.</td>
<td>task-general-purpose-audio-tagging-results#Ren2018</td>
<td>0.9405</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_1</td>
<td>Xu Kele</td>
<td>Department of Computer Science (NUDT), National University of Defense Technology, Changsha, China.</td>
<td>task-general-purpose-audio-tagging-results#Kele2018</td>
<td>0.9498</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_2</td>
<td>Xu Kele</td>
<td>Department of Computer Science (NUDT), National University of Defense Technology, Changsha, China.</td>
<td>task-general-purpose-audio-tagging-results#Kele2018</td>
<td>0.9441</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_1</td>
<td>Iurii Agafonov</td>
<td>Speech Information Systems (ITMO), ITMO University, Saint-Petersburg, Saint-Petersburg, Russia.</td>
<td>task-general-purpose-audio-tagging-results#Agafonov2018</td>
<td>0.9174</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_2</td>
<td>Iurii Agafonov</td>
<td>Speech Information Systems (ITMO), ITMO University, Saint-Petersburg, Saint-Petersburg, Russia.</td>
<td>task-general-purpose-audio-tagging-results#Agafonov2018</td>
<td>0.9275</td>
</tr>
<tr>
<td></td>
<td>Wilkinghoff_FKIE_task2_1</td>
<td>Kevin Wilkinghoff</td>
<td>Communication Systems (FKIE), Fraunhofer Institute for Communication, Information Processing and Ergonomics, Wachtberg, Germany.</td>
<td>task-general-purpose-audio-tagging-results#Wilkinghoff2018</td>
<td>0.9414</td>
</tr>
<tr>
<td></td>
<td>Pantic_ETF_task2_1</td>
<td>Bogdan Pantic</td>
<td>Signals and Systems Department (ETF), School of Electrical Engineering, Belgrade, Serbia.</td>
<td>task-general-purpose-audio-tagging-results#Pantic2018</td>
<td>0.9419</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_1</td>
<td>Maksim Khadkevich</td>
<td>AML (FB), Facebook, Menlo Park, CA, USA.</td>
<td>task-general-purpose-audio-tagging-results#Khadkevich2018</td>
<td>0.9188</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_2</td>
<td>Maksim Khadkevich</td>
<td>AML (FB), Facebook, Menlo Park, CA, USA.</td>
<td>task-general-purpose-audio-tagging-results#Khadkevich2018</td>
<td>0.9178</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_1</td>
<td>Turab Iqbal</td>
<td>Centre for Vision, Speech and Signal Processing (Surrey), University of Surrey, UK, Surrey, UK.</td>
<td>task-general-purpose-audio-tagging-results#Iqbal2018</td>
<td>0.9484</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_2</td>
<td>Turab Iqbal</td>
<td>Centre for Vision, Speech and Signal Processing (Surrey), University of Surrey, UK, Surrey, UK.</td>
<td>task-general-purpose-audio-tagging-results#Iqbal2018</td>
<td>0.9512</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_1</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP) (Surrey), University of Surrey, Guildford, UK.</td>
<td>task-general-purpose-audio-tagging-results#Kong2018</td>
<td>0.9034</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_2</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vission, Speech and Signal Processing (CVSSP) (Surrey), University of Surrey, Guildford, UK.</td>
<td>task-general-purpose-audio-tagging-results#Kong2018</td>
<td>0.8622</td>
</tr>
<tr>
<td></td>
<td>Dorfer_CPJKU_task2_1</td>
<td>Matthias Dorfer</td>
<td>Institute of Computational Perception (JKU), Johannes Kepler University Linz, Linz, Austria.</td>
<td>task-general-purpose-audio-tagging-results#Dorfer2018</td>
<td>0.9518</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2018/task-general-purpose-audio-tagging-results">results page</a></p>
<h1 id="baseline-system">Baseline system</h1>
<p>The baseline system provides a simple entry-level state-of-the-art approach that gives a sense of the performance possible with the <strong>FSDKaggle2018</strong> dataset of Task 2. It is a good starting point especially for entry-level researchers to get familiar with the task. Regardless of whether participants build their approaches on top of this baseline system or develop their own, DCASE organizers encourage all participants to open-source their code after the challenge. An overall description of the baseline system is included next. More detailed information can be found in the repository.</p>
<h2 id="repository">Repository</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task2" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task2" target="_blank">
<span style="font-size:20px;">DCASE2018 Task 2 <strong>Baseline</strong> <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="system-description">System description</h2>
<p>The baseline system implements a convolutional neural network (CNN) classifier similar to, but scaled down from, the deep CNN models that have been very successful in the vision domain. The model takes framed examples of log mel spectrogram as input and produces ranked predictions over the 41 classes in the dataset. The baseline system also allows training a simpler fully connected multi-layer perceptron (MLP) classifier. The baseline system is built on <a href="https://www.tensorflow.org/" target="_blank">TensorFlow</a>.</p>
<h3>Input features</h3>
<p>We use frames of log mel spectrogram as input features:</p>
<ul>
<li>computing spectrogram with a window size of 25ms and a hop size of 10ms</li>
<li>mapping the spectrogram to 64 mel bins covering the range 125-7500 Hz</li>
<li>log mel spectrogram is computed by applying log(mel spectrogram + 0.001)</li>
<li>log mel spectrogram is then framed into overlapping examples with a window size of 0.25s and a hop size of 0.125s</li>
</ul>
<h3>Architecture</h3>
<p>The baseline CNN model consists of three 2-D convolutional layers (with ReLU activations) and alternating 2-D max-pool layers, followed by a final max-reduction (to produce a single value per feature map), and a softmax layer. The Adam optimizer is used to train the model, with a learning rate of 1e-4. A batch size of 64 is used.</p>
<p>The layers are listed in the table below using notation Conv2D(kernel size, stride, # feature maps) and MaxPool2D(kernel size, stride). Both Conv2D and MaxPool2D use the <code>SAME</code> padding scheme. ReduceMax applies a maximum-value reduction across the first two dimensions. Activation shapes do not include the batch dimension.</p>
<div class="table-responsive col-md-6">
<table class="table table-striped">
<thead>
<tr>
<th>Layer</th>
<th>Activation shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td>(25, 64, 1)</td>
</tr>
<tr>
<td>Conv2D(7x7, 1, 100)</td>
<td>(25, 64, 100)</td>
</tr>
<tr>
<td>MaxPool2D(3x3, 2x2)</td>
<td>(13, 32, 100)</td>
</tr>
<tr>
<td>Conv2D(5x5, 1, 150)</td>
<td>(13, 32, 150)</td>
</tr>
<tr>
<td>MaxPool2D(3x3, 2x2)</td>
<td>(7, 16, 150)</td>
</tr>
<tr>
<td>Conv2D(3x3, 1, 200)</td>
<td>(7, 16, 200)</td>
</tr>
<tr>
<td>ReduceMax</td>
<td>(1, 1, 200)</td>
</tr>
<tr>
<td>Softmax</td>
<td>(41,) </td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<h3>Clip prediction</h3>
<p>The classifier predicts 41 scores for individual 0.25s-wide examples. In order to produce a ranked list of predicted classes for an entire clip, we average the predictions from all framed examples generated from the clip, and take the top 3 classes by score.</p>
<h2 id="system-performance">System performance</h2>
<p>The baseline system trains to achieve an <strong>MAP@3 of ~0.7</strong> on the <a href="https://www.kaggle.com/c/freesound-audio-tagging/leaderboard" target="_blank">public Kaggle leaderboard</a> after ~5 epochs of the entire training set which are completed in ~12 hours on an
<a href="https://cloud.google.com/compute/docs/machine-types#standard_machine_types" target="_blank"><code>n1-standard-8</code></a> Google Compute Engine machine with a quad-core Intel Xeon E5 v3 (Haswell) @ 2.3 GHz.</p>
<!-- <p class="bg-danger">
The baseline system will be released soon.
</p> -->
<h1 id="citation">Citation</h1>
<p>If you are using the <strong>FSDKaggle2018 dataset</strong> or <strong>baseline</strong> code, or want to refer <strong>challenge task</strong> please cite the following paper:</p>
<div class="btex-item" data-item="Fonseca2018_DCASE" data-source="content/data/challenge2018/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Fonseca2018_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis, Xavier Favory, Jordi Pons, and Xavier Serra.
<em>General-purpose tagging of freesound audio with audioset labels: task description, dataset, and baseline.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018), 69–73. November 2018.
URL: <a href="https://arxiv.org/abs/1807.09902">https://arxiv.org/abs/1807.09902</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/abs/1807.09902" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" class="panel-collapse collapse" id="collapseFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" role="tabpanel">
<h4>General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper describes Task 2 of the DCASE 2018 Challenge, titled "General-purpose audio tagging of Freesound content with AudioSet labels". This task was hosted on the Kaggle platform as "Freesound General-Purpose Audio Tagging Challenge". The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 heterogeneous categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system.</p>
<h5>Keywords</h5>
<p class="text-justify">Audio tagging, audio dataset, data collection</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/abs/1807.09902" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17label" class="modal fade" id="bibtexFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexFonseca2018_DCASE9e848b4864564496bb9c4f09e0503b17label">General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Fonseca2018_DCASE,
    Author = "Fonseca, Eduardo and Plakal, Manoj and Font, Frederic and Ellis, Daniel P. W. and Favory, Xavier and Pons, Jordi and Serra, Xavier",
    title = "General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    month = "November",
    pages = "69--73",
    year = "2018",
    keywords = "Audio tagging, audio dataset, data collection",
    abstract = {This paper describes Task 2 of the DCASE 2018 Challenge, titled "General-purpose audio tagging of Freesound content with AudioSet labels". This task was hosted on the Kaggle platform as "Freesound General-Purpose Audio Tagging Challenge". The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 heterogeneous categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system.},
    url = "https://arxiv.org/abs/1807.09902"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/>
<br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>