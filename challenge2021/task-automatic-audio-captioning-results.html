<!DOCTYPE html><html lang="en">
<head>
    <title>Automated Audio Captioning - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2021/task-automatic-audio-captioning-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description Automated audio captioning is the task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. Given the novelty of …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2021</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2021/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2021/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2021/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2021/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2021/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/stones-03.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-task1"></i><i class="fa dc-captioning fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Caption</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 6</span></span><img src="../images/logos/dcase/dcase2021_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Automated Audio Captioning</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#systems-ranking-all-metrics">Systems ranking, all metrics</a></li>
<li><a href="#systems-ranking-machine-translation-metrics">Systems ranking, machine translation metrics</a></li>
<li><a href="#systems-ranking-captioning-metrics">Systems ranking, captioning metrics</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#overview-of-characteristics">Overview of characteristics</a></li>
<li><a href="#detailed-characteristics">Detailed characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>Automated audio captioning is the task of general audio content
description using free text. It is an intermodal translation task
(not speech-to-text), where a system accepts as an input an audio
signal and outputs the textual description (i.e. the caption) of
that signal. Given the novelty of the task of audio captioning,
current focus is on exploring and developing different methods
that can provide some kind of captions for a general audio recording.
To this aim, the novel Clotho dataset is used, which provides
good quality captions, without speech transcription, named entities,
and hapax legomena (i.e. words that appear once in a split). </p>
<p>Participants used the freely available splits of Clotho development
and evaluation, which splits provide both audio and corresponding
captions. The systems are developed without the usage of any external
data. The developed systems are evaluated on their generated captions,
using the testing split of Clotho, which does not provide the corresponding
captions for the audio. More information about Task 6: Automated
Audio Captioning can be found at the
<a class="btn btn-primary" href="/challenge2021/task-automatic-audio-captioning" style="">task description page.</a></p>
<p>The ranking of the submitted systems is based on the achieved SPIDEr
metric. Though, in this page is provided a more thorough presentation,
grouping the metrics into those that are originated from machine translation
and to those that originated from captioning. </p>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Here are listed the best systems all from all teams. The ranking is based on the
SPIDEr metric. For more elaborated exploration of the performance of the
different systems, at the same table are listed the values achieved for
all the metrics employed in the task. The values for the metrics are for
the Clotho testing split and the Clotho evaluation split. The values for the
Clotho evaluation split, are provided in order to allow further comparison
with systems and methods developed outside of this task, since Clotho
evaluation split is freely available. </p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="tes_spider" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="4">Submission Information</th>
<th class="sep-left-cell text-center" colspan="9">Clotho testing split</th>
<th class="sep-left-cell text-center" colspan="9">Clotho evaluation split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
              Best official <br/>system rank
            </th>
<th data-field="corresponding_author" data-sortable="false">
              Corresponding author
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_bleu_1" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>1</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_2" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>2</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_3" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>3</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_4" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>4</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="tes_rouge_l" data-reversed="true" data-sortable="true" data-value-type="float3">
              ROUGE<sub>L</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eva_bleu_1" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>1</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_2" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>2</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_3" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>3</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_4" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>4</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="eva_rouge_l" data-reversed="true" data-sortable="true" data-value-type="float3">
              ROUGE<sub>L</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Yuan_t6_2</td>
<td>1</td>
<td>Weiqiang Yuan</td>
<td>yuan2021_t6</td>
<td>0.595</td>
<td>0.400</td>
<td>0.275</td>
<td>0.184</td>
<td>0.182</td>
<td>0.394</td>
<td>0.485</td>
<td>0.135</td>
<td>0.310</td>
<td>0.603</td>
<td>0.414</td>
<td>0.286</td>
<td>0.195</td>
<td>0.186</td>
<td>0.400</td>
<td>0.499</td>
<td>0.137</td>
<td>0.318</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_3</td>
<td>2</td>
<td>Xuenan Xu</td>
<td>xu2021_t6</td>
<td>0.650</td>
<td>0.420</td>
<td>0.271</td>
<td>0.171</td>
<td>0.182</td>
<td>0.405</td>
<td>0.463</td>
<td>0.129</td>
<td>0.296</td>
<td>0.659</td>
<td>0.424</td>
<td>0.275</td>
<td>0.176</td>
<td>0.182</td>
<td>0.411</td>
<td>0.472</td>
<td>0.124</td>
<td>0.298</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_1</td>
<td>3</td>
<td>Xinhao Mei</td>
<td>xinhao2021_t6</td>
<td>0.620</td>
<td>0.416</td>
<td>0.282</td>
<td>0.180</td>
<td>0.184</td>
<td>0.401</td>
<td>0.457</td>
<td>0.131</td>
<td>0.294</td>
<td>0.615</td>
<td>0.403</td>
<td>0.270</td>
<td>0.171</td>
<td>0.179</td>
<td>0.392</td>
<td>0.412</td>
<td>0.122</td>
<td>0.268</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_3</td>
<td>4</td>
<td>Zhongjie Ye</td>
<td>ye2021_t6</td>
<td>0.584</td>
<td>0.391</td>
<td>0.265</td>
<td>0.173</td>
<td>0.179</td>
<td>0.384</td>
<td>0.434</td>
<td>0.126</td>
<td>0.280</td>
<td>0.586</td>
<td>0.391</td>
<td>0.268</td>
<td>0.180</td>
<td>0.180</td>
<td>0.388</td>
<td>0.440</td>
<td>0.125</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_4</td>
<td>5</td>
<td>Zhiwen Chen</td>
<td>chen2021_t6</td>
<td>0.549</td>
<td>0.358</td>
<td>0.239</td>
<td>0.156</td>
<td>0.169</td>
<td>0.367</td>
<td>0.402</td>
<td>0.121</td>
<td>0.262</td>
<td>0.563</td>
<td>0.367</td>
<td>0.244</td>
<td>0.158</td>
<td>0.170</td>
<td>0.371</td>
<td>0.406</td>
<td>0.119</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Won_t6_4</td>
<td>6</td>
<td>Hyejin Won</td>
<td>won2021_t6</td>
<td>0.538</td>
<td>0.359</td>
<td>0.247</td>
<td>0.162</td>
<td>0.166</td>
<td>0.372</td>
<td>0.381</td>
<td>0.118</td>
<td>0.249</td>
<td>0.564</td>
<td>0.376</td>
<td>0.254</td>
<td>0.163</td>
<td>0.177</td>
<td>0.388</td>
<td>0.441</td>
<td>0.128</td>
<td>0.285</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_4</td>
<td>7</td>
<td>Chaitanya Narisetty</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.348</td>
<td>0.238</td>
<td>0.160</td>
<td>0.157</td>
<td>0.361</td>
<td>0.362</td>
<td>0.110</td>
<td>0.236</td>
<td>0.563</td>
<td>0.378</td>
<td>0.264</td>
<td>0.184</td>
<td>0.168</td>
<td>0.378</td>
<td>0.417</td>
<td>0.115</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_4</td>
<td>8</td>
<td>Etienne Labbe</td>
<td>labbe2021_t6</td>
<td>0.539</td>
<td>0.354</td>
<td>0.239</td>
<td>0.154</td>
<td>0.156</td>
<td>0.361</td>
<td>0.333</td>
<td>0.108</td>
<td>0.221</td>
<td>0.541</td>
<td>0.358</td>
<td>0.243</td>
<td>0.159</td>
<td>0.327</td>
<td>0.235</td>
<td>0.351</td>
<td>0.110</td>
<td>0.231</td>
</tr>
<tr>
<td></td>
<td>Liu_t6_1</td>
<td>9</td>
<td>Yang Liu</td>
<td>liu2021_t6</td>
<td>0.478</td>
<td>0.291</td>
<td>0.189</td>
<td>0.118</td>
<td>0.143</td>
<td>0.324</td>
<td>0.274</td>
<td>0.094</td>
<td>0.184</td>
<td>0.483</td>
<td>0.298</td>
<td>0.197</td>
<td>0.119</td>
<td>0.322</td>
<td>0.133</td>
<td>0.243</td>
<td>0.088</td>
<td>0.166</td>
</tr>
<tr>
<td></td>
<td>Eren_t6_1</td>
<td>10</td>
<td>Ayşegül Özkaya Eren</td>
<td>eren2021_t6</td>
<td>0.479</td>
<td>0.280</td>
<td>0.168</td>
<td>0.090</td>
<td>0.140</td>
<td>0.302</td>
<td>0.256</td>
<td>0.107</td>
<td>0.182</td>
<td>0.586</td>
<td>0.356</td>
<td>0.268</td>
<td>0.150</td>
<td>0.214</td>
<td>0.444</td>
<td>0.328</td>
<td>0.155</td>
<td>0.242</td>
</tr>
<tr>
<td></td>
<td>Gebhard_t6_1</td>
<td>11</td>
<td>Alexander Gebhard</td>
<td>gebhard2021_t6</td>
<td>0.447</td>
<td>0.169</td>
<td>0.072</td>
<td>0.029</td>
<td>0.099</td>
<td>0.287</td>
<td>0.105</td>
<td>0.047</td>
<td>0.076</td>
<td>0.449</td>
<td>0.167</td>
<td>0.068</td>
<td>0.029</td>
<td>0.097</td>
<td>0.284</td>
<td>0.098</td>
<td>0.043</td>
<td>0.071</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_2</td>
<td>12</td>
<td>Feiyang Xiao</td>
<td>xiao2021_t6</td>
<td>0.344</td>
<td>0.152</td>
<td>0.085</td>
<td>0.044</td>
<td>0.082</td>
<td>0.239</td>
<td>0.058</td>
<td>0.033</td>
<td>0.046</td>
<td>0.461</td>
<td>0.275</td>
<td>0.180</td>
<td>0.112</td>
<td>0.126</td>
<td>0.312</td>
<td>0.210</td>
<td>0.079</td>
<td>0.144</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_t6_1</td>
<td>13</td>
<td>Konstantinos Drossos</td>
<td>Baseline2021_t6</td>
<td>0.405</td>
<td>0.061</td>
<td>0.014</td>
<td>0.000</td>
<td>0.070</td>
<td>0.265</td>
<td>0.020</td>
<td>0.004</td>
<td>0.012</td>
<td>0.378</td>
<td>0.119</td>
<td>0.050</td>
<td>0.017</td>
<td>0.078</td>
<td>0.263</td>
<td>0.075</td>
<td>0.028</td>
<td>0.051</td>
</tr>
</tbody>
</table>
<h1 id="systems-ranking">Systems ranking</h1>
<p>Here are listed all systems and their ranking according to the different
metrics and grouping of metrics. First, is a table with all metrics and 
all systems. Then, is a table with all systems but with only machine
translation metrics, and then a table with all systems but with only
captioning metrics. </p>
<p>Detailed information of each system is at the next section.</p>
<h2 id="systems-ranking-all-metrics">Systems ranking, all metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="tes_spider" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="3">Submission Information</th>
<th class="sep-left-cell text-center" colspan="9">Clotho testing split</th>
<th class="sep-left-cell text-center" colspan="9">Clotho evaluation split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
              Best official <br/>system rank
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_bleu_1" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>1</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_2" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>2</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_3" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>3</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_4" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>4</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="tes_rouge_l" data-reversed="true" data-sortable="true" data-value-type="float3">
              ROUGE<sub>L</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eva_bleu_1" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>1</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_2" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>2</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_3" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>3</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_4" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>4</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="eva_rouge_l" data-reversed="true" data-sortable="true" data-value-type="float3">
              ROUGE<sub>L</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Yuan_t6_1</td>
<td>4</td>
<td>yuan2021_t6</td>
<td>0.586</td>
<td>0.387</td>
<td>0.261</td>
<td>0.170</td>
<td>0.181</td>
<td>0.384</td>
<td>0.457</td>
<td>0.136</td>
<td>0.296</td>
<td>0.595</td>
<td>0.402</td>
<td>0.278</td>
<td>0.189</td>
<td>0.184</td>
<td>0.392</td>
<td>0.495</td>
<td>0.136</td>
<td>0.315</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_2</td>
<td>1</td>
<td>yuan2021_t6</td>
<td>0.595</td>
<td>0.400</td>
<td>0.275</td>
<td>0.184</td>
<td>0.182</td>
<td>0.394</td>
<td>0.485</td>
<td>0.135</td>
<td>0.310</td>
<td>0.603</td>
<td>0.414</td>
<td>0.286</td>
<td>0.195</td>
<td>0.186</td>
<td>0.400</td>
<td>0.499</td>
<td>0.137</td>
<td>0.318</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_3</td>
<td>2</td>
<td>yuan2021_t6</td>
<td>0.590</td>
<td>0.396</td>
<td>0.271</td>
<td>0.176</td>
<td>0.181</td>
<td>0.388</td>
<td>0.471</td>
<td>0.133</td>
<td>0.302</td>
<td>0.635</td>
<td>0.444</td>
<td>0.310</td>
<td>0.211</td>
<td>0.197</td>
<td>0.420</td>
<td>0.569</td>
<td>0.151</td>
<td>0.360</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_4</td>
<td>3</td>
<td>yuan2021_t6</td>
<td>0.584</td>
<td>0.392</td>
<td>0.266</td>
<td>0.175</td>
<td>0.181</td>
<td>0.389</td>
<td>0.465</td>
<td>0.131</td>
<td>0.298</td>
<td>0.665</td>
<td>0.487</td>
<td>0.359</td>
<td>0.260</td>
<td>0.214</td>
<td>0.449</td>
<td>0.684</td>
<td>0.163</td>
<td>0.423</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_1</td>
<td>37</td>
<td>xiao2021_t6</td>
<td>0.351</td>
<td>0.150</td>
<td>0.079</td>
<td>0.041</td>
<td>0.082</td>
<td>0.238</td>
<td>0.057</td>
<td>0.029</td>
<td>0.043</td>
<td>0.471</td>
<td>0.282</td>
<td>0.182</td>
<td>0.112</td>
<td>0.128</td>
<td>0.317</td>
<td>0.208</td>
<td>0.078</td>
<td>0.143</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_2</td>
<td>36</td>
<td>xiao2021_t6</td>
<td>0.344</td>
<td>0.152</td>
<td>0.085</td>
<td>0.044</td>
<td>0.082</td>
<td>0.239</td>
<td>0.058</td>
<td>0.033</td>
<td>0.046</td>
<td>0.461</td>
<td>0.275</td>
<td>0.180</td>
<td>0.112</td>
<td>0.126</td>
<td>0.312</td>
<td>0.210</td>
<td>0.079</td>
<td>0.144</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_1</td>
<td>18</td>
<td>chen2021_t6</td>
<td>0.549</td>
<td>0.356</td>
<td>0.235</td>
<td>0.149</td>
<td>0.169</td>
<td>0.360</td>
<td>0.389</td>
<td>0.117</td>
<td>0.253</td>
<td>0.555</td>
<td>0.357</td>
<td>0.236</td>
<td>0.152</td>
<td>0.168</td>
<td>0.366</td>
<td>0.409</td>
<td>0.120</td>
<td>0.265</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_2</td>
<td>28</td>
<td>chen2021_t6</td>
<td>0.535</td>
<td>0.345</td>
<td>0.227</td>
<td>0.142</td>
<td>0.161</td>
<td>0.359</td>
<td>0.349</td>
<td>0.113</td>
<td>0.231</td>
<td>0.553</td>
<td>0.364</td>
<td>0.247</td>
<td>0.161</td>
<td>0.167</td>
<td>0.371</td>
<td>0.408</td>
<td>0.118</td>
<td>0.263</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_3</td>
<td>20</td>
<td>chen2021_t6</td>
<td>0.537</td>
<td>0.351</td>
<td>0.234</td>
<td>0.151</td>
<td>0.167</td>
<td>0.362</td>
<td>0.373</td>
<td>0.117</td>
<td>0.245</td>
<td>0.561</td>
<td>0.369</td>
<td>0.249</td>
<td>0.167</td>
<td>0.169</td>
<td>0.373</td>
<td>0.406</td>
<td>0.118</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_4</td>
<td>17</td>
<td>chen2021_t6</td>
<td>0.549</td>
<td>0.358</td>
<td>0.239</td>
<td>0.156</td>
<td>0.169</td>
<td>0.367</td>
<td>0.402</td>
<td>0.121</td>
<td>0.262</td>
<td>0.563</td>
<td>0.367</td>
<td>0.244</td>
<td>0.158</td>
<td>0.170</td>
<td>0.371</td>
<td>0.406</td>
<td>0.119</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_1</td>
<td>12</td>
<td>ye2021_t6</td>
<td>0.582</td>
<td>0.385</td>
<td>0.259</td>
<td>0.169</td>
<td>0.180</td>
<td>0.382</td>
<td>0.432</td>
<td>0.126</td>
<td>0.279</td>
<td>0.578</td>
<td>0.381</td>
<td>0.257</td>
<td>0.169</td>
<td>0.181</td>
<td>0.381</td>
<td>0.433</td>
<td>0.125</td>
<td>0.279</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_2</td>
<td>14</td>
<td>ye2021_t6</td>
<td>0.577</td>
<td>0.379</td>
<td>0.254</td>
<td>0.164</td>
<td>0.182</td>
<td>0.385</td>
<td>0.420</td>
<td>0.128</td>
<td>0.274</td>
<td>0.579</td>
<td>0.384</td>
<td>0.261</td>
<td>0.172</td>
<td>0.181</td>
<td>0.386</td>
<td>0.436</td>
<td>0.128</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_3</td>
<td>11</td>
<td>ye2021_t6</td>
<td>0.584</td>
<td>0.391</td>
<td>0.265</td>
<td>0.173</td>
<td>0.179</td>
<td>0.384</td>
<td>0.434</td>
<td>0.126</td>
<td>0.280</td>
<td>0.586</td>
<td>0.391</td>
<td>0.268</td>
<td>0.180</td>
<td>0.180</td>
<td>0.388</td>
<td>0.440</td>
<td>0.125</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_4</td>
<td>13</td>
<td>ye2021_t6</td>
<td>0.586</td>
<td>0.389</td>
<td>0.261</td>
<td>0.170</td>
<td>0.181</td>
<td>0.387</td>
<td>0.429</td>
<td>0.125</td>
<td>0.277</td>
<td>0.590</td>
<td>0.395</td>
<td>0.272</td>
<td>0.183</td>
<td>0.182</td>
<td>0.394</td>
<td>0.453</td>
<td>0.129</td>
<td>0.291</td>
</tr>
<tr>
<td></td>
<td>Liu_t6_1</td>
<td>31</td>
<td>liu2021_t6</td>
<td>0.478</td>
<td>0.291</td>
<td>0.189</td>
<td>0.118</td>
<td>0.143</td>
<td>0.324</td>
<td>0.274</td>
<td>0.094</td>
<td>0.184</td>
<td>0.483</td>
<td>0.298</td>
<td>0.197</td>
<td>0.119</td>
<td>0.322</td>
<td>0.133</td>
<td>0.243</td>
<td>0.088</td>
<td>0.166</td>
</tr>
<tr>
<td></td>
<td>Gebhard_t6_1</td>
<td>35</td>
<td>gebhard2021_t6</td>
<td>0.447</td>
<td>0.169</td>
<td>0.072</td>
<td>0.029</td>
<td>0.099</td>
<td>0.287</td>
<td>0.105</td>
<td>0.047</td>
<td>0.076</td>
<td>0.449</td>
<td>0.167</td>
<td>0.068</td>
<td>0.029</td>
<td>0.097</td>
<td>0.284</td>
<td>0.098</td>
<td>0.043</td>
<td>0.071</td>
</tr>
<tr>
<td></td>
<td>Eren_t6_1</td>
<td>32</td>
<td>eren2021_t6</td>
<td>0.479</td>
<td>0.280</td>
<td>0.168</td>
<td>0.090</td>
<td>0.140</td>
<td>0.302</td>
<td>0.256</td>
<td>0.107</td>
<td>0.182</td>
<td>0.586</td>
<td>0.356</td>
<td>0.268</td>
<td>0.150</td>
<td>0.214</td>
<td>0.444</td>
<td>0.328</td>
<td>0.155</td>
<td>0.242</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_1</td>
<td>7</td>
<td>xinhao2021_t6</td>
<td>0.620</td>
<td>0.416</td>
<td>0.282</td>
<td>0.180</td>
<td>0.184</td>
<td>0.401</td>
<td>0.457</td>
<td>0.131</td>
<td>0.294</td>
<td>0.615</td>
<td>0.403</td>
<td>0.270</td>
<td>0.171</td>
<td>0.179</td>
<td>0.392</td>
<td>0.412</td>
<td>0.122</td>
<td>0.268</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_2</td>
<td>9</td>
<td>xinhao2021_t6</td>
<td>0.653</td>
<td>0.423</td>
<td>0.282</td>
<td>0.176</td>
<td>0.180</td>
<td>0.408</td>
<td>0.439</td>
<td>0.136</td>
<td>0.287</td>
<td>0.635</td>
<td>0.406</td>
<td>0.268</td>
<td>0.166</td>
<td>0.176</td>
<td>0.400</td>
<td>0.412</td>
<td>0.121</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_3</td>
<td>8</td>
<td>xinhao2021_t6</td>
<td>0.644</td>
<td>0.420</td>
<td>0.278</td>
<td>0.170</td>
<td>0.181</td>
<td>0.406</td>
<td>0.447</td>
<td>0.136</td>
<td>0.291</td>
<td>0.621</td>
<td>0.407</td>
<td>0.273</td>
<td>0.177</td>
<td>0.179</td>
<td>0.395</td>
<td>0.431</td>
<td>0.122</td>
<td>0.277</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_4</td>
<td>10</td>
<td>xinhao2021_t6</td>
<td>0.627</td>
<td>0.407</td>
<td>0.269</td>
<td>0.166</td>
<td>0.182</td>
<td>0.399</td>
<td>0.436</td>
<td>0.129</td>
<td>0.283</td>
<td>0.625</td>
<td>0.412</td>
<td>0.278</td>
<td>0.178</td>
<td>0.176</td>
<td>0.401</td>
<td>0.428</td>
<td>0.126</td>
<td>0.277</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_1</td>
<td>26</td>
<td>narisetty2021_t6</td>
<td>0.531</td>
<td>0.346</td>
<td>0.235</td>
<td>0.157</td>
<td>0.160</td>
<td>0.361</td>
<td>0.362</td>
<td>0.108</td>
<td>0.235</td>
<td>0.546</td>
<td>0.356</td>
<td>0.243</td>
<td>0.165</td>
<td>0.163</td>
<td>0.369</td>
<td>0.381</td>
<td>0.110</td>
<td>0.246</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_2</td>
<td>27</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.347</td>
<td>0.235</td>
<td>0.157</td>
<td>0.158</td>
<td>0.359</td>
<td>0.358</td>
<td>0.109</td>
<td>0.234</td>
<td>0.558</td>
<td>0.373</td>
<td>0.261</td>
<td>0.181</td>
<td>0.167</td>
<td>0.376</td>
<td>0.410</td>
<td>0.114</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_3</td>
<td>25</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.347</td>
<td>0.235</td>
<td>0.157</td>
<td>0.159</td>
<td>0.362</td>
<td>0.360</td>
<td>0.110</td>
<td>0.235</td>
<td>0.562</td>
<td>0.377</td>
<td>0.261</td>
<td>0.182</td>
<td>0.169</td>
<td>0.377</td>
<td>0.416</td>
<td>0.116</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_4</td>
<td>23</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.348</td>
<td>0.238</td>
<td>0.160</td>
<td>0.157</td>
<td>0.361</td>
<td>0.362</td>
<td>0.110</td>
<td>0.236</td>
<td>0.563</td>
<td>0.378</td>
<td>0.264</td>
<td>0.184</td>
<td>0.168</td>
<td>0.378</td>
<td>0.417</td>
<td>0.115</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_1</td>
<td>34</td>
<td>labbe2021_t6</td>
<td>0.435</td>
<td>0.222</td>
<td>0.128</td>
<td>0.073</td>
<td>0.121</td>
<td>0.305</td>
<td>0.146</td>
<td>0.072</td>
<td>0.109</td>
<td>0.435</td>
<td>0.229</td>
<td>0.129</td>
<td>0.069</td>
<td>0.252</td>
<td>0.195</td>
<td>0.136</td>
<td>0.067</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_2</td>
<td>33</td>
<td>labbe2021_t6</td>
<td>0.454</td>
<td>0.270</td>
<td>0.176</td>
<td>0.109</td>
<td>0.122</td>
<td>0.310</td>
<td>0.178</td>
<td>0.078</td>
<td>0.128</td>
<td>0.452</td>
<td>0.262</td>
<td>0.168</td>
<td>0.102</td>
<td>0.249</td>
<td>0.193</td>
<td>0.172</td>
<td>0.071</td>
<td>0.122</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_3</td>
<td>30</td>
<td>labbe2021_t6</td>
<td>0.525</td>
<td>0.321</td>
<td>0.200</td>
<td>0.117</td>
<td>0.157</td>
<td>0.354</td>
<td>0.296</td>
<td>0.115</td>
<td>0.205</td>
<td>0.523</td>
<td>0.316</td>
<td>0.191</td>
<td>0.109</td>
<td>0.309</td>
<td>0.231</td>
<td>0.287</td>
<td>0.104</td>
<td>0.195</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_4</td>
<td>29</td>
<td>labbe2021_t6</td>
<td>0.539</td>
<td>0.354</td>
<td>0.239</td>
<td>0.154</td>
<td>0.156</td>
<td>0.361</td>
<td>0.333</td>
<td>0.108</td>
<td>0.221</td>
<td>0.541</td>
<td>0.358</td>
<td>0.243</td>
<td>0.159</td>
<td>0.327</td>
<td>0.235</td>
<td>0.351</td>
<td>0.110</td>
<td>0.231</td>
</tr>
<tr>
<td></td>
<td>Won_t6_1</td>
<td>21</td>
<td>won2021_t6</td>
<td>0.535</td>
<td>0.344</td>
<td>0.231</td>
<td>0.151</td>
<td>0.162</td>
<td>0.359</td>
<td>0.375</td>
<td>0.111</td>
<td>0.243</td>
<td>0.540</td>
<td>0.345</td>
<td>0.230</td>
<td>0.152</td>
<td>0.161</td>
<td>0.361</td>
<td>0.383</td>
<td>0.109</td>
<td>0.246</td>
</tr>
<tr>
<td></td>
<td>Won_t6_2</td>
<td>24</td>
<td>won2021_t6</td>
<td>0.516</td>
<td>0.338</td>
<td>0.226</td>
<td>0.145</td>
<td>0.161</td>
<td>0.359</td>
<td>0.357</td>
<td>0.114</td>
<td>0.236</td>
<td>0.550</td>
<td>0.361</td>
<td>0.244</td>
<td>0.160</td>
<td>0.172</td>
<td>0.375</td>
<td>0.401</td>
<td>0.121</td>
<td>0.261</td>
</tr>
<tr>
<td></td>
<td>Won_t6_3</td>
<td>22</td>
<td>won2021_t6</td>
<td>0.518</td>
<td>0.346</td>
<td>0.235</td>
<td>0.151</td>
<td>0.163</td>
<td>0.366</td>
<td>0.366</td>
<td>0.117</td>
<td>0.242</td>
<td>0.554</td>
<td>0.370</td>
<td>0.254</td>
<td>0.168</td>
<td>0.170</td>
<td>0.379</td>
<td>0.400</td>
<td>0.119</td>
<td>0.259</td>
</tr>
<tr>
<td></td>
<td>Won_t6_4</td>
<td>19</td>
<td>won2021_t6</td>
<td>0.538</td>
<td>0.359</td>
<td>0.247</td>
<td>0.162</td>
<td>0.166</td>
<td>0.372</td>
<td>0.381</td>
<td>0.118</td>
<td>0.249</td>
<td>0.564</td>
<td>0.376</td>
<td>0.254</td>
<td>0.163</td>
<td>0.177</td>
<td>0.388</td>
<td>0.441</td>
<td>0.128</td>
<td>0.285</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_1</td>
<td>15</td>
<td>xu2021_t6</td>
<td>0.560</td>
<td>0.366</td>
<td>0.245</td>
<td>0.159</td>
<td>0.177</td>
<td>0.376</td>
<td>0.403</td>
<td>0.127</td>
<td>0.265</td>
<td>0.576</td>
<td>0.377</td>
<td>0.252</td>
<td>0.164</td>
<td>0.178</td>
<td>0.382</td>
<td>0.421</td>
<td>0.122</td>
<td>0.271</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_2</td>
<td>16</td>
<td>xu2021_t6</td>
<td>0.556</td>
<td>0.365</td>
<td>0.245</td>
<td>0.161</td>
<td>0.178</td>
<td>0.375</td>
<td>0.404</td>
<td>0.125</td>
<td>0.265</td>
<td>0.572</td>
<td>0.374</td>
<td>0.251</td>
<td>0.165</td>
<td>0.178</td>
<td>0.381</td>
<td>0.418</td>
<td>0.122</td>
<td>0.270</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_3</td>
<td>5</td>
<td>xu2021_t6</td>
<td>0.650</td>
<td>0.420</td>
<td>0.271</td>
<td>0.171</td>
<td>0.182</td>
<td>0.405</td>
<td>0.463</td>
<td>0.129</td>
<td>0.296</td>
<td>0.659</td>
<td>0.424</td>
<td>0.275</td>
<td>0.176</td>
<td>0.182</td>
<td>0.411</td>
<td>0.472</td>
<td>0.124</td>
<td>0.298</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_4</td>
<td>6</td>
<td>xu2021_t6</td>
<td>0.651</td>
<td>0.421</td>
<td>0.271</td>
<td>0.170</td>
<td>0.182</td>
<td>0.403</td>
<td>0.461</td>
<td>0.128</td>
<td>0.295</td>
<td>0.660</td>
<td>0.427</td>
<td>0.276</td>
<td>0.177</td>
<td>0.181</td>
<td>0.411</td>
<td>0.471</td>
<td>0.123</td>
<td>0.297</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_t6_1</td>
<td>38</td>
<td>Baseline2021_t6</td>
<td>0.405</td>
<td>0.061</td>
<td>0.014</td>
<td>0.000</td>
<td>0.070</td>
<td>0.265</td>
<td>0.020</td>
<td>0.004</td>
<td>0.012</td>
<td>0.378</td>
<td>0.119</td>
<td>0.050</td>
<td>0.017</td>
<td>0.078</td>
<td>0.263</td>
<td>0.075</td>
<td>0.028</td>
<td>0.051</td>
</tr>
</tbody>
</table>
<h2 id="systems-ranking-machine-translation-metrics">Systems ranking, machine translation metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="tes_meteor" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="3">Submission Information</th>
<th class="sep-left-cell text-center" colspan="6">Clotho testing split</th>
<th class="sep-left-cell text-center" colspan="6">Clotho evaluation split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
              Best official <br/>system rank
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_bleu_1" data-reversed="true" data-sortable="true" data-value-type="float3">
              BLEU<sub>1</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_2" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>2</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_3" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>3</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_bleu_4" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>4</sub>
</th>
<th class="text-center" data-chartable="true" data-field="tes_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
                METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="tes_rouge_l" data-reversed="true" data-sortable="true" data-value-type="float3">
                ROUGE<sub>L</sub>
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eva_bleu_1" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>1</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_2" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>2</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_3" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>3</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_bleu_4" data-reversed="true" data-sortable="true" data-value-type="float3">
                BLEU<sub>4</sub>
</th>
<th class="text-center" data-chartable="true" data-field="eva_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
                METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="eva_rouge_l" data-reversed="true" data-sortable="true" data-value-type="float3">
                ROUGE<sub>L</sub>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Yuan_t6_1</td>
<td>4</td>
<td>yuan2021_t6</td>
<td>0.586</td>
<td>0.387</td>
<td>0.261</td>
<td>0.170</td>
<td>0.181</td>
<td>0.384</td>
<td>0.595</td>
<td>0.402</td>
<td>0.278</td>
<td>0.189</td>
<td>0.184</td>
<td>0.392</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_2</td>
<td>1</td>
<td>yuan2021_t6</td>
<td>0.595</td>
<td>0.400</td>
<td>0.275</td>
<td>0.184</td>
<td>0.182</td>
<td>0.394</td>
<td>0.603</td>
<td>0.414</td>
<td>0.286</td>
<td>0.195</td>
<td>0.186</td>
<td>0.400</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_3</td>
<td>2</td>
<td>yuan2021_t6</td>
<td>0.590</td>
<td>0.396</td>
<td>0.271</td>
<td>0.176</td>
<td>0.181</td>
<td>0.388</td>
<td>0.635</td>
<td>0.444</td>
<td>0.310</td>
<td>0.211</td>
<td>0.197</td>
<td>0.420</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_4</td>
<td>3</td>
<td>yuan2021_t6</td>
<td>0.584</td>
<td>0.392</td>
<td>0.266</td>
<td>0.175</td>
<td>0.181</td>
<td>0.389</td>
<td>0.665</td>
<td>0.487</td>
<td>0.359</td>
<td>0.260</td>
<td>0.214</td>
<td>0.449</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_1</td>
<td>37</td>
<td>xiao2021_t6</td>
<td>0.351</td>
<td>0.150</td>
<td>0.079</td>
<td>0.041</td>
<td>0.082</td>
<td>0.238</td>
<td>0.471</td>
<td>0.282</td>
<td>0.182</td>
<td>0.112</td>
<td>0.128</td>
<td>0.317</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_2</td>
<td>36</td>
<td>xiao2021_t6</td>
<td>0.344</td>
<td>0.152</td>
<td>0.085</td>
<td>0.044</td>
<td>0.082</td>
<td>0.239</td>
<td>0.461</td>
<td>0.275</td>
<td>0.180</td>
<td>0.112</td>
<td>0.126</td>
<td>0.312</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_1</td>
<td>18</td>
<td>chen2021_t6</td>
<td>0.549</td>
<td>0.356</td>
<td>0.235</td>
<td>0.149</td>
<td>0.169</td>
<td>0.360</td>
<td>0.555</td>
<td>0.357</td>
<td>0.236</td>
<td>0.152</td>
<td>0.168</td>
<td>0.366</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_2</td>
<td>28</td>
<td>chen2021_t6</td>
<td>0.535</td>
<td>0.345</td>
<td>0.227</td>
<td>0.142</td>
<td>0.161</td>
<td>0.359</td>
<td>0.553</td>
<td>0.364</td>
<td>0.247</td>
<td>0.161</td>
<td>0.167</td>
<td>0.371</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_3</td>
<td>20</td>
<td>chen2021_t6</td>
<td>0.537</td>
<td>0.351</td>
<td>0.234</td>
<td>0.151</td>
<td>0.167</td>
<td>0.362</td>
<td>0.561</td>
<td>0.369</td>
<td>0.249</td>
<td>0.167</td>
<td>0.169</td>
<td>0.373</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_4</td>
<td>17</td>
<td>chen2021_t6</td>
<td>0.549</td>
<td>0.358</td>
<td>0.239</td>
<td>0.156</td>
<td>0.169</td>
<td>0.367</td>
<td>0.563</td>
<td>0.367</td>
<td>0.244</td>
<td>0.158</td>
<td>0.170</td>
<td>0.371</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_1</td>
<td>12</td>
<td>ye2021_t6</td>
<td>0.582</td>
<td>0.385</td>
<td>0.259</td>
<td>0.169</td>
<td>0.180</td>
<td>0.382</td>
<td>0.578</td>
<td>0.381</td>
<td>0.257</td>
<td>0.169</td>
<td>0.181</td>
<td>0.381</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_2</td>
<td>14</td>
<td>ye2021_t6</td>
<td>0.577</td>
<td>0.379</td>
<td>0.254</td>
<td>0.164</td>
<td>0.182</td>
<td>0.385</td>
<td>0.579</td>
<td>0.384</td>
<td>0.261</td>
<td>0.172</td>
<td>0.181</td>
<td>0.386</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_3</td>
<td>11</td>
<td>ye2021_t6</td>
<td>0.584</td>
<td>0.391</td>
<td>0.265</td>
<td>0.173</td>
<td>0.179</td>
<td>0.384</td>
<td>0.586</td>
<td>0.391</td>
<td>0.268</td>
<td>0.180</td>
<td>0.180</td>
<td>0.388</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_4</td>
<td>13</td>
<td>ye2021_t6</td>
<td>0.586</td>
<td>0.389</td>
<td>0.261</td>
<td>0.170</td>
<td>0.181</td>
<td>0.387</td>
<td>0.590</td>
<td>0.395</td>
<td>0.272</td>
<td>0.183</td>
<td>0.182</td>
<td>0.394</td>
</tr>
<tr>
<td></td>
<td>Liu_t6_1</td>
<td>31</td>
<td>liu2021_t6</td>
<td>0.478</td>
<td>0.291</td>
<td>0.189</td>
<td>0.118</td>
<td>0.143</td>
<td>0.324</td>
<td>0.483</td>
<td>0.298</td>
<td>0.197</td>
<td>0.119</td>
<td>0.322</td>
<td>0.133</td>
</tr>
<tr>
<td></td>
<td>Gebhard_t6_1</td>
<td>35</td>
<td>gebhard2021_t6</td>
<td>0.447</td>
<td>0.169</td>
<td>0.072</td>
<td>0.029</td>
<td>0.099</td>
<td>0.287</td>
<td>0.449</td>
<td>0.167</td>
<td>0.068</td>
<td>0.029</td>
<td>0.097</td>
<td>0.284</td>
</tr>
<tr>
<td></td>
<td>Eren_t6_1</td>
<td>32</td>
<td>eren2021_t6</td>
<td>0.479</td>
<td>0.280</td>
<td>0.168</td>
<td>0.090</td>
<td>0.140</td>
<td>0.302</td>
<td>0.586</td>
<td>0.356</td>
<td>0.268</td>
<td>0.150</td>
<td>0.214</td>
<td>0.444</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_1</td>
<td>7</td>
<td>xinhao2021_t6</td>
<td>0.620</td>
<td>0.416</td>
<td>0.282</td>
<td>0.180</td>
<td>0.184</td>
<td>0.401</td>
<td>0.615</td>
<td>0.403</td>
<td>0.270</td>
<td>0.171</td>
<td>0.179</td>
<td>0.392</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_2</td>
<td>9</td>
<td>xinhao2021_t6</td>
<td>0.653</td>
<td>0.423</td>
<td>0.282</td>
<td>0.176</td>
<td>0.180</td>
<td>0.408</td>
<td>0.635</td>
<td>0.406</td>
<td>0.268</td>
<td>0.166</td>
<td>0.176</td>
<td>0.400</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_3</td>
<td>8</td>
<td>xinhao2021_t6</td>
<td>0.644</td>
<td>0.420</td>
<td>0.278</td>
<td>0.170</td>
<td>0.181</td>
<td>0.406</td>
<td>0.621</td>
<td>0.407</td>
<td>0.273</td>
<td>0.177</td>
<td>0.179</td>
<td>0.395</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_4</td>
<td>10</td>
<td>xinhao2021_t6</td>
<td>0.627</td>
<td>0.407</td>
<td>0.269</td>
<td>0.166</td>
<td>0.182</td>
<td>0.399</td>
<td>0.625</td>
<td>0.412</td>
<td>0.278</td>
<td>0.178</td>
<td>0.176</td>
<td>0.401</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_1</td>
<td>26</td>
<td>narisetty2021_t6</td>
<td>0.531</td>
<td>0.346</td>
<td>0.235</td>
<td>0.157</td>
<td>0.160</td>
<td>0.361</td>
<td>0.546</td>
<td>0.356</td>
<td>0.243</td>
<td>0.165</td>
<td>0.163</td>
<td>0.369</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_2</td>
<td>27</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.347</td>
<td>0.235</td>
<td>0.157</td>
<td>0.158</td>
<td>0.359</td>
<td>0.558</td>
<td>0.373</td>
<td>0.261</td>
<td>0.181</td>
<td>0.167</td>
<td>0.376</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_3</td>
<td>25</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.347</td>
<td>0.235</td>
<td>0.157</td>
<td>0.159</td>
<td>0.362</td>
<td>0.562</td>
<td>0.377</td>
<td>0.261</td>
<td>0.182</td>
<td>0.169</td>
<td>0.377</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_4</td>
<td>23</td>
<td>narisetty2021_t6</td>
<td>0.534</td>
<td>0.348</td>
<td>0.238</td>
<td>0.160</td>
<td>0.157</td>
<td>0.361</td>
<td>0.563</td>
<td>0.378</td>
<td>0.264</td>
<td>0.184</td>
<td>0.168</td>
<td>0.378</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_1</td>
<td>34</td>
<td>labbe2021_t6</td>
<td>0.435</td>
<td>0.222</td>
<td>0.128</td>
<td>0.073</td>
<td>0.121</td>
<td>0.305</td>
<td>0.435</td>
<td>0.229</td>
<td>0.129</td>
<td>0.069</td>
<td>0.252</td>
<td>0.195</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_2</td>
<td>33</td>
<td>labbe2021_t6</td>
<td>0.454</td>
<td>0.270</td>
<td>0.176</td>
<td>0.109</td>
<td>0.122</td>
<td>0.310</td>
<td>0.452</td>
<td>0.262</td>
<td>0.168</td>
<td>0.102</td>
<td>0.249</td>
<td>0.193</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_3</td>
<td>30</td>
<td>labbe2021_t6</td>
<td>0.525</td>
<td>0.321</td>
<td>0.200</td>
<td>0.117</td>
<td>0.157</td>
<td>0.354</td>
<td>0.523</td>
<td>0.316</td>
<td>0.191</td>
<td>0.109</td>
<td>0.309</td>
<td>0.231</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_4</td>
<td>29</td>
<td>labbe2021_t6</td>
<td>0.539</td>
<td>0.354</td>
<td>0.239</td>
<td>0.154</td>
<td>0.156</td>
<td>0.361</td>
<td>0.541</td>
<td>0.358</td>
<td>0.243</td>
<td>0.159</td>
<td>0.327</td>
<td>0.235</td>
</tr>
<tr>
<td></td>
<td>Won_t6_1</td>
<td>21</td>
<td>won2021_t6</td>
<td>0.535</td>
<td>0.344</td>
<td>0.231</td>
<td>0.151</td>
<td>0.162</td>
<td>0.359</td>
<td>0.540</td>
<td>0.345</td>
<td>0.230</td>
<td>0.152</td>
<td>0.161</td>
<td>0.361</td>
</tr>
<tr>
<td></td>
<td>Won_t6_2</td>
<td>24</td>
<td>won2021_t6</td>
<td>0.516</td>
<td>0.338</td>
<td>0.226</td>
<td>0.145</td>
<td>0.161</td>
<td>0.359</td>
<td>0.550</td>
<td>0.361</td>
<td>0.244</td>
<td>0.160</td>
<td>0.172</td>
<td>0.375</td>
</tr>
<tr>
<td></td>
<td>Won_t6_3</td>
<td>22</td>
<td>won2021_t6</td>
<td>0.518</td>
<td>0.346</td>
<td>0.235</td>
<td>0.151</td>
<td>0.163</td>
<td>0.366</td>
<td>0.554</td>
<td>0.370</td>
<td>0.254</td>
<td>0.168</td>
<td>0.170</td>
<td>0.379</td>
</tr>
<tr>
<td></td>
<td>Won_t6_4</td>
<td>19</td>
<td>won2021_t6</td>
<td>0.538</td>
<td>0.359</td>
<td>0.247</td>
<td>0.162</td>
<td>0.166</td>
<td>0.372</td>
<td>0.564</td>
<td>0.376</td>
<td>0.254</td>
<td>0.163</td>
<td>0.177</td>
<td>0.388</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_1</td>
<td>15</td>
<td>xu2021_t6</td>
<td>0.560</td>
<td>0.366</td>
<td>0.245</td>
<td>0.159</td>
<td>0.177</td>
<td>0.376</td>
<td>0.576</td>
<td>0.377</td>
<td>0.252</td>
<td>0.164</td>
<td>0.178</td>
<td>0.382</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_2</td>
<td>16</td>
<td>xu2021_t6</td>
<td>0.556</td>
<td>0.365</td>
<td>0.245</td>
<td>0.161</td>
<td>0.178</td>
<td>0.375</td>
<td>0.572</td>
<td>0.374</td>
<td>0.251</td>
<td>0.165</td>
<td>0.178</td>
<td>0.381</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_3</td>
<td>5</td>
<td>xu2021_t6</td>
<td>0.650</td>
<td>0.420</td>
<td>0.271</td>
<td>0.171</td>
<td>0.182</td>
<td>0.405</td>
<td>0.659</td>
<td>0.424</td>
<td>0.275</td>
<td>0.176</td>
<td>0.182</td>
<td>0.411</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_4</td>
<td>6</td>
<td>xu2021_t6</td>
<td>0.651</td>
<td>0.421</td>
<td>0.271</td>
<td>0.170</td>
<td>0.182</td>
<td>0.403</td>
<td>0.660</td>
<td>0.427</td>
<td>0.276</td>
<td>0.177</td>
<td>0.181</td>
<td>0.411</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_t6_1</td>
<td>38</td>
<td>Baseline2021_t6</td>
<td>0.405</td>
<td>0.061</td>
<td>0.014</td>
<td>0.000</td>
<td>0.070</td>
<td>0.265</td>
<td>0.378</td>
<td>0.119</td>
<td>0.050</td>
<td>0.017</td>
<td>0.078</td>
<td>0.263</td>
</tr>
</tbody>
</table>
<h2 id="systems-ranking-captioning-metrics">Systems ranking, captioning metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="tes_spider" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="3">Submission Information</th>
<th class="sep-left-cell text-center" colspan="3">Clotho testing split</th>
<th class="sep-left-cell text-center" colspan="3">Clotho evaluation split</th>
</tr>
<tr>
<th data-field="abbreviation" data-sortable="true">
              Submission code
            </th>
<th class="text-center" data-chartable="true" data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
        Best official <br/>system rank
        </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="tes_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="eva_cider" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="eva_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Yuan_t6_1</td>
<td>4</td>
<td>yuan2021_t6</td>
<td>0.457</td>
<td>0.136</td>
<td>0.296</td>
<td>0.495</td>
<td>0.136</td>
<td>0.315</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_2</td>
<td>1</td>
<td>yuan2021_t6</td>
<td>0.485</td>
<td>0.135</td>
<td>0.310</td>
<td>0.499</td>
<td>0.137</td>
<td>0.318</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_3</td>
<td>2</td>
<td>yuan2021_t6</td>
<td>0.471</td>
<td>0.133</td>
<td>0.302</td>
<td>0.569</td>
<td>0.151</td>
<td>0.360</td>
</tr>
<tr>
<td></td>
<td>Yuan_t6_4</td>
<td>3</td>
<td>yuan2021_t6</td>
<td>0.465</td>
<td>0.131</td>
<td>0.298</td>
<td>0.684</td>
<td>0.163</td>
<td>0.423</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_1</td>
<td>37</td>
<td>xiao2021_t6</td>
<td>0.057</td>
<td>0.029</td>
<td>0.043</td>
<td>0.208</td>
<td>0.078</td>
<td>0.143</td>
</tr>
<tr>
<td></td>
<td>Xiao_t6_2</td>
<td>36</td>
<td>xiao2021_t6</td>
<td>0.058</td>
<td>0.033</td>
<td>0.046</td>
<td>0.210</td>
<td>0.079</td>
<td>0.144</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_1</td>
<td>18</td>
<td>chen2021_t6</td>
<td>0.389</td>
<td>0.117</td>
<td>0.253</td>
<td>0.409</td>
<td>0.120</td>
<td>0.265</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_2</td>
<td>28</td>
<td>chen2021_t6</td>
<td>0.349</td>
<td>0.113</td>
<td>0.231</td>
<td>0.408</td>
<td>0.118</td>
<td>0.263</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_3</td>
<td>20</td>
<td>chen2021_t6</td>
<td>0.373</td>
<td>0.117</td>
<td>0.245</td>
<td>0.406</td>
<td>0.118</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Chen_t6_4</td>
<td>17</td>
<td>chen2021_t6</td>
<td>0.402</td>
<td>0.121</td>
<td>0.262</td>
<td>0.406</td>
<td>0.119</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_1</td>
<td>12</td>
<td>ye2021_t6</td>
<td>0.432</td>
<td>0.126</td>
<td>0.279</td>
<td>0.433</td>
<td>0.125</td>
<td>0.279</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_2</td>
<td>14</td>
<td>ye2021_t6</td>
<td>0.420</td>
<td>0.128</td>
<td>0.274</td>
<td>0.436</td>
<td>0.128</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_3</td>
<td>11</td>
<td>ye2021_t6</td>
<td>0.434</td>
<td>0.126</td>
<td>0.280</td>
<td>0.440</td>
<td>0.125</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>Ye_t6_4</td>
<td>13</td>
<td>ye2021_t6</td>
<td>0.429</td>
<td>0.125</td>
<td>0.277</td>
<td>0.453</td>
<td>0.129</td>
<td>0.291</td>
</tr>
<tr>
<td></td>
<td>Liu_t6_1</td>
<td>31</td>
<td>liu2021_t6</td>
<td>0.274</td>
<td>0.094</td>
<td>0.184</td>
<td>0.243</td>
<td>0.088</td>
<td>0.166</td>
</tr>
<tr>
<td></td>
<td>Gebhard_t6_1</td>
<td>35</td>
<td>gebhard2021_t6</td>
<td>0.105</td>
<td>0.047</td>
<td>0.076</td>
<td>0.098</td>
<td>0.043</td>
<td>0.071</td>
</tr>
<tr>
<td></td>
<td>Eren_t6_1</td>
<td>32</td>
<td>eren2021_t6</td>
<td>0.256</td>
<td>0.107</td>
<td>0.182</td>
<td>0.328</td>
<td>0.155</td>
<td>0.242</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_1</td>
<td>7</td>
<td>xinhao2021_t6</td>
<td>0.457</td>
<td>0.131</td>
<td>0.294</td>
<td>0.412</td>
<td>0.122</td>
<td>0.268</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_2</td>
<td>9</td>
<td>xinhao2021_t6</td>
<td>0.439</td>
<td>0.136</td>
<td>0.287</td>
<td>0.412</td>
<td>0.121</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_3</td>
<td>8</td>
<td>xinhao2021_t6</td>
<td>0.447</td>
<td>0.136</td>
<td>0.291</td>
<td>0.431</td>
<td>0.122</td>
<td>0.277</td>
</tr>
<tr>
<td></td>
<td>Xinhao_t6_4</td>
<td>10</td>
<td>xinhao2021_t6</td>
<td>0.436</td>
<td>0.129</td>
<td>0.283</td>
<td>0.428</td>
<td>0.126</td>
<td>0.277</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_1</td>
<td>26</td>
<td>narisetty2021_t6</td>
<td>0.362</td>
<td>0.108</td>
<td>0.235</td>
<td>0.381</td>
<td>0.110</td>
<td>0.246</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_2</td>
<td>27</td>
<td>narisetty2021_t6</td>
<td>0.358</td>
<td>0.109</td>
<td>0.234</td>
<td>0.410</td>
<td>0.114</td>
<td>0.262</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_3</td>
<td>25</td>
<td>narisetty2021_t6</td>
<td>0.360</td>
<td>0.110</td>
<td>0.235</td>
<td>0.416</td>
<td>0.116</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Narisetty_t6_4</td>
<td>23</td>
<td>narisetty2021_t6</td>
<td>0.362</td>
<td>0.110</td>
<td>0.236</td>
<td>0.417</td>
<td>0.115</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_1</td>
<td>34</td>
<td>labbe2021_t6</td>
<td>0.146</td>
<td>0.072</td>
<td>0.109</td>
<td>0.136</td>
<td>0.067</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_2</td>
<td>33</td>
<td>labbe2021_t6</td>
<td>0.178</td>
<td>0.078</td>
<td>0.128</td>
<td>0.172</td>
<td>0.071</td>
<td>0.122</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_3</td>
<td>30</td>
<td>labbe2021_t6</td>
<td>0.296</td>
<td>0.115</td>
<td>0.205</td>
<td>0.287</td>
<td>0.104</td>
<td>0.195</td>
</tr>
<tr>
<td></td>
<td>Labbe_t6_4</td>
<td>29</td>
<td>labbe2021_t6</td>
<td>0.333</td>
<td>0.108</td>
<td>0.221</td>
<td>0.351</td>
<td>0.110</td>
<td>0.231</td>
</tr>
<tr>
<td></td>
<td>Won_t6_1</td>
<td>21</td>
<td>won2021_t6</td>
<td>0.375</td>
<td>0.111</td>
<td>0.243</td>
<td>0.383</td>
<td>0.109</td>
<td>0.246</td>
</tr>
<tr>
<td></td>
<td>Won_t6_2</td>
<td>24</td>
<td>won2021_t6</td>
<td>0.357</td>
<td>0.114</td>
<td>0.236</td>
<td>0.401</td>
<td>0.121</td>
<td>0.261</td>
</tr>
<tr>
<td></td>
<td>Won_t6_3</td>
<td>22</td>
<td>won2021_t6</td>
<td>0.366</td>
<td>0.117</td>
<td>0.242</td>
<td>0.400</td>
<td>0.119</td>
<td>0.259</td>
</tr>
<tr>
<td></td>
<td>Won_t6_4</td>
<td>19</td>
<td>won2021_t6</td>
<td>0.381</td>
<td>0.118</td>
<td>0.249</td>
<td>0.441</td>
<td>0.128</td>
<td>0.285</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_1</td>
<td>15</td>
<td>xu2021_t6</td>
<td>0.403</td>
<td>0.127</td>
<td>0.265</td>
<td>0.421</td>
<td>0.122</td>
<td>0.271</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_2</td>
<td>16</td>
<td>xu2021_t6</td>
<td>0.404</td>
<td>0.125</td>
<td>0.265</td>
<td>0.418</td>
<td>0.122</td>
<td>0.270</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_3</td>
<td>5</td>
<td>xu2021_t6</td>
<td>0.463</td>
<td>0.129</td>
<td>0.296</td>
<td>0.472</td>
<td>0.124</td>
<td>0.298</td>
</tr>
<tr>
<td></td>
<td>Xu_t6_4</td>
<td>6</td>
<td>xu2021_t6</td>
<td>0.461</td>
<td>0.128</td>
<td>0.295</td>
<td>0.471</td>
<td>0.123</td>
<td>0.297</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Baseline_t6_1</td>
<td>38</td>
<td>Baseline2021_t6</td>
<td>0.020</td>
<td>0.004</td>
<td>0.012</td>
<td>0.075</td>
<td>0.028</td>
<td>0.051</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<p>In this section you can find the characteristics of the submitted systems. There are two tables
for easy reference, in the corresponding subsections. The first table has an onverview of the systems
and the second has a detailed presentation of each system. </p>
<h2 id="overview-of-characteristics">Overview of characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="abbreviation" data-filter-control="true" data-filter-show-clear="true" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="total_parameters" data-scatter-y="tes_spider" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_sys_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
            Rank
            </th>
<th class="sm-cell" data-field="abbreviation" data-sortable="true">
              Submission<br/>code
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="false" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Method scheme/architecture
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="total_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Data<br/>augmentation
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>Yuan_t6_1</td>
<td>0.296</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>986302137</td>
<td>PANNs</td>
<td>Transfomer</td>
<td>noise enhance</td>
</tr>
<tr>
<td>1</td>
<td>Yuan_t6_2</td>
<td>0.310</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>986302137</td>
<td>PANNs</td>
<td>Transfomer</td>
<td>noise enhance</td>
</tr>
<tr>
<td>2</td>
<td>Yuan_t6_3</td>
<td>0.302</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>986302137</td>
<td>PANNs</td>
<td>Transfomer</td>
<td>noise enhance</td>
</tr>
<tr>
<td>3</td>
<td>Yuan_t6_4</td>
<td>0.298</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>2572592190</td>
<td>PANNs</td>
<td>Transfomer</td>
<td>noise enhance</td>
</tr>
<tr>
<td>37</td>
<td>Xiao_t6_1</td>
<td>0.043</td>
<td>xiao2021_t6</td>
<td>encoder-decoder, Transformer, MLP-mixer, Residual, audio embedding</td>
<td>2448349</td>
<td>MLP-mixer encoder</td>
<td>Transformer decoder</td>
<td></td>
</tr>
<tr>
<td>36</td>
<td>Xiao_t6_2</td>
<td>0.046</td>
<td>xiao2021_t6</td>
<td>encoder-decoder, Transformer, MLP-mixer, Residual, audio embedding, pre-train encoder</td>
<td>2448349</td>
<td>MLP-mixer encoder</td>
<td>Transformer decoder</td>
<td></td>
</tr>
<tr>
<td>18</td>
<td>Chen_t6_1</td>
<td>0.253</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>93410432</td>
<td>CNN14, MemoryEncoder</td>
<td>MeshedDecoder</td>
<td>SpecAugment, Label Smoothing</td>
</tr>
<tr>
<td>28</td>
<td>Chen_t6_2</td>
<td>0.231</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>93410432</td>
<td>CNN14, MemoryEncoder</td>
<td>MeshedDecoder</td>
<td>SpecAugment, Label Smoothing</td>
</tr>
<tr>
<td>20</td>
<td>Chen_t6_3</td>
<td>0.245</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>22775528</td>
<td>CNN14, MemoryEncoder</td>
<td>MeshedDecoder</td>
<td>SpecAugment, Label Smoothing</td>
</tr>
<tr>
<td>17</td>
<td>Chen_t6_4</td>
<td>0.262</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>86440064</td>
<td>ResNet38, MemoryEncoder</td>
<td>MeshedDecoder</td>
<td>SpecAugment, Label Smoothing</td>
</tr>
<tr>
<td>12</td>
<td>Ye_t6_1</td>
<td>0.279</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>86643711</td>
<td>ResNet38</td>
<td>RNN</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
</tr>
<tr>
<td>14</td>
<td>Ye_t6_2</td>
<td>0.274</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>86643711</td>
<td>ResNet38</td>
<td>RNN</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
</tr>
<tr>
<td>11</td>
<td>Ye_t6_3</td>
<td>0.280</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>779793399</td>
<td>ResNet38</td>
<td>RNN</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
</tr>
<tr>
<td>13</td>
<td>Ye_t6_4</td>
<td>0.277</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>259931133</td>
<td>ResNet38</td>
<td>RNN</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
</tr>
<tr>
<td>31</td>
<td>Liu_t6_1</td>
<td>0.184</td>
<td>liu2021_t6</td>
<td>encoder-decoder</td>
<td>3045913</td>
<td>CNN</td>
<td>self-attention, Word2Vec</td>
<td></td>
</tr>
<tr>
<td>35</td>
<td>Gebhard_t6_1</td>
<td>0.076</td>
<td>gebhard2021_t6</td>
<td>encoder-decoder</td>
<td>13409747</td>
<td>CNN</td>
<td>RNN</td>
<td></td>
</tr>
<tr>
<td>32</td>
<td>Eren_t6_1</td>
<td>0.182</td>
<td>eren2021_t6</td>
<td>encoder-decoder</td>
<td>2511570</td>
<td>PANNs</td>
<td>RNN</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Xinhao_t6_1</td>
<td>0.294</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>7455570</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>9</td>
<td>Xinhao_t6_2</td>
<td>0.287</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>7455570</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>8</td>
<td>Xinhao_t6_3</td>
<td>0.291</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>8038703</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>10</td>
<td>Xinhao_t6_4</td>
<td>0.283</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>8038703</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>26</td>
<td>Narisetty_t6_1</td>
<td>0.235</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>143676488</td>
<td>Conformer</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>27</td>
<td>Narisetty_t6_2</td>
<td>0.234</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>143676488</td>
<td>Conformer</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>25</td>
<td>Narisetty_t6_3</td>
<td>0.235</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>167205128</td>
<td>Conformer</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>23</td>
<td>Narisetty_t6_4</td>
<td>0.236</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>167205128</td>
<td>Conformer</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>34</td>
<td>Labbe_t6_1</td>
<td>0.109</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>2887632</td>
<td>pyramidal bidirectional RNN-LSTM</td>
<td>RNN-LSTM, attention</td>
<td></td>
</tr>
<tr>
<td>33</td>
<td>Labbe_t6_2</td>
<td>0.128</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>2887632</td>
<td>pyramidal bidirectional RNN-LSTM</td>
<td>RNN-LSTM, attention</td>
<td></td>
</tr>
<tr>
<td>30</td>
<td>Labbe_t6_3</td>
<td>0.205</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>84521484</td>
<td>CNN14</td>
<td>RNN-LSTM, attention</td>
<td></td>
</tr>
<tr>
<td>29</td>
<td>Labbe_t6_4</td>
<td>0.221</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>84521484</td>
<td>CNN14</td>
<td>RNN-LSTM, attention</td>
<td></td>
</tr>
<tr>
<td>21</td>
<td>Won_t6_1</td>
<td>0.243</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>ResNet</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>24</td>
<td>Won_t6_2</td>
<td>0.236</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>22</td>
<td>Won_t6_3</td>
<td>0.242</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>19</td>
<td>Won_t6_4</td>
<td>0.249</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>CNN</td>
<td>Transformer</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>15</td>
<td>Xu_t6_1</td>
<td>0.265</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>36181131</td>
<td>CNN</td>
<td>RNN</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>16</td>
<td>Xu_t6_2</td>
<td>0.265</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>60301885</td>
<td>CNN</td>
<td>RNN</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>5</td>
<td>Xu_t6_3</td>
<td>0.296</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>48241508</td>
<td>CNN</td>
<td>RNN</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>6</td>
<td>Xu_t6_4</td>
<td>0.295</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>60301885</td>
<td>CNN</td>
<td>RNN</td>
<td>SpecAugment</td>
</tr>
<tr class="info" data-hline="true">
<td>38</td>
<td>Baseline_t6_1</td>
<td>0.012</td>
<td>Baseline2021_t6</td>
<td>encoder-decoder</td>
<td>5012931</td>
<td>RNN</td>
<td>RNN</td>
<td></td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="detailed-characteristics">Detailed characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="abbreviation" data-filter-control="true" data-filter-show-clear="true" data-id-field="abbreviation" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="total_parameters" data-scatter-y="tes_spider" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="anchor_sys_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th data-field="anchor_sys_rank" data-sortable="true" data-value-type="int">
            Rank
            </th>
<th class="sm-cell" data-field="abbreviation" data-sortable="true">
              Submission<br/>code
            </th>
<th class="text-center" data-chartable="true" data-field="tes_spider" data-reversed="false" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Method scheme/architecture
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="total_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="acoustic_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Acoustic<br/>features
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="word_embeddings" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word<br/>embeddings
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Data<br/>augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="input_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="learning_scheme" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning set-up
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="ensemble" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Ensemble method
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="loss_function" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Loss function
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="optimizer" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning set-up
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="learning_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="gradient_clipping" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Gradient clipping 
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="gradient_norm" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Gradient norm for clipping
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="metric_monitored" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Metric monitored for training
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="dataset_audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used for audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="dataset_word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used for word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="dataset_audio_similarity" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used for audio similarity
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>Yuan_t6_1</td>
<td>0.296</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>986302137</td>
<td>PANNs</td>
<td>log-mel energies</td>
<td>Transfomer</td>
<td>one-hot</td>
<td>noise enhance</td>
<td>36kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho</td>
</tr>
<tr>
<td>1</td>
<td>Yuan_t6_2</td>
<td>0.310</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>986302137</td>
<td>PANNs</td>
<td>log-mel energies</td>
<td>Transfomer</td>
<td>one-hot</td>
<td>noise enhance</td>
<td>36kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho</td>
</tr>
<tr>
<td>2</td>
<td>Yuan_t6_3</td>
<td>0.302</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>986302137</td>
<td>PANNs</td>
<td>log-mel energies</td>
<td>Transfomer</td>
<td>one-hot</td>
<td>noise enhance</td>
<td>36kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho</td>
</tr>
<tr>
<td>3</td>
<td>Yuan_t6_4</td>
<td>0.298</td>
<td>yuan2021_t6</td>
<td>encoder-decoder</td>
<td>2572592190</td>
<td>PANNs</td>
<td>log-mel energies</td>
<td>Transfomer</td>
<td>one-hot</td>
<td>noise enhance</td>
<td>36kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho, AudioCaps, Freesound</td>
<td>Clotho</td>
</tr>
<tr>
<td>37</td>
<td>Xiao_t6_1</td>
<td>0.043</td>
<td>xiao2021_t6</td>
<td>encoder-decoder, Transformer, MLP-mixer, Residual, audio embedding</td>
<td>2448349</td>
<td>MLP-mixer encoder</td>
<td>log-mel energies</td>
<td>Transformer decoder</td>
<td>learned embeddings</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>36</td>
<td>Xiao_t6_2</td>
<td>0.046</td>
<td>xiao2021_t6</td>
<td>encoder-decoder, Transformer, MLP-mixer, Residual, audio embedding, pre-train encoder</td>
<td>2448349</td>
<td>MLP-mixer encoder</td>
<td>log-mel energies</td>
<td>Transformer decoder</td>
<td>learned embeddings</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>18</td>
<td>Chen_t6_1</td>
<td>0.253</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>93410432</td>
<td>CNN14, MemoryEncoder</td>
<td>log-mel energies</td>
<td>MeshedDecoder</td>
<td>Word2Vec</td>
<td>SpecAugment, Label Smoothing</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>3e-5</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>28</td>
<td>Chen_t6_2</td>
<td>0.231</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>93410432</td>
<td>CNN14, MemoryEncoder</td>
<td>log-mel energies</td>
<td>MeshedDecoder</td>
<td>FastText</td>
<td>SpecAugment, Label Smoothing</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>3e-5</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>20</td>
<td>Chen_t6_3</td>
<td>0.245</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>22775528</td>
<td>CNN14, MemoryEncoder</td>
<td>log-mel energies</td>
<td>MeshedDecoder</td>
<td>learned embeddings</td>
<td>SpecAugment, Label Smoothing</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>3e-5</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>17</td>
<td>Chen_t6_4</td>
<td>0.262</td>
<td>chen2021_t6</td>
<td>encoder-decoder</td>
<td>86440064</td>
<td>ResNet38, MemoryEncoder</td>
<td>log-mel energies</td>
<td>MeshedDecoder</td>
<td>Word2Vec</td>
<td>SpecAugment, Label Smoothing</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>3e-5</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>Ye_t6_1</td>
<td>0.279</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>86643711</td>
<td>ResNet38</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>14</td>
<td>Ye_t6_2</td>
<td>0.274</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>86643711</td>
<td>ResNet38</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Ye_t6_3</td>
<td>0.280</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>779793399</td>
<td>ResNet38</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>Ye_t6_4</td>
<td>0.277</td>
<td>ye2021_t6</td>
<td>encoder-decoder</td>
<td>259931133</td>
<td>ResNet38</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>Mixup, SpecAugment, SpecAugment++</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>2e-5</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>31</td>
<td>Liu_t6_1</td>
<td>0.184</td>
<td>liu2021_t6</td>
<td>encoder-decoder</td>
<td>3045913</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>self-attention, Word2Vec</td>
<td>Word2Vec</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy, sentence-loss</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Training loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>35</td>
<td>Gebhard_t6_1</td>
<td>0.076</td>
<td>gebhard2021_t6</td>
<td>encoder-decoder</td>
<td>13409747</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>numeric-representation</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-4</td>
<td></td>
<td></td>
<td>Training loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>32</td>
<td>Eren_t6_1</td>
<td>0.182</td>
<td>eren2021_t6</td>
<td>encoder-decoder</td>
<td>2511570</td>
<td>PANNs</td>
<td>log-mel energies, PANNs</td>
<td>RNN</td>
<td>Word2Vec</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Xinhao_t6_1</td>
<td>0.294</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>7455570</td>
<td>CNN</td>
<td>PANNs</td>
<td>Transformer</td>
<td>random</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>Xinhao_t6_2</td>
<td>0.287</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>7455570</td>
<td>CNN</td>
<td>PANNs</td>
<td>Transformer</td>
<td>random</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>Xinhao_t6_3</td>
<td>0.291</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>8038703</td>
<td>CNN</td>
<td>PANNs</td>
<td>Transformer</td>
<td>Word2Vec</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>Xinhao_t6_4</td>
<td>0.283</td>
<td>xinhao2021_t6</td>
<td>encoder-decoder</td>
<td>8038703</td>
<td>CNN</td>
<td>PANNs</td>
<td>Transformer</td>
<td>Word2Vec</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>26</td>
<td>Narisetty_t6_1</td>
<td>0.235</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>143676488</td>
<td>Conformer</td>
<td>log-mel energies, PANNs, tags, embeddings</td>
<td>Transformer</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>noam</td>
<td>5e-1</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>27</td>
<td>Narisetty_t6_2</td>
<td>0.234</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>143676488</td>
<td>Conformer</td>
<td>log-mel energies, PANNs, tags, embeddings</td>
<td>Transformer</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>noam</td>
<td>5e-1</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>25</td>
<td>Narisetty_t6_3</td>
<td>0.235</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>167205128</td>
<td>Conformer</td>
<td>log-mel energies, PANNs, tags, embeddings</td>
<td>Transformer</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>noam</td>
<td>5e-1</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>23</td>
<td>Narisetty_t6_4</td>
<td>0.236</td>
<td>narisetty2021_t6</td>
<td>Conformer</td>
<td>167205128</td>
<td>Conformer</td>
<td>log-mel energies, PANNs, tags, embeddings</td>
<td>Transformer</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>16kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>noam</td>
<td>5e-1</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho, AudioCaps</td>
<td>Clotho, AudioCaps</td>
<td></td>
</tr>
<tr>
<td>34</td>
<td>Labbe_t6_1</td>
<td>0.109</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>2887632</td>
<td>pyramidal bidirectional RNN-LSTM</td>
<td>log-mel energies</td>
<td>RNN-LSTM, attention</td>
<td>learned embeddings</td>
<td></td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>33</td>
<td>Labbe_t6_2</td>
<td>0.128</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>2887632</td>
<td>pyramidal bidirectional RNN-LSTM</td>
<td>log-mel energies</td>
<td>RNN-LSTM, attention</td>
<td>learned embeddings</td>
<td></td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>30</td>
<td>Labbe_t6_3</td>
<td>0.205</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>84521484</td>
<td>CNN14</td>
<td>log-mel energies</td>
<td>RNN-LSTM, attention</td>
<td>learned embeddings</td>
<td></td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>29</td>
<td>Labbe_t6_4</td>
<td>0.221</td>
<td>labbe2021_t6</td>
<td>encoder-decoder</td>
<td>84521484</td>
<td>CNN14</td>
<td>log-mel energies</td>
<td>RNN-LSTM, attention</td>
<td>learned embeddings</td>
<td></td>
<td>32kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>21</td>
<td>Won_t6_1</td>
<td>0.243</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>ResNet</td>
<td>log-mel energies</td>
<td>Transformer</td>
<td>Word2Vec</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam, SWA</td>
<td>3e-4, 1e-4</td>
<td></td>
<td></td>
<td>Training SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>24</td>
<td>Won_t6_2</td>
<td>0.236</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>Transformer</td>
<td>Word2Vec</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam, SWA</td>
<td>3e-4, 1e-4</td>
<td></td>
<td></td>
<td>Training SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>22</td>
<td>Won_t6_3</td>
<td>0.242</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>Transformer</td>
<td>Word2Vec</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam, SWA</td>
<td>3e-4, 1e-4</td>
<td></td>
<td></td>
<td>Training SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>19</td>
<td>Won_t6_4</td>
<td>0.249</td>
<td>won2021_t6</td>
<td>encoder-decoder</td>
<td>8445139</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>Transformer</td>
<td>Word2Vec</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam, SWA</td>
<td>3e-4, 1e-4</td>
<td></td>
<td></td>
<td>Training SPIDEr score</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>15</td>
<td>Xu_t6_1</td>
<td>0.265</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>36181131</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learning</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioSet</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>16</td>
<td>Xu_t6_2</td>
<td>0.265</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>60301885</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learning</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioSet</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>Xu_t6_3</td>
<td>0.296</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>48241508</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learning</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioSet</td>
<td>Clotho</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>Xu_t6_4</td>
<td>0.295</td>
<td>xu2021_t6</td>
<td>seq2seq</td>
<td>60301885</td>
<td>CNN</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td>SpecAugment</td>
<td>44.1kHz</td>
<td>supervised, reinforcement learning</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>5e-4</td>
<td></td>
<td></td>
<td>Validation SPIDEr score</td>
<td>Clotho, AudioSet</td>
<td>Clotho</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td>38</td>
<td>Baseline_t6_1</td>
<td>0.012</td>
<td>Baseline2021_t6</td>
<td>encoder-decoder</td>
<td>5012931</td>
<td>RNN</td>
<td>log-mel energies</td>
<td>RNN</td>
<td>learned embeddings</td>
<td></td>
<td>44.1kHz</td>
<td>supervised</td>
<td></td>
<td>crossentropy</td>
<td>adam</td>
<td>1e-3</td>
<td></td>
<td></td>
<td>Validation loss</td>
<td>Clotho</td>
<td>Clotho</td>
<td></td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2021/technical_reports_task6.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="chen2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-chen2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Audio Captioning With Meshed-Memory Transformer
       </h4>
<p style="text-align:left">
        Zhiwen Chen, Dawei Zhang, Jun Wang, and Feng Deng
       </p>
<p style="text-align:left">
<em>
         University of Chinese Academy of Sciences, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chen_t6_1</span> <span class="label label-primary">Chen_t6_2</span> <span class="label label-primary">Chen_t6_3</span> <span class="label label-primary">Chen_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-chen2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-chen2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-chen2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Chen_17_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-chen2021_t6" class="panel-collapse collapse" id="collapse-chen2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Audio Captioning With Meshed-Memory Transformer
      </h4>
<p style="text-align:left">
<small>
        Zhiwen Chen, Dawei Zhang, Jun Wang, and Feng Deng
       </small>
<br/>
<small>
<em>
         University of Chinese Academy of Sciences, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Automated audio captioning is the task of describing the audio content of a given audio signal in natural language. Through advancing for years, transformer-based language models are widely used in audio captioning. However, most architectures based on transformer, cannot learn prior knowledge between samples well, leading to worse text decoding. For better acoustic event and language modeling, a sequence-to-sequence model is proposed which consists of a CNN-based encoder, a memory-augmented refiner and a meshed decoder. The proposed architecture refines a multi-level representation of the relationships between audio features integrating learned a priori knowledge. At decoding stage, it exploit low- and high-level features with a mesh-like connectivity. Experiments show that the proposed model can achieve a SPIDEr score of 0.2645 on Clotho V2 dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment, Label Smoothing
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-chen2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Chen_17_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-chen2021_t6label" class="modal fade" id="bibtex-chen2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexchen2021_t6label">
        Audio Captioning With Meshed-Memory Transformer
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{chen2021_t6,
    Author = "Chen, Zhiwen and Zhang, Dawei and Wang, Jun and Deng, Feng",
    title = "Audio Captioning With Meshed-Memory Transformer",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "Automated audio captioning is the task of describing the audio content of a given audio signal in natural language. Through advancing for years, transformer-based language models are widely used in audio captioning. However, most architectures based on transformer, cannot learn prior knowledge between samples well, leading to worse text decoding. For better acoustic event and language modeling, a sequence-to-sequence model is proposed which consists of a CNN-based encoder, a memory-augmented refiner and a meshed decoder. The proposed architecture refines a multi-level representation of the relationships between audio features integrating learned a priori knowledge. At decoding stage, it exploit low- and high-level features with a mesh-like connectivity. Experiments show that the proposed model can achieve a SPIDEr score of 0.2645 on Clotho V2 dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="eren2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-eren2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Audio Captioning Using Sound Event Detection
       </h4>
<p style="text-align:left">
        Ayşegül Özkaya Eren and Mustafa Sert
       </p>
<p style="text-align:left">
<em>
         Department of Computer Engineering, Baskent University, Ankara, Turkey
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Eren_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-eren2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-eren2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-eren2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Eren_83_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-eren2021_t6" class="panel-collapse collapse" id="collapse-eren2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Audio Captioning Using Sound Event Detection
      </h4>
<p style="text-align:left">
<small>
        Ayşegül Özkaya Eren and Mustafa Sert
       </small>
<br/>
<small>
<em>
         Department of Computer Engineering, Baskent University, Ankara, Turkey
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report proposes an audio captioning system for DCASE 2021 Task 6 audio captioning challenge. Our proposed model is based on an encoder-decoder architecture with bi-directional Gated Recurrent Units (BiGRU) using pretrained audio features and sound event detection. A pretrained neural network (PANN) is used to extract audio features and Word2Vec is selected with the aim of extracting word embeddings from the audio captions. To create semantically meaningful captions, we extract sound events from the audio clips and feed the encoder-decoder architecture with sound events in addition to PANNs features. Our experiments on the Clotho dataset show that our proposed method significantly achieves better results than the challenge baseline model across all evaluation metrics.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-eren2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Eren_83_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-eren2021_t6label" class="modal fade" id="bibtex-eren2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexeren2021_t6label">
        Audio Captioning Using Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{eren2021_t6,
    Author = {\"{O}zkaya Eren, Ay\c{s}eg\"{u}l and Sert, Mustafa},
    title = "Audio Captioning Using Sound Event Detection",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This technical report proposes an audio captioning system for DCASE 2021 Task 6 audio captioning challenge. Our proposed model is based on an encoder-decoder architecture with bi-directional Gated Recurrent Units (BiGRU) using pretrained audio features and sound event detection. A pretrained neural network (PANN) is used to extract audio features and Word2Vec is selected with the aim of extracting word embeddings from the audio captions. To create semantically meaningful captions, we extract sound events from the audio clips and feed the encoder-decoder architecture with sound events in addition to PANNs features. Our experiments on the Clotho dataset show that our proposed method significantly achieves better results than the challenge baseline model across all evaluation metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="gebhard2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-gebhard2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        An Automated Audio Captioning Approach Utilising a ResNet-based Encoder
       </h4>
<p style="text-align:left">
        Alexander Gebhard<sup>1</sup>, Andreas Triantafyllopoulos<sup>1,2</sup>, Alice Baird<sup>1</sup>, and Björn Schuller<sup>1,2,3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup> EIHW, Chair of Embedded Intelligence for Healthcare and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup> audEERING GmbH, Gilching, Germany, <sup>3</sup> GLAM, Group on Language, Audio, and Music, Imperial College, London, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Gebhard_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-gebhard2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-gebhard2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-gebhard2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Gebhard_80_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-gebhard2021_t6" class="panel-collapse collapse" id="collapse-gebhard2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       An Automated Audio Captioning Approach Utilising a ResNet-based Encoder
      </h4>
<p style="text-align:left">
<small>
        Alexander Gebhard<sup>1</sup>, Andreas Triantafyllopoulos<sup>1,2</sup>, Alice Baird<sup>1</sup>, and Björn Schuller<sup>1,2,3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup> EIHW, Chair of Embedded Intelligence for Healthcare and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup> audEERING GmbH, Gilching, Germany, <sup>3</sup> GLAM, Group on Language, Audio, and Music, Imperial College, London, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our submission system to TASK6 of the DCASE2021 Challenge. The main module is based on the baseline architecture for the automated audio captioning (AAC) task, which was provided by the challenge organisers. We exchange the encoder part of the baseline architecture and replace it by a Residual Neural Network (ResNet)-18 encoder adapted to the AAC task. Results from our proposed architecture have shown an average increase of 35.7\% over the baseline system, reaching a BLEU1 score of 0.449 on the development set, demonstrating the effectiveness of the proposed encoder for this task.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-gebhard2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Gebhard_80_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-gebhard2021_t6label" class="modal fade" id="bibtex-gebhard2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexgebhard2021_t6label">
        An Automated Audio Captioning Approach Utilising a ResNet-based Encoder
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{gebhard2021_t6,
    Author = {Gebhard, Alexander and Triantafyllopoulos, Andreas and Baird, Alice and Schuller, Bj\"{o}rn},
    title = "An Automated Audio Captioning Approach Utilising a ResNet-based Encoder",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "In this report, we present our submission system to TASK6 of the DCASE2021 Challenge. The main module is based on the baseline architecture for the automated audio captioning (AAC) task, which was provided by the challenge organisers. We exchange the encoder part of the baseline architecture and replace it by a Residual Neural Network (ResNet)-18 encoder adapted to the AAC task. Results from our proposed architecture have shown an average increase of 35.7\\% over the baseline system, reaching a BLEU1 score of 0.449 on the development set, demonstrating the effectiveness of the proposed encoder for this task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="labbe2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-labbe2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        IRIT-UPS DCASE 2021 Audio Captioning System
       </h4>
<p style="text-align:left">
        Etienne Labbé and Thomas Pellegrini
       </p>
<p style="text-align:left">
<em>
         IRIT (UMR 5505), Université Paul Sabatier, CNRS, Toulouse, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Labbe_t6_1</span> <span class="label label-primary">Labbe_t6_2</span> <span class="label label-primary">Labbe_t6_3</span> <span class="label label-primary">Labbe_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-labbe2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-labbe2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-labbe2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Labbe_102_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-labbe2021_t6" class="panel-collapse collapse" id="collapse-labbe2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       IRIT-UPS DCASE 2021 Audio Captioning System
      </h4>
<p style="text-align:left">
<small>
        Etienne Labbé and Thomas Pellegrini
       </small>
<br/>
<small>
<em>
         IRIT (UMR 5505), Université Paul Sabatier, CNRS, Toulouse, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This document provides a description of our seq-to-seq models used for audio captioning in the task 6 of the DCASE 2021 challenge. Four submissions were made with two different models, a “Listen- Attend-Tell” and a “CNN-Tell”, and two different algorithms for inference, greedy and beam search.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-labbe2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Labbe_102_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-labbe2021_t6label" class="modal fade" id="bibtex-labbe2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexlabbe2021_t6label">
        IRIT-UPS DCASE 2021 Audio Captioning System
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{labbe2021_t6,
    Author = "Labb\'{e}, Etienne and Pellegrini, Thomas",
    title = "{IRIT-UPS} {DCASE} 2021 Audio Captioning System",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This document provides a description of our seq-to-seq models used for audio captioning in the task 6 of the DCASE 2021 challenge. Four submissions were made with two different models, a “Listen- Attend-Tell” and a “CNN-Tell”, and two different algorithms for inference, greedy and beam search."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="liu2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-liu2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The DCASE2021 Challenge Task 6 System : Automated Audio Caption
       </h4>
<p style="text-align:left">
        Liu Yang and Bi Sijun
       </p>
<p style="text-align:left">
<em>
         Beijing Institute of Technology
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-liu2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-liu2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-liu2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-liu2021_t6" class="panel-collapse collapse" id="collapse-liu2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The DCASE2021 Challenge Task 6 System : Automated Audio Caption
      </h4>
<p style="text-align:left">
<small>
        Liu Yang and Bi Sijun
       </small>
<br/>
<small>
<em>
         Beijing Institute of Technology
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the system participating to the Automated Audio Captioning (DCASE) 2021 Challenge, Task 6: automated audio captioning. In this work, We employ several learnable stack CNNs to extract audio features in the encoder layer, meanwhile, we employ the decoder of the widely used Transformer structure to generate captions. For optimize system, we use the sentence-level cosine loss function and crossentropy loss. The experimental results show that our system could achieve the SPIDEr of 0.166 on the evaluation split of the Clotho dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-liu2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-liu2021_t6label" class="modal fade" id="bibtex-liu2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexliu2021_t6label">
        The DCASE2021 Challenge Task 6 System : Automated Audio Caption
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{liu2021_t6,
    Author = "Yang, Liu and Sijun, Bi",
    title = "The {DCASE2021} Challenge Task 6 System : Automated Audio Caption",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This technical report describes the system participating to the Automated Audio Captioning (DCASE) 2021 Challenge, Task 6: automated audio captioning. In this work, We employ several learnable stack CNNs to extract audio features in the encoder layer, meanwhile, we employ the decoder of the widely used Transformer structure to generate captions. For optimize system, we use the sentence-level cosine loss function and crossentropy loss. The experimental results show that our system could achieve the SPIDEr of 0.166 on the evaluation split of the Clotho dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="narisetty2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-narisetty2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Leveraging State-of-the-art ASR Techniques to Audio Captioning
       </h4>
<p style="text-align:left">
        Chaitanya Narisetty<sup>1</sup>, Tomoki Hayashi<sup>2</sup>, Ryunosuke Ishizaki<sup>2</sup>, Shinji Watanabe<sup>1</sup>, and Kazuya Takeda<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup> Carnegie Mellon University, Pittsburgh, USA, <sup>2</sup> Nagoya University, Nagoya, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Narisetty_t6_1</span> <span class="label label-primary">Narisetty_t6_2</span> <span class="label label-primary">Narisetty_t6_3</span> <span class="label label-primary">Narisetty_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-narisetty2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-narisetty2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-narisetty2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Narisetty_94_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-narisetty2021_t6" class="panel-collapse collapse" id="collapse-narisetty2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Leveraging State-of-the-art ASR Techniques to Audio Captioning
      </h4>
<p style="text-align:left">
<small>
        Chaitanya Narisetty<sup>1</sup>, Tomoki Hayashi<sup>2</sup>, Ryunosuke Ishizaki<sup>2</sup>, Shinji Watanabe<sup>1</sup>, and Kazuya Takeda<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup> Carnegie Mellon University, Pittsburgh, USA, <sup>2</sup> Nagoya University, Nagoya, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report presents a summary of our submission to the 2021 DCASE challenge Task 6: Automated Audio Captioning. Our ap- proach to this task is derived from state-of-the-art ASR techniques available in the ESPNet toolkit. Specifically, we train a convolution-augmented Transformer (Conformer) model to generate captions from input acoustic features in an end-to-end manner. In addition to the prescribed challenge dataset: Clotho-v2, we also augment the AudioCaps external dataset. To overcome the limited availability of training data, we further incorporate the Audioset-tags and audio-embeddings obtained by pretrained audio neural networks (PANNs) as an auxiliary input to our model. An ensemble of models trained over various architectures and input embeddings is selected as our final submission system. Experimental results indicate that our models achieve a SPIDEr score of 0.224 and 0.246 on the development-validation and development-evaluation sets respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-narisetty2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Narisetty_94_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-narisetty2021_t6label" class="modal fade" id="bibtex-narisetty2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexnarisetty2021_t6label">
        Leveraging State-of-the-art ASR Techniques to Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{narisetty2021_t6,
    Author = "Narisetty, Chaitanya and Hayashi, Tomoki and Ishizaki, Ryunosuke and Watanabe, Shinji and Takeda, Kazuya",
    title = "Leveraging State-of-the-art {ASR} Techniques to Audio Captioning",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This report presents a summary of our submission to the 2021 DCASE challenge Task 6: Automated Audio Captioning. Our ap- proach to this task is derived from state-of-the-art ASR techniques available in the ESPNet toolkit. Specifically, we train a convolution-augmented Transformer (Conformer) model to generate captions from input acoustic features in an end-to-end manner. In addition to the prescribed challenge dataset: Clotho-v2, we also augment the AudioCaps external dataset. To overcome the limited availability of training data, we further incorporate the Audioset-tags and audio-embeddings obtained by pretrained audio neural networks (PANNs) as an auxiliary input to our model. An ensemble of models trained over various architectures and input embeddings is selected as our final submission system. Experimental results indicate that our models achieve a SPIDEr score of 0.224 and 0.246 on the development-validation and development-evaluation sets respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="won2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-won2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CAU Submission to DCASE 2021 Task6: Transformer Followed by Transfer Learning for Audio Captioning
       </h4>
<p style="text-align:left">
        Hyejin Won, Baekseung Kim, Il-Youp Kwak, and Changwon Lim
       </p>
<p style="text-align:left">
<em>
         Chung-Ang University, Department of Applied Statistics, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Won_t6_1</span> <span class="label label-primary">Won_t6_2</span> <span class="label label-primary">Won_t6_3</span> <span class="label label-primary">Won_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-won2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-won2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-won2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-won2021_t6" class="panel-collapse collapse" id="collapse-won2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CAU Submission to DCASE 2021 Task6: Transformer Followed by Transfer Learning for Audio Captioning
      </h4>
<p style="text-align:left">
<small>
        Hyejin Won, Baekseung Kim, Il-Youp Kwak, and Changwon Lim
       </small>
<br/>
<small>
<em>
         Chung-Ang University, Department of Applied Statistics, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report proposes an automated audio captioning model for the 2021 DCASE audio captioning challenge. In this challenge, a model is required to generate natural language descriptions of a given audio signal. We use pre-trained models trained using AudioSet data, a large-scale dataset of manually annotated audio events. The large amount of audio events data would help capturing important au- dio feature representation. To make use of the learned feature from AudioSet data, we explored several transfer learning approaches. Our proposed sequence-to-sequence model consists of a CNN14 or ResNet54 encoder and a Transformer decoder. Experiments show that the proposed model can achieve a SPIDEr score of 0.246 and 0.285 on audio captioning performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-won2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-won2021_t6label" class="modal fade" id="bibtex-won2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexwon2021_t6label">
        CAU Submission to DCASE 2021 Task6: Transformer Followed by Transfer Learning for Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{won2021_t6,
    Author = "Won, Hyejin and Kim, Baekseung and Kwak, Il-Youp and Lim, Changwon",
    title = "{CAU} Submission to {DCASE} 2021 Task6: Transformer Followed by Transfer Learning for Audio Captioning",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This report proposes an automated audio captioning model for the 2021 DCASE audio captioning challenge. In this challenge, a model is required to generate natural language descriptions of a given audio signal. We use pre-trained models trained using AudioSet data, a large-scale dataset of manually annotated audio events. The large amount of audio events data would help capturing important au- dio feature representation. To make use of the learned feature from AudioSet data, we explored several transfer learning approaches. Our proposed sequence-to-sequence model consists of a CNN14 or ResNet54 encoder and a Transformer decoder. Experiments show that the proposed model can achieve a SPIDEr score of 0.246 and 0.285 on audio captioning performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="xiao2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-xiao2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Automated Audio Captioning With MLP-mixer and Pre-trained Encoder
       </h4>
<p style="text-align:left">
        Feiyang Xiao<sup>1</sup>, Jian Guan<sup>1</sup>, and Qiuqiang Kong<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup> Group of Intelligent Signal Processing, College of Computer Science and Technology Harbin Engineering University, Harbin, China, <sup>2</sup> ByteDance, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xiao_t6_2</span> <span class="label label-primary">Xiao_t6_2</span> <span class="label label-primary">Xiao_t6_3</span> <span class="label label-primary">Xiao_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-xiao2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-xiao2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-xiao2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Xiao_10_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-xiao2021_t6" class="panel-collapse collapse" id="collapse-xiao2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Automated Audio Captioning With MLP-mixer and Pre-trained Encoder
      </h4>
<p style="text-align:left">
<small>
        Feiyang Xiao<sup>1</sup>, Jian Guan<sup>1</sup>, and Qiuqiang Kong<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup> Group of Intelligent Signal Processing, College of Computer Science and Technology Harbin Engineering University, Harbin, China, <sup>2</sup> ByteDance, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the submission from the Group of Intelligent Signal Processing (GISP) for Task6 of DCASE2021 challenge (automated audio captioning). Our audio captioning system is based on the sequence-to-sequence autoencoder model. Previous recurrent neural network (RNN) and Transformer based methods just perceive the time dimension information but ignore the frequency information. To utilize both time dimension and frequency dimension information, multi-layer perceptrons mixer (MLP-Mixer) is used as the encoder. For caption prediction, a Transformer decoder structure is used as the decoder. No extra data is employed. In addition, to highlight the content information, we use a pre-trained encoder with multi-label content information. The experimental results show that our system can achieve the SPIDEr of 0.144 (official baseline: 0.051) on the evaluation split of the Clotho dataset. In addition, comparing with Transformer methods, our system has fewer training time.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         None
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-xiao2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Xiao_10_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-xiao2021_t6label" class="modal fade" id="bibtex-xiao2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexxiao2021_t6label">
        Automated Audio Captioning With MLP-mixer and Pre-trained Encoder
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{xiao2021_t6,
    Author = "Xiao, Feiyang and Guan, Jian and Kong, Qiuqiang",
    title = "Automated Audio Captioning With {MLP}-mixer and Pre-trained Encoder",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This technical report describes the submission from the Group of Intelligent Signal Processing (GISP) for Task6 of DCASE2021 challenge (automated audio captioning). Our audio captioning system is based on the sequence-to-sequence autoencoder model. Previous recurrent neural network (RNN) and Transformer based methods just perceive the time dimension information but ignore the frequency information. To utilize both time dimension and frequency dimension information, multi-layer perceptrons mixer (MLP-Mixer) is used as the encoder. For caption prediction, a Transformer decoder structure is used as the decoder. No extra data is employed. In addition, to highlight the content information, we use a pre-trained encoder with multi-label content information. The experimental results show that our system can achieve the SPIDEr of 0.144 (official baseline: 0.051) on the evaluation split of the Clotho dataset. In addition, comparing with Transformer methods, our system has fewer training time."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="xinhao2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-xinhao2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        An Encoder-Decoder Based Audio Captioning System With Transfer and Reinforcement Learning for DCASE Challenge 2021 Task 6
       </h4>
<p style="text-align:left">
        Xinhao Mei<sup>1</sup>, Qiushi Huang<sup>1</sup>, Xubo Liu<sup>1</sup>, Gengyun Chen<sup>2</sup>, Jingqian Wu<sup>3∗</sup>, Yusong Wu<sup>3†</sup>, Jinzheng Zhao<sup>1</sup>, Shengchen Li<sup>3</sup>, Tom Ko<sup>4</sup>, H Lilian Tang<sup>1</sup>, Xi Shao<sup>2</sup>, Mark D. Plumbley<sup>1</sup>, Wenwu Wang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup> University of Surrey, Guildford, United Kingdom, <sup>2</sup> Nanjing University of Posts and Telecommunications, Nanjing, China, <sup>3</sup> Xi’an Jiaotong-Liverpool University, Suzhou, China, <sup>4</sup> Southern University of Science and Technology, Shenzhen, China, <sup>∗</sup> Jingqian Wu is currently with Wake Forest University, USA, <sup>†</sup>Yusong Wu is currently with University of Montreal, Canada
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xinhao_t6_1</span> <span class="label label-primary">Xinhao_t6_2</span> <span class="label label-primary">Xinhao_t6_3</span> <span class="label label-primary">Xinhao_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-xinhao2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-xinhao2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-xinhao2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Mei_88_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-xinhao2021_t6').collapse('show');window.location.hash='#xinhao2021_t6';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-xinhao2021_t6" class="panel-collapse collapse" id="collapse-xinhao2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       An Encoder-Decoder Based Audio Captioning System With Transfer and Reinforcement Learning for DCASE Challenge 2021 Task 6
      </h4>
<p style="text-align:left">
<small>
        Xinhao Mei<sup>1</sup>, Qiushi Huang<sup>1</sup>, Xubo Liu<sup>1</sup>, Gengyun Chen<sup>2</sup>, Jingqian Wu<sup>3∗</sup>, Yusong Wu<sup>3†</sup>, Jinzheng Zhao<sup>1</sup>, Shengchen Li<sup>3</sup>, Tom Ko<sup>4</sup>, H Lilian Tang<sup>1</sup>, Xi Shao<sup>2</sup>, Mark D. Plumbley<sup>1</sup>, Wenwu Wang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup> University of Surrey, Guildford, United Kingdom, <sup>2</sup> Nanjing University of Posts and Telecommunications, Nanjing, China, <sup>3</sup> Xi’an Jiaotong-Liverpool University, Suzhou, China, <sup>4</sup> Southern University of Science and Technology, Shenzhen, China, <sup>∗</sup> Jingqian Wu is currently with Wake Forest University, USA, <sup>†</sup>Yusong Wu is currently with University of Montreal, Canada
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Audio captioning aims to use natural language to describe the content of audio data. This technical report presents an automated audio captioning system submitted to Task 6 of the DCASE 2021 challenge. The proposed system is based on an encoder-decoder architecture, consisting of a convolutional neural network (CNN) encoder and a Transformer decoder. We further improve the system with two techniques, namely, pre-training the model via transfer learning techniques, either on upstream audio-related tasks or large in-domain datasets, and incorporating evaluation metrics into the optimization of the model with reinforcement learning techniques, which help address the problem caused by the mismatch between the evaluation metrics and the loss function. The results show that both techniques can further improve the performance of the captioning system. The overall system achieves a SPIDEr score of 0.277 on the Clotho evaluation set, which outperforms the top-ranked system from the DCASE 2020 challenge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-xinhao2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Mei_88_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/XinhaoMei/DCASE2021_task6_v2.git" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-xinhao2021_t6label" class="modal fade" id="bibtex-xinhao2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexxinhao2021_t6label">
        An Encoder-Decoder Based Audio Captioning System With Transfer and Reinforcement Learning for DCASE Challenge 2021 Task 6
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{xinhao2021_t6,
    Author = "Mei, Xinhao and Huang, Qiushi and Liu, Xubo and Chen, Gengyun and Wu, Jingqian and Wu, Yusong and Zhao, Jinzheng and Li, Shengchen and Ko, Tom and Tang, H. Lilian and Shao, Xi and Plumbley, Mark D. and Wang, Wenwu",
    title = "An Encoder-Decoder Based Audio Captioning System With Transfer and Reinforcement Learning for {DCASE} Challenge 2021 Task 6",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "Audio captioning aims to use natural language to describe the content of audio data. This technical report presents an automated audio captioning system submitted to Task 6 of the DCASE 2021 challenge. The proposed system is based on an encoder-decoder architecture, consisting of a convolutional neural network (CNN) encoder and a Transformer decoder. We further improve the system with two techniques, namely, pre-training the model via transfer learning techniques, either on upstream audio-related tasks or large in-domain datasets, and incorporating evaluation metrics into the optimization of the model with reinforcement learning techniques, which help address the problem caused by the mismatch between the evaluation metrics and the loss function. The results show that both techniques can further improve the performance of the captioning system. The overall system achieves a SPIDEr score of 0.277 on the Clotho evaluation set, which outperforms the top-ranked system from the DCASE 2020 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="xu2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-xu2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The SJTU System for DCASE2021 Challenge Task 6: Audio Captioning Based on Encoder Pre-training and Reinforcement Learning
       </h4>
<p style="text-align:left">
        Xuenan Xu, Zeyu Xie, Mengyue Wu, and Kai Yu
       </p>
<p style="text-align:left">
<em>
         MoE Key Lab of Artificial Intelligence X-LANCE Lab, Department of Computer Science and Engineering AI Institute, Shanghai Jiao Tong University, Shanghai, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xiao_t6_1</span> <span class="label label-primary">Xiao_t6_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-xu2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-xu2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-xu2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Xu_119_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-xu2021_t6').collapse('show');window.location.hash='#xu2021_t6';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-xu2021_t6" class="panel-collapse collapse" id="collapse-xu2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The SJTU System for DCASE2021 Challenge Task 6: Audio Captioning Based on Encoder Pre-training and Reinforcement Learning
      </h4>
<p style="text-align:left">
<small>
        Xuenan Xu, Zeyu Xie, Mengyue Wu, and Kai Yu
       </small>
<br/>
<small>
<em>
         MoE Key Lab of Artificial Intelligence X-LANCE Lab, Department of Computer Science and Engineering AI Institute, Shanghai Jiao Tong University, Shanghai, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report proposes an audio captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 challenge task Task 6. Our audio captioning system consists of a 10-layer convolution neural network (CNN) encoder and a temporal attentional single layer gated recurrent unit (GRU) decoder. In this challenge, there is no restriction on the usage of external data and pre-trained models. To better model the concepts in an audio clip, we pre-train the CNN encoder with audio tagging on AudioSet. After standard cross entropy based training, we further fine-tune the model with reinforcement learning to directly optimize the evaluation metric. Experiments show that our proposed system achieves a SPIDEr of 28.6 on the public evaluation split without ensemble.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-xu2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Xu_119_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/wsntxxn/AudioCaption" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-xu2021_t6label" class="modal fade" id="bibtex-xu2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexxu2021_t6label">
        The SJTU System for DCASE2021 Challenge Task 6: Audio Captioning Based on Encoder Pre-training and Reinforcement Learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{xu2021_t6,
    Author = "Xu, Xuenan and Xie, Zeyu and Wu, Mengyue and Yu, Kai",
    title = "The {SJTU} System for {DCASE2021} Challenge Task 6: Audio Captioning Based on Encoder Pre-training and Reinforcement Learning",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This report proposes an audio captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 challenge task Task 6. Our audio captioning system consists of a 10-layer convolution neural network (CNN) encoder and a temporal attentional single layer gated recurrent unit (GRU) decoder. In this challenge, there is no restriction on the usage of external data and pre-trained models. To better model the concepts in an audio clip, we pre-train the CNN encoder with audio tagging on AudioSet. After standard cross entropy based training, we further fine-tune the model with reinforcement learning to directly optimize the evaluation metric. Experiments show that our proposed system achieves a SPIDEr of 28.6 on the public evaluation split without ensemble."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="ye2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-ye2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Textual Information
       </h4>
<p style="text-align:left">
        Zhongjie Ye<sup>1</sup>, Helin Wang<sup>1</sup>, Dongchao Yang<sup>1</sup>, and Yuexian Zou<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup> ADSPLAB, School of ECE, Peking University, Shenzhen, China, <sup>2</sup> Peng Cheng Laboratory, Shenzhen, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ye_t6_1</span> <span class="label label-primary">Ye_t6_2</span> <span class="label label-primary">Ye_t6_3</span> <span class="label label-primary">Ye_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-ye2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-ye2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-ye2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Ye_21_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-ye2021_t6').collapse('show');window.location.hash='#ye2021_t6';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-ye2021_t6" class="panel-collapse collapse" id="collapse-ye2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Textual Information
      </h4>
<p style="text-align:left">
<small>
        Zhongjie Ye<sup>1</sup>, Helin Wang<sup>1</sup>, Dongchao Yang<sup>1</sup>, and Yuexian Zou<sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup> ADSPLAB, School of ECE, Peking University, Shenzhen, China, <sup>2</sup> Peng Cheng Laboratory, Shenzhen, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes an automated audio captioning (AAC) model for the the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 Task 6 Challenge. In order to utilize more acoustic and textual information, we propose a novel sequence-to-sequence model named KPE-MAD, with a keyword pre-trained encoder and a multi-modal attention decoder. For the encoder, we use pre-trained classification model on the AudioSet dataset, and finetune it with keywords of nouns and verbs as labels. In addition, a multi-modal attention module is proposed to integrate the acoustic and textual information in the decoder. Our single model achieves the SPIDEr score of 0.279 in the evaluation splits. And our best ensemble model by optimizing CIDEr-D via the reinforcement learning, achieves the SPIDEr score of 0.291. Our code1 and models will be released after the competition.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Mixup, SpecAugment, SpecAugment++
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-ye2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Ye_21_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/XinhaoMei/DCASE2021_task6_v2.git" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-ye2021_t6label" class="modal fade" id="bibtex-ye2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexye2021_t6label">
        Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Textual Information
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{ye2021_t6,
    Author = "Ye, Zhongjie and Wang, Helin and Yang, Dongchao and Zou, Yuexian",
    title = "Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Textual Information",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This technical report describes an automated audio captioning (AAC) model for the the Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 Task 6 Challenge. In order to utilize more acoustic and textual information, we propose a novel sequence-to-sequence model named KPE-MAD, with a keyword pre-trained encoder and a multi-modal attention decoder. For the encoder, we use pre-trained classification model on the AudioSet dataset, and finetune it with keywords of nouns and verbs as labels. In addition, a multi-modal attention module is proposed to integrate the acoustic and textual information in the decoder. Our single model achieves the SPIDEr score of 0.279 in the evaluation splits. And our best ensemble model by optimizing CIDEr-D via the reinforcement learning, achieves the SPIDEr score of 0.291. Our code1 and models will be released after the competition."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="yuan2021_t6" style="box-shadow: none">
<div class="panel-heading" id="heading-yuan2021_t6" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The DCASE 2021 Challenge Task 6 System: Automated Audio Captioning With Weakly Supervised Pre-traing and Word Selection Methods
       </h4>
<p style="text-align:left">
        Weiqiang Yuan, Qichen Han, Dong Liu, Xiang Li, and Zhen Yang
       </p>
<p style="text-align:left">
<em>
         NetEase (Hangzhou) Network Co., Ltd., China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yuan_t6_1</span> <span class="label label-primary">Yuan_t6_2</span> <span class="label label-primary">Yuan_t6_3</span> <span class="label label-primary">Yuan_t6_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-yuan2021_t6" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-yuan2021_t6" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-yuan2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yuan_2_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-yuan2021_t6" class="panel-collapse collapse" id="collapse-yuan2021_t6" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The DCASE 2021 Challenge Task 6 System: Automated Audio Captioning With Weakly Supervised Pre-traing and Word Selection Methods
      </h4>
<p style="text-align:left">
<small>
        Weiqiang Yuan, Qichen Han, Dong Liu, Xiang Li, and Zhen Yang
       </small>
<br/>
<small>
<em>
         NetEase (Hangzhou) Network Co., Ltd., China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the system participating to the De- tection and Classification of Acoustic Scenes and Events (DCASE) 2021 Challenge, Task 6: automated audio captioning. We use encoder-decoder modeling framework for audio understanding and caption generation. Our solution focuses on solving two problems in automated audio captioning: data insufficiency and word selection indeterminacy. As the amount of training data is limited, we collect large-scale weakly labeled dataset from Web with heuristic methods. Then we pre-train the encoder-decoder models with this dataset followed by fine-tuning on Clotho dataset. To solve the word selection indeterminacy problem, we use keywords extracted from captions of similar audios and audio event tags produced by pre-trained audio tagging models to guide words generation in decoding stage. We tested our submissions using the development-testing dataset. Our best submission achieved 31.8 SPIDEr score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Noise enhance
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-yuan2021_t6" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yuan_2_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-yuan2021_t6label" class="modal fade" id="bibtex-yuan2021_t6" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexyuan2021_t6label">
        The DCASE 2021 Challenge Task 6 System: Automated Audio Captioning With Weakly Supervised Pre-traing and Word Selection Methods
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{yuan2021_t6,
    Author = "Yuan, Weiqiang and Han, Qichen and Liu, Dong and Li, Xiang and Yang, Zhen",
    title = "The {DCASE} 2021 Challenge Task 6 System: Automated Audio Captioning With Weakly Supervised Pre-traing and Word Selection Methods",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "July",
    abstract = "This technical report describes the system participating to the De- tection and Classification of Acoustic Scenes and Events (DCASE) 2021 Challenge, Task 6: automated audio captioning. We use encoder-decoder modeling framework for audio understanding and caption generation. Our solution focuses on solving two problems in automated audio captioning: data insufficiency and word selection indeterminacy. As the amount of training data is limited, we collect large-scale weakly labeled dataset from Web with heuristic methods. Then we pre-train the encoder-decoder models with this dataset followed by fine-tuning on Clotho dataset. To solve the word selection indeterminacy problem, we use keywords extracted from captions of similar audios and audio event tags produced by pre-trained audio tagging models to guide words generation in decoding stage. We tested our submissions using the development-testing dataset. Our best submission achieved 31.8 SPIDEr score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>