<!DOCTYPE html><html lang="en">
<head>
    <title>Audio tagging with noisy labels and minimal supervision - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2019/task-audio-tagging-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description This task evaluates systems for multi-label audio tagging using a small set of manually-labeled data, and a larger set of noisy-labeled data, under a large vocabulary setting. This task will provide insight towards the development of broadly-applicable sound event classifiers able to cope with label noise and minimal â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2019</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2019/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2019/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2019/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2019/task-urban-sound-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2019/task-urban-sound-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2019/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2019/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2019/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/marina-bay-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-success"></i><i class="fa dc-tags fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Tags</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 2</span></span><img src="../images/logos/dcase/dcase2019_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Audio tagging with noisy labels and minimal supervision</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#input-characteristics">Input characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
<li><a href="#other-resources-generated-in-the-kaggle-competition">Other resources generated in the Kaggle competition</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>This task evaluates systems for multi-label audio tagging using a small set of manually-labeled data, and a larger set of noisy-labeled data, under a large vocabulary setting. This task will provide insight towards the development of broadly-applicable sound event classifiers able to cope with label noise and minimal supervision conditions.</p>
<p>More detailed task description can be found in the <a href="/challenge2019/task-audio-tagging">task description page</a> or in the <a href="http://kaggle.com/c/freesound-audio-tagging-2019/">competition page in Kaggle</a>.</p>
<p><strong>IMPORTANT NOTE</strong>: the task results shown in this page <strong>only include</strong> the submissions that were made using the DCASE submission system. Therefore, there are entries appearing in the <a href="http://kaggle.com/c/freesound-audio-tagging-2019/leaderboard">official Kaggle leaderboard</a> that do not appear here and the two rankings do not match.</p>
<p><strong>IMPORTANT NOTE 2</strong>: Some of the submitted systems failed running when presented with the data of the private test set (mainly because of kernels taking longer to compute than the maximum time allowed). For these systems Kaggle does not provide us a score for the private LB set, and are <strong>disqualified from the official ranking</strong>. Also, Kaggle only provides us private LB score for the two selected submissions per team. Hence, the third system that some teams submitted to DCASE does not have a private LB score attached (only public LB). We have asked the authors of these systems to provide us with private LB scores so we can show them in the tables below. Disqualified systems are shown in the tables below <strong>highlighted in red</strong>. </p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission code
            </th>
<th data-field="kaggle_team_name" data-sortable="true">
                Kaggle team name
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (public LB)" data-chartable="true" data-field="eval_score_public_lb" data-sortable="true" data-value-type="float4">
                lwlrap <br/><br/>(public LB)
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (private LB)" data-chartable="true" data-field="eval_score" data-sortable="true" data-value-type="float4">
                lwlrap <br/><br/>(private LB*)
            </th>
</tr>
</thead>
<tbody>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_2</td>
<td>THUEE</td>
<td>THUEE</td>
<td>Zhang2019</td>
<td>0.7392</td>
<td>0.7577</td>
</tr>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_1</td>
<td>THUEE</td>
<td>THUEE</td>
<td>Zhang2019</td>
<td>0.7423</td>
<td>0.7575</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_1</td>
<td>TEMP</td>
<td>Multi-label Audio tagging system 1</td>
<td>Boqing2019</td>
<td>0.7253</td>
<td>0.7240</td>
</tr>
<tr>
<td></td>
<td>Boqing_NUDT_task2_3</td>
<td>TEMP</td>
<td>Multi-label Audio tagging system 3</td>
<td>Boqing2019</td>
<td>0.7119</td>
<td>0.6777</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_2</td>
<td>TEMP</td>
<td>Multi-label Audio tagging system 2</td>
<td>Boqing2019</td>
<td>0.7235</td>
<td>0.7232</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_3</td>
<td>3x6min</td>
<td>DCASE2019 Task2 | Teacher-Student V3</td>
<td>Zhang2019b</td>
<td>0.7126</td>
<td>0.7144</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_2</td>
<td>3x6min</td>
<td>DCASE2019 Task2 | Teacher-Student V2</td>
<td>Zhang2019b</td>
<td>0.7298</td>
<td>0.7338</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_1</td>
<td>3x6min</td>
<td>DCASE2019 Task2 | Teacher-Student V1</td>
<td>Zhang2019b</td>
<td>0.7304</td>
<td>0.7338</td>
</tr>
<tr class="danger">
<td></td>
<td>Kong_SURREY_task2_1</td>
<td>cvssp_baseline</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>0.5803</td>
<td>0.0000</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_2</td>
<td>Eric Bouteillon</td>
<td>BOUTEILLON Warm-up pipeline and spec-mix 2.2</td>
<td>Bouteillon2019</td>
<td>0.7331</td>
<td>0.7419</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_1</td>
<td>Eric Bouteillon</td>
<td>BOUTEILLON Warm-up pipeline and spec-mix 2.1</td>
<td>Bouteillon2019</td>
<td>0.7389</td>
<td>0.7519</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_2</td>
<td>[kaggler-ja/AIMS] OUmed</td>
<td>resnet34_envnet_ensemble  Raw-Audio and Spectrogram</td>
<td>Akiyama2019</td>
<td>0.7474</td>
<td>0.7579</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_1</td>
<td>[kaggler-ja/AIMS] OUmed</td>
<td>resnet34_envnet_ensemble on Raw-Audio and Spectrogram</td>
<td>Akiyama2019</td>
<td>0.7504</td>
<td>0.7577</td>
</tr>
<tr>
<td></td>
<td>Sun_BNU_task2_1</td>
<td>Penghao</td>
<td>CNN+MeanTeacher</td>
<td>Sun2019</td>
<td>0.6320</td>
<td>0.6443</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_3</td>
<td>Janek Ebbers</td>
<td>DCASE2019 UPB system 3</td>
<td>Ebbers2019</td>
<td>0.7071</td>
<td>0.0000</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_2</td>
<td>Janek Ebbers</td>
<td>DCASE2019 UPB system 2</td>
<td>Ebbers2019</td>
<td>0.7262</td>
<td>0.7456</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_1</td>
<td>Janek Ebbers</td>
<td>DCASE2019 UPB system 1</td>
<td>Ebbers2019</td>
<td>0.7305</td>
<td>0.7552</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_1</td>
<td>HongXiaoFeng</td>
<td>HongXiaoFeng_BUPT_task2_1</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7152</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_2</td>
<td>HongXiaoFeng</td>
<td>HongXiaoFeng_BUPT_task2_2</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7149</td>
</tr>
<tr class="danger">
<td></td>
<td>Kharin_MePhI_task2_1</td>
<td>Alexander Khar</td>
<td>Kharin_noisy_annealing</td>
<td>Kharin2019</td>
<td>0.6637</td>
<td>0.6819</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_1</td>
<td>CP-JKU</td>
<td>CP JKU 1</td>
<td>Koutini2019</td>
<td>0.7282</td>
<td>0.7351</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_2</td>
<td>CP-JKU</td>
<td>CP JKU 2</td>
<td>Koutini2019</td>
<td>0.7254</td>
<td>0.7374</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Fonseca_UPF_task2_1</td>
<td>Challenge Baseline</td>
<td>DCASE2019 baseline system</td>
<td>Fonseca2019</td>
<td>0.5370</td>
<td>0.5379</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_1</td>
<td>CPJKUStudents</td>
<td>CPJKU Students submission</td>
<td>Paischer2019</td>
<td>0.7222</td>
<td>0.7033</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_2</td>
<td>CPJKUStudents</td>
<td>CPJKU Students submission</td>
<td>Paischer2019</td>
<td>0.7216</td>
<td>0.7099</td>
</tr>
<tr>
<td></td>
<td>PaischerPrinz_CPJKU_task2_3</td>
<td>CPJKUStudents</td>
<td>CPJKU Students submission</td>
<td>Paischer2019</td>
<td>0.7158</td>
<td>0.7018</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_1</td>
<td>Kuaiyu</td>
<td>Kuaiyu Tagging System</td>
<td>Liu2019</td>
<td>0.7348</td>
<td>0.7414</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_2</td>
<td>Kuaiyu</td>
<td>Kuaiyu Tagging System</td>
<td>Liu2019</td>
<td>0.7311</td>
<td>0.7366</td>
</tr>
</tbody>
</table>
<p><b>*</b> Unless stated otherwise, all reported scores are computed using the ground truth for the private leaderboard.</p>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="eval_score" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission code
            </th>
<th data-field="kaggle_team_name" data-sortable="true">
                Kaggle team name
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (public LB)" data-chartable="true" data-field="eval_score_public_lb" data-sortable="true" data-value-type="float4">
                lwlrap <br/><br/>(public LB)
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (private LB)" data-chartable="true" data-field="eval_score" data-sortable="true" data-value-type="float4">
                lwlrap <br/><br/>(private LB)
            </th>
</tr>
</thead>
<tbody>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_2</td>
<td>THUEE</td>
<td>THUEE</td>
<td>Zhang2019</td>
<td>0.7392</td>
<td>0.7577</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_1</td>
<td>TEMP</td>
<td>Multi-label Audio tagging system 1</td>
<td>Boqing2019</td>
<td>0.7253</td>
<td>0.7240</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_2</td>
<td>3x6min</td>
<td>DCASE2019 Task2 | Teacher-Student V2</td>
<td>Zhang2019b</td>
<td>0.7298</td>
<td>0.7338</td>
</tr>
<tr class="danger">
<td></td>
<td>Kong_SURREY_task2_1</td>
<td>cvssp_baseline</td>
<td>CVSSP cross-task CNN baseline</td>
<td>Kong2019</td>
<td>0.5803</td>
<td>0.0000</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_1</td>
<td>Eric Bouteillon</td>
<td>BOUTEILLON Warm-up pipeline and spec-mix 2.1</td>
<td>Bouteillon2019</td>
<td>0.7389</td>
<td>0.7519</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_2</td>
<td>[kaggler-ja/AIMS] OUmed</td>
<td>resnet34_envnet_ensemble  Raw-Audio and Spectrogram</td>
<td>Akiyama2019</td>
<td>0.7474</td>
<td>0.7579</td>
</tr>
<tr>
<td></td>
<td>Sun_BNU_task2_1</td>
<td>Penghao</td>
<td>CNN+MeanTeacher</td>
<td>Sun2019</td>
<td>0.6320</td>
<td>0.6443</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_1</td>
<td>Janek Ebbers</td>
<td>DCASE2019 UPB system 1</td>
<td>Ebbers2019</td>
<td>0.7305</td>
<td>0.7552</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_1</td>
<td>HongXiaoFeng</td>
<td>HongXiaoFeng_BUPT_task2_1</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7152</td>
</tr>
<tr class="danger">
<td></td>
<td>Kharin_MePhI_task2_1</td>
<td>Alexander Khar</td>
<td>Kharin_noisy_annealing</td>
<td>Kharin2019</td>
<td>0.6637</td>
<td>0.6819</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_2</td>
<td>CP-JKU</td>
<td>CP JKU 2</td>
<td>Koutini2019</td>
<td>0.7254</td>
<td>0.7374</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Fonseca_UPF_task2_1</td>
<td>Challenge Baseline</td>
<td>DCASE2019 baseline system</td>
<td>Fonseca2019</td>
<td>0.5370</td>
<td>0.5379</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_2</td>
<td>CPJKUStudents</td>
<td>CPJKU Students submission</td>
<td>Paischer2019</td>
<td>0.7216</td>
<td>0.7099</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_1</td>
<td>Kuaiyu</td>
<td>Kuaiyu Tagging System</td>
<td>Liu2019</td>
<td>0.7348</td>
<td>0.7414</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="input-characteristics">Input characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_score" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (public LB)" data-chartable="true" data-field="eval_score_public_lb" data-sortable="true" data-value-type="float4">
                lwlrap  <br/><br/>(public LB)
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (private LB)" data-chartable="true" data-field="eval_score" data-sortable="true" data-value-type="float4">
                lwlrap  <br/><br/>(private LB)
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Acoustic features
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_noisy_subset" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Use of noisy subset
            </th>
<th class="text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling rate
            </th>
</tr>
</thead>
<tbody>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_2</td>
<td>Zhang2019</td>
<td>0.7392</td>
<td>0.7577</td>
<td>log-mel energies, CQT</td>
<td>mixup, SpecAugment</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_1</td>
<td>Zhang2019</td>
<td>0.7423</td>
<td>0.7575</td>
<td>log-mel energies, CQT</td>
<td>mixup, SpecAugment</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_1</td>
<td>Boqing2019</td>
<td>0.7253</td>
<td>0.7240</td>
<td>log-mel energies</td>
<td>SpecAugment</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Boqing_NUDT_task2_3</td>
<td>Boqing2019</td>
<td>0.7119</td>
<td>0.6777</td>
<td>log-mel energies</td>
<td>SpecAugment</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_2</td>
<td>Boqing2019</td>
<td>0.7235</td>
<td>0.7232</td>
<td>log-mel energies</td>
<td>SpecAugment</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_3</td>
<td>Zhang2019b</td>
<td>0.7126</td>
<td>0.7144</td>
<td>log-mel energies</td>
<td>frequency masking, time masking, time reversal, mixup</td>
<td>using provided labels, automatic re-labeling</td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_2</td>
<td>Zhang2019b</td>
<td>0.7298</td>
<td>0.7338</td>
<td>log-mel energies, PCEN</td>
<td>frequency masking, time masking, time reversal, mixup</td>
<td>using provided labels, automatic re-labeling</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_1</td>
<td>Zhang2019b</td>
<td>0.7304</td>
<td>0.7338</td>
<td>log-mel energies, PCEN</td>
<td>frequency masking, time masking, time reversal, mixup</td>
<td>using provided labels, automatic re-labeling</td>
<td>44.1kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>Kong_SURREY_task2_1</td>
<td>Kong2019</td>
<td>0.5803</td>
<td>0.0000</td>
<td>log-mel energies</td>
<td></td>
<td>using provided labels</td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_2</td>
<td>Bouteillon2019</td>
<td>0.7331</td>
<td>0.7419</td>
<td>log-mel energies</td>
<td>spec-mix</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_1</td>
<td>Bouteillon2019</td>
<td>0.7389</td>
<td>0.7519</td>
<td>log-mel energies</td>
<td>spec-mix</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_2</td>
<td>Akiyama2019</td>
<td>0.7474</td>
<td>0.7579</td>
<td>log-mel energies, waveform</td>
<td>mixup, cutout, random gain, flip, highpass</td>
<td>semisupervised, multitask learning</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_1</td>
<td>Akiyama2019</td>
<td>0.7504</td>
<td>0.7577</td>
<td>log-mel energies, waveform</td>
<td>mixup, cutout, random gain, flip, highpass</td>
<td>semisupervised, multitask learning</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Sun_BNU_task2_1</td>
<td>Sun2019</td>
<td>0.6320</td>
<td>0.6443</td>
<td>log-mel energies</td>
<td>resample, gaussian noise</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_3</td>
<td>Ebbers2019</td>
<td>0.7071</td>
<td>0.0000</td>
<td>log-mel energies</td>
<td>mixup, frequency warping, frequency masking, time masking</td>
<td>automatic re-labeling</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_2</td>
<td>Ebbers2019</td>
<td>0.7262</td>
<td>0.7456</td>
<td>log-mel energies</td>
<td>mixup, frequency warping, frequency masking, time masking</td>
<td>automatic re-labeling</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_1</td>
<td>Ebbers2019</td>
<td>0.7305</td>
<td>0.7552</td>
<td>log-mel energies</td>
<td>mixup, frequency warping, frequency masking, time masking</td>
<td>automatic re-labeling</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_1</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7152</td>
<td>log-mel energies</td>
<td>mixup</td>
<td>Semi-Supervised Learning</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_2</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7149</td>
<td>log-mel energies</td>
<td>mixup</td>
<td>Semi-Supervised Learning</td>
<td>44.1kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>Kharin_MePhI_task2_1</td>
<td>Kharin2019</td>
<td>0.6637</td>
<td>0.6819</td>
<td>log-mel energies</td>
<td>random crops</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_1</td>
<td>Koutini2019</td>
<td>0.7282</td>
<td>0.7351</td>
<td>log-mel energies</td>
<td>mixup</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_2</td>
<td>Koutini2019</td>
<td>0.7254</td>
<td>0.7374</td>
<td>log-mel energies</td>
<td>mixup</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Fonseca_UPF_task2_1</td>
<td>Fonseca2019</td>
<td>0.5370</td>
<td>0.5379</td>
<td>log-mel energies</td>
<td></td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_1</td>
<td>Paischer2019</td>
<td>0.7222</td>
<td>0.7033</td>
<td>log-mel energies, perceptually weighted mel, perceptually weighted CQT</td>
<td>Mixup Augmentation</td>
<td>using provided labels</td>
<td>44.1kHz, 32kHz</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_2</td>
<td>Paischer2019</td>
<td>0.7216</td>
<td>0.7099</td>
<td>log-mel energies, perceptually weighted mel, perceptually weighted CQT</td>
<td>Mixup Augmentation</td>
<td>using provided labels</td>
<td>44.1kHz, 32kHz</td>
</tr>
<tr>
<td></td>
<td>PaischerPrinz_CPJKU_task2_3</td>
<td>Paischer2019</td>
<td>0.7158</td>
<td>0.7018</td>
<td>log-mel energies, perceptually weighted mel, perceptually weighted CQT</td>
<td>Mixup Augmentation</td>
<td>using provided labels</td>
<td>44.1kHz, 32kHz</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_1</td>
<td>Liu2019</td>
<td>0.7348</td>
<td>0.7414</td>
<td>log-mel energies</td>
<td>mixup,</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_2</td>
<td>Liu2019</td>
<td>0.7311</td>
<td>0.7366</td>
<td>log-mel energies</td>
<td>mixup,</td>
<td>using provided labels</td>
<td>44.1kHz</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_score" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_score" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (public LB)" data-chartable="true" data-field="eval_score_public_lb" data-sortable="true" data-value-type="float4">
                lwlrap <br/><br/>(public LB)
            </th>
<th class="sep-left-cell text-center" data-axis-label="lwlrap (private LB)" data-chartable="true" data-field="eval_score" data-sortable="true" data-value-type="float4">
                lwlrap <br/><br/>(private LB)
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision making
            </th>
<th class="text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                System complexity
            </th>
</tr>
</thead>
<tbody>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_2</td>
<td>Zhang2019</td>
<td>0.7392</td>
<td>0.7577</td>
<td>CNN, RNN, ensemble</td>
<td>15</td>
<td>geometric mean</td>
<td>17000000</td>
</tr>
<tr class="danger">
<td></td>
<td>Zhang_THU_task2_1</td>
<td>Zhang2019</td>
<td>0.7423</td>
<td>0.7575</td>
<td>CNN, RNN, ensemble</td>
<td>15</td>
<td>geometric mean</td>
<td>17000000</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_1</td>
<td>Boqing2019</td>
<td>0.7253</td>
<td>0.7240</td>
<td>CNN</td>
<td>5</td>
<td>arithmetic mean</td>
<td>16800000</td>
</tr>
<tr>
<td></td>
<td>Boqing_NUDT_task2_3</td>
<td>Boqing2019</td>
<td>0.7119</td>
<td>0.6777</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>2300000</td>
</tr>
<tr class="danger">
<td></td>
<td>Boqing_NUDT_task2_2</td>
<td>Boqing2019</td>
<td>0.7235</td>
<td>0.7232</td>
<td>CNN</td>
<td>5</td>
<td>arithmetic mean</td>
<td>16800000</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_3</td>
<td>Zhang2019b</td>
<td>0.7126</td>
<td>0.7144</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>5500000</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_2</td>
<td>Zhang2019b</td>
<td>0.7298</td>
<td>0.7338</td>
<td>CNN</td>
<td>13</td>
<td>arithmetic mean</td>
<td>71500000</td>
</tr>
<tr>
<td></td>
<td>Zhang_BIsmart_task2_1</td>
<td>Zhang2019b</td>
<td>0.7304</td>
<td>0.7338</td>
<td>CNN</td>
<td>12</td>
<td>arithmetic mean</td>
<td>66000000</td>
</tr>
<tr class="danger">
<td></td>
<td>Kong_SURREY_task2_1</td>
<td>Kong2019</td>
<td>0.5803</td>
<td>0.0000</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>4686144</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_2</td>
<td>Bouteillon2019</td>
<td>0.7331</td>
<td>0.7419</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>5250000</td>
</tr>
<tr>
<td></td>
<td>BOUTEILLON_NOORG_task2_1</td>
<td>Bouteillon2019</td>
<td>0.7389</td>
<td>0.7519</td>
<td>CNN</td>
<td>2</td>
<td>arithmetic mean</td>
<td>143000000</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_2</td>
<td>Akiyama2019</td>
<td>0.7474</td>
<td>0.7579</td>
<td>CNN, ensemble</td>
<td>95</td>
<td>weighted average</td>
<td>21800000</td>
</tr>
<tr>
<td></td>
<td>Akiyama_OU_task2_1</td>
<td>Akiyama2019</td>
<td>0.7504</td>
<td>0.7577</td>
<td>CNN, ensemble</td>
<td>170</td>
<td>weighted average</td>
<td>21800000</td>
</tr>
<tr>
<td></td>
<td>Sun_BNU_task2_1</td>
<td>Sun2019</td>
<td>0.6320</td>
<td>0.6443</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>20700000</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_3</td>
<td>Ebbers2019</td>
<td>0.7071</td>
<td>0.0000</td>
<td>CRNN</td>
<td></td>
<td>arithmetic mean</td>
<td>2600000</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_2</td>
<td>Ebbers2019</td>
<td>0.7262</td>
<td>0.7456</td>
<td>CRNN</td>
<td>3</td>
<td>arithmetic mean</td>
<td>7900000</td>
</tr>
<tr>
<td></td>
<td>Ebbers_UPB_task2_1</td>
<td>Ebbers2019</td>
<td>0.7305</td>
<td>0.7552</td>
<td>CRNN</td>
<td>6</td>
<td>arithmetic mean</td>
<td>15900000</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_1</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7152</td>
<td>CNN, CRNN, ensemble</td>
<td>27</td>
<td>geometric mean</td>
<td>300000000</td>
</tr>
<tr>
<td></td>
<td>HongXiaoFeng_BUPT_task2_2</td>
<td>Hong2019</td>
<td>0.6991</td>
<td>0.7149</td>
<td>CNN, ensemble</td>
<td>26</td>
<td>geometric mean</td>
<td>280000000</td>
</tr>
<tr class="danger">
<td></td>
<td>Kharin_MePhI_task2_1</td>
<td>Kharin2019</td>
<td>0.6637</td>
<td>0.6819</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>4700000</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_1</td>
<td>Koutini2019</td>
<td>0.7282</td>
<td>0.7351</td>
<td>CNN, Receptive Field Regularization</td>
<td>39</td>
<td>arithmetic mean</td>
<td>90000000</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task2_2</td>
<td>Koutini2019</td>
<td>0.7254</td>
<td>0.7374</td>
<td>CNN, Receptive Field Regularization</td>
<td>24</td>
<td>arithmetic mean</td>
<td>90000000</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Fonseca_UPF_task2_1</td>
<td>Fonseca2019</td>
<td>0.5370</td>
<td>0.5379</td>
<td>CNN</td>
<td></td>
<td>arithmetic mean</td>
<td>3300000</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_1</td>
<td>Paischer2019</td>
<td>0.7222</td>
<td>0.7033</td>
<td>CNN</td>
<td>5</td>
<td>arithmetic mean</td>
<td>33700000</td>
</tr>
<tr class="danger">
<td></td>
<td>PaischerPrinz_CPJKU_task2_2</td>
<td>Paischer2019</td>
<td>0.7216</td>
<td>0.7099</td>
<td>CNN</td>
<td>6</td>
<td>arithmetic mean</td>
<td>48600000</td>
</tr>
<tr>
<td></td>
<td>PaischerPrinz_CPJKU_task2_3</td>
<td>Paischer2019</td>
<td>0.7158</td>
<td>0.7018</td>
<td>CNN</td>
<td>5</td>
<td>arithmetic mean</td>
<td>47300000</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_1</td>
<td>Liu2019</td>
<td>0.7348</td>
<td>0.7414</td>
<td>CNN</td>
<td>5</td>
<td>geometric mean</td>
<td>55000000</td>
</tr>
<tr>
<td></td>
<td>Liu_Kuaiyu_task2_2</td>
<td>Liu2019</td>
<td>0.7311</td>
<td>0.7366</td>
<td>CNN</td>
<td>2</td>
<td>geometric mean</td>
<td>30000000</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2019/technical_reports_task2.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Akiyama2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Akiyama2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTITASK LEARNING AND SEMI-SUPERVISED LEARNING WITH NOISY DATA FOR AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Osamu Akiyama and Junya Sato
       </p>
<p style="text-align:left">
<em>
         Faculty of Medicine (OU), Osaka University, Osaka, Japan.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Akiyama_OU_task2_2</span><span class="label label-primary">Akiyama_OU_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Akiyama2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Akiyama2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Akiyama2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Akiyama_94_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Akiyama2019').collapse('show');window.location.hash='#Akiyama2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Akiyama2019" class="panel-collapse collapse" id="collapse-Akiyama2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTITASK LEARNING AND SEMI-SUPERVISED LEARNING WITH NOISY DATA FOR AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Osamu Akiyama and Junya Sato
       </small>
<br/>
<small>
<em>
         Faculty of Medicine (OU), Osaka University, Osaka, Japan.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes our submission to the DCASE 2019 challenge Task 2 "Audio tagging with noisy labels and minimal supervision" [1]. This task is a multi-label audio classification with 80 classes. The training data is composed of a small amount of reliably labeled data (curated data) and a larger amount of data with unreliable labels (noisy data). Additionally, there is a difference between data distribution between curated data and noisy data. To tackle this difficulty, we propose three strategies. The first is multitask learning using noisy data. The second is semi-supervised learning (SSL) using input data with a different distribution from labeled input data. The third is an ensemble method that averages models learned with different time windows. By using these methods, we achieved a score of 0.750 with label-weighted label-ranking average precision (lwlrap), which is in the top 1% on the public leaderboard (LB).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, cutout, random gain, flip, highpass
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, waveform
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted average
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         170
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         21800000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         17h (1 x Tesla P-100)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Akiyama2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Akiyama_94_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/OsciiArt/Freesound-Audio-Tagging-2019" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Akiyama2019label" class="modal fade" id="bibtex-Akiyama2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAkiyama2019label">
        MULTITASK LEARNING AND SEMI-SUPERVISED LEARNING WITH NOISY DATA FOR AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Akiyama2019,
    Author = "Akiyama, Osamu and Sato, Junya",
    title = "MULTITASK LEARNING AND SEMI-SUPERVISED LEARNING WITH NOISY DATA FOR AUDIO TAGGING",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = {This paper describes our submission to the DCASE 2019 challenge Task 2 "Audio tagging with noisy labels and minimal supervision" [1]. This task is a multi-label audio classification with 80 classes. The training data is composed of a small amount of reliably labeled data (curated data) and a larger amount of data with unreliable labels (noisy data). Additionally, there is a difference between data distribution between curated data and noisy data. To tackle this difficulty, we propose three strategies. The first is multitask learning using noisy data. The second is semi-supervised learning (SSL) using input data with a different distribution from labeled input data. The third is an ensemble method that averages models learned with different time windows. By using these methods, we achieved a score of 0.750 with label-weighted label-ranking average precision (lwlrap), which is in the top 1\% on the public leaderboard (LB).}
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Boqing2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Boqing2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTI-LABEL AUDIO TAGGING WITH NOISY LABELS AND VARIABLE LENGTH
       </h4>
<p style="text-align:left">
        Zhu Boqing, Xu Kele, Wang Dezhi and Mathurin ACHE
       </p>
<p style="text-align:left">
<em>
         College of Computer (NUDT), National University of Defense Technology, Changsha, Changsha, China. College of Meteorology and Oceanography (NUDT), National University of Defense Technology, Changsha, Changsha, China. None, None, Paris, France.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Boqing_NUDT_task2_1</span><span class="label label-primary">Boqing_NUDT_task2_3</span><span class="label label-primary">Boqing_NUDT_task2_2</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Boqing2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Boqing2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Boqing2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Boqing_88_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Boqing2019" class="panel-collapse collapse" id="collapse-Boqing2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTI-LABEL AUDIO TAGGING WITH NOISY LABELS AND VARIABLE LENGTH
      </h4>
<p style="text-align:left">
<small>
        Zhu Boqing, Xu Kele, Wang Dezhi and Mathurin ACHE
       </small>
<br/>
<small>
<em>
         College of Computer (NUDT), National University of Defense Technology, Changsha, Changsha, China. College of Meteorology and Oceanography (NUDT), National University of Defense Technology, Changsha, Changsha, China. None, None, Paris, France.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes our approach for DCASE 2019 Task2: Audio tagging with noisy labels and minimal supervision. This challenge uses a smaller set of manually labeled data and a larger set of noise-labeled data to enable the system to perform multi-label audio tagging tasks with minimal supervision conditions. We aim to tagging the audio clips with convolutional neural network under a limited computation and storage resources. To tackle the problem of noisy label data, we propose a data generation method named Dominate Mixup. It can restrain the impact of incorrect label during back propagation and itâ€™s suitable for multi-class classification problem. In response to the variable length of audio clips, we conduct an efficient learning method with cyclical audio length which allow us to learn more pattern from widely diverse sound events. On the public leaderboard for the competition, our single model and simple ensemble of 5 models score 0.711 and 0.725 respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         5
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         16800000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         25h (1 x GeForce RTX 2080Ti)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Boqing2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Boqing_88_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Boqing2019label" class="modal fade" id="bibtex-Boqing2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBoqing2019label">
        MULTI-LABEL AUDIO TAGGING WITH NOISY LABELS AND VARIABLE LENGTH
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Boqing2019,
    Author = "Boqing, Zhu and Kele, Xu and Dezhi, Wang and ACHE, Mathurin",
    title = "MULTI-LABEL AUDIO TAGGING WITH NOISY LABELS AND VARIABLE LENGTH",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper describes our approach for DCASE 2019 Task2: Audio tagging with noisy labels and minimal supervision. This challenge uses a smaller set of manually labeled data and a larger set of noise-labeled data to enable the system to perform multi-label audio tagging tasks with minimal supervision conditions. We aim to tagging the audio clips with convolutional neural network under a limited computation and storage resources. To tackle the problem of noisy label data, we propose a data generation method named Dominate Mixup. It can restrain the impact of incorrect label during back propagation and itâ€™s suitable for multi-class classification problem. In response to the variable length of audio clips, we conduct an efficient learning method with cyclical audio length which allow us to learn more pattern from widely diverse sound events. On the public leaderboard for the competition, our single model and simple ensemble of 5 models score 0.711 and 0.725 respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bouteillon2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Bouteillon2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SPECMIX: A SIMPLE DATA AUGMENTATION TO LEVERAGE CLEAN AND NOISY SET FOR EFFICIENT AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Eric Bouteillon
       </p>
<p style="text-align:left">
<em>
         NOORG, No Organization, France.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">BOUTEILLON_NOORG_task2_2</span><span class="label label-primary">BOUTEILLON_NOORG_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bouteillon2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bouteillon2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bouteillon2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Bouteillon_27_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Bouteillon2019').collapse('show');window.location.hash='#Bouteillon2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bouteillon2019" class="panel-collapse collapse" id="collapse-Bouteillon2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SPECMIX: A SIMPLE DATA AUGMENTATION TO LEVERAGE CLEAN AND NOISY SET FOR EFFICIENT AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Eric Bouteillon
       </small>
<br/>
<small>
<em>
         NOORG, No Organization, France.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents a semi-supervised warm-up pipeline used to create an efficient audio tagging system as well as a novel data augmentation technique for multi-labels audio tagging named by the author SpecMix. These new techniques were applied to our submitted audio tagging system to the Freesound Audio Tagging 2019 challenge carried out within the DCASE 2019 Task 2 challenge [3]. Purpose of this challenge consist of predicting the audio labels for every test clips using machine learning techniques trained on a small amount of reliable, manually-labeled data, and a larger quantity of noisy web audio data in a multi-label audio tagging task with a large vocabulary setting.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         spec-mix
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         143000000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         72h (1 x rtx2080ti)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bouteillon2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Bouteillon_27_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/ebouteillon/freesound-audio-tagging-2019" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bouteillon2019label" class="modal fade" id="bibtex-Bouteillon2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBouteillon2019label">
        SPECMIX: A SIMPLE DATA AUGMENTATION TO LEVERAGE CLEAN AND NOISY SET FOR EFFICIENT AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bouteillon2019,
    Author = "Bouteillon, Eric",
    title = "SPECMIX: A SIMPLE DATA AUGMENTATION TO LEVERAGE CLEAN AND NOISY SET FOR EFFICIENT AUDIO TAGGING",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper presents a semi-supervised warm-up pipeline used to create an efficient audio tagging system as well as a novel data augmentation technique for multi-labels audio tagging named by the author SpecMix. These new techniques were applied to our submitted audio tagging system to the Freesound Audio Tagging 2019 challenge carried out within the DCASE 2019 Task 2 challenge [3]. Purpose of this challenge consist of predicting the audio labels for every test clips using machine learning techniques trained on a small amount of reliable, manually-labeled data, and a larger quantity of noisy web audio data in a multi-label audio tagging task with a large vocabulary setting."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ebbers2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Ebbers2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CONVOLUTIONAL RECURRENT NEURAL NETWORK AND DATA AUGMENTATION FOR AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION
       </h4>
<p style="text-align:left">
        Janek Ebbers and Reinhold Haeb-Umbach
       </p>
<p style="text-align:left">
<em>
         Communications Engineering (UPB), Paderborn University, Paderborn, Germany.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ebbers_UPB_task2_3</span><span class="label label-primary">Ebbers_UPB_task2_2</span><span class="label label-primary">Ebbers_UPB_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ebbers2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ebbers2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ebbers2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Ebbers_92_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ebbers2019" class="panel-collapse collapse" id="collapse-Ebbers2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CONVOLUTIONAL RECURRENT NEURAL NETWORK AND DATA AUGMENTATION FOR AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION
      </h4>
<p style="text-align:left">
<small>
        Janek Ebbers and Reinhold Haeb-Umbach
       </small>
<br/>
<small>
<em>
         Communications Engineering (UPB), Paderborn University, Paderborn, Germany.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report presents our Audio Tagging system for the DCASE 2019 Challenge Task 2. Our proposed neural network architecture consists of a convolutional front end using log-mel-energies as input features, a recurrent neural network sequence encoder outputting a single vector for a whole sequence and finally a fully connected classifier network outputting an activity probability for each of the 80 event classes. Due to the limited amount of available data we use various data augmentation techniques to prevent overfitting and improve generalization. Our best system achieves a label-weighted label-ranking average precision (lwlrap) of 73.0% on the public test set which is an absolute improvement of 19.3% over the baseline.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, frequency warping, frequency masking, time masking
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         6
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         15900000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         10h (6 x GTX 980)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ebbers2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Ebbers_92_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ebbers2019label" class="modal fade" id="bibtex-Ebbers2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEbbers2019label">
        CONVOLUTIONAL RECURRENT NEURAL NETWORK AND DATA AUGMENTATION FOR AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ebbers2019,
    Author = "Ebbers, Janek and Haeb-Umbach, Reinhold",
    title = "CONVOLUTIONAL RECURRENT NEURAL NETWORK AND DATA AUGMENTATION FOR AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report presents our Audio Tagging system for the DCASE 2019 Challenge Task 2. Our proposed neural network architecture consists of a convolutional front end using log-mel-energies as input features, a recurrent neural network sequence encoder outputting a single vector for a whole sequence and finally a fully connected classifier network outputting an activity probability for each of the 80 event classes. Due to the limited amount of available data we use various data augmentation techniques to prevent overfitting and improve generalization. Our best system achieves a label-weighted label-ranking average precision (lwlrap) of 73.0\% on the public test set which is an absolute improvement of 19.3\% over the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fonseca2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Fonseca2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION
       </h4>
<p style="text-align:left">
        Eduardo Fonseca, Frederic Font, Manoj Plakal and Daniel P. W. Ellis
       </p>
<p style="text-align:left">
<em>
         Machine Perception Team (GOOGLE), Google Research, New York, USA. Music Technology Group (UPF), Universitat Pompeu Fabra, Barcelona, Barcelona, Spain.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Fonseca_UPF_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fonseca2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fonseca2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fonseca2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://arxiv.org/abs/1906.02975" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Fonseca2019').collapse('show');window.location.hash='#Fonseca2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fonseca2019" class="panel-collapse collapse" id="collapse-Fonseca2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION
      </h4>
<p style="text-align:left">
<small>
        Eduardo Fonseca, Frederic Font, Manoj Plakal and Daniel P. W. Ellis
       </small>
<br/>
<small>
<em>
         Machine Perception Team (GOOGLE), Google Research, New York, USA. Music Technology Group (UPF), Universitat Pompeu Fabra, Barcelona, Barcelona, Spain.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper introduces Task 2 of the DCASE2019 Challenge, titled â€œAudio tagging with noisy labels and minimal supervisionâ€. This task was hosted on the Kaggle platform as â€œFreesound Audio Tagging 2019â€. The task evaluates systems for multi-label audio tagging using a large set of noisy-labeled data, and a much smaller set of manually-labeled data, under a large vocabulary setting of 80 everyday sound classes. In addition, the proposed dataset poses an acoustic mismatch problem between the noisy train set and the test set due to the fact that they come from different web audio sources. This can correspond to a realistic scenario given by the difficulty of gathering large amounts of manually labeled data. We present the task setup, the FSDKaggle2019 dataset prepared for this scientific evaluation, and a baseline system consisting of a convolutional neural network. All these resources are freely available.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         3300000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         8h (1 x Tesla V-100)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fonseca2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://arxiv.org/abs/1906.02975" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/DCASE-REPO/dcase2019_task2_baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fonseca2019label" class="modal fade" id="bibtex-Fonseca2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFonseca2019label">
        AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fonseca2019,
    Author = "Fonseca, Eduardo and Font, Frederic and Plakal, Manoj and Ellis, Daniel P. W.",
    title = "AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This paper introduces Task 2 of the DCASE2019 Challenge, titled â€œAudio tagging with noisy labels and minimal supervisionâ€. This task was hosted on the Kaggle platform as â€œFreesound Audio Tagging 2019â€. The task evaluates systems for multi-label audio tagging using a large set of noisy-labeled data, and a much smaller set of manually-labeled data, under a large vocabulary setting of 80 everyday sound classes. In addition, the proposed dataset poses an acoustic mismatch problem between the noisy train set and the test set due to the fact that they come from different web audio sources. This can correspond to a realistic scenario given by the difficulty of gathering large amounts of manually labeled data. We present the task setup, the FSDKaggle2019 dataset prepared for this scientific evaluation, and a baseline system consisting of a convolutional neural network. All these resources are freely available."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hong2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Hong2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTI-LABEL AUDIO TAGGING SYSTEM FOR FREESOUND 2019: FOCUSING ON NETWORK ARCHITECTURES, LABEL NOISY AND LOSS FUNCTIONS
       </h4>
<p style="text-align:left">
        Xiaofeng Hong and Gang Liu
       </p>
<p style="text-align:left">
<em>
         Pattern Recognition and Intelligent System Laboratory (PRIS Lab) (BUPT), Beijing University of Posts and Telecommunications, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">HongXiaoFeng_BUPT_task2_1</span><span class="label label-primary">HongXiaoFeng_BUPT_task2_2</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hong2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hong2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Hong_82_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hong2019" class="panel-collapse collapse" id="collapse-Hong2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTI-LABEL AUDIO TAGGING SYSTEM FOR FREESOUND 2019: FOCUSING ON NETWORK ARCHITECTURES, LABEL NOISY AND LOSS FUNCTIONS
      </h4>
<p style="text-align:left">
<small>
        Xiaofeng Hong and Gang Liu
       </small>
<br/>
<small>
<em>
         Pattern Recognition and Intelligent System Laboratory (PRIS Lab) (BUPT), Beijing University of Posts and Telecommunications, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we propose our solutions applied to our submission for DCASE2019 Task2. We focus on the model architectures which can efficiently tag the audio with multi-label and noisy label. Furthermore, we use multi-label models based on convolutional network and recurrent network to unify detection of audio events. Graph representation is also utilized to take the audio event co-occurrence into account which is reflected in the loss functions. We also tried Semi-Supervised Learning to use the noisy data. Finally, we tried an ensemble of CNNs and CRNN, trained by using cross validation folds. Compared to the baseline score of 0.537, we achieved a score of 0.700 on the public leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         26
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         280000000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         10h (1 x TITAN Xp)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Hong_82_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hong2019label" class="modal fade" id="bibtex-Hong2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHong2019label">
        MULTI-LABEL AUDIO TAGGING SYSTEM FOR FREESOUND 2019: FOCUSING ON NETWORK ARCHITECTURES, LABEL NOISY AND LOSS FUNCTIONS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hong2019,
    Author = "Hong, Xiaofeng and Liu, Gang",
    title = "MULTI-LABEL AUDIO TAGGING SYSTEM FOR FREESOUND 2019: FOCUSING ON NETWORK ARCHITECTURES, LABEL NOISY AND LOSS FUNCTIONS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this technical report, we propose our solutions applied to our submission for DCASE2019 Task2. We focus on the model architectures which can efficiently tag the audio with multi-label and noisy label. Furthermore, we use multi-label models based on convolutional network and recurrent network to unify detection of audio events. Graph representation is also utilized to take the audio event co-occurrence into account which is reflected in the loss functions. We also tried Semi-Supervised Learning to use the noisy data. Finally, we tried an ensemble of CNNs and CRNN, trained by using cross validation folds. Compared to the baseline score of 0.537, we achieved a score of 0.700 on the public leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kharin2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kharin2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019 CHALLENGE NOISY_ANNEALING SYSTEM TECHNICAL REPORT
       </h4>
<p style="text-align:left">
        Alexander Kharin
       </p>
<p style="text-align:left">
<em>
         laboratory of bionanophotonics (MePhI), National Research Nuclear University MePhI, Moscow, Moscow, Russia.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kharin_MePhI_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kharin2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kharin2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kharin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kharin_70_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kharin2019" class="panel-collapse collapse" id="collapse-Kharin2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019 CHALLENGE NOISY_ANNEALING SYSTEM TECHNICAL REPORT
      </h4>
<p style="text-align:left">
<small>
        Alexander Kharin
       </small>
<br/>
<small>
<em>
         laboratory of bionanophotonics (MePhI), National Research Nuclear University MePhI, Moscow, Moscow, Russia.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Multy-layer convolutional neural network with following Dense layer with 4.7 millions of parameters was used for training on mel-spectrograms of audio data. Such large number of parameters and small dataset (~9k samples without augumentation) leads to vulnerability of model to overfitting. Agumentation of audiofiles (i.e cropping of spectrograms) was not found wery effective way to get rid of overfitting. The following ways found to be reasonoble: standard Kfold technique with training on 5 Kfolds and averaging of the results and so-called â€˜noisy data annealingâ€™. That method lies on sequential training of the model on general set for several epochs (30 in our case) followed by training on poorly labeled, but larger dataset for 5 epochs. After several cycles we can observe significant reduction of the overfitting (lwrap scores 0.61 for base model, 0.66 for noisydata-annealed model). Such increase is caused by partial â€˜resetâ€™ of the trainable parameters during training on poorly-labelled set. The more set-specific parameters are, the higher is â€˜resetâ€™ rate, so such annealing enhances the significance of non-overfitting-responsible features and reduces the impact of highly dataset-specific features.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         random crops
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         4700000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         5h (1 x TESLA K80)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kharin2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kharin_70_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kharin2019label" class="modal fade" id="bibtex-Kharin2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKharin2019label">
        DCASE 2019 CHALLENGE NOISY_ANNEALING SYSTEM TECHNICAL REPORT
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kharin2019,
    Author = "Kharin, Alexander",
    title = "DCASE 2019 CHALLENGE NOISY\_ANNEALING SYSTEM TECHNICAL REPORT",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "Multy-layer convolutional neural network with following Dense layer with 4.7 millions of parameters was used for training on mel-spectrograms of audio data. Such large number of parameters and small dataset (\textasciitilde 9k samples without augumentation) leads to vulnerability of model to overfitting. Agumentation of audiofiles (i.e cropping of spectrograms) was not found wery effective way to get rid of overfitting. The following ways found to be reasonoble: standard Kfold technique with training on 5 Kfolds and averaging of the results and so-called â€˜noisy data annealingâ€™. That method lies on sequential training of the model on general set for several epochs (30 in our case) followed by training on poorly labeled, but larger dataset for 5 epochs. After several cycles we can observe significant reduction of the overfitting (lwrap scores 0.61 for base model, 0.66 for noisydata-annealed model). Such increase is caused by partial â€˜resetâ€™ of the trainable parameters during training on poorly-labelled set. The more set-specific parameters are, the higher is â€˜resetâ€™ rate, so such annealing enhances the significance of non-overfitting-responsible features and reduces the impact of highly dataset-specific features."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Yin Cao, Turab Iqbal, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vision, Speech and Signal Processing (CVSSP) (SURREY), University of Surrey, Guildford, England.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kong_SURREY_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2019').collapse('show');window.location.hash='#Kong2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2019" class="panel-collapse collapse" id="collapse-Kong2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
      </h4>
<p style="text-align:left">
<small>
        Qiuqiang Kong, Yin Cao, Turab Iqbal, Wenwu Wang and Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         Centre for Vision, Speech and Signal Processing (CVSSP) (SURREY), University of Surrey, Guildford, England.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is taskdependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         4686144 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         2h (1 x GTX Titan Xp)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Kong_20_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2019_task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2019label" class="modal fade" id="bibtex-Kong2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2019label">
        CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2019,
    Author = "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.",
    title = "CROSS-TASK LEARNING FOR AUDIO TAGGING, SOUND EVENT DETECTION AND SPATIAL LOCALIZATION: DCASE 2019 BASELINE SYSTEMS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several audio recognition tasks without exploiting the specific characteristics of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the optimal architecture is taskdependent. For the systems we considered, we found that the 9-layer CNN with average pooling after convolutional layers is a good model for a majority of the DCASE 2019 tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koutini2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Koutini2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CP-JKU SUBMISSIONS TO DCASEâ€™19: ACOUSTIC SCENE CLASSIFICATION AND AUDIO TAGGING WITH RECEPTIVE-FIELD-REGULARIZED CNNS
       </h4>
<p style="text-align:left">
        Khaled Koutini, Hamid Eghbal-zadeh and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception (JKU), Johannes Kepler University Linz, Linz, Austria.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Koutini_CPJKU_task2_1</span><span class="label label-primary">Koutini_CPJKU_task2_2</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Koutini2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koutini2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koutini2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Koutini_99_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Koutini2019').collapse('show');window.location.hash='#Koutini2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koutini2019" class="panel-collapse collapse" id="collapse-Koutini2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CP-JKU SUBMISSIONS TO DCASEâ€™19: ACOUSTIC SCENE CLASSIFICATION AND AUDIO TAGGING WITH RECEPTIVE-FIELD-REGULARIZED CNNS
      </h4>
<p style="text-align:left">
<small>
        Khaled Koutini, Hamid Eghbal-zadeh and Gerhard Widmer
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception (JKU), Johannes Kepler University Linz, Linz, Austria.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we detail the CP-JKU submissions to the DCASE2019 challenge Task 1 (acoustic scene classification) and Task 2 (audio tagging with noisy labels and minimal supervision). In all of our submissions, we use fully convolutional deep neural networks architectures that are regularized with Receptive Field (RF) adjustments. We adjust the RF of variants of Resnet and Densenet architectures to best fit the various audio processing tasks that use the spectrogram features as input. Additionally, we propose novel CNN layers such as Frequency-Aware CNNs, and new noise compensation techniques such as Adaptive Weighting for Learning from Noisy Labels to cope with the complexities of each task. We prepared all of our submissions without the use of any external data. Our focus in this yearâ€™s submissions is to provide the best-performing single-model submission, using our proposed approaches.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, Receptive Field Regularization
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         24
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         90000000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         18h (1 x 1080ti)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koutini2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Koutini_99_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/kkoutini/cpjku_dcase19" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koutini2019label" class="modal fade" id="bibtex-Koutini2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoutini2019label">
        CP-JKU SUBMISSIONS TO DCASEâ€™19: ACOUSTIC SCENE CLASSIFICATION AND AUDIO TAGGING WITH RECEPTIVE-FIELD-REGULARIZED CNNS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Koutini2019,
    Author = "Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard",
    title = "CP-JKU SUBMISSIONS TO DCASEâ€™19: ACOUSTIC SCENE CLASSIFICATION AND AUDIO TAGGING WITH RECEPTIVE-FIELD-REGULARIZED CNNS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we detail the CP-JKU submissions to the DCASE2019 challenge Task 1 (acoustic scene classification) and Task 2 (audio tagging with noisy labels and minimal supervision). In all of our submissions, we use fully convolutional deep neural networks architectures that are regularized with Receptive Field (RF) adjustments. We adjust the RF of variants of Resnet and Densenet architectures to best fit the various audio processing tasks that use the spectrogram features as input. Additionally, we propose novel CNN layers such as Frequency-Aware CNNs, and new noise compensation techniques such as Adaptive Weighting for Learning from Noisy Labels to cope with the complexities of each task. We prepared all of our submissions without the use of any external data. Our focus in this yearâ€™s submissions is to provide the best-performing single-model submission, using our proposed approaches."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Liu2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        STACKED CONVOLUTIONAL NEURAL NETWORKS FOR AUDIO TAGGING WITH NOISE LABELS
       </h4>
<p style="text-align:left">
        Yanfang Liu and Qingkai Wei
       </p>
<p style="text-align:left">
<em>
         Kuaiyu, Beijing Kuaiyu Electronics Co., Ltd., Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liu_Kuaiyu_task2_1</span><span class="label label-primary">Liu_Kuaiyu_task2_2</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liu2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Liu_38_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Liu2019').collapse('show');window.location.hash='#Liu2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2019" class="panel-collapse collapse" id="collapse-Liu2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       STACKED CONVOLUTIONAL NEURAL NETWORKS FOR AUDIO TAGGING WITH NOISE LABELS
      </h4>
<p style="text-align:left">
<small>
        Yanfang Liu and Qingkai Wei
       </small>
<br/>
<small>
<em>
         Kuaiyu, Beijing Kuaiyu Electronics Co., Ltd., Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the system we used to participate in task 2 of the DCASE 2019 challenge. The task is to predict the tags of audio recordings with using a small number of manually-verified labels and a much larger number of noisy labels. In this task, we propose serveral convolutional neural networks to learn from log-mel spectrogram features. To improve the performance, different techniques preprocessing, data augmentations, loss functions and cross-validation are involved. The prediction results are then ensembled using geometric mean. On the test set used for evaluation, our system achieved a score of 0.734.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup,
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         30000000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         10h (1 x Titan xp)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Liu_38_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/sailor88128/dcase2019-task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2019label" class="modal fade" id="bibtex-Liu2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2019label">
        STACKED CONVOLUTIONAL NEURAL NETWORKS FOR AUDIO TAGGING WITH NOISE LABELS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liu2019,
    Author = "Liu, Yanfang and Wei, Qingkai",
    title = "STACKED CONVOLUTIONAL NEURAL NETWORKS FOR AUDIO TAGGING WITH NOISE LABELS",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes the system we used to participate in task 2 of the DCASE 2019 challenge. The task is to predict the tags of audio recordings with using a small number of manually-verified labels and a much larger number of noisy labels. In this task, we propose serveral convolutional neural networks to learn from log-mel spectrogram features. To improve the performance, different techniques preprocessing, data augmentations, loss functions and cross-validation are involved. The prediction results are then ensembled using geometric mean. On the test set used for evaluation, our system achieved a score of 0.734."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Paischer2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Paischer2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUDIO TAGGING WITH CONVOLUTIONAL NEURAL NETWORKS TRAINED WITH NOISY DATA
       </h4>
<p style="text-align:left">
        Fabian Paischer, Katharina Prinz and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception (CPJKU), Johannes Kepler University, Linz, Linz, Austria.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">PaischerPrinz_CPJKU_task2_1</span><span class="label label-primary">PaischerPrinz_CPJKU_task2_2</span><span class="label label-primary">PaischerPrinz_CPJKU_task2_3</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Paischer2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Paischer2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Paischer2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Paischer_37_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Paischer2019').collapse('show');window.location.hash='#Paischer2019';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Paischer2019" class="panel-collapse collapse" id="collapse-Paischer2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUDIO TAGGING WITH CONVOLUTIONAL NEURAL NETWORKS TRAINED WITH NOISY DATA
      </h4>
<p style="text-align:left">
<small>
        Fabian Paischer, Katharina Prinz and Gerhard Widmer
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception (CPJKU), Johannes Kepler University, Linz, Linz, Austria.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report is a description of our submission to the 2019 DCASE Challenge, Task 2. The task at hand is to predict one or more audiotags, out of the 80 available tags, for the audio clips of different lengths, originating from two different datasets. For training, a total number of 4970 audio clips is provided with trustworthy labels, whereas 19815 samples contain a substantial amount of label noise with unknown noise ratio. To tackle this task, we propose two different convolutional neural network (CNN) architectures trained on different features to capture different aspects of the data. Stochastic Weight Averaging is used in order to improve generalisation. By averaging over the predictions of all five networks, we obtain an ensemble that provides us with the likelihood of 80 different labels being present in an input audio clip. On the unseen data of the Public Kaggle Leaderboard, our system reaches a Label Weighted Label Ranking Average Precision (Lwlrap) of 0.722.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz, 32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         Mixup Augmentation
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, perceptually weighted mel, perceptually weighted CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         5
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         47300000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         192h (2 x NVIDIA GeForce GTX 1080)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Paischer2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Paischer_37_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Ambress92/DCASE-Audio-Tagging" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Paischer2019label" class="modal fade" id="bibtex-Paischer2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPaischer2019label">
        AUDIO TAGGING WITH CONVOLUTIONAL NEURAL NETWORKS TRAINED WITH NOISY DATA
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Paischer2019,
    Author = "Paischer, Fabian and Prinz, Katharina and Widmer, Gerhard",
    title = "AUDIO TAGGING WITH CONVOLUTIONAL NEURAL NETWORKS TRAINED WITH NOISY DATA",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This report is a description of our submission to the 2019 DCASE Challenge, Task 2. The task at hand is to predict one or more audiotags, out of the 80 available tags, for the audio clips of different lengths, originating from two different datasets. For training, a total number of 4970 audio clips is provided with trustworthy labels, whereas 19815 samples contain a substantial amount of label noise with unknown noise ratio. To tackle this task, we propose two different convolutional neural network (CNN) architectures trained on different features to capture different aspects of the data. Stochastic Weight Averaging is used in order to improve generalisation. By averaging over the predictions of all five networks, we obtain an ensemble that provides us with the likelihood of 80 different labels being present in an input audio clip. On the unseen data of the Public Kaggle Leaderboard, our system reaches a Label Weighted Label Ranking Average Precision (Lwlrap) of 0.722."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sun2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Sun2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Audio Tagging with Minimal Supervision Based on Mean Teacher for DCASE 2019 Challenge
       </h4>
<p style="text-align:left">
        Jun He, Penghao Rao, Bo Sun and Lejun Yu
       </p>
<p style="text-align:left">
<em>
         College of Information Science and Technology (BNU), Bejing Normal University, Beijing, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Sun_BNU_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sun2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sun2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sun2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Sun_113_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sun2019" class="panel-collapse collapse" id="collapse-Sun2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Audio Tagging with Minimal Supervision Based on Mean Teacher for DCASE 2019 Challenge
      </h4>
<p style="text-align:left">
<small>
        Jun He, Penghao Rao, Bo Sun and Lejun Yu
       </small>
<br/>
<small>
<em>
         College of Information Science and Technology (BNU), Bejing Normal University, Beijing, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we describe the mean teacher based audio tagging system and performance applied to the task 2 of DCASE 2018 challenge, where the task evaluates systems for audio tagging with noisy labels and minimal supervision. The proposed system is based on a VGG16 network with attention mechanism and gated CNN. Following data augmentation techniques are used to increase model robustness: a) Scaling the signal with 0.75 to 1.5 time, b) Adding Gaussian white noise with 20dB to 40dB. Samples with noisy labels are regarded as unlabeled and are utilized with semi-supervision method namely mean teacher. The proposed system is trained using 5-fold cross-validation, and the final result is the arithmetic mean of the five models. Finally, the method provides lwlrap score of 0.631, which is measured through the Kaggle platform.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         resample, gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         20700000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         24h (1 x GTX 1080 Ti)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sun2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Sun_113_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sun2019label" class="modal fade" id="bibtex-Sun2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSun2019label">
        Audio Tagging with Minimal Supervision Based on Mean Teacher for DCASE 2019 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sun2019,
    Author = "He, Jun and Rao, Penghao and Sun, Bo and Yu, Lejun",
    title = "Audio Tagging with Minimal Supervision Based on Mean Teacher for DCASE 2019 Challenge",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we describe the mean teacher based audio tagging system and performance applied to the task 2 of DCASE 2018 challenge, where the task evaluates systems for audio tagging with noisy labels and minimal supervision. The proposed system is based on a VGG16 network with attention mechanism and gated CNN. Following data augmentation techniques are used to increase model robustness: a) Scaling the signal with 0.75 to 1.5 time, b) Adding Gaussian white noise with 20dB to 40dB. Samples with noisy labels are regarded as unlabeled and are utilized with semi-supervision method namely mean teacher. The proposed system is trained using 5-fold cross-validation, and the final result is the arithmetic mean of the five models. Finally, the method provides lwlrap score of 0.631, which is measured through the Kaggle platform."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhang2019" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhang2019" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        THUEE SYSTEM FOR DCASE 2019 CHALLENGE TASK 2
       </h4>
<p style="text-align:left">
        Kexin He, Yuhan Shen and Weiqiang Zhang
       </p>
<p style="text-align:left">
<em>
         Department of Electronic Engineering (THU), Tsinghua University, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_THU_task2_2</span><span class="label label-primary">Zhang_THU_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhang2019" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhang2019" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_100_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhang2019" class="panel-collapse collapse" id="collapse-Zhang2019" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       THUEE SYSTEM FOR DCASE 2019 CHALLENGE TASK 2
      </h4>
<p style="text-align:left">
<small>
        Kexin He, Yuhan Shen and Weiqiang Zhang
       </small>
<br/>
<small>
<em>
         Department of Electronic Engineering (THU), Tsinghua University, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we described our submission for the task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge: Audio tagging with noisy labels and minimal supervision. Our methods are mainly based on two types of deep learning models: Convolutional Recurrent Neural Network (CRNN) and DenseNet. In order to prevent overfitting, we adopted data augmentation using mixup strategy and SpecAugment. Besides, we designed a staged loss function to train our models using both curated and noisy data. We also used various acoustic features, including log-mel energies and perceptual Constant-Q transform (p-CQT), and tried an ensemble of multiple subsystems to enhance the generalization capability of our system. Our final system achieved a lwlrap score of 0.742 on the public leaderboard in Kaggle.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, RNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         15
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         17000000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         10h (1 x Tesla P-100)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhang2019" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_100_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhang2019label" class="modal fade" id="bibtex-Zhang2019" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhang2019label">
        THUEE SYSTEM FOR DCASE 2019 CHALLENGE TASK 2
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhang2019,
    Author = "He, Kexin and Shen, Yuhan and Zhang, Weiqiang",
    title = "THUEE SYSTEM FOR DCASE 2019 CHALLENGE TASK 2",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "In this report, we described our submission for the task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge: Audio tagging with noisy labels and minimal supervision. Our methods are mainly based on two types of deep learning models: Convolutional Recurrent Neural Network (CRNN) and DenseNet. In order to prevent overfitting, we adopted data augmentation using mixup strategy and SpecAugment. Besides, we designed a staged loss function to train our models using both curated and noisy data. We also used various acoustic features, including log-mel energies and perceptual Constant-Q transform (p-CQT), and tried an ensemble of multiple subsystems to enhance the generalization capability of our system. Our final system achieved a lwlrap score of 0.742 on the public leaderboard in Kaggle."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhang2019b" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhang2019b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2019 TASK 2: SEMI-SUPERVISED NETWORKS WITH HEAVY DATA AUGMENTATIONS TO BATTLE AGAINST LABEL NOISE IN AUDIO TAGGING TASK
       </h4>
<p style="text-align:left">
        Jihang Zhang and Jie Wu
       </p>
<p style="text-align:left">
<em>
         Data Analyst (BIsmart), getBIsmart, Irvine, California, USA. Data Scientist (ENGINE), ENGINE | Transformation, London, UK.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_BIsmart_task2_3</span><span class="label label-primary">Zhang_BIsmart_task2_2</span><span class="label label-primary">Zhang_BIsmart_task2_1</span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhang2019b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhang2019b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhang2019b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_87_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhang2019b" class="panel-collapse collapse" id="collapse-Zhang2019b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2019 TASK 2: SEMI-SUPERVISED NETWORKS WITH HEAVY DATA AUGMENTATIONS TO BATTLE AGAINST LABEL NOISE IN AUDIO TAGGING TASK
      </h4>
<p style="text-align:left">
<small>
        Jihang Zhang and Jie Wu
       </small>
<br/>
<small>
<em>
         Data Analyst (BIsmart), getBIsmart, Irvine, California, USA. Data Scientist (ENGINE), ENGINE | Transformation, London, UK.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes a system used for DCASE 2019 Task 2: Audio tagging with noisy labels and minimal supervision. Building a large-scale multi-label dataset normally requires extensive amount of manual effort, especially for general-purpose audio tagging system. To tackle the problem, we use a semi-supervised teacher-student convolutional neural network (CNN) to leverage substantial noisy labels and small curated labels in dataset. To further regularize the system, we exploit multiple data augmentation methods, including SpecAugment [1], mixup [2], and an innovative time reversal augmentation approach. Moreover, a combination of binary Focal [3] and ArcFace [4] losses are used to increase the accuracy of pseudo labels produced by the semi-supervised network, and accelerate the training process. Aadaptive test time augmentation (TTA) based on the lengths of audio samples is used as a final approach to improve the system. We choose a single system that generates the submission file Zhang BIsmart task2 3.output.csv to be the candidate model considered for the Judgesâ€™ Award. Other two systems use ensemble approach to furthur improve the performance.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         frequency masking, time masking, time reversal, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, PCEN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         arithmetic mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         12
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         66000000 parameters
        </td>
</tr>
<tr>
<td class="col-md-3">
         Training time
        </td>
<td>
         60h (1 x GeForce RTX 2070) + 5h (1 x Tesla P100)
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhang2019b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2019/technical_reports/DCASE2019_Zhang_87_t2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhang2019blabel" class="modal fade" id="bibtex-Zhang2019b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhang2019blabel">
        DCASE 2019 TASK 2: SEMI-SUPERVISED NETWORKS WITH HEAVY DATA AUGMENTATIONS TO BATTLE AGAINST LABEL NOISE IN AUDIO TAGGING TASK
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhang2019b,
    Author = "Zhang, Jihang and Wu, Jie",
    title = "DCASE 2019 TASK 2: SEMI-SUPERVISED NETWORKS WITH HEAVY DATA AUGMENTATIONS TO BATTLE AGAINST LABEL NOISE IN AUDIO TAGGING TASK",
    institution = "DCASE2019 Challenge",
    year = "2019",
    month = "June",
    abstract = "This technical report describes a system used for DCASE 2019 Task 2: Audio tagging with noisy labels and minimal supervision. Building a large-scale multi-label dataset normally requires extensive amount of manual effort, especially for general-purpose audio tagging system. To tackle the problem, we use a semi-supervised teacher-student convolutional neural network (CNN) to leverage substantial noisy labels and small curated labels in dataset. To further regularize the system, we exploit multiple data augmentation methods, including SpecAugment [1], mixup [2], and an innovative time reversal augmentation approach. Moreover, a combination of binary Focal [3] and ArcFace [4] losses are used to increase the accuracy of pseudo labels produced by the semi-supervised network, and accelerate the training process. Aadaptive test time augmentation (TTA) based on the lengths of audio samples is used as a final approach to improve the system. We choose a single system that generates the submission file Zhang BIsmart task2 3.output.csv to be the candidate model considered for the Judgesâ€™ Award. Other two systems use ensemble approach to furthur improve the performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
<h1 id="other-resources-generated-in-the-kaggle-competition">Other resources generated in the Kaggle competition</h1>
<p>The table below shows additional resources that were created and made accessible by Kaggle participants during the competition but that were not submitted to DCASE Challenge. Note that teams who submitted to DCASE Challenge are deliverably ommitted from this table as their generated resources are referenced in the sections above.</p>
<table class="table table-hover table-condensed">
<tbody>
<tr>
<th>Team name</th>
<th>Kaggle ranking</th>
<th>Kaggle score</th>
<th>Code</th>
<th>Report</th>
</tr>
<tr>
<td>Ruslan Baikulov</td>
<td>1</td>
<td>0.75980</td>
<td><a href="https://github.com/lRomul/argus-freesound">https://github.com/lRomul/argus-freesound</a></td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/95924">kaggle writeup</a></td>
</tr>
<tr>
<td>the art of ensemble</td>
<td>2</td>
<td>0.75913</td>
<td><a href="https://github.com/qrfaction/2nd-Freesound-Audio-Tagging-2019">https://github.com/qrfaction/2nd-Freesound-Audio-Tagging-2019</a></td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/97815">kaggle writeup</a></td>
</tr>
<tr>
<td>Dmitriy Danevskiy</td>
<td>3</td>
<td>0.75892</td>
<td><a href="https://github.com/ex4sperans/freesound-classification">https://github.com/ex4sperans/freesound-classification</a></td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/97926">kaggle writeup</a></td>
</tr>
<!--tr>
            <td>[kaggler-ja/AIMS] OUmed</td>
            <td>4</td>
            <td>0.75787</td>
            <td colspan=2 style="text-align: center;">Listed in results table above under name <i>Akiyama_OU_task2_X</i></td>
        </tr-->
<!--tr>
            <td>Janek Ebbers</td>
            <td>5</td>
            <td>0.75521</td>
            <td colspan=2 style="text-align: center;">Listed in results table above under name <i>Ebbers_UPB_task2_X</i></td>
        </tr-->
<tr>
<td>Miguel Pinto</td>
<td>6</td>
<td>0.75421</td>
<td><a href="https://github.com/mnpinto/audiotagging2019">https://github.com/mnpinto/audiotagging2019</a></td>
<td><a href="https://medium.com/@mp.music93/multi-label-audio-classification-7th-place-public-lb-solution-for-freesound-audio-tagging-2019-a7ccc0e0a02f">Medium blog post</a></td>
</tr>
<tr>
<td>[kaggler-ja] Shirogane</td>
<td>7</td>
<td>0.75302</td>
<td><a href="https://www.kaggle.com/hidehisaarai1213/freesound-7th-place-solution">https://www.kaggle.com/hidehisaarai1213/freesound-7th-place-solution</a></td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/97812">kaggle writeup</a></td>
</tr>
<!--tr>
            <td>Eric Bouteillon</td>
            <td>8</td>
            <td>0.7519</td>
            <td colspan=2 style="text-align: center;">Listed in results table above under name <i>BOUTEILLON_NOORG_task2_X</i></td>
        </tr-->
<tr>
<td>4 people</td>
<td>9</td>
<td>0.74835</td>
<td><a href="https://www.kaggle.com/theoviel/9th-place-modeling-kernel">https://www.kaggle.com/theoviel/9th-place-modeling-kernel</a></td>
<td><a href="https://drive.google.com/file/d/12tZgELxClvbw2ICvB8TWiED51DPSaJkW/view?usp=sharing">technical report</a></td>
</tr>
<!--tr>
            <td>Kuaiyu</td>
            <td>11</td>
            <td>0.7414</td>
            <td colspan=2 style="text-align: center;">Listed in results table above under name <i>Liu_Kuaiyu_task2_X</i></td>
        </tr-->
<tr>
<td>VFA</td>
<td>13</td>
<td>0.73993</td>
<td>-</td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/95429#latest-555313">kaggle writeup</a></td>
</tr>
<tr>
<td>å’Œä½ ä¸€èµ·è™šåº¦æ—¶å…‰</td>
<td>19</td>
<td>0.73399</td>
<td>-</td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/95785#latest-554808">kaggle writeup</a></td>
</tr>
<tr>
<td>[dsmlkz] Dombra Power</td>
<td>21</td>
<td>0.73371</td>
<td>-</td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/95347#latest-553077">kaggle writeup</a></td>
</tr>
<tr>
<td>daisukelab</td>
<td>38</td>
<td>0.72308</td>
<td><a href="https://github.com/daisukelab/freesound-audio-tagging-2019">https://github.com/daisukelab/freesound-audio-tagging-2019</a></td>
<td><a href="https://drive.google.com/file/d/1g_xsQtvm5_m16a2fjxNma5CJb-tFyDWX/view?usp=sharing">technical report</a></td>
</tr>
<tr>
<td>Audio4Fun</td>
<td>77</td>
<td>0.70251</td>
<td>-</td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/95890#latest-553669">kaggle writeup</a></td>
</tr>
<tr>
<td>Robert Bracco</td>
<td>210</td>
<td>0.54651</td>
<td>-</td>
<td><a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/95202#latest-552774">kaggle writeup</a></td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td colspan="2" style="text-align: center;">Tutorial on Medium - How to Participate in a Kaggle Competition with Zero Code <a href="https://towardsdatascience.com/f017918d2f08">https://towardsdatascience.com/f017918d2f08</a></td>
</tr>
</tbody>
</table>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>