<!DOCTYPE html><html lang="en">
<head>
    <title>Proceedings - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2022/proceedings">
        <meta name="author" content="Kevin Wilkinghoff" />
        <meta name="description" content="The proceedings of the DCASE2022 Workshop have been published as an electronic publication: Mathieu Lagrange, Annamaria Mesaros, Thomas Pellegrini, Gaël Richard, Romain Serizel and Dan Stowell (eds.), Proceedings of the 7th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2022), Nov. 2022.. ISBN (Electronic): 978-952-03-2677-7 Link PDF …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2022</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2022/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-list fa-fw"></i>&nbsp;Program&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2022/technical-program"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule & Talks</a>
    </li>
            <li class="">
        <a href="/workshop2022/keynotes"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Keynotes</a>
    </li>
            <li class="">
        <a href="/workshop2022/session_I"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Session I</a>
    </li>
            <li class="">
        <a href="/workshop2022/session_II"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Session II</a>
    </li>
            <li class="">
        <a href="/workshop2022/session_III"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Session III</a>
    </li>
            <li class="">
        <a href="/workshop2022/session_IV"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Session IV</a>
    </li>
        </ul>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2022/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="">
        <a href="/workshop2022/registration"><i class="fa fa-key fa-fw"></i>&nbsp;Registration</a>
    </li><li class="">
        <a href="/workshop2022/travel-grants"><i class="fa fa-info fa-fw"></i>&nbsp;Travel Grants</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2022/submission"><i class="fa fa-upload fa-fw"></i>&nbsp;Submission</a>
    </li>
            <li class="">
        <a href="/workshop2022/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2022/presenter-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Presenters</a>
    </li>
            <li class="">
        <a href="/workshop2022/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2022/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metropol-sevilla-03.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><h1 class="bold">Proceedings</h1></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p>The proceedings of the DCASE2022 Workshop have been published as an electronic publication:</p>
<div class="row" style="display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;">
<div class="col-xs-2">
<a data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/143430/978-952-03-2677-7.pdf?sequence=2&amp;isAllowed=y" rel="tooltip" target="_blank" title="PDF"><img class="img-responsive img-thumbnail" src="../images/covers/DCASE2022Workshop_proceedings_cover.png"/></a>
</div>
<div class="col-xs-10 bg-light-gray">
<p>Mathieu Lagrange, Annamaria Mesaros, Thomas Pellegrini, Gaël Richard,
Romain Serizel and Dan Stowell (eds.), Proceedings of the 7th Workshop on
Detection and Classification of Acoustic Scenes and Events (DCASE 2022),
Nov. 2022..</p>
<p>ISBN (Electronic): 978-952-03-2677-7<br/>
<!-- DOI: <a href="https://doi.org/10.5281/zenodo.7306527">10.5281/zenodo.7306527</a></p>
        <br> -->
<div class="btn-group">
<a class="btn btn-xs btn-primary" data-placement="bottom" href="https://trepo.tuni.fi/handle/10024/143430" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" target="_blank" title="Permanent link"><i class="fa fa-link fa-1x"></i> Link</a>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/143430/978-952-03-2677-7.pdf?sequence=2&amp;isAllowed=y" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" title="PDF"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button class="btn btn-xs btn-danger" data-target="#bibtex_proceedings" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
</p></div>
</div>
<div aria-hidden="true" aria-labelledby="bibtex_proceedingslabel" class="modal fade" id="bibtex_proceedings" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtex_proceedingslabel">Proceedings of the 7th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2022)</h4>
</div>
<div class="modal-body">
<pre>
@book{DCASE2022Workshop,
    title = "Proceedings of the 7th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2022)",
    author = "Mathieu Lagrange, Annamaria Mesaros, Thomas Pellegrini, Gaël Richard,
Romain Serizel and Dan Stowell",
    year = "2022",
    publisher = "Tampere University",
    isbn = "978-952-03-2677-7",
    month = "November",
    address = "Nancy, France"
}
                </pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
<div class="clearfix"></div>
<div class="btex" data-scholar-cite-counts="true" data-source="content/data/workshop2022/proceedings.bib" data-stats="true">
<em>
  Total cites: 333 (updated 30.11.2023)
 </em>
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Abdollahi2022" style="box-shadow: none">
<div class="panel-heading" id="headingAbdollahi2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Integrating Isolated Examples with Weakly-Supervised Sound Event Detection: A Direct Approach
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Mohammad Abdollahi<sup>1</sup>, Romain Serizel<sup>1</sup>, Alain Rakotomamonjy<sup>1</sup>, and Gilles Gasso<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universite de Rouen
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Abdollahi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Abdollahi_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Abdollahi2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Abdollahi2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Abdollahi2022" class="panel-collapse collapse" id="collapse-Abdollahi2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In an attempt to mitigate the need for high quality strong annotations for Sound Event Detection (SED), an approach has been to resort to a mix of weakly-labelled, unlabelled and a small set of representative (isolated) examples. The common approach to integrate the set of representative examples into the training process is to use them for creating synthetic soundscapes. The process of synthesizing soundscapes however could come with its own artefacts and mismatch to real recordings and harm the overall performance. Alternatively, a rather direct way would be to use the isolated examples in a form of template matching. To this end in this paper we propose to train an isolated event classifier using the representative examples. By sliding the classifier across a recording, we use its output as an auxiliary feature vector concatenated with intermediate spectro-temporal representations extracted by the SED system. Experimental results on DESED dataset demonstrate improvements in segmentation performance when using auxiliary features and comparable results to the baseline when using them without synthetic soundscapes. Furthermore we show that this auxiliary feature vector block could act as a gateway to integrate external annotated datasets in order to further boost SED system’s performance.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Abdollahi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Abdollahi_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Abdollahi2022label" class="modal fade" id="bibtex-Abdollahi2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAbdollahi2022label">
        Integrating Isolated Examples with Weakly-Supervised Sound Event Detection: A Direct Approach
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Abdollahi2022,
    author = "Abdollahi, Mohammad and Serizel, Romain and Rakotomamonjy, Alain and Gasso, Gilles",
    title = "Integrating Isolated Examples with Weakly-Supervised Sound Event Detection: A Direct Approach",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In an attempt to mitigate the need for high quality strong annotations for Sound Event Detection (SED), an approach has been to resort to a mix of weakly-labelled, unlabelled and a small set of representative (isolated) examples. The common approach to integrate the set of representative examples into the training process is to use them for creating synthetic soundscapes. The process of synthesizing soundscapes however could come with its own artefacts and mismatch to real recordings and harm the overall performance. Alternatively, a rather direct way would be to use the isolated examples in a form of template matching. To this end in this paper we propose to train an isolated event classifier using the representative examples. By sliding the classifier across a recording, we use its output as an auxiliary feature vector concatenated with intermediate spectro-temporal representations extracted by the SED system. Experimental results on DESED dataset demonstrate improvements in segmentation performance when using auxiliary features and comparable results to the baseline when using them without synthetic soundscapes. Furthermore we show that this auxiliary feature vector block could act as a gateway to integrate external annotated datasets in order to further boost SED system’s performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Boes2022" style="box-shadow: none">
<div class="panel-heading" id="headingBoes2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Impact of Temporal Resolution on Convolutional Recurrent Networks for Audio Tagging and Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Wim Boes<sup>1</sup> and Hugo Van hamme<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>ESAT, KU Leuven, Belgium
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Boes2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Boes_41.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Boes2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Boes2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Boes2022" class="panel-collapse collapse" id="collapse-Boes2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Many state-of-the-art systems for audio tagging and sound event detection employ convolutional recurrent neural architectures. Typically, they are trained in a mean teacher setting to deal with the heterogeneous annotation of the available data. In this work, we present a thorough analysis of how changing the temporal resolution of these convolutional recurrent neural networks - which can be done by simply adapting their pooling operations - impacts their performance. By using a variety of evaluation metrics, we investigate the effects of adapting this design parameter under several sound recognition scenarios involving different needs in terms of temporal localization.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Boes2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Boes_41.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Boes2022label" class="modal fade" id="bibtex-Boes2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBoes2022label">
        Impact of Temporal Resolution on Convolutional Recurrent Networks for Audio Tagging and Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Boes2022,
    author = "Boes, Wim and Van hamme, Hugo",
    title = "Impact of Temporal Resolution on Convolutional Recurrent Networks for Audio Tagging and Sound Event Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Many state-of-the-art systems for audio tagging and sound event detection employ convolutional recurrent neural architectures. Typically, they are trained in a mean teacher setting to deal with the heterogeneous annotation of the available data. In this work, we present a thorough analysis of how changing the temporal resolution of these convolutional recurrent neural networks - which can be done by simply adapting their pooling operations - impacts their performance. By using a variety of evaluation metrics, we investigate the effects of adapting this design parameter under several sound recognition scenarios involving different needs in terms of temporal localization."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Choi2022" style="box-shadow: none">
<div class="panel-heading" id="headingChoi2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Confidence Regularized Entropy for Polyphonic Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Won-Gook Choi<sup>1</sup> and Joon-Hyuk Chang<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Choi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Choi_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Choi2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Choi2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Choi2022" class="panel-collapse collapse" id="collapse-Choi2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       One of the main issues of polyphonic sound event detection (PSED) is the class imbalance problem caused by the proportions of active and inactive frames. Since the target sounds occasionally appear, binary cross-entropy makes the model mainly fit on inactive frames. This paper introduces an effective objective function, confidence regularized entropy, which regularizes the confidence level to prevent overfitting of the dominant classes. The proposed method exhibits less overfitted samples and better detection performance than the binary cross-entropy. Also, we compare our method with the other objective function, the asymmetric focal loss also designed to solve the class imbalance problem in PSED. The two objective functions show different system characteristics. From an end-user perspective, we suggest choosing a proper objective function for the purposes.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Choi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Choi_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Choi2022label" class="modal fade" id="bibtex-Choi2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChoi2022label">
        Confidence Regularized Entropy for Polyphonic Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Choi2022,
    author = "Choi, Won-Gook and Chang, Joon-Hyuk",
    title = "Confidence Regularized Entropy for Polyphonic Sound Event Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "One of the main issues of polyphonic sound event detection (PSED) is the class imbalance problem caused by the proportions of active and inactive frames. Since the target sounds occasionally appear, binary cross-entropy makes the model mainly fit on inactive frames. This paper introduces an effective objective function, confidence regularized entropy, which regularizes the confidence level to prevent overfitting of the dominant classes. The proposed method exhibits less overfitted samples and better detection performance than the binary cross-entropy. Also, we compare our method with the other objective function, the asymmetric focal loss also designed to solve the class imbalance problem in PSED. The two objective functions show different system characteristics. From an end-user perspective, we suggest choosing a proper objective function for the purposes."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Coban2022" style="box-shadow: none">
<div class="panel-heading" id="headingCoban2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         EDANSA-2019: The Ecoacoustic Dataset from Arctic North Slope Alaska
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Enis Berk Çoban<sup>1</sup>, Megan Perra<sup>2</sup>, Dara Pir<sup>3</sup>, and Michael I Mandel<sup>1,4</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>The Graduate Center, CUNY, New York, NY, USA, <sup>2</sup>Institute of Arctic Biology, UAF, Fairbanks, AK, USA, <sup>3</sup>Guttman Community College, CUNY, New York, NY, USA, <sup>4</sup>Brooklyn College, CUNY, Brooklyn, NY, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Coban2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Coban_17.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Coban2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Coban2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Coban2022" class="panel-collapse collapse" id="collapse-Coban2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The arctic is warming at three times the rate of the global average, affecting the habitat and lifecycles of migratory species that reproduce there, like birds and caribou. Ecoacoustic monitoring can help efficiently track changes in animal phenology and behavior over large areas so that the impacts of climate change on these species can be better understood and potentially mitigated. We introduce here the Ecoacoustic Dataset from Arctic North Slope Alaska (EDANSA-2019), a dataset collected by a network of 100 autonomous recording units covering an area of 9000 square miles over the course of the 2019 summer season on the North Slope of Alaska and neighboring regions. We labeled over 27 hours of this dataset according to 28 tags with enough instances of 9 important environmental classes to train baseline convolutional recognizers. We are releasing this dataset and the corresponding baseline to the community to accelerate the recognition of these sounds and facilitate automated analyses of large-scale ecoacoustic databases.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Coban2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Coban_17.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Coban2022label" class="modal fade" id="bibtex-Coban2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCoban2022label">
        EDANSA-2019: The Ecoacoustic Dataset from Arctic North Slope Alaska
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Coban2022,
    author = "\c{C}oban, Enis Berk and Perra, Megan and Pir, Dara and Mandel, Michael I.",
    title = "EDANSA-2019: The Ecoacoustic Dataset from Arctic North Slope Alaska",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "The arctic is warming at three times the rate of the global average, affecting the habitat and lifecycles of migratory species that reproduce there, like birds and caribou. Ecoacoustic monitoring can help efficiently track changes in animal phenology and behavior over large areas so that the impacts of climate change on these species can be better understood and potentially mitigated. We introduce here the Ecoacoustic Dataset from Arctic North Slope Alaska (EDANSA-2019), a dataset collected by a network of 100 autonomous recording units covering an area of 9000 square miles over the course of the 2019 summer season on the North Slope of Alaska and neighboring regions. We labeled over 27 hours of this dataset according to 28 tags with enough instances of 9 important environmental classes to train baseline convolutional recognizers. We are releasing this dataset and the corresponding baseline to the community to accelerate the recognition of these sounds and facilitate automated analyses of large-scale ecoacoustic databases."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Deng2022" style="box-shadow: none">
<div class="panel-heading" id="headingDeng2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Ensemble of Multiple Anomalous Sound Detectors
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yufeng Deng<sup>1</sup>, Anbai Jiang<sup>1</sup>, Yuchen Duan<sup>1</sup>, Jitao Ma<sup>3</sup>, Xuchu Chen<sup>1</sup>, Jia Liu<sup>1,2</sup>, Pingyi Fan<sup>1</sup>, Cheng Lu<sup>3</sup>, and Wei-Qiang Zhang<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Department of Electronic Engineering, Tsinghua University, Beijing, China, <sup>2</sup>Tsinghua AI Plus, Beijing, China, <sup>3</sup>School of Economics and Management, North China Electric Power University, Beijing, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Deng2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Deng_4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Deng2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Deng2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Deng2022" class="panel-collapse collapse" id="collapse-Deng2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents our submission to DCASE 2022 Challenge Task 2, which aims to detect anomalous machine status via sounding by using machine learning methods, where the training dataset itself does not contain any examples of anomalies. We build six subsystems, including three self-supervised classification methods, two probabilistic methods and one generative adversarial network (GAN) based method. Our final submissions are four ensemble systems, which are different combinations of the six subsystems. The best official score of the ensemble systems can achieve 86.81% on the development dataset, whereas the corresponding Autoencoderbased baseline and the MobileNetV2-based baseline are with scores of 52.61% and 56.01%, respectively. In addition, our methods rank top on the development dataset and fourth on the evaluation dataset in this challenge.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Deng2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Deng_4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Deng2022label" class="modal fade" id="bibtex-Deng2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDeng2022label">
        Ensemble of Multiple Anomalous Sound Detectors
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Deng2022,
    author = "Deng, Yufeng and Jiang, Anbai and Duan, Yuchen and Ma, Jitao and Chen, Xuchu and Liu, Jia and Fan, Pingyi and Lu, Cheng and Zhang, Wei-Qiang",
    title = "Ensemble of Multiple Anomalous Sound Detectors",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "This paper presents our submission to DCASE 2022 Challenge Task 2, which aims to detect anomalous machine status via sounding by using machine learning methods, where the training dataset itself does not contain any examples of anomalies. We build six subsystems, including three self-supervised classification methods, two probabilistic methods and one generative adversarial network (GAN) based method. Our final submissions are four ensemble systems, which are different combinations of the six subsystems. The best official score of the ensemble systems can achieve 86.81\% on the development dataset, whereas the corresponding Autoencoderbased baseline and the MobileNetV2-based baseline are with scores of 52.61\% and 56.01\%, respectively. In addition, our methods rank top on the development dataset and fourth on the evaluation dataset in this challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dohi2022" style="box-shadow: none">
<div class="panel-heading" id="headingDohi2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         MIMII DG: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection for Domain Ggeneralization Task
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kota Dohi<sup>1</sup>, Tomoya Nishida<sup>1</sup>, Harsh Purohit<sup>1</sup>, Ryo Tanabe<sup>1</sup>, Takashi Endo<sup>1</sup>, Masaaki Yamamoto<sup>1</sup>, Yuki Nikaido<sup>1</sup>, and Yohei Kawaguchi<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Research and Development Group, Hitachi, Ltd., Tokyo, Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          70 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dohi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Dohi_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Dohi2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dohi2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dohi2022" class="panel-collapse collapse" id="collapse-Dohi2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present a machine sound dataset to benchmark domain generalization techniques for anomalous sound detection (ASD). Domain shifts are differences in data distributions that can degrade the detection performance, and handling them is a major issue for the application of ASD systems. While currently available datasets for ASD tasks assume that occurrences of domain shifts are known, in practice, they can be difficult to detect. To handle such domain shifts, domain generalization techniques that perform well regardless of the domains should be investigated. In this paper, we present the first ASD dataset for the domain generalization techniques, called MIMII DG. The dataset consists of five machine types and three domain shift scenarios for each machine type. The dataset is dedicated to the domain generalization task with features such as multiple different values for parameters that cause domain shifts and introduction of domain shifts that can be difficult to detect, such as shifts in the background noise. Experimental results using two baseline systems indicate that the dataset reproduces domain shift scenarios and is useful for benchmarking domain generalization techniques.
      </p>
<p>
<strong>
        Cites:
       </strong>
       70 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dohi2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Dohi_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dohi2022label" class="modal fade" id="bibtex-Dohi2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDohi2022label">
        MIMII DG: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection for Domain Ggeneralization Task
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dohi2022,
    author = "Dohi, Kota and Nishida, Tomoya and Purohit, Harsh and Tanabe, Ryo and Endo, Takashi and Yamamoto, Masaaki and Nikaido, Yuki and Kawaguchi, Yohei",
    title = "MIMII DG: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection for Domain Ggeneralization Task",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "We present a machine sound dataset to benchmark domain generalization techniques for anomalous sound detection (ASD). Domain shifts are differences in data distributions that can degrade the detection performance, and handling them is a major issue for the application of ASD systems. While currently available datasets for ASD tasks assume that occurrences of domain shifts are known, in practice, they can be difficult to detect. To handle such domain shifts, domain generalization techniques that perform well regardless of the domains should be investigated. In this paper, we present the first ASD dataset for the domain generalization techniques, called MIMII DG. The dataset consists of five machine types and three domain shift scenarios for each machine type. The dataset is dedicated to the domain generalization task with features such as multiple different values for parameters that cause domain shifts and introduction of domain shifts that can be difficult to detect, such as shifts in the background noise. Experimental results using two baseline systems indicate that the dataset reproduces domain shift scenarios and is useful for benchmarking domain generalization techniques."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dohi2022-2" style="box-shadow: none">
<div class="panel-heading" id="headingDohi2022-2" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kota Dohi<sup>1</sup>, Keisuke Imoto<sup>2</sup>, Noboru Harada<sup>3</sup>, Daisuke Niizumi<sup>3</sup>, Yuma Koizumi<sup>4</sup>, Tomoya Nishida<sup>1</sup>, Harsh Purohit<sup>1</sup>, Ryo Tanabe<sup>1</sup>, Takashi Endo<sup>1</sup>, Masaaki Yamamoto<sup>1</sup>, and Yohei Kawaguchi<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Hitachi, Ltd., Japan, <sup>2</sup>Doshisha University, Japan, <sup>3</sup>NTT Corporation, Japan, <sup>4</sup>Google, Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          69 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dohi2022-2" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Dohi_63.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Dohi2022-2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dohi2022-2" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dohi2022-2" class="panel-collapse collapse" id="collapse-Dohi2022-2" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present the task description and discussion on the results of the DCASE 2022 Challenge Task 2: “Unsupervised anomalous sound detection (ASD) for machine condition monitoring applying domain generalization techniques”. Domain shifts are a critical problem for the application of ASD systems. Because domain shifts can change the acoustic characteristics of data, a model trained in a source domain performs poorly for a target domain. In DCASE 2021 Challenge Task 2, we organized an ASD task for handling domain shifts. In this task, it was assumed that the occurrences of domain shifts are known. However, in practice, the domain of each sample may not be given, and the domain shifts can occur implicitly. In 2022 Task 2, we focus on domain generalization techniques that detects anomalies regardless of the domain shifts. Specifically, the domain of each sample is not given in the test data and only one threshold is allowed for all domains. Analysis of 81 submissions from 31 teams revealed two remarkable types of domain generalization techniques: 1) domain-mixing-based approach that obtains generalized representations and 2) domain-classification-based approach that explicitly or implicitly classifies different domains to improve detection performance for each domain.
      </p>
<p>
<strong>
        Cites:
       </strong>
       69 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dohi2022-2" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Dohi_63.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dohi2022-2label" class="modal fade" id="bibtex-Dohi2022-2" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDohi2022-2label">
        Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dohi2022-2,
    author = "Dohi, Kota and Imoto, Keisuke and Harada, Noboru and Niizumi, Daisuke and Koizumi, Yuma and Nishida, Tomoya and Purohit, Harsh and Tanabe, Ryo and Endo, Takashi and Yamamoto, Masaaki and Kawaguchi, Yohei",
    title = "Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "We present the task description and discussion on the results of the DCASE 2022 Challenge Task 2: “Unsupervised anomalous sound detection (ASD) for machine condition monitoring applying domain generalization techniques”. Domain shifts are a critical problem for the application of ASD systems. Because domain shifts can change the acoustic characteristics of data, a model trained in a source domain performs poorly for a target domain. In DCASE 2021 Challenge Task 2, we organized an ASD task for handling domain shifts. In this task, it was assumed that the occurrences of domain shifts are known. However, in practice, the domain of each sample may not be given, and the domain shifts can occur implicitly. In 2022 Task 2, we focus on domain generalization techniques that detects anomalies regardless of the domain shifts. Specifically, the domain of each sample is not given in the test data and only one threshold is allowed for all domains. Analysis of 81 submissions from 31 teams revealed two remarkable types of domain generalization techniques: 1) domain-mixing-based approach that obtains generalized representations and 2) domain-classification-based approach that explicitly or implicitly classifies different domains to improve detection performance for each domain."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Effa2022" style="box-shadow: none">
<div class="panel-heading" id="headingEffa2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Convolutional Neural Network for Audibility Assessment of Acoustic Alarms
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         François Effa<sup>1,2,3</sup>, Romain Serizel<sup>3</sup>, Jean-Pierre Arz<sup>1</sup>, and Nicolas Grimault<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institut National de Recherche et de Sécurité, F-54000 Nancy, France, <sup>2</sup>Centre de Recherche en Neurosciences de Lyon, CNRS, F-69500 Bron, France, <sup>3</sup>Université de Lorraine, CNRS, Inria, Loria, F-54000 Nancy, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Effa2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Effa_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Effa2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Effa2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Effa2022" class="panel-collapse collapse" id="collapse-Effa2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In noisy workplaces, the audibility of acoustic alarms is essential to ensure worker safety. In practice, some criteria are required in international standards to make sure that the alarms are “clearly audible”. However, the recommendations may lead to overly loud alarms, thereby exposing workers to unnecessary high sound levels, especially when ambient sound levels are high themselves. For this reason, it appears necessary to properly assess the audibility of alarms at design stage. Existing psychoacoustical methods rely on repeated subjective measurements at different sound levels and therefore require time-consuming procedures. In addition, they must be repeated each time the alarm or sound environment changes. To overcome this issue, we propose a data-driven approach to estimate the audibility of new alarm signals without having to test each new condition experimentally. In this study, a convolutional neural network model is trained to perform a binary classification task on short sound clips labeled with the outcomes of psychoacoustical experiments. We propose a proof of concept of this approach and analyze its performance depending on the data used at training and the temporal context used by the networks to predict the audibility of the alarm.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Effa2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Effa_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Effa2022label" class="modal fade" id="bibtex-Effa2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEffa2022label">
        Convolutional Neural Network for Audibility Assessment of Acoustic Alarms
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Effa2022,
    author = "Effa, Fran\c{c}ois and Serizel, Romain and Arz, Jean-Pierre and Grimault, Nicolas",
    title = "Convolutional Neural Network for Audibility Assessment of Acoustic Alarms",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In noisy workplaces, the audibility of acoustic alarms is essential to ensure worker safety. In practice, some criteria are required in international standards to make sure that the alarms are “clearly audible”. However, the recommendations may lead to overly loud alarms, thereby exposing workers to unnecessary high sound levels, especially when ambient sound levels are high themselves. For this reason, it appears necessary to properly assess the audibility of alarms at design stage. Existing psychoacoustical methods rely on repeated subjective measurements at different sound levels and therefore require time-consuming procedures. In addition, they must be repeated each time the alarm or sound environment changes. To overcome this issue, we propose a data-driven approach to estimate the audibility of new alarm signals without having to test each new condition experimentally. In this study, a convolutional neural network model is trained to perform a binary classification task on short sound clips labeled with the outcomes of psychoacoustical experiments. We propose a proof of concept of this approach and analyze its performance depending on the data used at training and the temporal context used by the networks to predict the audibility of the alarm."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fourer2022" style="box-shadow: none">
<div class="panel-heading" id="headingFourer2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Detection and Identification of Beehive Piping Audio Signals
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Dominique Fourer<sup>1</sup>, and Agnieszka Orlowska<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IBISC (EA 4526) - University of Evry / Paris-Saclay, Evry-Courcouronnes, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fourer2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Fourer_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Fourer2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fourer2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fourer2022" class="panel-collapse collapse" id="collapse-Fourer2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Piping signals are particular sounds emitted by honey bees during the swarming season or sometimes when bees are exposed to specific factors during the life of the colony. Such sounds are of interest for beekeepers for predicting an imminent swarming of a beehive. The present study introduces a novel publicly available dataset made of several honey bee piping recordings allowing for the evaluation of future audio-based detection and recognition methods. First, we propose an analysis of the most relevant timbre features for discriminating between tooting and quacking sounds which are two distinct types of piping signals. Second, we comparatively assess several machine-learning-based methods designed for the detection and the identification of piping signals through a beehive independent 3-fold cross-validation methodology.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fourer2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Fourer_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fourer2022label" class="modal fade" id="bibtex-Fourer2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFourer2022label">
        Detection and Identification of Beehive Piping Audio Signals
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Fourer2022,
    author = "Fourer, Dominique and Orlowska, Agnieszka",
    title = "Detection and Identification of Beehive Piping Audio Signals",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Piping signals are particular sounds emitted by honey bees during the swarming season or sometimes when bees are exposed to specific factors during the life of the colony. Such sounds are of interest for beekeepers for predicting an imminent swarming of a beehive. The present study introduces a novel publicly available dataset made of several honey bee piping recordings allowing for the evaluation of future audio-based detection and recognition methods. First, we propose an analysis of the most relevant timbre features for discriminating between tooting and quacking sounds which are two distinct types of piping signals. Second, we comparatively assess several machine-learning-based methods designed for the detection and the identification of piping signals through a beehive independent 3-fold cross-validation methodology."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hu2022" style="box-shadow: none">
<div class="panel-heading" id="headingHu2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound Event Localization and Detection for Real Spatial Sound Scenes: Event-Independent Network and Data Augmentation Chains
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jinbo Hu<sup>1,2</sup>, Yin Cao<sup>3</sup>, Ming Wu<sup>1</sup>, Qiuqiang Kong<sup>4</sup>, Feiran Yang<sup>1</sup>, Mark D. Plumbley<sup>5</sup>, and Jun Yang<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Xi’an Jiaotong Liverpool University, Suzhou, China, <sup>4</sup>ByteDance Shanghai, China, <sup>5</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          6 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Hu_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Hu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hu2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hu2022" class="panel-collapse collapse" id="collapse-Hu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event localization and detection (SELD) is a joint task of sound event detection and direction-of-arrival estimation. In DCASE 2022 Task 3, types of data transform from computationally generated spatial recordings to recordings of real-sound scenes. Our system submitted to the DCASE 2022 Task 3 is based on our previous proposed Event-Independent Network V2 (EINV2) with a novel data augmentation method. Our method employs EINV2 with a track-wise output format, permutation-invariant training, and a soft parameter-sharing strategy, to detect different sound events of the same class but in different locations. The Conformer structure is used for extending EINV2 to learn local and global features. A data augmentation method, which contains several data augmentation chains composed of stochastic combinations of several different data augmentation operations, is utilized to generalize the model. To mitigate the lack of real-scene recordings in the development dataset and the presence of sound events being unbalanced, we exploit FSD50K, AudioSet, and TAU Spatial Room Impulse Response Database (TAU-SRIR DB) to generate simulated datasets for training. We present results on the validation set of Sony-TAu Realistic Spatial Soundscapes 2022 (STARSS22) in detail. Experimental results indicate that the ability to generalize to different environments and unbalanced performance among different classes are two main challenges. We evaluate our proposed method in Task 3 of the DCASE 2022 challenge and obtain the second rank in the teams ranking. Source code is released.
      </p>
<p>
<strong>
        Cites:
       </strong>
       6 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Hu_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hu2022label" class="modal fade" id="bibtex-Hu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHu2022label">
        Sound Event Localization and Detection for Real Spatial Sound Scenes: Event-Independent Network and Data Augmentation Chains
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Hu2022,
    author = "Hu, Jinbo and Cao, Yin and Wu, Ming and Kong, Qiuqiang and Yang, Feiran and Plumbley, Mark D. and Yang, Jun",
    title = "Sound Event Localization and Detection for Real Spatial Sound Scenes: Event-Independent Network and Data Augmentation Chains",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Sound event localization and detection (SELD) is a joint task of sound event detection and direction-of-arrival estimation. In DCASE 2022 Task 3, types of data transform from computationally generated spatial recordings to recordings of real-sound scenes. Our system submitted to the DCASE 2022 Task 3 is based on our previous proposed Event-Independent Network V2 (EINV2) with a novel data augmentation method. Our method employs EINV2 with a track-wise output format, permutation-invariant training, and a soft parameter-sharing strategy, to detect different sound events of the same class but in different locations. The Conformer structure is used for extending EINV2 to learn local and global features. A data augmentation method, which contains several data augmentation chains composed of stochastic combinations of several different data augmentation operations, is utilized to generalize the model. To mitigate the lack of real-scene recordings in the development dataset and the presence of sound events being unbalanced, we exploit FSD50K, AudioSet, and TAU Spatial Room Impulse Response Database (TAU-SRIR DB) to generate simulated datasets for training. We present results on the validation set of Sony-TAu Realistic Spatial Soundscapes 2022 (STARSS22) in detail. Experimental results indicate that the ability to generalize to different environments and unbalanced performance among different classes are two main challenges. We evaluate our proposed method in Task 3 of the DCASE 2022 challenge and obtain the second rank in the teams ranking. Source code is released."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kapka2022" style="box-shadow: none">
<div class="panel-heading" id="headingKapka2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         CoLoC: Conditioned Localizer and Classifier for Sound Event Localization and Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sławomir Kapka<sup>1</sup>, and Jakub Tkaczuk<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Samsung R&amp;D Institute Poland, Warsaw, Poland
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kapka2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Kapka_10.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Kapka2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kapka2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kapka2022" class="panel-collapse collapse" id="collapse-Kapka2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this article we describe Conditioned Localizer and Classifier (CoLoC) which is a novel solution for Sound Event Localization and Detection (SELD). The solution constitutes of two stages: the localization is done first and is followed by classification conditioned by the output of the localizer. In order to resolve the problem of unknown number of sources we incorporate the idea borrowed from Sequential Set Generation (SSG). Models from both stages are SELDnet-like CRNNs, but with single outputs. Conducted reasoning shows that such two single output models are fit for SELD task. We show that our solution improves on the baseline system in most metrics on the STARSS22 Dataset.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kapka2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Kapka_10.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kapka2022label" class="modal fade" id="bibtex-Kapka2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKapka2022label">
        CoLoC: Conditioned Localizer and Classifier for Sound Event Localization and Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kapka2022,
    author = "Kapka, S\l{}awomir and Tkaczuk, Jakub",
    title = "CoLoC: Conditioned Localizer and Classifier for Sound Event Localization and Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In this article we describe Conditioned Localizer and Classifier (CoLoC) which is a novel solution for Sound Event Localization and Detection (SELD). The solution constitutes of two stages: the localization is done first and is followed by classification conditioned by the output of the localizer. In order to resolve the problem of unknown number of sources we incorporate the idea borrowed from Sequential Set Generation (SSG). Models from both stages are SELDnet-like CRNNs, but with single outputs. Conducted reasoning shows that such two single output models are fit for SELD task. We show that our solution improves on the baseline system in most metrics on the STARSS22 Dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koga2022" style="box-shadow: none">
<div class="panel-heading" id="headingKoga2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Model Training that Prioritizes Rare Overlapped Labels for Polyphonic Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Rie Koga<sup>1</sup>, Sawa Takamuku<sup>1</sup>, Keisuke Imoto<sup>2</sup>, and Naotake Natori<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>AISIN CORPORATION, 1-1-20 Aomi, Koto-ku, Tokyo, Japan, <sup>2</sup>Doshisha University, 1-3 Tatara Miyakodani, Kyotanabe, Kyoto, Japan
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koga2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Koga_64.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Koga2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koga2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koga2022" class="panel-collapse collapse" id="collapse-Koga2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this study, we propose a model training method for polyphonic sound event detection (polyphonic SED) that prioritizes rare event label frames during multiple overlapping sound events. Multi-label classification typically utilized in polyphonic SED often fails to recognize such events. To overcome this problem, the proposed method is designed to represent event overlaps of rare labels easily without a complicated network structure. During model training, we periodically apply either binary cross-entropy loss (BCE) for multi-label classification or softmax cross-entropy loss (Softmax-CE) for multi-class classification. When multi-class classification is performed using Softmax-CE, the labels of the overlapping frames are reconstructed from the target labels to include the rarest ones and exclude the frequent ones. The model was evaluated on strongly labeled AudioSet data, from which only human voice segments were extracted. The proposed method achieves an improvement of 0.23 percentage points over the baseline, which only used the BCE, in terms of the mean average precision. In particular, the proposed method outperforms the baseline with respect to rare labels, with an average precision of 1.18 percentage points. The experimental results also demonstrate the effectiveness of the proposed method for both overlap of sound events and rare labels.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koga2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Koga_64.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koga2022label" class="modal fade" id="bibtex-Koga2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoga2022label">
        Model Training that Prioritizes Rare Overlapped Labels for Polyphonic Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Koga2022,
    author = "Koga, Rie and Takamuku, Sawa and Imoto, Keisuke and Natori, Naotake",
    title = "Model Training that Prioritizes Rare Overlapped Labels for Polyphonic Sound Event Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In this study, we propose a model training method for polyphonic sound event detection (polyphonic SED) that prioritizes rare event label frames during multiple overlapping sound events. Multi-label classification typically utilized in polyphonic SED often fails to recognize such events. To overcome this problem, the proposed method is designed to represent event overlaps of rare labels easily without a complicated network structure. During model training, we periodically apply either binary cross-entropy loss (BCE) for multi-label classification or softmax cross-entropy loss (Softmax-CE) for multi-class classification. When multi-class classification is performed using Softmax-CE, the labels of the overlapping frames are reconstructed from the target labels to include the rarest ones and exclude the frequent ones. The model was evaluated on strongly labeled AudioSet data, from which only human voice segments were extracted. The proposed method achieves an improvement of 0.23 percentage points over the baseline, which only used the BCE, in terms of the mean average precision. In particular, the proposed method outperforms the baseline with respect to rare labels, with an average precision of 1.18 percentage points. The experimental results also demonstrate the effectiveness of the proposed method for both overlap of sound events and rare labels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kushwaha2022" style="box-shadow: none">
<div class="panel-heading" id="headingKushwaha2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Analyzing the Effect of Equal-Angle Spatial Discretization on Sound Event Localization and Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Saksham Singh Kushwaha<sup>1</sup>, Iran R. Roman<sup>2</sup>, and Juan Pablo Bello<sup>2,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Courant Institute of Mathematical Sciences, New York University, NY, USA, <sup>2</sup>Music and Audio Research Lab, New York University, NY, USA, <sup>3</sup>Center for Urban Science and Progress, New York University, NY, USA
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kushwaha2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Kushwaha_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Kushwaha2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kushwaha2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kushwaha2022" class="panel-collapse collapse" id="collapse-Kushwaha2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event localization and detection (SELD) models detect and localize sound events in space and time. Datasets for SELD often discretize spatial sound events along the polar coordinates of azimuth (integers from -180º to 180º) and elevation (integers from -90º to 90º). This discretization, known as equal-angle, results in more dense points at the poles (±90º elevation) than at the equator (0º elevation). We first analyzed the effect of equal-angle discretization on the 2022 DCASE SELD baseline model. Since the STARSS 2022 dataset that accompanies the model shows unbalanced sampling of spatial sound events along the elevation axis, we created a synthetic dataset. Our dataset has spatial sound events uniformly distributed along the elevation axis. We created two versions: one with targets spatially discretized using equal-angle, and another one with a uniform spatial discretization (both versions had the same audio). The model trained with equal-angle showed a greater angular localization error for targets around the equator compared to the poles, while the model trained with uniform spatial discretization showed a uniform localization error along the elevation axis. To train the model with the STARSS2022 dataset and reduce the effect of its equal-angle-discretized targets, we modified the model’s loss function to penalize localization errors above an angular distance threshold around each target. Using this loss we fine-tuned a model trained with the original loss, and also trained the same model from scratch. Results showed improved localization metrics in both models compared to baseline, while retaining classification metrics. Our results show that equal-angle discretization yields models with nonuniform localization errors for targets along the elevation axis. Finally, our proposed loss function penalizes the SELD model’s angular localization errors, regardless of which spatial discretization was used to annotate the dataset targets.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kushwaha2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Kushwaha_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kushwaha2022label" class="modal fade" id="bibtex-Kushwaha2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKushwaha2022label">
        Analyzing the Effect of Equal-Angle Spatial Discretization on Sound Event Localization and Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kushwaha2022,
    author = "Kushwaha, Saksham Singh and Roman, Iran R. and Bello, Juan Pablo",
    title = "Analyzing the Effect of Equal-Angle Spatial Discretization on Sound Event Localization and Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Sound event localization and detection (SELD) models detect and localize sound events in space and time. Datasets for SELD often discretize spatial sound events along the polar coordinates of azimuth (integers from -180º to 180º) and elevation (integers from -90º to 90º). This discretization, known as equal-angle, results in more dense points at the poles (±90º elevation) than at the equator (0º elevation). We first analyzed the effect of equal-angle discretization on the 2022 DCASE SELD baseline model. Since the STARSS 2022 dataset that accompanies the model shows unbalanced sampling of spatial sound events along the elevation axis, we created a synthetic dataset. Our dataset has spatial sound events uniformly distributed along the elevation axis. We created two versions: one with targets spatially discretized using equal-angle, and another one with a uniform spatial discretization (both versions had the same audio). The model trained with equal-angle showed a greater angular localization error for targets around the equator compared to the poles, while the model trained with uniform spatial discretization showed a uniform localization error along the elevation axis. To train the model with the STARSS2022 dataset and reduce the effect of its equal-angle-discretized targets, we modified the model’s loss function to penalize localization errors above an angular distance threshold around each target. Using this loss we fine-tuned a model trained with the original loss, and also trained the same model from scratch. Results showed improved localization metrics in both models compared to baseline, while retaining classification metrics. Our results show that equal-angle discretization yields models with nonuniform localization errors for targets along the elevation axis. Finally, our proposed loss function penalizes the SELD model’s angular localization errors, regardless of which spatial discretization was used to annotate the dataset targets."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Labbe2022" style="box-shadow: none">
<div class="panel-heading" id="headingLabbe2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Is my Automatic Audio Captioning System so Bad? SPIDEr-max: A Metric to Consider Several Caption Candidates
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Etienne Labbé<sup>1</sup>, Thomas Pellegrini<sup>1</sup>, and Julien Pinquier<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IRIT, Université Paul Sabatier, CNRS, Toulouse, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          5 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Labbe2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Labbe_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Labbe2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Labbe2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Labbe2022" class="panel-collapse collapse" id="collapse-Labbe2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automatic Audio Captioning (AAC) is the task that aims to describe an audio signal using natural language. AAC systems take as input an audio signal and output a free-form text sentence, called a caption. Evaluating such systems is not trivial, since there are many ways to express the same idea. For this reason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr, are used to compare a single automatic caption to one or several captions of reference, produced by a human annotator. Nevertheless, an automatic system can produce several caption candidates, either using some randomness in the sentence generation process, or by considering the various competing hypothesized captions during decoding with beam-search, for instance. If we consider an end-user of an AAC system, presenting several captions instead of a single one seems relevant to provide some diversity, similarly to information retrieval systems. In this work, we explore the possibility to consider several predicted captions in the evaluation process instead of one. For this purpose, we propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the scores of several caption candidates. To advocate for our metric, we report experiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On AudioCaps for example, this system reached a SPIDEr-max value (with 5 candidates) close to the SPIDEr human score of reference.
      </p>
<p>
<strong>
        Cites:
       </strong>
       5 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Labbe2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Labbe_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Labbe2022label" class="modal fade" id="bibtex-Labbe2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLabbe2022label">
        Is my Automatic Audio Captioning System so Bad? SPIDEr-max: A Metric to Consider Several Caption Candidates
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Labbe2022,
    author = "Labb\'{e}, Etienne and Pellegrini, Thomas and Pinquier, Julien",
    title = "Is my Automatic Audio Captioning System so Bad? SPIDEr-max: A Metric to Consider Several Caption Candidates",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Automatic Audio Captioning (AAC) is the task that aims to describe an audio signal using natural language. AAC systems take as input an audio signal and output a free-form text sentence, called a caption. Evaluating such systems is not trivial, since there are many ways to express the same idea. For this reason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr, are used to compare a single automatic caption to one or several captions of reference, produced by a human annotator. Nevertheless, an automatic system can produce several caption candidates, either using some randomness in the sentence generation process, or by considering the various competing hypothesized captions during decoding with beam-search, for instance. If we consider an end-user of an AAC system, presenting several captions instead of a single one seems relevant to provide some diversity, similarly to information retrieval systems. In this work, we explore the possibility to consider several predicted captions in the evaluation process instead of one. For this purpose, we propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the scores of several caption candidates. To advocate for our metric, we report experiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On AudioCaps for example, this system reached a SPIDEr-max value (with 5 candidates) close to the SPIDEr human score of reference."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2022" style="box-shadow: none">
<div class="panel-heading" id="headingLee2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Multi-Scale Architecture and Device-Aware Data-Random-Drop Based Fine-Tuning Method for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Joo-Hyun Lee<sup>1</sup>, Jeong-Hwan Choi<sup>1</sup>, Pil Moo Byun<sup>1</sup>, and Joon-Hyuk Chang<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea
         </em>
</small>
</p>
<p class="text-left">
<span class="label label-success">
         Best paper award
        </span>
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          5 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Lee_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lee2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2022" class="panel-collapse collapse" id="collapse-Lee2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a low-complexity acoustic scene classification (ASC) model structure suitable for short-segmented audio and fine-tuning methods for generalization to multiple recording devices. Based on the state-of-the-art architecture of the ASC, broadcasting-ResNet (BC-ResNet), we introduce BC-Res2Net that uses hierarchical residual-like connections within the frequency- and temporal-wise convolutions to extract multiscale features while using fewer parameters. We also incorporate the attention and aggregation method proposed in short-utterance speaker verification with BC-Res2Net to achieve high performance. In addition, we train the model with a novel fine-tuning method using a device-aware data-random-drop to avoid optimization for only a few devices. When the amount of data differed for each device in the training dataset, the proposed method gradually dropped the data of the primary device from the mini-batch. The experimental results on the TAU Urban Acoustic Scenes 2022 Mobile development dataset demonstrated the effectiveness of multi-scale modeling in short audio. Furthermore, the proposed training strategy significantly reduced the multi-class cross-entropy loss for various devices.
      </p>
<p>
<strong>
        Awards:
       </strong>
       Best paper award
      </p>
<p>
<strong>
        Cites:
       </strong>
       5 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Lee_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2022label" class="modal fade" id="bibtex-Lee2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2022label">
        Multi-Scale Architecture and Device-Aware Data-Random-Drop Based Fine-Tuning Method for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2022,
    author = "Lee, Joo-Hyun and Choi, Jeong-Hwan and Byun, Pil Moo and Chang, Joon-Hyuk",
    title = "Multi-Scale Architecture and Device-Aware Data-Random-Drop Based Fine-Tuning Method for Acoustic Scene Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "We propose a low-complexity acoustic scene classification (ASC) model structure suitable for short-segmented audio and fine-tuning methods for generalization to multiple recording devices. Based on the state-of-the-art architecture of the ASC, broadcasting-ResNet (BC-ResNet), we introduce BC-Res2Net that uses hierarchical residual-like connections within the frequency- and temporal-wise convolutions to extract multiscale features while using fewer parameters. We also incorporate the attention and aggregation method proposed in short-utterance speaker verification with BC-Res2Net to achieve high performance. In addition, we train the model with a novel fine-tuning method using a device-aware data-random-drop to avoid optimization for only a few devices. When the amount of data differed for each device in the training dataset, the proposed method gradually dropped the data of the primary device from the mini-batch. The experimental results on the TAU Urban Acoustic Scenes 2022 Mobile development dataset demonstrated the effectiveness of multi-scale modeling in short audio. Furthermore, the proposed training strategy significantly reduced the multi-class cross-entropy loss for various devices."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022" style="box-shadow: none">
<div class="panel-heading" id="headingLi2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Hybrid System of Sound Event Detection Transformer and Frame-Wise Model for DCASE 2022 Task 4
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yiming Li<sup>1,2</sup>, Zhifang Guo<sup>1,2</sup>, Zhirong Ye<sup>1,2</sup>, Xiangdong Wang<sup>1</sup>, Hong Liu<sup>1</sup>, Yueliang Qian<sup>1</sup>, Rui Tao<sup>3</sup>, Long Yan<sup>3</sup>, and Kazushige Ouchi<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, <sup>2</sup>University of Chinese Academy of Sciences, Beijing, China, <sup>3</sup>Toshiba China R&amp;D Center, Beijing, China
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Li_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Li2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022" class="panel-collapse collapse" id="collapse-Li2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe in detail our system for DCASE 2022 Task4. The system combines two considerably different models: an end-to-end Sound Event Detection Transformer (SEDT) and a frame-wise model, Metric Learning and Focal Loss CNN (MLFL-CNN). The former is an event-wise model which learns event-level representations and predicts sound event categories and boundaries directly, while the latter is based on the widely-adopted frame-classification scheme, under which each frame is classified into event categories and event boundaries are obtained by postprocessing such as thresholding and smoothing. For SEDT, selfsupervised pre-training using unlabeled data is applied, and semisupervised learning is adopted by using an online teacher, which is updated from the student model using the Exponential Moving Average (EMA) strategy and generates reliable pseudo labels for weakly-labeled and unlabeled data. For the frame-wise model, the ICT-TOSHIBA system of DCASE 2021 Task 4 is used. Experimental results show that the hybrid system considerably outperforms either individual model, and achieves psds1 of 0.420 and psds2 of 0.783 on the validation set without external data. The code is available at https://github.com/965694547/Hybrid-system-of-frame-wise-model-and-SEDT.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Li_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022label" class="modal fade" id="bibtex-Li2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022label">
        A Hybrid System of Sound Event Detection Transformer and Frame-Wise Model for DCASE 2022 Task 4
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Li2022,
    author = "Li, Yiming and Guo, Zhifang and Ye, Zhirong and Wang, Xiangdong and Liu, Hong and Qian, Yueliang and Tao, Rui and Yan, Long and Ouchi, Kazushige",
    title = "A Hybrid System of Sound Event Detection Transformer and Frame-Wise Model for DCASE 2022 Task 4",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In this paper, we describe in detail our system for DCASE 2022 Task4. The system combines two considerably different models: an end-to-end Sound Event Detection Transformer (SEDT) and a frame-wise model, Metric Learning and Focal Loss CNN (MLFL-CNN). The former is an event-wise model which learns event-level representations and predicts sound event categories and boundaries directly, while the latter is based on the widely-adopted frame-classification scheme, under which each frame is classified into event categories and event boundaries are obtained by postprocessing such as thresholding and smoothing. For SEDT, selfsupervised pre-training using unlabeled data is applied, and semisupervised learning is adopted by using an online teacher, which is updated from the student model using the Exponential Moving Average (EMA) strategy and generates reliable pseudo labels for weakly-labeled and unlabeled data. For the frame-wise model, the ICT-TOSHIBA system of DCASE 2021 Task 4 is used. Experimental results show that the hybrid system considerably outperforms either individual model, and achieves psds1 of 0.420 and psds2 of 0.783 on the validation set without external data. The code is available at https://github.com/965694547/Hybrid-system-of-frame-wise-model-and-SEDT."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022-2" style="box-shadow: none">
<div class="panel-heading" id="headingLi2022-2" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Using Temporal Modulation Features on Gammatone Auditory Filterbank
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kai Li<sup>1</sup>, Quoc-Huy Nguyen<sup>1</sup>, Yasuji Ota<sup>1</sup>, and Masashi Unoki<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Information Science, Japan Advanced Institute of Science and Technology
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022-2" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Li_25.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Li2022-2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022-2" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022-2" class="panel-collapse collapse" id="collapse-Li2022-2" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Anomalous sound detection (ASD) is a technique to determine whether the sound emitted from a target machine is anomalous or not. Subjectively, timbral attributes, such as sharpness and roughness, are crucial cues for human beings to distinguish anomalous and normal sounds. However, the feature frequently used in existing methods for ASD is the log-Mel-spectrogram, which is difficult to capture temporal information. This paper proposes an ASD method using temporal modulation features on the gammatone auditory filterbank (TMGF) to provide temporal characteristics for machine-learning-based methods. We evaluated the proposed method using the area under the ROC curve (AUC) and the partial area under the ROC curve (pAUC) with sounds recorded from seven kinds of machines. Compared with the baseline method of the DCASE2022 challenge, the proposed method provides a better ability for domain generalization, especially for machine sounds recorded from the valve.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022-2" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Li_25.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022-2label" class="modal fade" id="bibtex-Li2022-2" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022-2label">
        Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Using Temporal Modulation Features on Gammatone Auditory Filterbank
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Li2022-2,
    author = "Li, Kai and Nguyen, Quoc-Huy and Ota, Yasuji and Unoki, Masashi",
    title = "Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Using Temporal Modulation Features on Gammatone Auditory Filterbank",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Anomalous sound detection (ASD) is a technique to determine whether the sound emitted from a target machine is anomalous or not. Subjectively, timbral attributes, such as sharpness and roughness, are crucial cues for human beings to distinguish anomalous and normal sounds. However, the feature frequently used in existing methods for ASD is the log-Mel-spectrogram, which is difficult to capture temporal information. This paper proposes an ASD method using temporal modulation features on the gammatone auditory filterbank (TMGF) to provide temporal characteristics for machine-learning-based methods. We evaluated the proposed method using the area under the ROC curve (AUC) and the partial area under the ROC curve (pAUC) with sounds recorded from seven kinds of machines. Compared with the baseline method of the DCASE2022 challenge, the proposed method provides a better ability for domain generalization, especially for machine sounds recorded from the valve."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2022-3" style="box-shadow: none">
<div class="panel-heading" id="headingLi2022-3" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Few-Shot Bioacoustic Event Detection: Enhanced Classifiers for Prototypical Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ren Li<sup>1</sup>, Jinhua Liang<sup>1</sup>, and Huy Phan<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom, <sup>2</sup>The Alan Turing Institute, United Kingdom
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2022-3" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Li_50.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Li2022-3" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2022-3" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2022-3" class="panel-collapse collapse" id="collapse-Li2022-3" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Few-shot learning has emerged as a novel approach to bioacoustic event detection since it is useful when training data is insufficient, and the cost of labelling data is high. In this paper, we explore the Prototypical Networks for developing a few-shot learning system to detect mammal and bird sounds from audio recordings. To en-hance the deep networks, we use a ResNet-18 variant as the clas-sifier, which can learn the embedding mapping better with stronger architecture. Another method is proposed to focus on do-main shift problem during learning the embedding by taking ad-vantage of autoencoders to learn the low-dimensional representa-tions of input data. A reconstruction loss is added to the training loss to perform regularization. We also utilize various data augmentation techniques to boost the performance. Our proposed sys-tems are evaluated on the validation set of DCASE 2022 task 5 and improve the F1-score from 29.59% to 47.88%.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2022-3" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Li_50.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2022-3label" class="modal fade" id="bibtex-Li2022-3" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2022-3label">
        Few-Shot Bioacoustic Event Detection: Enhanced Classifiers for Prototypical Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Li2022-3,
    author = "Li, Ren and Liang, Jinhua and Phan, Huy",
    title = "Few-Shot Bioacoustic Event Detection: Enhanced Classifiers for Prototypical Networks",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Few-shot learning has emerged as a novel approach to bioacoustic event detection since it is useful when training data is insufficient, and the cost of labelling data is high. In this paper, we explore the Prototypical Networks for developing a few-shot learning system to detect mammal and bird sounds from audio recordings. To en-hance the deep networks, we use a ResNet-18 variant as the clas-sifier, which can learn the embedding mapping better with stronger architecture. Another method is proposed to focus on do-main shift problem during learning the embedding by taking ad-vantage of autoencoders to learn the low-dimensional representa-tions of input data. A reconstruction loss is added to the training loss to perform regularization. We also utilize various data augmentation techniques to boost the performance. Our proposed sys-tems are evaluated on the validation set of DCASE 2022 task 5 and improve the F1-score from 29.59\% to 47.88\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liang2022" style="box-shadow: none">
<div class="panel-heading" id="headingLiang2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Leveraging Label Hierachies for Few-Shot Everyday Sound Recognition
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jinhua Liang<sup>1</sup>, Huy Phan<sup>1</sup>, and Emmanouil Benetos<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Digital Music, Queen Mary University of London, United Kingdom
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Liang_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Liang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liang2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liang2022" class="panel-collapse collapse" id="collapse-Liang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Everyday sounds cover a considerable range of sound categories in our daily life, yet for certain sound categories it is hard to collect sufficient data. Although existing works have applied few-shot learning paradigms to sound recognition successfully, most of them have not exploited the relationship between labels in audio taxonomies. This work adopts a hierarchical prototypical network to leverage the knowledge rooted in audio taxonomies. Specifically, a VGG-like convolutional neural network is used to extract acoustic features. Prototypical nodes are then calculated in each level of the tree structure. A multi-level loss is obtained by multiplying a weight decay with multiple losses. Experimental results demonstrate our hierarchical prototypical networks not only outperform prototypical networks with no hierarchy information but yield a better result than other state-of-the-art algorithms. Our code is available in: https://github.com/JinhuaLiang/HPNs_tagging
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Liang_5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liang2022label" class="modal fade" id="bibtex-Liang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiang2022label">
        Leveraging Label Hierachies for Few-Shot Everyday Sound Recognition
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Liang2022,
    author = "Liang, Jinhua and Phan, Huy and Benetos, Emmanouil",
    title = "Leveraging Label Hierachies for Few-Shot Everyday Sound Recognition",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Everyday sounds cover a considerable range of sound categories in our daily life, yet for certain sound categories it is hard to collect sufficient data. Although existing works have applied few-shot learning paradigms to sound recognition successfully, most of them have not exploited the relationship between labels in audio taxonomies. This work adopts a hierarchical prototypical network to leverage the knowledge rooted in audio taxonomies. Specifically, a VGG-like convolutional neural network is used to extract acoustic features. Prototypical nodes are then calculated in each level of the tree structure. A multi-level loss is obtained by multiplying a weight decay with multiple losses. Experimental results demonstrate our hierarchical prototypical networks not only outperform prototypical networks with no hierarchy information but yield a better result than other state-of-the-art algorithms. Our code is available in: https://github.com/JinhuaLiang/HPNs\_tagging"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2022" style="box-shadow: none">
<div class="panel-heading" id="headingLiu2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Segment-Level Metric Learning for Few-Shot Bioacoustic Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Haohe Liu<sup>1</sup>, Xubo Liu<sup>1</sup>, Xinhao Mei<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Wenwu Wang<sup>1</sup>, Mark D. Plumbley<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Vision, Speech, and Signal Processing (CVSSP), University of Surrey, UK, <sup>2</sup>Speech, Audio, and Music Intelligence (SAMI) Group, ByteDance, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          5 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Liu_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Liu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2022" class="panel-collapse collapse" id="collapse-Liu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Few-shot bioacoustic event detection is a task that detects the occurrence time of a novel sound given a few examples. Previous methods employ metric learning to build a latent space with the labeled part of different sound classes, also known as positive events. In this study, we propose a segment-level few-shot learning framework that utilizes both the positive and negative events during model optimization. Training with negative events, which are larger in volume than positive events, can increase the generalization ability of the model. In addition, we use transductive learning on the validation set during training for better adaptation to novel classes. We conduct ablation studies on our proposed method with different setups on input features, training data, and hyper-parameters. Our final system achieves an F-measure of 62.73 on the DCASE 2022 challenge task 5 (DCASE2022-T5) validation set, outperforming the performance of the baseline prototypical network 34.02 by a large margin. Using the proposed method, our submitted system ranks 2nd in DCASE2022-T5 with an F-measure of 48.2 on the evaluation set. The code of this paper is open-sourced.
      </p>
<p>
<strong>
        Cites:
       </strong>
       5 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Liu_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2022label" class="modal fade" id="bibtex-Liu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2022label">
        Segment-Level Metric Learning for Few-Shot Bioacoustic Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Liu2022,
    author = "Liu, Haohe and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D.",
    title = "Segment-Level Metric Learning for Few-Shot Bioacoustic Event Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Few-shot bioacoustic event detection is a task that detects the occurrence time of a novel sound given a few examples. Previous methods employ metric learning to build a latent space with the labeled part of different sound classes, also known as positive events. In this study, we propose a segment-level few-shot learning framework that utilizes both the positive and negative events during model optimization. Training with negative events, which are larger in volume than positive events, can increase the generalization ability of the model. In addition, we use transductive learning on the validation set during training for better adaptation to novel classes. We conduct ablation studies on our proposed method with different setups on input features, training data, and hyper-parameters. Our final system achieves an F-measure of 62.73 on the DCASE 2022 challenge task 5 (DCASE2022-T5) validation set, outperforming the performance of the baseline prototypical network 34.02 by a large margin. Using the proposed method, our submitted system ranks 2nd in DCASE2022-T5 with an F-measure of 48.2 on the evaluation set. The code of this paper is open-sourced."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mai2022" style="box-shadow: none">
<div class="panel-heading" id="headingMai2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Explaining the Decision of Anomalous Sound Detectors
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kimberly T. Mai<sup>1,3</sup>, Toby Davies<sup>1,3</sup>, Lewis D. Griffin<sup>1</sup>, and Emmanouil Benetos<sup>2,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>University College London, UK, <sup>2</sup>Queen Mary University of London, UK, <sup>3</sup>The Alan Turing Institute, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mai2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Mai_2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Mai2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mai2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mai2022" class="panel-collapse collapse" id="collapse-Mai2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deciding whether a sound is anomalous is accomplished by comparing it to a learnt distribution of inliers. Therefore, learning a distribution close to the true population of inliers is vital for anomalous sound detection (ASD). Data engineering is a common strategy to aid training and improve generalisation. However, in the context of ASD, it is debatable whether data engineering indeed facilitates generalisation or whether it obscures characteristics that distinguish anomalies from inliers. We conduct an exploratory investigation into this by focusing on frequency-related data engineering. We adapt local model explanations to anomaly detectors and show that models rely on higher frequencies to distinguish anomalies from inliers. We verify this by filtering the input data’s frequencies and observing the change in ASD performance. Our results indicate that sifting out low frequencies by applying high-pass filters aids downstream performance, and this could serve as a simple pre-processing step for improving anomaly detectors.
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mai2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Mai_2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mai2022label" class="modal fade" id="bibtex-Mai2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMai2022label">
        Explaining the Decision of Anomalous Sound Detectors
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mai2022,
    author = "Mai, Kimberly T. and Davies, Toby and Griffin, Lewis D. and Benetos, Emmanouil",
    title = "Explaining the Decision of Anomalous Sound Detectors",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Deciding whether a sound is anomalous is accomplished by comparing it to a learnt distribution of inliers. Therefore, learning a distribution close to the true population of inliers is vital for anomalous sound detection (ASD). Data engineering is a common strategy to aid training and improve generalisation. However, in the context of ASD, it is debatable whether data engineering indeed facilitates generalisation or whether it obscures characteristics that distinguish anomalies from inliers. We conduct an exploratory investigation into this by focusing on frequency-related data engineering. We adapt local model explanations to anomaly detectors and show that models rely on higher frequencies to distinguish anomalies from inliers. We verify this by filtering the input data’s frequencies and observing the change in ASD performance. Our results indicate that sifting out low frequencies by applying high-pass filters aids downstream performance, and this could serve as a simple pre-processing step for improving anomaly detectors."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mars2022" style="box-shadow: none">
<div class="panel-heading" id="headingMars2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Device Classification-Aided Multi-Task Framework for Low-Complexity Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Rohith Mars<sup>1</sup> and Rohan Kumar Das<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Fortemedia Singapore, Singapore
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mars2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Mars_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Mars2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mars2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mars2022" class="panel-collapse collapse" id="collapse-Mars2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we investigate the use of a multi-task learning framework to address the DCASE 2022 Task 1 on Low-complexity Acoustic Scene Classification (ASC). Specifically, we employ classification of the recording devices as an additional task to improve the performance of the ASC task. Both these tasks utilize our proposed convolutional neural network with a shared layer along with strided and separable convolution operations designed to comply with the model parameter and computational constraints imposed by Task 1 organizers. We also explore the use of various data augmentation techniques to improve the generalization of the model. Evaluations on the development dataset show that our proposed ASC system consisting of 127.2k parameters with 27.5 million multiply and accumulate operations provides a significant improvement on overall log-loss and accuracy over the baseline system.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mars2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Mars_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mars2022label" class="modal fade" id="bibtex-Mars2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMars2022label">
        A Device Classification-Aided Multi-Task Framework for Low-Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mars2022,
    author = "Mars, Rohith and Das, Rohan Kumar",
    title = "A Device Classification-Aided Multi-Task Framework for Low-Complexity Acoustic Scene Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In this paper, we investigate the use of a multi-task learning framework to address the DCASE 2022 Task 1 on Low-complexity Acoustic Scene Classification (ASC). Specifically, we employ classification of the recording devices as an additional task to improve the performance of the ASC task. Both these tasks utilize our proposed convolutional neural network with a shared layer along with strided and separable convolution operations designed to comply with the model parameter and computational constraints imposed by Task 1 organizers. We also explore the use of various data augmentation techniques to improve the generalization of the model. Evaluations on the development dataset show that our proposed ASC system consisting of 127.2k parameters with 27.5 million multiply and accumulate operations provides a significant improvement on overall log-loss and accuracy over the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Martin-Morato2022" style="box-shadow: none">
<div class="panel-heading" id="headingMartin-Morato2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Low-Complexity Acoustic Scene Classification in DCASE 2022 Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Irene Martín-Morató<sup>1</sup>, Francesco Paissan<sup>2</sup>, Alberto Ancilotto<sup>2</sup>, Toni Heittola<sup>1</sup>, Annamaria Mesaros<sup>1</sup>, Elisabetta Farella<sup>2</sup>, Alessio Brutti<sup>2</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Computing Sciences, Tampere University, Finland, <sup>2</sup>Fondazione Bruno Kessler, Italy
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          30 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Martin-Morato2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Martin-Morato_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Martin-Morato2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Martin-Morato2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Martin-Morato2022" class="panel-collapse collapse" id="collapse-Martin-Morato2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents an analysis of the Low-Complexity Acoustic Scene Classification task in DCASE 2022 Challenge. The task was a continuation from the previous years, but the low-complexity requirements were changed to the following: the maximum number of allowed parameters, including the zero-valued ones, was 128 K, with parameters being represented using INT8 numerical format; and the maximum number of multiply-accumulate operations at inference time was 30 million. Despite using the same previous year dataset, the audio samples have been shortened to 1 second instead of 10 second for this year challenge. The provided baseline system is a convolutional neural network which employs post-training quantization of parameters, resulting in 46.5 K parameters, and 29.23 million multiply-and-accumulate operations (MMACs). Its performance on the evaluation data is 44.2% accuracy and 1.532 log-loss. In comparison, the top system in the challenge obtained an accuracy of 59.6% and a log loss of 1.091, having 121 K parameters and 28 MMACs. The task received 48 submissions from 19 different teams, most of which outperformed the baseline system.
      </p>
<p>
<strong>
        Cites:
       </strong>
       30 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Martin-Morato2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Martin-Morato_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Martin-Morato2022label" class="modal fade" id="bibtex-Martin-Morato2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMartin-Morato2022label">
        Low-Complexity Acoustic Scene Classification in DCASE 2022 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Martin-Morato2022,
    author = "Mart\'{i}n-Morat\'{o}, Irene and Paissan, Francesco and Ancilotto, Alberto and Heittola, Toni and Mesaros, Annamaria and Farella, Elisabetta and Brutti, Alessio and Virtanen, Tuomas",
    title = "Low-Complexity Acoustic Scene Classification in DCASE 2022 Challenge",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "This paper presents an analysis of the Low-Complexity Acoustic Scene Classification task in DCASE 2022 Challenge. The task was a continuation from the previous years, but the low-complexity requirements were changed to the following: the maximum number of allowed parameters, including the zero-valued ones, was 128 K, with parameters being represented using INT8 numerical format; and the maximum number of multiply-accumulate operations at inference time was 30 million. Despite using the same previous year dataset, the audio samples have been shortened to 1 second instead of 10 second for this year challenge. The provided baseline system is a convolutional neural network which employs post-training quantization of parameters, resulting in 46.5 K parameters, and 29.23 million multiply-and-accumulate operations (MMACs). Its performance on the evaluation data is 44.2\% accuracy and 1.532 log-loss. In comparison, the top system in the challenge obtained an accuracy of 59.6\% and a log loss of 1.091, having 121 K parameters and 28 MMACs. The task received 48 submissions from 19 different teams, most of which outperformed the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Martin-Morato2022-2" style="box-shadow: none">
<div class="panel-heading" id="headingMartin-Morato2022-2" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Summarization Approach to Evaluating Audio Captioning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Irene Martín-Morató<sup>1</sup>, Manu Harju<sup>1</sup>, and Annamaria Mesaros<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Computing Sciences, Tampere University, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Martin-Morato2022-2" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Martin-Morato_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Martin-Morato2022-2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Martin-Morato2022-2" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Martin-Morato2022-2" class="panel-collapse collapse" id="collapse-Martin-Morato2022-2" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio captioning is currently evaluated with metrics originating from machine translation and image captioning, but their suitability for audio has recently been questioned. This work proposes content-based scoring of audio captions, an approach that considers the specific sound events content of the captions. Inspired from text summarization, the proposed measure gives relevance scores to the sound events present in the reference, and scores candidates based on the relevance of the retrieved sounds. In this work we use a simple, consensus-based definition of relevance, but different weighing schemes can be easily incorporated to change the importance of terms accordingly. Our experiments use two datasets and three different audio captioning systems and show that the proposed measure behaves consistently with the data: captions that correctly capture the most relevant sounds obtain a score of 1, while the ones containing less relevant sounds score lower. While the proposed content-based score is not concerned with the fluency or semantic content of the captions, it can be incorporated into a compound metric, similar to SPIDEr being a linear combination of a semantic and a syntactic fluency score.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Martin-Morato2022-2" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Martin-Morato_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Martin-Morato2022-2label" class="modal fade" id="bibtex-Martin-Morato2022-2" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMartin-Morato2022-2label">
        A Summarization Approach to Evaluating Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Martin-Morato2022-2,
    author = "Mart\'{i}n-Morat\'{o}, Irene and Harju, Manu and Mesaros, Annamaria",
    title = "A Summarization Approach to Evaluating Audio Captioning",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Audio captioning is currently evaluated with metrics originating from machine translation and image captioning, but their suitability for audio has recently been questioned. This work proposes content-based scoring of audio captions, an approach that considers the specific sound events content of the captions. Inspired from text summarization, the proposed measure gives relevance scores to the sound events present in the reference, and scores candidates based on the relevance of the retrieved sounds. In this work we use a simple, consensus-based definition of relevance, but different weighing schemes can be easily incorporated to change the importance of terms accordingly. Our experiments use two datasets and three different audio captioning systems and show that the proposed measure behaves consistently with the data: captions that correctly capture the most relevant sounds obtain a score of 1, while the ones containing less relevant sounds score lower. While the proposed content-based score is not concerned with the fluency or semantic content of the captions, it can be incorporated into a compound metric, similar to SPIDEr being a linear combination of a semantic and a syntactic fluency score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Martinsson2022" style="box-shadow: none">
<div class="panel-heading" id="headingMartinsson2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Few-Shot Bioacoustic Event Detection Using an Event-Length Adapted Ensemble of Prototypical Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         John Martinsson<sup>1,2</sup>, Martin Willbo<sup>1</sup>, Aleksis Pirinen<sup>1</sup>, Olof Mogren<sup>1</sup>, and Maria Sandsten<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Computer Science, RISE Research Institutes of Sweden, Sweden, <sup>2</sup>Centre for Mathematical Sciences, Lund University, Sweden
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Martinsson2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Martinsson_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Martinsson2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Martinsson2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Martinsson2022" class="panel-collapse collapse" id="collapse-Martinsson2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper we study two major challenges in few-shot bioacoustic event detection: variable event lengths and false-positives. We use prototypical networks where the embedding function is trained using a multi-label sound event detection model instead of using episodic training as the proxy task on the provided training dataset. This is motivated by polyphonic sound events being present in the base training data. We propose a method to choose the embedding function based on the average event length of the few-shot examples and show that this makes the method more robust towards variable event lengths. Further, we show that an ensemble of prototypical neural networks trained on different training and validation splits of time-frequency images with different loudness normalizations reduces false-positives. In addition, we present an analysis on the effect that the studied loudness normalization techniques have on the performance of the prototypical network ensemble. Overall, per-channel energy normalization (PCEN) outperforms the standard log transform for this task. The method uses no data augmentation and no external data. The proposed approach achieves a F-score of 48.0% when evaluated on the hidden test set of the Detection and Classification of Acoustic Scenes and Events (DCASE) task 5.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Martinsson2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Martinsson_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Martinsson2022label" class="modal fade" id="bibtex-Martinsson2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMartinsson2022label">
        Few-Shot Bioacoustic Event Detection Using an Event-Length Adapted Ensemble of Prototypical Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Martinsson2022,
    author = "Martinsson, John and Willbo, Martin and Pirinen, Aleksis and Mogren, Olof and Sandsten, Maria",
    title = "Few-Shot Bioacoustic Event Detection Using an Event-Length Adapted Ensemble of Prototypical Networks",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "In this paper we study two major challenges in few-shot bioacoustic event detection: variable event lengths and false-positives. We use prototypical networks where the embedding function is trained using a multi-label sound event detection model instead of using episodic training as the proxy task on the provided training dataset. This is motivated by polyphonic sound events being present in the base training data. We propose a method to choose the embedding function based on the average event length of the few-shot examples and show that this makes the method more robust towards variable event lengths. Further, we show that an ensemble of prototypical neural networks trained on different training and validation splits of time-frequency images with different loudness normalizations reduces false-positives. In addition, we present an analysis on the effect that the studied loudness normalization techniques have on the performance of the prototypical network ensemble. Overall, per-channel energy normalization (PCEN) outperforms the standard log transform for this task. The method uses no data augmentation and no external data. The proposed approach achieves a F-score of 48.0\% when evaluated on the hidden test set of the Detection and Classification of Acoustic Scenes and Events (DCASE) task 5."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Napoli2022" style="box-shadow: none">
<div class="panel-heading" id="headingNapoli2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Quantity Over Quality? Investigating the Effects of Volume and Strength of Training Data in Marine Bioacoustics
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Andrea Napoli<sup>1</sup>, Paul R. White<sup>1</sup>, and Thomas Blumensath<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Sound and Vibration Research, University of Southampton, UK
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Napoli2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Napoli_48.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Napoli2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Napoli2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Napoli2022" class="panel-collapse collapse" id="collapse-Napoli2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The trade-off between the quality and quantity of training data is considered for the detection of minke whale (Balaenoptera acutorostrata) vocalisations. The performance of two different detectors is measured across a range of label strengths using training sets of different sizes. A detector based on spectrogram correlation and a convolutional neural network (CNN) are considered. The results show that increasing label strength does not benefit either detector past a certain point, corresponding here to a label density of 60 to 70%. Performance is found to be good even when labels are extremely weak (4% label density). Additionally, it is noted that performance of the spectrogram correlation plateaus beyond the use of 5 training calls, whereas the CNN’s performance continues to increase up to the maximum training set size tested. Finally, interaction effects are observed between label strength and quantity, indicating that larger training sets are more robust to weaker labels. Overall, these findings suggest that there is indeed a benefit to collecting more, lower quality data when training a CNN, but that for a correlation-based detector this is not the case.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Napoli2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Napoli_48.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Napoli2022label" class="modal fade" id="bibtex-Napoli2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNapoli2022label">
        Quantity Over Quality? Investigating the Effects of Volume and Strength of Training Data in Marine Bioacoustics
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Napoli2022,
    author = "Napoli, Andrea and White, Paul R. and Blumensath, Thomas",
    title = "Quantity Over Quality? Investigating the Effects of Volume and Strength of Training Data in Marine Bioacoustics",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "The trade-off between the quality and quantity of training data is considered for the detection of minke whale (Balaenoptera acutorostrata) vocalisations. The performance of two different detectors is measured across a range of label strengths using training sets of different sizes. A detector based on spectrogram correlation and a convolutional neural network (CNN) are considered. The results show that increasing label strength does not benefit either detector past a certain point, corresponding here to a label density of 60 to 70\%. Performance is found to be good even when labels are extremely weak (4\% label density). Additionally, it is noted that performance of the spectrogram correlation plateaus beyond the use of 5 training calls, whereas the CNN’s performance continues to increase up to the maximum training set size tested. Finally, interaction effects are observed between label strength and quantity, indicating that larger training sets are more robust to weaker labels. Overall, these findings suggest that there is indeed a benefit to collecting more, lower quality data when training a CNN, but that for a correlation-based detector this is not the case."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nejjar2022" style="box-shadow: none">
<div class="panel-heading" id="headingNejjar2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DG-Mix: Domain Generalization for Anomalous Sound Detection Based on Self-Supervised Learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ismail Nejjar<sup>1,2</sup>, Jean Meunier-Pion<sup>1,3</sup>, Gaetan Frusque<sup>1</sup>, and Olga Fink<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>EPFL, IMOS, Lausanne, Switerzland, <sup>2</sup>ETH Zürich, Chair of Intelligent Maintenance Systems, Zürich, Switzerland, <sup>3</sup>CentraleSupélec, Gif-sur-Yvette, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nejjar2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Nejjar_31.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Nejjar2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nejjar2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nejjar2022" class="panel-collapse collapse" id="collapse-Nejjar2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Detecting anomalies in sound data has recently received significant attention due to the increasing number of implementations of sound condition monitoring solutions for critical assets. In this context, changing operating conditions impose significant domain shifts resulting in performance drops if a model trained on a set of operating conditions is applied to a new operating condition. An essential challenge is distinguishing between anomalies due to faults and new operating conditions. Therefore, the high variability of operating conditions or even the emergence of new operating conditions requires algorithms that can be applied under all conditions. Therefore, domain generalization approaches need to be developed to tackle this challenge. In this paper, we propose a novel framework that leads to a representation that separates the health state from changes in operating conditions in the latent space. This research introduces DG-Mix (Domain Generalization Mixup), an algorithm inspired by the recent Variance-Invariance-Covariance Regularization (VICReg) framework. Extending the original VICReg algorithm, we propose to use Mixup between two samples of the same machine type as a transformation and apply a geometric constraint instead of an invariance loss. This approach allows us to learn a representation that distinguishes between the operating conditions in an unsupervised way. The proposed DG-Mix enables the generalization between different machine types and diverse operating conditions without an additional adaptation of the hyperparameters or an ensemble method. DG-Mix provides superior performance and outperforms the baselines on the development dataset of DCASE 2022 challenge task 2. We also demonstrate that training using DG-Mix and then fine-tuning the model to a specific task significantly improves the model’s performance.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nejjar2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Nejjar_31.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nejjar2022label" class="modal fade" id="bibtex-Nejjar2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNejjar2022label">
        DG-Mix: Domain Generalization for Anomalous Sound Detection Based on Self-Supervised Learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nejjar2022,
    author = "Nejjar, Ismail and Meunier-Pion, Jean and Frusque, Gaetan and Fink, Olga",
    title = "DG-Mix: Domain Generalization for Anomalous Sound Detection Based on Self-Supervised Learning",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Detecting anomalies in sound data has recently received significant attention due to the increasing number of implementations of sound condition monitoring solutions for critical assets. In this context, changing operating conditions impose significant domain shifts resulting in performance drops if a model trained on a set of operating conditions is applied to a new operating condition. An essential challenge is distinguishing between anomalies due to faults and new operating conditions. Therefore, the high variability of operating conditions or even the emergence of new operating conditions requires algorithms that can be applied under all conditions. Therefore, domain generalization approaches need to be developed to tackle this challenge. In this paper, we propose a novel framework that leads to a representation that separates the health state from changes in operating conditions in the latent space. This research introduces DG-Mix (Domain Generalization Mixup), an algorithm inspired by the recent Variance-Invariance-Covariance Regularization (VICReg) framework. Extending the original VICReg algorithm, we propose to use Mixup between two samples of the same machine type as a transformation and apply a geometric constraint instead of an invariance loss. This approach allows us to learn a representation that distinguishes between the operating conditions in an unsupervised way. The proposed DG-Mix enables the generalization between different machine types and diverse operating conditions without an additional adaptation of the hyperparameters or an ensemble method. DG-Mix provides superior performance and outperforms the baselines on the development dataset of DCASE 2022 challenge task 2. We also demonstrate that training using DG-Mix and then fine-tuning the model to a specific task significantly improves the model’s performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nolasco2022" style="box-shadow: none">
<div class="panel-heading" id="headingNolasco2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Few-Shot Bioacoustic Event Detection at the DCASE 2022 Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         I. Nolasco<sup>1</sup>, S. Singh<sup>1</sup>, E. Vidaña-Vila <sup>2</sup>, E. Grout<sup>3,4</sup>, J. Morford<sup>5</sup>, M.G. Emmerson<sup>6</sup> F. H. Jensen<sup>7</sup>, I. Kiskin<sup>8</sup>, H. Whitehead<sup>9</sup>, A. Strandburg-Peshkin<sup>3,4</sup>, L. Gill<sup>10</sup>, H. Pamuła<sup>11</sup>, V. Lostanlen<sup>12</sup>, V. Morfi<sup>1,13</sup>, and D. Stowell<sup>14</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Digital Music (C4DM), Queen Mary University of London, London, UK, <sup>2</sup>La Salle, University Ramon Llull, Barcelona, ES, <sup>3</sup>Dept. of Biology &amp; Centre for the Advanced Study of Collective Behaviour, University of Konstanz, DE, <sup>4</sup>Dept. for the Ecology of Animal Societies, Max Planck Institute of Animal Behavior, DE, <sup>5</sup>The Oxford Navigation group, Dept. of Zoology, Oxford University, Oxford, UK, <sup>6</sup>School of Biological and Behavioural Sciences, Queen Mary University of London, London, UK, <sup>7</sup>Biology Dept, Syracuse University, NY, USA, <sup>8</sup>Institute for People-Centred AI, FHMS, University of Surrey, Surrey, UK, <sup>9</sup>School of Science, Engineering and Environment, University of Salford, Manchester, UK, <sup>10</sup>BIOTOPIA Naturkundemuseum Bayern, Munich, DE, <sup>11</sup>AGH University of Science and Technology, Kraków, PL, <sup>12</sup>Nantes Université, École Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, FR, <sup>13</sup>Sonantic Limited, London, UK, <sup>14</sup>Tilburg University, Tilburg, The Netherlands; Naturalis Biodiversity Centre, Leiden, NL
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          14 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nolasco2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Nolasco_14.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Nolasco2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nolasco2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nolasco2022" class="panel-collapse collapse" id="collapse-Nolasco2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Few-shot sound event detection is the task of detecting sound events, despite having only a few labelled examples of the class of interest. This framework is particularly useful in bioacoustics, where often there is a need to annotate very long recordings but the expert annotator time is limited. This paper presents an overview of the second edition of the few-shot bioacoustic sound event detection task included in the DCASE 2022 challenge. A detailed description of the task objectives, dataset, and baselines is presented, together with the main results obtained and characteristics of the submitted systems. This task received submissions from 15 different teams from which 13 scored higher than the baselines. The highest Fscore was of 60.2% on the evaluation set, which leads to a huge improvement over last year’s edition. Highly-performing methods made use of prototypical networks, transductive learning, and addressed the variable length of events from all target classes. Furthermore, by analysing results on each of the subsets we can identify the main difficulties that the systems face, and conclude that few-show bioacoustic sound event detection remains an open challenge.
      </p>
<p>
<strong>
        Cites:
       </strong>
       14 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nolasco2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Nolasco_14.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nolasco2022label" class="modal fade" id="bibtex-Nolasco2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNolasco2022label">
        Few-Shot Bioacoustic Event Detection at the DCASE 2022 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nolasco2022,
    author = "Nolasco, I. and Singh, S. and Vida\l{}a-Vila, E. and Grout, E. and Morford, J. and Emmerson, M.G. and Jensen, F. H. and Kiskin, I. and Whitehead, H. and Strandburg-Peshkin, A. and Gill, L. and Pamu\l{}a, H. and Lostanlen, V. and Morfi, V. and Stowell, D.",
    title = "Few-Shot Bioacoustic Event Detection at the DCASE 2022 Challenge",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Few-shot sound event detection is the task of detecting sound events, despite having only a few labelled examples of the class of interest. This framework is particularly useful in bioacoustics, where often there is a need to annotate very long recordings but the expert annotator time is limited. This paper presents an overview of the second edition of the few-shot bioacoustic sound event detection task included in the DCASE 2022 challenge. A detailed description of the task objectives, dataset, and baselines is presented, together with the main results obtained and characteristics of the submitted systems. This task received submissions from 15 different teams from which 13 scored higher than the baselines. The highest Fscore was of 60.2\% on the evaluation set, which leads to a huge improvement over last year’s edition. Highly-performing methods made use of prototypical networks, transductive learning, and addressed the variable length of events from all target classes. Furthermore, by analysing results on each of the subsets we can identify the main difficulties that the systems face, and conclude that few-show bioacoustic sound event detection remains an open challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Outidrarine2022" style="box-shadow: none">
<div class="panel-heading" id="headingOutidrarine2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Exploring Eco-Acoustic Data with K-Determinantal Point Processes
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Mohamed Outidrarine<sup>1</sup>, Pierre Baudet<sup>1</sup>, Vincent Lostanlen<sup>1</sup>, Mathieu Lagrange<sup>1</sup>, and Juan Sebastián Ulloa<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nantes Université, École Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France, <sup>2</sup>Instituto de Investigación de Recursos Biológicos Alexander von Humboldt, Bogotá, Colombia
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Outidrarine2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Outidrarine_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Outidrarine2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Outidrarine2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Outidrarine2022" class="panel-collapse collapse" id="collapse-Outidrarine2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The deployment of acoustic sensor networks in a natural environment contributes to the understanding and the conservation of biodiversity. Yet, the sheer size of audio data which result from these recordings prevents listening them in full. In order to skim through an ecoacoustic corpus, one may typically draw K snippets uniformly at random. In this article, we present an alternative method, based on K-determinantal point processes (K-DPP). This method weights the sampling of K-tuples according to a two-fold criterion of relevance and diversity. To study the eco-acoustics of a tropical dry forest in Colombia, we define relevance in terms of time–frequency second derivative (TFSD) and diversity in terms of scattering transform. Hence, we show that K-DPP offers a better tradeoff than K-means clustering. Furthermore, we estimate the species richness of the K selected snippets by means of the BirdNET birdsong classifier, which is based on a deep neural network. We find that, for K &gt; 10, K-DPP and K-means tend to produce a species checklist that is richer than sampling K snippets independently without replacement.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Outidrarine2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Outidrarine_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Outidrarine2022label" class="modal fade" id="bibtex-Outidrarine2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexOutidrarine2022label">
        Exploring Eco-Acoustic Data with K-Determinantal Point Processes
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Outidrarine2022,
    author = "Outidrarine, Mohamed and Baudet, Pierre and Lostanlen, Vincent and Lagrange, Mathieu and Ulloa, Juan Sebasti\'{a}n",
    title = "Exploring Eco-Acoustic Data with K-Determinantal Point Processes",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "The deployment of acoustic sensor networks in a natural environment contributes to the understanding and the conservation of biodiversity. Yet, the sheer size of audio data which result from these recordings prevents listening them in full. In order to skim through an ecoacoustic corpus, one may typically draw K snippets uniformly at random. In this article, we present an alternative method, based on K-determinantal point processes (K-DPP). This method weights the sampling of K-tuples according to a two-fold criterion of relevance and diversity. To study the eco-acoustics of a tropical dry forest in Colombia, we define relevance in terms of time–frequency second derivative (TFSD) and diversity in terms of scattering transform. Hence, we show that K-DPP offers a better tradeoff than K-means clustering. Furthermore, we estimate the species richness of the K selected snippets by means of the BirdNET birdsong classifier, which is based on a deep neural network. We find that, for K &gt; 10, K-DPP and K-means tend to produce a species checklist that is richer than sampling K snippets independently without replacement."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Parrilla2022" style="box-shadow: none">
<div class="panel-heading" id="headingParrilla2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Polyphonic Sound Event Detection for Highly Dense Birdsong Scenes
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Alberto García Arroba Parrilla<sup>1</sup>, and Dan Stowell<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Tilburg University, the Netherlands, <sup>2</sup>Naturalis Biodiversity Center, Leiden, the Netherlands
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Parrilla2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Parrilla_12.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Parrilla2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Parrilla2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Parrilla2022" class="panel-collapse collapse" id="collapse-Parrilla2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       One hour before sunrise, one can experience the dawn chorus where birds from different species sing together. In this scenario, high levels of polyphony, as in the number of overlapping sound sources, are prone to happen resulting in a complex acoustic out-come. Sound Event Detection (SED) tasks analyze acoustic sce-narios in order to identify the occurring events and their respective temporal information. However, highly dense scenarios can be hard to process and have not been studied in depth. Here we show, using a Convolutional Recurrent Neural Network (CRNN), how birdsong polyphonic scenarios can be detected when dealing with higher polyphony and how effectively this type of model can face a very dense scene with up to 10 overlapping birds. We found that models trained with denser examples (i.e., higher polyphony) learn at a similar rate as models that used simpler samples in their train-ing set. Additionally, the model trained with the densest samples maintained a consistent score for all polyphonies, while the model trained with the least dense samples degraded as the polyphony increased. Our results demonstrate that highly dense acoustic sce-narios can be dealt with using CRNNs. We expect that this study serves as a starting point for working on highly populated bird sce-narios such as dawn chorus or other dense acoustic problems.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Parrilla2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Parrilla_12.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP5.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Parrilla2022label" class="modal fade" id="bibtex-Parrilla2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexParrilla2022label">
        Polyphonic Sound Event Detection for Highly Dense Birdsong Scenes
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Parrilla2022,
    author = "Parrilla, Alberto Garc\'{i}a Arroba and Stowell, Dan",
    title = "Polyphonic Sound Event Detection for Highly Dense Birdsong Scenes",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "One hour before sunrise, one can experience the dawn chorus where birds from different species sing together. In this scenario, high levels of polyphony, as in the number of overlapping sound sources, are prone to happen resulting in a complex acoustic out-come. Sound Event Detection (SED) tasks analyze acoustic sce-narios in order to identify the occurring events and their respective temporal information. However, highly dense scenarios can be hard to process and have not been studied in depth. Here we show, using a Convolutional Recurrent Neural Network (CRNN), how birdsong polyphonic scenarios can be detected when dealing with higher polyphony and how effectively this type of model can face a very dense scene with up to 10 overlapping birds. We found that models trained with denser examples (i.e., higher polyphony) learn at a similar rate as models that used simpler samples in their train-ing set. Additionally, the model trained with the densest samples maintained a consistent score for all polyphonies, while the model trained with the least dense samples degraded as the polyphony increased. Our results demonstrate that highly dense acoustic sce-narios can be dealt with using CRNNs. We expect that this study serves as a starting point for working on highly populated bird sce-narios such as dawn chorus or other dense acoustic problems."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pellegrini2022" style="box-shadow: none">
<div class="panel-heading" id="headingPellegrini2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Language-Based Audio Retrieval with Textual Embeddings of Tag Names
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Thomas Pellegrini<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IRIT, Université de Toulouse, CNRS, Toulouse INP, UT3, Toulouse, France,<sup>2</sup>Artificial and Natural Intelligence Toulouse Institute (ANITI)
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pellegrini2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Pellegrini_28.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Pellegrini2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pellegrini2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pellegrini2022" class="panel-collapse collapse" id="collapse-Pellegrini2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Language-based audio retrieval aims to retrieve audio recordings based on a queried caption, formulated as a free-form sentence written in natural language. To perform this task, a system is expected to project both modalities (text and audio) onto the same subspace, where they can be compared in terms of a distance. In this work, we propose a first system based on large scale pretrained models to extract audio and text embeddings. As audio embeddings, we use logits predicted over the set of 527 AudioSet tag categories, instead of the most commonly used 2-d feature maps extracted from earlier layers in a deep neural network. We improved this system by adding information from audio tag text embeddings. Experiments were conducted on Clotho v2. A 0.234 mean average precision at top 10 (mAP@10) was obtained on the development-testing split when using the tags, compared to 0.229 without. We also present experiments to justify our architectural design choices.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pellegrini2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Pellegrini_28.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pellegrini2022label" class="modal fade" id="bibtex-Pellegrini2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPellegrini2022label">
        Language-Based Audio Retrieval with Textual Embeddings of Tag Names
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Pellegrini2022,
    author = "Pellegrini, Thomas",
    title = "Language-Based Audio Retrieval with Textual Embeddings of Tag Names",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Language-based audio retrieval aims to retrieve audio recordings based on a queried caption, formulated as a free-form sentence written in natural language. To perform this task, a system is expected to project both modalities (text and audio) onto the same subspace, where they can be compared in terms of a distance. In this work, we propose a first system based on large scale pretrained models to extract audio and text embeddings. As audio embeddings, we use logits predicted over the set of 527 AudioSet tag categories, instead of the most commonly used 2-d feature maps extracted from earlier layers in a deep neural network. We improved this system by adding information from audio tag text embeddings. Experiments were conducted on Clotho v2. A 0.234 mean average precision at top 10 (mAP@10) was obtained on the development-testing split when using the tags, compared to 0.229 without. We also present experiments to justify our architectural design choices."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Perera2022" style="box-shadow: none">
<div class="panel-heading" id="headingPerera2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Latent and Adversarial Data Augmentations for Sound Event Detection and Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         David Perera<sup>1</sup>, Slim Essid<sup>1</sup>, and Gaël Richard<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>LTCI, Télécom Paris, Institut Polytechnique de Paris, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Perera2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Perera_55.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Perera2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Perera2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Perera2022" class="panel-collapse collapse" id="collapse-Perera2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Invariance-based learning is a promising approach in deep learning. Among other benefits, it can mitigate the lack of diversity of available datasets and increase the interpretability of trained models. To this end, practitioners often use a consistency cost penalizing the sensitivity of a model to a set of carefully selected data augmentations. However, there is no consensus about how these augmentations should be selected. In this paper, we study the behavior of several augmentation strategies. We consider the task of sound event detection and classification for our experiments. In particular, we show that transformations operating on the internal layers of a deep neural network are beneficial for this task.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Perera2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Perera_55.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Perera2022label" class="modal fade" id="bibtex-Perera2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPerera2022label">
        Latent and Adversarial Data Augmentations for Sound Event Detection and Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Perera2022,
    author = "Perera, David and Essid, Slim and Richard, Ga\''{e}l",
    title = "Latent and Adversarial Data Augmentations for Sound Event Detection and Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Invariance-based learning is a promising approach in deep learning. Among other benefits, it can mitigate the lack of diversity of available datasets and increase the interpretability of trained models. To this end, practitioners often use a consistency cost penalizing the sensitivity of a model to a set of carefully selected data augmentations. However, there is no consensus about how these augmentations should be selected. In this paper, we study the behavior of several augmentation strategies. We consider the task of sound event detection and classification for our experiments. In particular, we show that transformations operating on the internal layers of a deep neural network are beneficial for this task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Politis2022" style="box-shadow: none">
<div class="panel-heading" id="headingPolitis2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         STARSS22: A Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Archontis Politis<sup>1</sup>, Kazuki Shimada<sup>2</sup>, Parthasaarathy Sudarsanam<sup>1</sup>, Sharath Adavanne<sup>1</sup>, Daniel Krause<sup>1</sup>, Yuichiro Koyama<sup>2</sup>, Naoya Takahashi<sup>2</sup>, Shusuke Takahashi<sup>2</sup>, Yuki Mitsufuji<sup>2</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Audio Research Group, Tampere University, Tampere, Finland, <sup>2</sup>Sony Group Corporation, Tokyo, Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          51 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Politis2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Politis_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Politis2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Politis2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Politis2022" class="panel-collapse collapse" id="collapse-Politis2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARS22) dataset of spatial recordings of real sound scenes collected in various interiors at two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events belonging to 13 target classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. STARSS22 serves as the development and evaluation dataset for Task 3 (Sound Event Localization and Detection) of the DCASE2022 Challenge and it introduces significant new challenges with regard to the previous iterations, which were based on synthetic data. Additionally, the report introduces the baseline system that accompanies the dataset with emphasis on its differences to the baseline of the previous challenge. Baseline results indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in https://zenodo.org/record/6600531.
      </p>
<p>
<strong>
        Cites:
       </strong>
       51 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Politis2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Politis_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Politis2022label" class="modal fade" id="bibtex-Politis2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPolitis2022label">
        STARSS22: A Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Politis2022,
    author = "Politis, Archontis and Shimada, Kazuki and Sudarsanam, Parthasaarathy and Adavanne, Sharath and Krause, Daniel and Koyama, Yuichiro and Takahashi, Naoya and Takahashi, Shusuke and Mitsufuji, Yuki and Virtanen, Tuomas",
    title = "STARSS22: A Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARS22) dataset of spatial recordings of real sound scenes collected in various interiors at two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events belonging to 13 target classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. STARSS22 serves as the development and evaluation dataset for Task 3 (Sound Event Localization and Detection) of the DCASE2022 Challenge and it introduces significant new challenges with regard to the previous iterations, which were based on synthetic data. Additionally, the report introduces the baseline system that accompanies the dataset with emphasis on its differences to the baseline of the previous challenge. Baseline results indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in https://zenodo.org/record/6600531."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Primus2022" style="box-shadow: none">
<div class="panel-heading" id="headingPrimus2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Improving Natural-Language-Based Audio Retrieval with Transfer Learning and Audio &amp; Text Augmentations
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Paul Primus<sup>1</sup> and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Computational Perception (CP-JKU), <sup>2</sup>LIT Artificial Intelligence Lab
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Primus2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Primus_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Primus2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Primus2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Primus2022" class="panel-collapse collapse" id="collapse-Primus2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The absence of large labeled datasets remains a significant challenge in many application areas of deep learning. Researchers and practitioners typically resort to transfer learning and data augmentation to alleviate this issue. We study these strategies in the context of audio retrieval with natural language queries (Task 6b of the DCASE 2022 Challenge). Our proposed system uses pretrained embedding models to project recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. We employ various data augmentation techniques on audio and text inputs and systematically tune their corresponding hyperparameters with sequential model-based optimization. Our results show that the used augmentations strategies reduce overfitting and improve retrieval performance.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Primus2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Primus_40.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_I/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Primus2022label" class="modal fade" id="bibtex-Primus2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPrimus2022label">
        Improving Natural-Language-Based Audio Retrieval with Transfer Learning and Audio &amp; Text Augmentations
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Primus2022,
    author = "Primus, Paul and Widmer, Gerhard",
    title = "Improving Natural-Language-Based Audio Retrieval with Transfer Learning and Audio \&amp; Text Augmentations",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "The absence of large labeled datasets remains a significant challenge in many application areas of deep learning. Researchers and practitioners typically resort to transfer learning and data augmentation to alleviate this issue. We study these strategies in the context of audio retrieval with natural language queries (Task 6b of the DCASE 2022 Challenge). Our proposed system uses pretrained embedding models to project recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. We employ various data augmentation techniques on audio and text inputs and systematically tune their corresponding hyperparameters with sequential model-based optimization. Our results show that the used augmentations strategies reduce overfitting and improve retrieval performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ronchini2022" style="box-shadow: none">
<div class="panel-heading" id="headingRonchini2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Description and Analysis of Novelties Introduced in DCASE Task 4 2022 on the Baseline System
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Francesca Ronchini<sup>1</sup>, Samuele Cornell<sup>2</sup>, Romain Serizel<sup>1</sup>, Nicolas Turpault<sup>1</sup>, Eduardo Fonseca<sup>3</sup>, and Daniel P. W. Ellis<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universite de Lorraine, CNRS, Inria, Loria, Nancy, France, <sup>2</sup>Department of Information Engineering, Universita Politecnica delle Marche, Italy, <sup>3</sup>Google Research, United States
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ronchini2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Ronchini_68.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Ronchini2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ronchini2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ronchini2022" class="panel-collapse collapse" id="collapse-Ronchini2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The aim of the Detection and Classification of Acoustic Scenes and Events Challenge Task 4 is to evaluate systems for the detection of sound events in domestic environments using an heterogeneous dataset. The systems need to be able to correctly detect the sound events present in a recorded audio clip, as well as localize the events in time. This year’s task is a follow-up of DCASE 2021 Task 4, with some important novelties. The goal of this paper is to describe and motivate these new additions, and report an analysis of their impact on the baseline system. We introduced three main novelties: the use of external datasets, including recently released strongly annotated clips from Audioset, the possibility of leveraging pre-trained models, and a new energy consumption metric to raise awareness about the ecological impact of training sound events detectors. The results on the baseline system show that leveraging open-source pretrained on AudioSet improves the results significantly in terms of event classification but not in terms of event segmentation.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ronchini2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Ronchini_68.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ronchini2022label" class="modal fade" id="bibtex-Ronchini2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRonchini2022label">
        Description and Analysis of Novelties Introduced in DCASE Task 4 2022 on the Baseline System
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ronchini2022,
    author = "Ronchini, Francesca and Cornell, Samuele and Serizel, Romain and Turpault, Nicolas and Fonseca, Eduardo and Ellis, Daniel P. W.",
    title = "Description and Analysis of Novelties Introduced in DCASE Task 4 2022 on the Baseline System",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "The aim of the Detection and Classification of Acoustic Scenes and Events Challenge Task 4 is to evaluate systems for the detection of sound events in domestic environments using an heterogeneous dataset. The systems need to be able to correctly detect the sound events present in a recorded audio clip, as well as localize the events in time. This year’s task is a follow-up of DCASE 2021 Task 4, with some important novelties. The goal of this paper is to describe and motivate these new additions, and report an analysis of their impact on the baseline system. We introduced three main novelties: the use of external datasets, including recently released strongly annotated clips from Audioset, the possibility of leveraging pre-trained models, and a new energy consumption metric to raise awareness about the ecological impact of training sound events detectors. The results on the baseline system show that leveraging open-source pretrained on AudioSet improves the results significantly in terms of event classification but not in terms of event segmentation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Scheibler2022" style="box-shadow: none">
<div class="panel-heading" id="headingScheibler2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound Event Localization and Detection with Pre-Trained Audio Spectrogram Transformer and Multichannel Seperation Network
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Robin Scheibler<sup>1</sup>, Tatsuya Komatsu<sup>1</sup>, Yusuke Fujita<sup>1</sup>, and Michael Hentschel<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>LINE Corporation, Tokyo, Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          5 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Scheibler2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Scheibler_24.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Scheibler2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Scheibler2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Scheibler2022" class="panel-collapse collapse" id="collapse-Scheibler2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a sound event localization and detection system based on a CNN-Conformer base network. Our main contribution is to evaluate the use of pre-trained elements in this system. First, a pretrained multichannel separation network allows to separate overlapping events. Second, a fine-tuned self-supervised audio spectrogram transformer provides a priori classification of sound events in the mixture and separated channels. We propose three different architectures combining these extra features into the base network. We first train on the STARSS22 dataset extended by simulation using events from FSD50K and room impulse responses from previous challenges. To bridge the gap between the simulated dataset and the STARSS22 dataset, we fine-tune the models on the training part of the STARSS22 development dataset only before the final evaluation. Experiments reveal that both the pre-trained separation and classification models enhance the final performance, but the extent depends on the adopted network architecture.
      </p>
<p>
<strong>
        Cites:
       </strong>
       5 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Scheibler2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Scheibler_24.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Scheibler2022label" class="modal fade" id="bibtex-Scheibler2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexScheibler2022label">
        Sound Event Localization and Detection with Pre-Trained Audio Spectrogram Transformer and Multichannel Seperation Network
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Scheibler2022,
    author = "Scheibler, Robin and Komatsu, Tatsuya and Fujita, Yusuke and Hentschel, Michael",
    title = "Sound Event Localization and Detection with Pre-Trained Audio Spectrogram Transformer and Multichannel Seperation Network",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "We propose a sound event localization and detection system based on a CNN-Conformer base network. Our main contribution is to evaluate the use of pre-trained elements in this system. First, a pretrained multichannel separation network allows to separate overlapping events. Second, a fine-tuned self-supervised audio spectrogram transformer provides a priori classification of sound events in the mixture and separated channels. We propose three different architectures combining these extra features into the base network. We first train on the STARSS22 dataset extended by simulation using events from FSD50K and room impulse responses from previous challenges. To bridge the gap between the simulated dataset and the STARSS22 dataset, we fine-tune the models on the training part of the STARSS22 development dataset only before the final evaluation. Experiments reveal that both the pre-trained separation and classification models enhance the final performance, but the extent depends on the adopted network architecture."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schmid2022" style="box-shadow: none">
<div class="panel-heading" id="headingSchmid2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Knowledge Distillation from Transformers for Low-Complexity Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Florian Schmid<sup>1,2</sup>, Shahed Masoudian<sup>2</sup>, Khaled Koutini<sup>2</sup>, and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Computational Perception (CP-JKU), <sup>2</sup>LIT Artificial Intelligence Lab
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          9 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schmid2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Schmid_27.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Schmid2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schmid2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schmid2022" class="panel-collapse collapse" id="collapse-Schmid2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Knowledge Distillation (KD) is known for its ability to compress large models into low-complexity solutions while preserving high predictive performance. In Acoustic Scene Classification (ASC), this ability has recently been exploited successfully, as underlined by three of the top four systems in the low-complexity ASC task of the DCASE‘21 challenge relying on KD. Current KD solutions for ASC mainly use large-scale CNNs or specialist ensembles to derive superior teacher predictions. In this work, we use the Audio Spectrogram Transformer model PaSST, pre-trained on Audioset, as a teacher model. We show how the pre-trained PaSST model can be properly trained downstream on the TAU Urban Acoustic Scenes 2022 Mobile development dataset and how to distill the knowledge into a low-complexity CNN student. We study the effect of using teacher ensembles, using teacher predictions on extended audio sequences, and using Audioset as an additional dataset for knowledge transfer. Additionally, we compare the effectiveness of Mixup and Freq-MixStyle to improve performance and enhance device generalization. The described system achieved rank 1 in the Low-complexity ASC Task of the DCASE‘22 challenge.
      </p>
<p>
<strong>
        Cites:
       </strong>
       9 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schmid2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Schmid_27.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_IV/SP2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schmid2022label" class="modal fade" id="bibtex-Schmid2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchmid2022label">
        Knowledge Distillation from Transformers for Low-Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schmid2022,
    author = "Schmid, Florian and Masoudian, Shahed and Koutini, Khaled and Widmer, Gerhard",
    title = "Knowledge Distillation from Transformers for Low-Complexity Acoustic Scene Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Knowledge Distillation (KD) is known for its ability to compress large models into low-complexity solutions while preserving high predictive performance. In Acoustic Scene Classification (ASC), this ability has recently been exploited successfully, as underlined by three of the top four systems in the low-complexity ASC task of the DCASE‘21 challenge relying on KD. Current KD solutions for ASC mainly use large-scale CNNs or specialist ensembles to derive superior teacher predictions. In this work, we use the Audio Spectrogram Transformer model PaSST, pre-trained on Audioset, as a teacher model. We show how the pre-trained PaSST model can be properly trained downstream on the TAU Urban Acoustic Scenes 2022 Mobile development dataset and how to distill the knowledge into a low-complexity CNN student. We study the effect of using teacher ensembles, using teacher predictions on extended audio sequences, and using Audioset as an additional dataset for knowledge transfer. Additionally, we compare the effectiveness of Mixup and Freq-MixStyle to improve performance and enhance device generalization. The described system achieved rank 1 in the Low-complexity ASC Task of the DCASE‘22 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schmidt2022" style="box-shadow: none">
<div class="panel-heading" id="headingSchmidt2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Feature Selection Using Alternating Direction Method of Multiplier for Low-Complexity Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Lorenz P. Schmidt<sup>1,2</sup>, Beran Kiliç<sup>1,2</sup>, and Nils Peters<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, <sup>2</sup>International Audio Laboratories, Erlangen, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          1 cite
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schmidt2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Schmidt_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Schmidt2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schmidt2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schmidt2022" class="panel-collapse collapse" id="collapse-Schmidt2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic Scene Classification (ASC) is a common task for many resource-constrained devices, e.g., mobile phones or hearing aids. Limiting the complexity and memory footprint of the classifier is crucial. The number of input features directly relates to these two metrics. In this contribution, we evaluate a feature selection algorithm which we also used in this year’s challenge. We propose binary search with hard constraints on the feature set and solve the optimization problem with Alternating Direction Method of Multipliers (ADMM).With minimal impact on accuracy and log loss, results show the model complexity is halved by masking 50% of the Mel input features. Further, we found that training convergence is more stable across random seeds. This also facilitates the hyperparameter search. Finally, the remaining Mel features provide an insight into the properties of the DCASE ASC data set.
      </p>
<p>
<strong>
        Cites:
       </strong>
       1 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schmidt2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Schmidt_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schmidt2022label" class="modal fade" id="bibtex-Schmidt2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchmidt2022label">
        Feature Selection Using Alternating Direction Method of Multiplier for Low-Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schmidt2022,
    author = "Schmidt, Lorenz P. and Kili\c{c}, Beran and Peters, Nils",
    title = "Feature Selection Using Alternating Direction Method of Multiplier for Low-Complexity Acoustic Scene Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Acoustic Scene Classification (ASC) is a common task for many resource-constrained devices, e.g., mobile phones or hearing aids. Limiting the complexity and memory footprint of the classifier is crucial. The number of input features directly relates to these two metrics. In this contribution, we evaluate a feature selection algorithm which we also used in this year’s challenge. We propose binary search with hard constraints on the feature set and solve the optimization problem with Alternating Direction Method of Multipliers (ADMM).With minimal impact on accuracy and log loss, results show the model complexity is halved by masking 50\% of the Mel input features. Further, we found that training convergence is more stable across random seeds. This also facilitates the hyperparameter search. Finally, the remaining Mel features provide an insight into the properties of the DCASE ASC data set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Singh2022" style="box-shadow: none">
<div class="panel-heading" id="headingSingh2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Low-Complexity CNNs for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Arshdeep Singh<sup>1</sup>, and Mark D. Plumbley<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          12 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Singh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Singh_33.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Singh2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Singh2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Singh2022" class="panel-collapse collapse" id="collapse-Singh2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents a low-complexity framework for acoustic scene classification (ASC). Most of the frameworks designed for ASC use convolutional neural networks (CNNs) due to their learning ability and improved performance compared to hand-engineered features. However, CNNs are resource hungry due to their large size and high computational complexity. Therefore, CNNs are difficult to deploy on resource constrained devices. This paper addresses the problem of reducing the computational complexity and memory requirement in CNNs. We propose a low-complexity CNN architecture, and apply pruning and quantization to further reduce the parameters and memory. We then propose an ensemble framework that combines various low-complexity CNNs to improve the overall performance. An experimental evaluation of the proposed framework is performed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The proposed ensemble framework has approximately 60K parameters, requires 19M multiply-accumulate operations and improves the performance by approximately 2-4 percentage points compared to the DCASE 2022 Task 1 baseline network.
      </p>
<p>
<strong>
        Cites:
       </strong>
       12 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Singh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Singh_33.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Singh2022label" class="modal fade" id="bibtex-Singh2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSingh2022label">
        Low-Complexity CNNs for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Singh2022,
    author = "Singh, Arshdeep and Plumbley, Mark D.",
    title = "Low-Complexity CNNs for Acoustic Scene Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "This paper presents a low-complexity framework for acoustic scene classification (ASC). Most of the frameworks designed for ASC use convolutional neural networks (CNNs) due to their learning ability and improved performance compared to hand-engineered features. However, CNNs are resource hungry due to their large size and high computational complexity. Therefore, CNNs are difficult to deploy on resource constrained devices. This paper addresses the problem of reducing the computational complexity and memory requirement in CNNs. We propose a low-complexity CNN architecture, and apply pruning and quantization to further reduce the parameters and memory. We then propose an ensemble framework that combines various low-complexity CNNs to improve the overall performance. An experimental evaluation of the proposed framework is performed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The proposed ensemble framework has approximately 60K parameters, requires 19M multiply-accumulate operations and improves the performance by approximately 2-4 percentage points compared to the DCASE 2022 Task 1 baseline network."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Venkatesh2022" style="box-shadow: none">
<div class="panel-heading" id="headingVenkatesh2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Improved Domain Generalization via Disentangled Multi-Task Learning in Unsupervised Anomalous Sound Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Satvik Venkatesh<sup>1,2</sup>, Gordon Wichern<sup>1</sup>, Aswin Subramanian<sup>1</sup>, and Jonathan Le Roux<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA, <sup>2</sup>Interdisciplinary Centre for Computer Music Research, University of Plymouth, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Venkatesh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Venkatesh_43.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Venkatesh2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Venkatesh2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Venkatesh2022" class="panel-collapse collapse" id="collapse-Venkatesh2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We investigate a novel multi-task learning framework that disentangles domain-shared features and domain-specific features for domain generalization in anomalous sound detection. Disentanglement leads to better latent features and also increases flexibility in post-processing due to the availability of multiple embedding spaces. The framework was at the core of our submissions to the DCASE2022 Challenge Task 2. We ranked 5th out of 32 teams in the competition, obtaining an overall harmonic mean of 67.57% on the blind evaluation set, surpassing the baseline by 13.5% and trailing the top rank by 3.4%. We also explored machine-specific loss functions and domain generalization methods, which showed improvements on the development set, but were less effective on the evaluation set.
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Venkatesh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Venkatesh_43.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Venkatesh2022label" class="modal fade" id="bibtex-Venkatesh2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVenkatesh2022label">
        Improved Domain Generalization via Disentangled Multi-Task Learning in Unsupervised Anomalous Sound Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Venkatesh2022,
    author = "Venkatesh, Satvik and Wichern, Gordon and Subramanian, Aswin and Le Roux, Jonathan",
    title = "Improved Domain Generalization via Disentangled Multi-Task Learning in Unsupervised Anomalous Sound Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "We investigate a novel multi-task learning framework that disentangles domain-shared features and domain-specific features for domain generalization in anomalous sound detection. Disentanglement leads to better latent features and also increases flexibility in post-processing due to the availability of multiple embedding spaces. The framework was at the core of our submissions to the DCASE2022 Challenge Task 2. We ranked 5th out of 32 teams in the competition, obtaining an overall harmonic mean of 67.57\% on the blind evaluation set, surpassing the baseline by 13.5\% and trailing the top rank by 3.4\%. We also explored machine-specific loss functions and domain generalization methods, which showed improvements on the development set, but were less effective on the evaluation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wang2022" style="box-shadow: none">
<div class="panel-heading" id="headingWang2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Detect What You Want: Target Sound Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Helin Wang<sup>1</sup>, Dongchao Yang<sup>1</sup>, Yuexian Zou<sup>1</sup> , Fan Cui<sup>2</sup>, and Yujun Wang<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>ADSPLAB, School of ECE, Peking University, Shenzhen, China, <sup>2</sup>Xiaomi Corporation, Beijing, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          7 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Wang_3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wang2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wang2022" class="panel-collapse collapse" id="collapse-Wang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Human beings can perceive a target sound type from a multi-source mixture signal by the selective auditory attention, however, such functionality was hardly ever explored in machine hearing. This paper addresses the target sound detection (TSD) task, which aims to detect the target sound signal from a mixture audio when a target sound’s reference audio is given. We present a novel target sound detection network (TSDNet) which consists of two main parts: A conditional network which aims at generating a sound-discriminative conditional embedding vector representing the target sound, and a detection network which takes both the mixture audio and the conditional embedding vector as inputs and produces the detection result of the target sound. These two networks can be jointly optimized with a multi-task learning approach to further improve the performance. In addition, we study both strong-supervised and weakly-supervised strategies to train TSDNet and propose a data augmentation method by mixing two samples. To facilitate this research, we build a target sound detection dataset (i.e. URBANTSD) based on URBAN-SED and UrbanSound8K datasets, and experimental results indicate our method could get the segment-based F scores of 76.3% and 56.8% on the strongly-labelled and weakly-labelled data respectively.
      </p>
<p>
<strong>
        Cites:
       </strong>
       7 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Wang_3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wang2022label" class="modal fade" id="bibtex-Wang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWang2022label">
        Detect What You Want: Target Sound Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wang2022,
    author = "Wang, Helin and Yang, Dongchao and Zou, Yuexian and Cui, Fan Cui and Wang, Yujun",
    title = "Detect What You Want: Target Sound Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Human beings can perceive a target sound type from a multi-source mixture signal by the selective auditory attention, however, such functionality was hardly ever explored in machine hearing. This paper addresses the target sound detection (TSD) task, which aims to detect the target sound signal from a mixture audio when a target sound’s reference audio is given. We present a novel target sound detection network (TSDNet) which consists of two main parts: A conditional network which aims at generating a sound-discriminative conditional embedding vector representing the target sound, and a detection network which takes both the mixture audio and the conditional embedding vector as inputs and produces the detection result of the target sound. These two networks can be jointly optimized with a multi-task learning approach to further improve the performance. In addition, we study both strong-supervised and weakly-supervised strategies to train TSDNet and propose a data augmentation method by mixing two samples. To facilitate this research, we build a target sound detection dataset (i.e. URBANTSD) based on URBAN-SED and UrbanSound8K datasets, and experimental results indicate our method could get the segment-based F scores of 76.3\% and 56.8\% on the strongly-labelled and weakly-labelled data respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Weck2022" style="box-shadow: none">
<div class="panel-heading" id="headingWeck2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Matching Text and Audio Embeddings: Exploring Transfer-Learning Strategies for Language-Based Audio Retrieval
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Benno Weck<sup>1,2</sup>, Miguel Pérez Fernández<sup>1,2</sup>, Holger Kirchhoff<sup>1</sup>, and Xavier Serra<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Huawei Technologies, Munich Research Center, Germany, <sup>2</sup>Universitat Pompeu Fabra, Music Technology Group, Spain
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Weck2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Weck_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Weck2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Weck2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Weck2022" class="panel-collapse collapse" id="collapse-Weck2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present an analysis of large-scale pretrained deep learning models used for cross-modal (text-to-audio) retrieval. We use embeddings extracted by these models in a metric learning framework to connect matching pairs of audio and text. Shallow neural networks map the embeddings to a common dimensionality. Our system, which is an extension of our submission to the Language-based Audio Retrieval Task of the DCASE Challenge 2022, employs the RoBERTa foundation model as the text embedding extractor. A pretrained PANNs model extracts the audio embeddings. To improve the generalisation of our model, we investigate how pretraining with audio and associated noisy text collected from the online platform Freesound improves the performance of our method. Furthermore, our ablation study reveals that the proper choice of the loss function and fine-tuning the pretrained models are essential in training a competitive retrieval system.
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Weck2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Weck_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_III/SP9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Weck2022label" class="modal fade" id="bibtex-Weck2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWeck2022label">
        Matching Text and Audio Embeddings: Exploring Transfer-Learning Strategies for Language-Based Audio Retrieval
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Weck2022,
    author = "Weck, Benno and Fern'{a}ndez, Miguel P'{e}rez and Kirchhoff, Holger and Serra, Xavier",
    title = "Matching Text and Audio Embeddings: Exploring Transfer-Learning Strategies for Language-Based Audio Retrieval",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "We present an analysis of large-scale pretrained deep learning models used for cross-modal (text-to-audio) retrieval. We use embeddings extracted by these models in a metric learning framework to connect matching pairs of audio and text. Shallow neural networks map the embeddings to a common dimensionality. Our system, which is an extension of our submission to the Language-based Audio Retrieval Task of the DCASE Challenge 2022, employs the RoBERTa foundation model as the text embedding extractor. A pretrained PANNs model extracts the audio embeddings. To improve the generalisation of our model, we investigate how pretraining with audio and associated noisy text collected from the online platform Freesound improves the performance of our method. Furthermore, our ablation study reveals that the proper choice of the loss function and fine-tuning the pretrained models are essential in training a competitive retrieval system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xiao2022" style="box-shadow: none">
<div class="panel-heading" id="headingXiao2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Continual Learning for On-Device Environmental Sound Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yang Xiao<sup>1</sup>, Xubo Liu<sup>2</sup>, James King<sup>2</sup>, Arshdeep Singh<sup>2</sup>, Eng Siong Chng<sup>1</sup>, Mark D. Plumbley<sup>2</sup>, Wenwu Wang<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Computer Science and Engineering, Nanyang Technological University, Singapore, <sup>2</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xiao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Xiao_47.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Xiao2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xiao2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xiao2022" class="panel-collapse collapse" id="collapse-Xiao2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device environmental sound classification given the restrictions on computation resources (e.g., model size, running memory). To address this issue, we propose a simple and efficient continual learning method. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. Specifically, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification.
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xiao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Xiao_47.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/session_II/SP2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xiao2022label" class="modal fade" id="bibtex-Xiao2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXiao2022label">
        Continual Learning for On-Device Environmental Sound Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xiao2022,
    author = "Xiao, Yang and Liu, Xubo and King, James and Singh, Arshdeep and Chng, Eng Siong and Plumbley, Mark D. and Wang, Wenwu",
    title = "Continual Learning for On-Device Environmental Sound Classification",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device environmental sound classification given the restrictions on computation resources (e.g., model size, running memory). To address this issue, we propose a simple and efficient continual learning method. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. Specifically, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xie2022" style="box-shadow: none">
<div class="panel-heading" id="headingXie2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Language-Based Audio Retrieval Task in DCASE 2022 Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Huang Xie<sup>1</sup>, Samuel Lipping<sup>1</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Audio Research Group, Tampere University, Tampere, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          10 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xie2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Xie_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T6b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Xie2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xie2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xie2022" class="panel-collapse collapse" id="collapse-Xie2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Language-based audio retrieval is a task, where natural language textual captions are used as queries to retrieve audio signals from a dataset. It has been first introduced into DCASE 2022 Challenge as Subtask 6B of task 6, which aims at developing computational systems to model relationships between audio signals and free-form textual descriptions. Compared with audio captioning (Subtask 6A), which is about generating audio captions for audio signals, language-based audio retrieval (Subtask 6B) focuses on ranking audio signals according to their relevance to natural language textual captions. In DCASE 2022 Challenge, the provided baseline system for Subtask 6B was significantly outperformed, with top performance being 0.276 in mAP@10. This paper presents the outcome of Subtask 6B in terms of submitted systems' performance and analysis.
      </p>
<p>
<strong>
        Cites:
       </strong>
       10 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xie2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Xie_56.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2022/slides/challenge/T6b.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xie2022label" class="modal fade" id="bibtex-Xie2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXie2022label">
        Language-Based Audio Retrieval Task in DCASE 2022 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xie2022,
    author = "Xie, Huang and Lipping, Samuel and Virtanen, Tuomas",
    title = "Language-Based Audio Retrieval Task in DCASE 2022 Challenge",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Language-based audio retrieval is a task, where natural language textual captions are used as queries to retrieve audio signals from a dataset. It has been first introduced into DCASE 2022 Challenge as Subtask 6B of task 6, which aims at developing computational systems to model relationships between audio signals and free-form textual descriptions. Compared with audio captioning (Subtask 6A), which is about generating audio captions for audio signals, language-based audio retrieval (Subtask 6B) focuses on ranking audio signals according to their relevance to natural language textual captions. In DCASE 2022 Challenge, the provided baseline system for Subtask 6B was significantly outperformed, with top performance being 0.276 in mAP@10. This paper presents the outcome of Subtask 6B in terms of submitted systems' performance and analysis."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yang2022" style="box-shadow: none">
<div class="panel-heading" id="headingYang2022" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Mixed Supervised Learning Framework For Target Sound Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Dongchao Yang<sup>1</sup>, Helin Wang<sup>1</sup>, Wenwu Wang<sup>2</sup>, and Yuexian Zou<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>ADSPLAB, School of ECE, Peking University, Shenzhen, China, <sup>2</sup>Center for Vision, Speech and Signal Processing, University of Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Yang_23.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Yang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yang2022" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yang2022" class="panel-collapse collapse" id="collapse-Yang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Target sound detection (TSD) aims to detect the target sound from mixture audio given the reference information. Previous works have shown that TSD models can be trained on fully-annotated (frame-level label) or weakly-annotated (clip-level label) data. However, there are some clear evidences show that the performance of the model trained on weakly-annotated data is worse than that trained on fully-annotated data. To fill this gap, we provide a mixed supervision perspective, in which learning novel categories (target domain) using weak annotations with the help of full annotations of existing base categories (source domain). To realize this, a mixed supervised learning framework is proposed, which contains two mutually-helping student models (f student and w student) that learn from fully-annotated and weakly-annotated data, respectively. The motivation is that f student learned from fully-annotated data has a better ability to capture detailed information than w student. Thus, we first let f student guide w student to learn the ability to capture details, so w student can perform better in the target domain. Then we let w student guide f student to fine-tune on the target domain. The process can be repeated several times so that the two students perform very well in the target domain. To evaluate our method, we built three TSD datasets based on UrbanSound and Audioset. Experimental results show that our methods offer about 8% improvement in event-based F-score as compared with a recent baseline.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2022/proceedings/DCASE2022Workshop_Yang_23.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yang2022label" class="modal fade" id="bibtex-Yang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYang2022label">
        A Mixed Supervised Learning Framework For Target Sound Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Yang2022,
    author = "Yang, Dongchao and Wang, Helin and Wang, Wenwu and Zou, Yuexian",
    title = "A Mixed Supervised Learning Framework For Target Sound Detection",
    booktitle = "Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    abstract = "Target sound detection (TSD) aims to detect the target sound from mixture audio given the reference information. Previous works have shown that TSD models can be trained on fully-annotated (frame-level label) or weakly-annotated (clip-level label) data. However, there are some clear evidences show that the performance of the model trained on weakly-annotated data is worse than that trained on fully-annotated data. To fill this gap, we provide a mixed supervision perspective, in which learning novel categories (target domain) using weak annotations with the help of full annotations of existing base categories (source domain). To realize this, a mixed supervised learning framework is proposed, which contains two mutually-helping student models (f student and w student) that learn from fully-annotated and weakly-annotated data, respectively. The motivation is that f student learned from fully-annotated data has a better ability to capture detailed information than w student. Thus, we first let f student guide w student to learn the ability to capture details, so w student can perform better in the target domain. Then we let w student guide f student to fine-tune on the target domain. The process can be repeated several times so that the two students perform very well in the target domain. To evaluate our method, we built three TSD datasets based on UrbanSound and Audioset. Experimental results show that our methods offer about 8\% improvement in event-based F-score as compared with a recent baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>