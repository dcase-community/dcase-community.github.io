<!DOCTYPE html><html lang="en">
<head>
    <title>Proceedings - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2018/proceedings">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="The proceedings of the DCASE2018 Workshop have been published as electronic publication of Tampere University of Technology series: Plumbley, M. D., Kroos, C., Bello, J. P., Richard, G., Ellis, D.P.W. &amp; Mesaros, A. (Eds.) (2018). Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018 …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2018/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-list fa-fw"></i>&nbsp;Program&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2018/technical-program"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule & Talks</a>
    </li>
            <li class="">
        <a href="/workshop2018/keynotes"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Keynotes</a>
    </li>
            <li class="">
        <a href="/workshop2018/poster_session_I"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Poster session I</a>
    </li>
            <li class="">
        <a href="/workshop2018/poster_session_II"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Poster session II</a>
    </li>
        </ul>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2018/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Registration">
        <a href="/workshop2018/registration"><i class="fa fa-key fa-fw"></i>&nbsp;Registration</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Travel & Accommodation">
        <a href="/workshop2018/travel"><i class="fa fa-info fa-fw"></i>&nbsp;Travel & Accommodation</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2018/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2018/presentations"><i class="fa fa-comments-o fa-fw"></i>&nbsp;Talks & Posters</a>
    </li>
            <li class="">
        <a href="/workshop2018/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
            <li class="">
        <a href="/workshop2018/submission"><i class="fa fa-upload fa-fw"></i>&nbsp;Submission</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2018/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metropol-sevilla-03.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><h1 class="bold">Proceedings</h1></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p>The proceedings of the DCASE2018 Workshop have been published as electronic publication of Tampere University of Technology series:</p>
<div class="row" style="display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;">
<div class="col-xs-2">
<a data-placement="bottom" href="https://tutcris.tut.fi/portal/files/17110946/DCASE_2018_proceedings.pdf" rel="tooltip" target="_blank" title="PDF"><img class="img-responsive img-thumbnail" src="../images/covers/DCASE2018Workshop_proceedings_cover.png"/></a>
</div>
<div class="col-xs-10 bg-light-gray">
<p>Plumbley, M. D., Kroos, C., Bello, J. P., Richard, G., Ellis, D.P.W. &amp; Mesaros, A. (Eds.) (2018). <em>Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018).</em>
</p>
<p>ISBN (Electronic): 978-952-15-4262-6</p>
<br/>
<div class="btn-group">
<a class="btn btn-xs btn-primary" data-placement="bottom" href="http://urn.fi/URN:ISBN:978-952-15-4262-6" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" target="_blank" title="Permanent link"><i class="fa fa-link fa-1x"></i> Link</a>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://tutcris.tut.fi/portal/files/17110946/DCASE_2018_proceedings.pdf" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" title="PDF"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button class="btn btn-xs btn-danger" data-target="#bibtex_proceedings" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
</div>
</div>
<div aria-hidden="true" aria-labelledby="bibtex_proceedingslabel" class="modal fade" id="bibtex_proceedings" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtex_proceedingslabel">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)</h4>
</div>
<div class="modal-body">
<pre>
@book{DCASE2018Workshop,
    title = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    author = "Mark D. Plumbley and Christian Kroos and Juan P. Bello and Gaël Richard and Daniel P.W. Ellis and Annamaria Mesaros",
    year = "2018",
    publisher = "Tampere University of Technology. Laboratory of Signal Processing",
}
                </pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
<div class="clearfix"></div>
<div class="btex" data-scholar-cite-counts="true" data-source="content/data/workshop2018/proceedings.bib" data-stats="true">
<em>
  Total cites: 1682 (updated 30.11.2023)
 </em>
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Bear2018" style="box-shadow: none">
<div class="panel-heading" id="headingBear2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         An extensible cluster-graph taxonomy for open set sound scene analysis
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Helen L. Bear and Emmanouil Benetos
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Queen Mary University of London, School of Electronic Engineering and Computer Science, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bear2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Bear_144.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Bear2018').collapse('show');window.location.hash='#Bear2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Bear2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bear2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bear2018" class="panel-collapse collapse" id="collapse-Bear2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present a new extensible and divisible taxonomy for open set sound scene analysis. This new model allows complex scene analysis with tangible descriptors and perception labels. Its novel structure is a cluster graph such that each cluster (or subset) can stand alone for targeted analyses such as office sound event detection, whilst maintaining integrity over the whole graph (superset) of labels. The key design benefit is its extensibility as new labels are needed during new data capture. Furthermore, datasets which use the same taxonomy are easily augmented, saving future data collection effort. We balance the details needed for complex scene analysis with avoiding ‘the taxonomy of everything’ with our framework to ensure no duplicity in the superset of labels and demonstrate this with DCASE challenge classifications.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Taxonomy, ontology, sound scenes, sound events, sound scene analysis, open set
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="https://scholar.google.com/scholar?cites=6806435851081176486" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bear2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Bear_144.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="http://soundscape.eecs.qmul.ac.uk/the-extensible-taxonomy/" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        The Extensible Taxonomy
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bear2018label" class="modal fade" id="bibtex-Bear2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBear2018label">
        An extensible cluster-graph taxonomy for open set sound scene analysis
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Bear2018,
    author = "Bear, Helen and Benetos, Emmanouil",
    title = "An extensible cluster-graph taxonomy for open set sound scene analysis",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "183--187",
    keywords = "Taxonomy, ontology, sound scenes, sound events, sound scene analysis, open set",
    abstract = "We present a new extensible and divisible taxonomy for open set sound scene analysis. This new model allows complex scene analysis with tangible descriptors and perception labels. Its novel structure is a cluster graph such that each cluster (or subset) can stand alone for targeted analyses such as office sound event detection, whilst maintaining integrity over the whole graph (superset) of labels. The key design benefit is its extensibility as new labels are needed during new data capture. Furthermore, datasets which use the same taxonomy are easily augmented, saving future data collection effort. We balance the details needed for complex scene analysis with avoiding ‘the taxonomy of everything’ with our framework to ensure no duplicity in the superset of labels and demonstrate this with DCASE challenge classifications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cances2018" style="box-shadow: none">
<div class="panel-heading" id="headingCances2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound event detection from weak annotations: weighted-GRU versus multi-instance-learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Léo Cances, Thomas Pellegrini, and Patrice Guyot
        </em>
</p>
<p class="text-muted">
<small>
<em>
          IRIT, Universite de Toulouse, CNRS, Toulouse, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          9 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cances2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Cances_113.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Cances2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cances2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cances2018" class="panel-collapse collapse" id="collapse-Cances2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we address the detection of audio events in domestic environments in the case where a weakly annotated dataset is available for training. The weak annotations provide tags from audio events but do not provide temporal boundaries. We report experiments in the framework of the task four of the DCASE 2018 challenge. The objective is twofold: detect audio events (multicategorical classification at recording level), localize the events precisely within the recordings. We explored two approaches: 1) a ”weighted-GRU” (WGRU), in which we train a Convolutional Recurrent Neural Network (CRNN) for classification and then exploit its frame-based predictions at the output of the time-distributed dense layer to perform localization. We propose to lower the influence of the hidden states to avoid predicting a same score throughout a recording. 2) An approach inspired by Multi-Instance Learning (MIL), in which we train a CRNN to give predictions at framelevel, using a custom loss function based on the weak label and statistics of the frame-based predictions. Both approaches outperform the baseline of 14.06% in F-measure by a large margin, with values of respectively 16.77% and 24.58% for combined WGRUs and MIL, on a test set comprised of 288 recordings.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, weakly supervised learning, multi-instance learning, convolutional neural networks, weighted gate recurrent unit
      </p>
<p>
<strong>
        Cites:
       </strong>
       9 (
       <a href="https://scholar.google.com/scholar?cites=8651878833146359183" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cances2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Cances_113.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cances2018label" class="modal fade" id="bibtex-Cances2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCances2018label">
        Sound event detection from weak annotations: weighted-GRU versus multi-instance-learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cances2018,
    author = "Cances, Léo and Pellegrini, Thomas and Guyot, Patrice",
    title = "Sound event detection from weak annotations: weighted-GRU versus multi-instance-learning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "64--68",
    keywords = "Sound event detection, weakly supervised learning, multi-instance learning, convolutional neural networks, weighted gate recurrent unit",
    abstract = "In this paper, we address the detection of audio events in domestic environments in the case where a weakly annotated dataset is available for training. The weak annotations provide tags from audio events but do not provide temporal boundaries. We report experiments in the framework of the task four of the DCASE 2018 challenge. The objective is twofold: detect audio events (multicategorical classification at recording level), localize the events precisely within the recordings. We explored two approaches: 1) a ”weighted-GRU” (WGRU), in which we train a Convolutional Recurrent Neural Network (CRNN) for classification and then exploit its frame-based predictions at the output of the time-distributed dense layer to perform localization. We propose to lower the influence of the hidden states to avoid predicting a same score throughout a recording. 2) An approach inspired by Multi-Instance Learning (MIL), in which we train a CRNN to give predictions at framelevel, using a custom loss function based on the weak label and statistics of the frame-based predictions. Both approaches outperform the baseline of 14.06\% in F-measure by a large margin, with values of respectively 16.77\% and 24.58\% for combined WGRUs and MIL, on a test set comprised of 288 recordings."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dorfer2018" style="box-shadow: none">
<div class="panel-heading" id="headingDorfer2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Training general-purpose audio tagging networks with noisy labels and iterative self-verification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Matthias Dorfer and Gerhard Widmer
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Johannes Kepler University, Institute of Computational Perception, Linz, Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          34 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dorfer2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Dorfer_143.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Dorfer2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dorfer2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Dorfer2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dorfer2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dorfer2018" class="panel-collapse collapse" id="collapse-Dorfer2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes our submission to the first Freesound generalpurpose audio tagging challenge carried out within the DCASE 2018 challenge. Our proposal is based on a fully convolutional neural network that predicts one out of 41 possible audio class labels when given an audio spectrogram excerpt as an input. What makes this classification dataset and the task in general special, is the fact that only 3,700 of the 9,500 provided training examples are delivered with manually verified ground truth labels. The remaining non-verified observations are expected to contain a substantial amount of label noise (up to 30-35% in the “worst” categories). We propose to address this issue by a simple, iterative self-verification process, which gradually shifts unverified labels into the verified, trusted training set. The decision criterion for self-verifying a training example is the prediction consensus of a previous snapshot of the network on multiple short sliding window excerpts of the training example at hand. On the unseen test data, an ensemble of three networks trained with this self-verification approach achieves a mean average precision (MAP@3) of 0.951. This is the second best out of 558 submissions to the corresponding Kaggle challenge.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio-tagging, Fully Convolutional Neural Networks, Noisy Labels, Label Self-Verification
      </p>
<p>
<strong>
        Cites:
       </strong>
       34 (
       <a href="https://scholar.google.com/scholar?cites=9514897985443310768" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dorfer2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Dorfer_143.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/CPJKU/dcase_task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Code">
<i class="fa fa-git">
</i>
        Code
       </a>
<a class="btn btn-sm btn-info" href="https://cpjku.github.io/dcase_task2/" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Blog">
<i class="fa fa-external-link-square">
</i>
        Blog
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dorfer2018label" class="modal fade" id="bibtex-Dorfer2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDorfer2018label">
        Training general-purpose audio tagging networks with noisy labels and iterative self-verification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dorfer2018,
    author = "Dorfer, Matthias and Widmer, Gerhard",
    title = "Training general-purpose audio tagging networks with noisy labels and iterative self-verification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "178--182",
    keywords = "Audio-tagging, Fully Convolutional Neural Networks, Noisy Labels, Label Self-Verification",
    abstract = "This paper describes our submission to the first Freesound generalpurpose audio tagging challenge carried out within the DCASE 2018 challenge. Our proposal is based on a fully convolutional neural network that predicts one out of 41 possible audio class labels when given an audio spectrogram excerpt as an input. What makes this classification dataset and the task in general special, is the fact that only 3,700 of the 9,500 provided training examples are delivered with manually verified ground truth labels. The remaining non-verified observations are expected to contain a substantial amount of label noise (up to 30-35\% in the “worst” categories). We propose to address this issue by a simple, iterative self-verification process, which gradually shifts unverified labels into the verified, trusted training set. The decision criterion for self-verifying a training example is the prediction consensus of a previous snapshot of the network on multiple short sliding window excerpts of the training example at hand. On the unseen test data, an ensemble of three networks trained with this self-verification approach achieves a mean average precision (MAP@3) of 0.951. This is the second best out of 558 submissions to the corresponding Kaggle challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fonseca2018" style="box-shadow: none">
<div class="panel-heading" id="headingFonseca2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         General-purpose tagging of Freesound audio with AudioSet labels: task description, dataset, and baseline
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Eduardo Fonseca<sup>1</sup>, Manoj Plakal<sup>2</sup>, Frederic Font<sup>1</sup>, Daniel P. W. Ellis<sup>2</sup>, Xavier Favory<sup>1</sup>, Jordi Pons<sup>1</sup>, and Xavier Serra<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Music Technology Group, Universitat Pompeu Fabra, Barcelona, <sup>2</sup>Google, Inc., New York, NY, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          163 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fonseca2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Fonseca_114.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Fonseca2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fonseca2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fonseca2018" class="panel-collapse collapse" id="collapse-Fonseca2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes Task 2 of the DCASE 2018 Challenge, titled “General-purpose audio tagging of Freesound content with AudioSet labels”. This task was hosted on the Kaggle platform as “Freesound General-Purpose Audio Tagging Challenge”. The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 diverse categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio tagging, audio dataset, data collection
      </p>
<p>
<strong>
        Cites:
       </strong>
       163 (
       <a href="https://scholar.google.com/scholar?cites=4240160875528169622" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fonseca2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Fonseca_114.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" href="https://www.kaggle.com/c/freesound-audio-tagging" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Kaggle competition">
<i class="fa fa-external-link-square">
</i>
        Kaggle competition
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fonseca2018label" class="modal fade" id="bibtex-Fonseca2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFonseca2018label">
        General-purpose tagging of Freesound audio with AudioSet labels: task description, dataset, and baseline
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Fonseca2018,
    author = "Fonseca, Eduardo and Plakal, Manoj and Font, Frederic and Ellis, Daniel P.W. and Favory, Xavier and Pons, Jordi and Serra, Xavier",
    title = "General-purpose tagging of Freesound audio with AudioSet labels: task description, dataset, and baseline",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "69--73",
    keywords = "Audio tagging, audio dataset, data collection",
    abstract = "This paper describes Task 2 of the DCASE 2018 Challenge, titled “General-purpose audio tagging of Freesound content with AudioSet labels”. This task was hosted on the Kaggle platform as “Freesound General-Purpose Audio Tagging Challenge”. The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 diverse categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gharib2018" style="box-shadow: none">
<div class="panel-heading" id="headingGharib2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Unsupervised adversarial domain adaptation for acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Shayan Gharib<sup>1</sup>, Konstantinos Drossos<sup>1</sup>, Emre Cakir<sup>1</sup>, Dmitriy Serdyuk<sup>2</sup>, and Tuomas Virtanen<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Laboratory of Signal Processing, Tampere University of Technology, Finland, <sup>2</sup>Montreal Institute for Learning Algorithms, Canada
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          56 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gharib2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Gharib_132.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Gharib2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gharib2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Gharib2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gharib2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gharib2018" class="panel-collapse collapse" id="collapse-Gharib2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       A general problem in acoustic scene classification task is the mismatched conditions between training and testing data, which significantly reduces the performance of the developed methods on classification accuracy. As a countermeasure, we present the first method of unsupervised adversarial domain adaptation for acoustic scene classification. We employ a model pre-trained on data from one set of conditions and by using data from other set of conditions, we adapt the model in order that its output cannot be used for classifying the set of conditions that input data belong to. We use a freely available dataset from the DCASE 2018 challenge Task 1, subtask B, that contains data from mismatched recording devices. We consider the scenario where the annotations are available for the data recorded from one device, but not for the rest. Our results show that with our model agnostic method we can achieve ∼ 10% increase at the accuracy on an unseen and unlabeled dataset, while keeping almost the same performance on the labeled dataset.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Adversarial domain adaptation, acoustic scene classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       56 (
       <a href="https://scholar.google.com/scholar?cites=2978918002945582503" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gharib2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Gharib_132.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/shayangharib/AUDASC" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-git">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gharib2018label" class="modal fade" id="bibtex-Gharib2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGharib2018label">
        Unsupervised adversarial domain adaptation for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gharib2018,
    author = "Gharib, Shayan and Drossos, Konstantinos and Cakir, Emre and Serdyuk, Dmitriy and Virtanen, Tuomas",
    title = "Unsupervised adversarial domain adaptation for acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "138--142",
    keywords = "Adversarial domain adaptation, acoustic scene classification",
    abstract = "A general problem in acoustic scene classification task is the mismatched conditions between training and testing data, which significantly reduces the performance of the developed methods on classification accuracy. As a countermeasure, we present the first method of unsupervised adversarial domain adaptation for acoustic scene classification. We employ a model pre-trained on data from one set of conditions and by using data from other set of conditions, we adapt the model in order that its output cannot be used for classifying the set of conditions that input data belong to. We use a freely available dataset from the DCASE 2018 challenge Task 1, subtask B, that contains data from mismatched recording devices. We consider the scenario where the annotations are available for the data recorded from one device, but not for the rest. Our results show that with our model agnostic method we can achieve ∼ 10\% increase at the accuracy on an unseen and unlabeled dataset, while keeping almost the same performance on the labeled dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gontier2018" style="box-shadow: none">
<div class="panel-heading" id="headingGontier2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Towards perceptual soundscape characterization using event detection algorithms
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Félix Gontier<sup>1</sup>, Pierre Aumond<sup>2,3</sup>, Mathieu Lagrange<sup>1</sup>, Catherine Lavandier<sup>2</sup>, and Jean-Francois Petiot<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>LS2N, UMR 6004, Ecole Centrale de Nantes, Nantes, France, <sup>2</sup>ETIS, UMR 8051, Universite Paris Seine, Université de Cergy-Pontoise, ENSEA, CNRS, France, <sup>3</sup>IFSTTAR, CEREMA, UMRAE, Bouguenais, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gontier2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Gontier_9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Gontier2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gontier2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gontier2018" class="panel-collapse collapse" id="collapse-Gontier2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Assessing properties about specific sound sources is important to characterize better the perception of urban sound environments. In order to produce perceptually motivated noise maps, we argue that it is possible to consider the data produced by acoustic sensor networks to gather information about sources of interest and predict their perceptual attributes. To validate this important assumption, this paper reports on a perceptual test on simulated sound scenes for which both perceptual and acoustic source properties are known. Results show that it is indeed feasible to predict perceptual source-specific quantities of interest from recordings, leading to the introduction of two predictors of perceptual judgments from acoustic data. The use of those predictors in the new task of automatic soundscape characterization is finally discussed.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Soundscape, urban acoustic monitoring, event detection
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="https://scholar.google.com/scholar?cites=14355077165832744230" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gontier2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Gontier_9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gontier2018label" class="modal fade" id="bibtex-Gontier2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGontier2018label">
        Towards perceptual soundscape characterization using event detection algorithms
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gontier2018,
    author = "Gontier, Felix and Aumond, Pierre and Lagrange, Mathieu and Lavandier, Catherine and Petiot, Jean-François",
    title = "Towards perceptual soundscape characterization using event detection algorithms",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "14--18",
    keywords = "Soundscape, urban acoustic monitoring, event detection",
    abstract = "Assessing properties about specific sound sources is important to characterize better the perception of urban sound environments. In order to produce perceptually motivated noise maps, we argue that it is possible to consider the data produced by acoustic sensor networks to gather information about sources of interest and predict their perceptual attributes. To validate this important assumption, this paper reports on a perceptual test on simulated sound scenes for which both perceptual and acoustic source properties are known. Results show that it is indeed feasible to predict perceptual source-specific quantities of interest from recordings, leading to the introduction of two predictors of perceptual judgments from acoustic data. The use of those predictors in the new task of automatic soundscape characterization is finally discussed."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Guo2018" style="box-shadow: none">
<div class="panel-heading" id="headingGuo2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yingmei Guo<sup>1</sup>, Mingxing Xu<sup>1</sup>, Jianming Wu<sup>2</sup>, Yanan Wang<sup>2</sup>, and Keiichiro Hoashi<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Tsinghua University, Department of Computer Science and Technology, Beijing, China, <sup>2</sup>KDDI Research, Inc., Saitama,Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          17 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Guo2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Guo_121.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Guo2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Guo2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Guo2018" class="panel-collapse collapse" id="collapse-Guo2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe our contributions to the challenge of detection and classification of acoustic scenes and events 2018(DCASE2018). We propose multi-scale convolutional recurrent neural network(Multi-scale CRNN), a novel weakly-supervised learning framework for sound event detection. By integrating information from different time resolutions, the multi-scale method can capture both the fine-grained and coarse-grained features of sound events and model the temporal dependencies including fine-grained dependencies and long-term dependencies. CRNN using learnable gated linear units(GLUs) can also help to select the most related features corresponding to the audio labels. Furthermore, the ensemble method proposed in the paper can help to correct the frame-level prediction errors with classification results, as identifying the sound events occurred in the audio is much easier than providing the event time boundaries. The proposed method achieves 29.2% in the event-based F1-score and 1.40 in event-based error rate in development set of DCASE2018 task4 compared to the baseline of 14.1% F-value and 1.54 error rate[1].
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, Weakly-supervised learning, Deep learning, Convolutional recurrent neural network, Multi-scale model
      </p>
<p>
<strong>
        Cites:
       </strong>
       17 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Guo2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Guo_121.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Guo2018label" class="modal fade" id="bibtex-Guo2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGuo2018label">
        Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Guo2018,
    author = "Guo, Yingmei and Xu, Mingxing and Wu, ianming and Wang, Yanan and Hoashi, Keiichiro",
    title = "Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "98--102",
    keywords = "Sound event detection, Weakly-supervised learning, Deep learning, Convolutional recurrent neural network, Multi-scale model",
    abstract = "In this paper, we describe our contributions to the challenge of detection and classification of acoustic scenes and events 2018(DCASE2018). We propose multi-scale convolutional recurrent neural network(Multi-scale CRNN), a novel weakly-supervised learning framework for sound event detection. By integrating information from different time resolutions, the multi-scale method can capture both the fine-grained and coarse-grained features of sound events and model the temporal dependencies including fine-grained dependencies and long-term dependencies. CRNN using learnable gated linear units(GLUs) can also help to select the most related features corresponding to the audio labels. Furthermore, the ensemble method proposed in the paper can help to correct the frame-level prediction errors with classification results, as identifying the sound events occurred in the audio is much easier than providing the event time boundaries. The proposed method achieves 29.2\% in the event-based F1-score and 1.40 in event-based error rate in development set of DCASE2018 task4 compared to the baseline of 14.1\% F-value and 1.54 error rate[1]."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hammond2018" style="box-shadow: none">
<div class="panel-heading" id="headingHammond2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Robust median-plane binaural sound source localization
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Benjamin R. Hammond and Philip J.B. Jackson
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision, Speech and Signal Processing, Guildford, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hammond2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Hammond_139.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Hammond2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hammond2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hammond2018" class="panel-collapse collapse" id="collapse-Hammond2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       For a sound source on the median-plane of a binaural system, interaural localization cues are absent. So, for robust binaural localization of sound sources on the median-plane, localization methods need to be designed with this in consideration. We compare four median-plane binaural sound source localization methods. Where appropriate, adjustments to the methods have been made to improve their robustness to real world recording conditions. The methods are tested using different HRTF datasets to generate the test data and training data. Each method uses a different combination of spectral and interaural localization cues, allowing for a comparison of the effect of spectral and interaural cues on median-plane localization. The methods are tested for their robustness to different levels of additive noise and different categories of sound.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Binaural, Localization, Median-Plane
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="https://scholar.google.com/scholar?cites=11556213384045031976" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hammond2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Hammond_139.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hammond2018label" class="modal fade" id="bibtex-Hammond2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHammond2018label">
        Robust median-plane binaural sound source localization
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Hammond2018,
    author = "Hammond, Benjamin R. and Jackson, Philip J.B.",
    title = "Robust median-plane binaural sound source localization",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "168--172",
    keywords = "Binaural, Localization, Median-Plane",
    abstract = "For a sound source on the median-plane of a binaural system, interaural localization cues are absent. So, for robust binaural localization of sound sources on the median-plane, localization methods need to be designed with this in consideration. We compare four median-plane binaural sound source localization methods. Where appropriate, adjustments to the methods have been made to improve their robustness to real world recording conditions. The methods are tested using different HRTF datasets to generate the test data and training data. Each method uses a different combination of spectral and interaural localization cues, allowing for a comparison of the effect of spectral and interaural cues on median-plane localization. The methods are tested for their robustness to different levels of additive noise and different categories of sound."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Harb2018" style="box-shadow: none">
<div class="panel-heading" id="headingHarb2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound event detection using weakly labelled semi-supervised data with GCRNNs, VAT and self-adaptive label refinement
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Robert Harb<sup>1</sup> and Franz Pernkopf<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Graz University of Technology, Austria, <sup>2</sup>Graz University of Technology, Signal Processing and Speech Communication Laboratory, Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          11 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Harb2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Harb_118.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Harb2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Harb2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Harb2018" class="panel-collapse collapse" id="collapse-Harb2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present a gated convolutional recurrent neural network based approach to solve task 4, large-scale weakly labelled semi-supervised sound event detection in domestic environments, of the DCASE 2018 challenge. Gated linear units and a temporal attention layer are used to predict the onset and offset of sound events in 10s long audio clips. Whereby for training only weaklylabelled data is used. Virtual adversarial training is used for regularization, utilizing both labelled and unlabelled data. Furthermore, we introduce self-adaptive label refinement, a method which allows unsupervised adaption of our trained system to refine the accuracy of frame-level class predictions. The proposed system reaches an overall macro averaged event-based F-score of 34:6%, resulting in a relative improvement of 20:5% over the baseline system.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2018, Convolutional neural networks, Sound event detection, Weakly-supervised learning, Semisupervised learning
      </p>
<p>
<strong>
        Cites:
       </strong>
       11 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Harb2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Harb_118.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Harb2018label" class="modal fade" id="bibtex-Harb2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHarb2018label">
        Sound event detection using weakly labelled semi-supervised data with GCRNNs, VAT and self-adaptive label refinement
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Harb2018,
    author = "Harb, Robert and Pernkopf, Franz",
    title = "Sound event detection using weakly labelled semi-supervised data with GCRNNs, VAT and self-adaptive label refinement",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "83--87",
    keywords = "DCASE 2018, Convolutional neural networks, Sound event detection, Weakly-supervised learning, Semisupervised learning",
    abstract = "In this paper, we present a gated convolutional recurrent neural network based approach to solve task 4, large-scale weakly labelled semi-supervised sound event detection in domestic environments, of the DCASE 2018 challenge. Gated linear units and a temporal attention layer are used to predict the onset and offset of sound events in 10s long audio clips. Whereby for training only weaklylabelled data is used. Virtual adversarial training is used for regularization, utilizing both labelled and unlabelled data. Furthermore, we introduce self-adaptive label refinement, a method which allows unsupervised adaption of our trained system to refine the accuracy of frame-level class predictions. The proposed system reaches an overall macro averaged event-based F-score of 34:6\%, resulting in a relative improvement of 20:5\% over the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Himawan2018" style="box-shadow: none">
<div class="panel-heading" id="headingHimawan2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         3D convolutional recurrent neural networks for bird sound detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ivan Himawan, Michael Towsey, and Paul Roe
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Queensland University of Technology, Science and Engineering Faculty, Brisbane, Australia
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          24 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Himawan2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Himawan_124.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Himawan2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Himawan2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Himawan2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Himawan2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Himawan2018" class="panel-collapse collapse" id="collapse-Himawan2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       With the increasing use of a high quality acoustic device to monitor wildlife population, it has become imperative to develop techniques for analyzing animals’ calls automatically. Bird sound detection is one example of a long-term monitoring project where data are collected in continuous periods, often cover multiple sites at the same time. Inspired by the success of deep learning approaches in various audio classification tasks, this paper first reviews previous works exploiting deep learning for bird audio detection, and then proposes a novel 3-dimensional (3D) convolutional and recurrent neural networks. We propose 3D convolutions for extracting long-term and short-term information in frequency simultaneously. In order to leverage powerful and compact features of 3D convolution, we employ separate recurrent neural networks (RNN), acting on each filter of the last convolutional layers rather than stacking the feature maps in the typical combined convolution and recurrent architectures. Our best model achieved a preview of 88.70% Area Under ROC Curve (AUC) score on the unseen evaluation data in the second edition of bird audio detection challenge. Further improvement with model adaptation led to a 89.58% AUC score.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       bird sound detection, deep learning, 3D CNN, GRU, biodiversity
      </p>
<p>
<strong>
        Cites:
       </strong>
       24 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Himawan2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Himawan_124.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/himaivan/BAD2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Code">
<i class="fa fa-git">
</i>
        Code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Himawan2018label" class="modal fade" id="bibtex-Himawan2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHimawan2018label">
        3D convolutional recurrent neural networks for bird sound detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Himawan2018,
    author = "Himawan, Ivan and Towsey, Michael and Roe, Paul",
    title = "3D convolutional recurrent neural networks for bird sound detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "108--112",
    keywords = "bird sound detection, deep learning, 3D CNN, GRU, biodiversity",
    abstract = "With the increasing use of a high quality acoustic device to monitor wildlife population, it has become imperative to develop techniques for analyzing animals’ calls automatically. Bird sound detection is one example of a long-term monitoring project where data are collected in continuous periods, often cover multiple sites at the same time. Inspired by the success of deep learning approaches in various audio classification tasks, this paper first reviews previous works exploiting deep learning for bird audio detection, and then proposes a novel 3-dimensional (3D) convolutional and recurrent neural networks. We propose 3D convolutions for extracting long-term and short-term information in frequency simultaneously. In order to leverage powerful and compact features of 3D convolution, we employ separate recurrent neural networks (RNN), acting on each filter of the last convolutional layers rather than stacking the feature maps in the typical combined convolution and recurrent architectures. Our best model achieved a preview of 88.70\% Area Under ROC Curve (AUC) score on the unseen evaluation data in the second edition of bird audio detection challenge. Further improvement with model adaptation led to a 89.58\% AUC score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hou2018" style="box-shadow: none">
<div class="panel-heading" id="headingHou2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Polyphonic audio tagging with sequentially labelled data using CRNN with learnable gated linear units
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yuanbo Hou<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Jun Wang<sup>1</sup>, and Shengchen Li<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup> Beijing University of Posts and Telecommunications, Beijing, China, <sup>2</sup> Centre for Vision, Speech and Signal Processing, University of Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hou2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Hou_116.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Hou2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hou2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Hou2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hou2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hou2018" class="panel-collapse collapse" id="collapse-Hou2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio tagging aims to detect the types of sound events occurring in an audio recording. To tag the polyphonic audio recordings, we propose to use Connectionist Temporal Classification (CTC) loss function on the top of Convolutional Recurrent Neural Network (CRNN) with learnable Gated Linear Units (GLUCTC), based on a new type of audio label data: Sequentially Labelled Data (SLD). In GLU-CTC, CTC objective function maps the frame-level probability of labels to clip-level probability of labels. To compare the mapping ability of GLU-CTC for sound events, we train a CRNN with GLU based on Global Max Pooling (GLU-GMP) and a CRNN with GLU based on Global Average Pooling (GLU-GAP). And we also compare the proposed GLU-CTC system with the baseline system, which is a CRNN trained using CTC loss function without GLU. The experiments show that the GLU-CTC achieves an Area Under Curve (AUC) score of 0.882 in audio tagging, outperforming the GLU-GMP of 0.803, GLU-GAP of 0.766 and baseline system of 0.837. That means based on the same CRNN model with GLU, the performance of CTC mapping is better than the GMP and GAP mapping. Given both based on the CTC mapping, the CRNN with GLU outperforms the CRNN without GLU.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio tagging, Convolutional Recurrent Neural Network (CRNN), Gated Linear Units (GLU), Connectionist Temporal Classification (CTC), Sequentially Labelled Data (SLD)
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=2288884109074723017" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hou2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Hou_116.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/moses1994/DCASE2018-Task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Annotations">
<i class="fa fa-git">
</i>
        Annotations
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hou2018label" class="modal fade" id="bibtex-Hou2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHou2018label">
        Polyphonic audio tagging with sequentially labelled data using CRNN with learnable gated linear units
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Hou2018,
    author = "Hou, Yuanbo and Kong, Qiuqiang and Wang, Jun and Li, Shengchen",
    title = "Polyphonic audio tagging with sequentially labelled data using CRNN with learnable gated linear units",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "78--82",
    keywords = "Audio tagging, Convolutional Recurrent Neural Network (CRNN), Gated Linear Units (GLU), Connectionist Temporal Classification (CTC), Sequentially Labelled Data (SLD)",
    abstract = "Audio tagging aims to detect the types of sound events occurring in an audio recording. To tag the polyphonic audio recordings, we propose to use Connectionist Temporal Classification (CTC) loss function on the top of Convolutional Recurrent Neural Network (CRNN) with learnable Gated Linear Units (GLUCTC), based on a new type of audio label data: Sequentially Labelled Data (SLD). In GLU-CTC, CTC objective function maps the frame-level probability of labels to clip-level probability of labels. To compare the mapping ability of GLU-CTC for sound events, we train a CRNN with GLU based on Global Max Pooling (GLU-GMP) and a CRNN with GLU based on Global Average Pooling (GLU-GAP). And we also compare the proposed GLU-CTC system with the baseline system, which is a CRNN trained using CTC loss function without GLU. The experiments show that the GLU-CTC achieves an Area Under Curve (AUC) score of 0.882 in audio tagging, outperforming the GLU-GMP of 0.803, GLU-GAP of 0.766 and baseline system of 0.837. That means based on the same CRNN model with GLU, the performance of CTC mapping is better than the GMP and GAP mapping. Given both based on the CTC mapping, the CRNN with GLU outperforms the CRNN without GLU."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ikawa2018" style="box-shadow: none">
<div class="panel-heading" id="headingIkawa2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic event search with an onomatopoeic query: measuring distance between onomatopoeic words and sounds
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Shota Ikawa<sup>1</sup> and Kunio Kashino<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Graduate School of Information Science and Technology, University of Tokyo, Japan, <sup>2</sup>NTT Communication Science Laboratories, NTT Corporation, Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          11 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ikawa2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Ikawa_112.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Ikawa2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ikawa2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ikawa2018" class="panel-collapse collapse" id="collapse-Ikawa2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       As a means of searching for desired audio signals stored in a database, we consider using a string of an onomatopoeic word, namely a word that imitates a sound, as a query, which allows the user to specify the desired sound by verbally mimicking the sound or typing the sound word, or the word containing sounds similar to the desired sound. However, it is generally difficult to realize such a system based on text similarities between the onomatopoeic query and the onomatopoeic tags associated with each section of the audio signals in the database. In this paper, we propose a novel audio signal search method that uses a latent variable space obtained through a learning process. By employing an encoderdecoder onomatopoeia generation model and an encoder model for the onomatopoeias, both audio signals and onomatopoeias are mapped within the space, allowing us to directly measure the distance between them. Subjective tests show that the search results obtained with the proposed method correspond to the onomatopoeic queries reasonably well, and the method has a generalization capability when searching. We also confirm that users preferred the audio signals obtained with this approach to those obtained with a text-based similarity search.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio signal search, onomatopoeia, latent variable, encoder-decoder model
      </p>
<p>
<strong>
        Cites:
       </strong>
       11 (
       <a href="https://scholar.google.com/scholar?cites=9433256468722829884" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ikawa2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Ikawa_112.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ikawa2018label" class="modal fade" id="bibtex-Ikawa2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexIkawa2018label">
        Acoustic event search with an onomatopoeic query: measuring distance between onomatopoeic words and sounds
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ikawa2018,
    author = "Ikawa, Shota and Kashino, Kunio",
    title = "Acoustic event search with an onomatopoeic query: measuring distance between onomatopoeic words and sounds",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "59--63",
    keywords = "audio signal search, onomatopoeia, latent variable, encoder-decoder model",
    abstract = "As a means of searching for desired audio signals stored in a database, we consider using a string of an onomatopoeic word, namely a word that imitates a sound, as a query, which allows the user to specify the desired sound by verbally mimicking the sound or typing the sound word, or the word containing sounds similar to the desired sound. However, it is generally difficult to realize such a system based on text similarities between the onomatopoeic query and the onomatopoeic tags associated with each section of the audio signals in the database. In this paper, we propose a novel audio signal search method that uses a latent variable space obtained through a learning process. By employing an encoderdecoder onomatopoeia generation model and an encoder model for the onomatopoeias, both audio signals and onomatopoeias are mapped within the space, allowing us to directly measure the distance between them. Subjective tests show that the search results obtained with the proposed method correspond to the onomatopoeic queries reasonably well, and the method has a generalization capability when searching. We also confirm that users preferred the audio signals obtained with this approach to those obtained with a text-based similarity search."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Iqbal2018" style="box-shadow: none">
<div class="panel-heading" id="headingIqbal2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         General-purpose audio tagging from noisy labels using convolutional neural networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Turab Iqbal, Qiuqiang Kong, Mark D. Plumbley, and Wenwu Wang
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision, Speech and Signal Processing, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          15 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Iqbal2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Iqbal_151.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Iqbal2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Iqbal2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Iqbal2018" class="panel-collapse collapse" id="collapse-Iqbal2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       General-purpose audio tagging refers to classifying sounds that are of a diverse nature, and is relevant in many applications where domain-specific information cannot be exploited. The DCASE 2018 challenge introduces Task 2 for this very problem. In this task, there are a large number of classes and the audio clips vary in duration. Moreover, a subset of the labels are noisy. In this paper, we propose a system to address these challenges. The basis of our system is an ensemble of convolutional neural networks trained on log-scaled mel spectrograms. We use preprocessing and data augmentation methods to improve the performance further. To reduce the effects of label noise, two techniques are proposed: loss function weighting and pseudo-labeling. Experiments on the private test set of this task show that our system achieves state-of-the-art performance with a mean average precision score of 0:951.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio classification, convolutional network, recurrent network, deep learning, data augmentation, label noise
      </p>
<p>
<strong>
        Cites:
       </strong>
       15 (
       <a href="https://scholar.google.com/scholar?cites=16660388754749711609" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Iqbal2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Iqbal_151.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Iqbal2018label" class="modal fade" id="bibtex-Iqbal2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexIqbal2018label">
        General-purpose audio tagging from noisy labels using convolutional neural networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Iqbal2018,
    author = "Iqbal, Turab and Kong, Qiuqiang and Plumbley, Mark D. and Wang, Wenwu",
    title = "General-purpose audio tagging from noisy labels using convolutional neural networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "212--216",
    keywords = "Audio classification, convolutional network, recurrent network, deep learning, data augmentation, label noise",
    abstract = "General-purpose audio tagging refers to classifying sounds that are of a diverse nature, and is relevant in many applications where domain-specific information cannot be exploited. The DCASE 2018 challenge introduces Task 2 for this very problem. In this task, there are a large number of classes and the audio clips vary in duration. Moreover, a subset of the labels are noisy. In this paper, we propose a system to address these challenges. The basis of our system is an ensemble of convolutional neural networks trained on log-scaled mel spectrograms. We use preprocessing and data augmentation methods to improve the performance further. To reduce the effects of label noise, two techniques are proposed: loss function weighting and pseudo-labeling. Experiments on the private test set of this task show that our system achieves state-of-the-art performance with a mean average precision score of 0:951."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jeong2018" style="box-shadow: none">
<div class="panel-heading" id="headingJeong2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Audio tagging system using densely connected convolutional networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Il-Young Jeong and Hyungui Lim
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Cochlear.ai, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          12 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jeong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Jeong_148.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Jeong2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jeong2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jeong2018" class="panel-collapse collapse" id="collapse-Jeong2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe the techniques and models applied to our submission for DCASE 2018 task 2: General-purpose audio tagging of Freesound content with AudioSet labels. We mainly focus on how to train deep learning models efficiently against strong augmentation and label noise. First, we conducted a single-block DenseNet architecture and multi-head softmax classifier for efficient learning with mixup augmentation. For the label noise, we applied the batch-wise loss masking to eliminate the loss of outliers in a mini-batch. We also tried an ensemble of various models, trained by using different sampling rate or audio representation.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio tagging, DenseNet, Mixup, Multi-head softmax, Batch-wise loss masking
      </p>
<p>
<strong>
        Cites:
       </strong>
       12 (
       <a href="https://scholar.google.com/scholar?cites=10883737846143147757" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jeong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Jeong_148.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jeong2018label" class="modal fade" id="bibtex-Jeong2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJeong2018label">
        Audio tagging system using densely connected convolutional networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jeong2018,
    author = "Jeong, Il-Young and Lim, Hyungui",
    title = "Audio tagging system using densely connected convolutional networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "197--201",
    keywords = "Audio tagging, DenseNet, Mixup, Multi-head softmax, Batch-wise loss masking",
    abstract = "In this paper, we describe the techniques and models applied to our submission for DCASE 2018 task 2: General-purpose audio tagging of Freesound content with AudioSet labels. We mainly focus on how to train deep learning models efficiently against strong augmentation and label noise. First, we conducted a single-block DenseNet architecture and multi-head softmax classifier for efficient learning with mixup augmentation. For the label noise, we applied the batch-wise loss masking to eliminate the loss of outliers in a mini-batch. We also tried an ensemble of various models, trained by using different sampling rate or audio representation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jung2018" style="box-shadow: none">
<div class="panel-heading" id="headingJung2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DNN based multi-level feature ensemble for acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jee-weon Jung, Hee-soo Heo, Hye-jin Shim, and Ha-jin Yu
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Seoul, School of Computer Science, South Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jung2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Jung_128.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Jung2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jung2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jung2018" class="panel-collapse collapse" id="collapse-Jung2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Various characteristics can be used to define an acoustic scene, such as long-term context information and short-term events. This makes it difficult to select input features and pre-processing methods suitable for acoustic scene classification. In this paper, we propose an ensemble model that exploits various input features in which the strength for classifying an acoustic scene varies: i-vectors are used for segment-level representations of long-term context, spectrograms are used for frame-level short-term events, and raw waveforms are used to extract features that could be missed by existing methods. For each feature, we used deep neural network based models to extract a representation from an input segment. A separated scoring phase was then exploited to extract class-wise scores on a scale of 0 to 1 that could be used as confidence measures. Scores were extracted using Gaussian models and support vector machines. We tested the validity of the proposed framework using task 1 of detection, and classification of acoustic scenes and events 2018 dataset. The proposed framework had an accuracy of 73.82% for the pre-defined fold-1 validation setup and 74.8% for the evaluation setup which is 7th in team ranking.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, DNN, raw waveform, i-vector
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=3058632906581611346" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jung2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Jung_128.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jung2018label" class="modal fade" id="bibtex-Jung2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJung2018label">
        DNN based multi-level feature ensemble for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jung2018,
    author = "Jung, Jee-weon and Heo, Hee-soo and Shim, Hye-jin and Yu, Ha-jin",
    title = "{DNN} based multi-level feature ensemble for acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "113--117",
    keywords = "Acoustic scene classification, DNN, raw waveform, i-vector",
    abstract = "Various characteristics can be used to define an acoustic scene, such as long-term context information and short-term events. This makes it difficult to select input features and pre-processing methods suitable for acoustic scene classification. In this paper, we propose an ensemble model that exploits various input features in which the strength for classifying an acoustic scene varies: i-vectors are used for segment-level representations of long-term context, spectrograms are used for frame-level short-term events, and raw waveforms are used to extract features that could be missed by existing methods. For each feature, we used deep neural network based models to extract a representation from an input segment. A separated scoring phase was then exploited to extract class-wise scores on a scale of 0 to 1 that could be used as confidence measures. Scores were extracted using Gaussian models and support vector machines. We tested the validity of the proposed framework using task 1 of detection, and classification of acoustic scenes and events 2018 dataset. The proposed framework had an accuracy of 73.82\% for the pre-defined fold-1 validation setup and 74.8\% for the evaluation setup which is 7th in team ranking."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2018" style="box-shadow: none">
<div class="panel-heading" id="headingKim2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Vocal Imitation Set: a dataset of vocally imitated sound events using the AudioSet ontology
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Bongjun Kim<sup>1</sup>, Madhav Ghei<sup>1</sup>, Bryan Pardo<sup>1</sup>, and Zhiyao Duan<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Northwestern University, Department of Electrical Engineering and Computer Science, Evanston, USA, <sup>2</sup>University of Rochester, Department of Electrical and Computer Engineering, Rochester, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          26 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Kim_135.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kim2018').collapse('show');window.location.hash='#Kim2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Kim2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2018" class="panel-collapse collapse" id="collapse-Kim2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Query-By-Vocal Imitation (QBV) search systems enable searching a collection of audio files using a vocal imitation as a query. This can be useful when sounds do not have commonly agreed-upon textlabels, or many sounds share a label. As deep learning approaches have been successfully applied to QBV systems, datasets to build models have become more important. We present Vocal Imitation Set, a new vocal imitation dataset containing 11; 242 crowd-sourced vocal imitations of 302 sound event classes in the AudioSet sound event ontology. It is the largest publicly-available dataset of vocal imitations as well as the first to adopt the widely-used AudioSet ontology for a vocal imitation dataset. Each imitation recording in Vocal Imitation Set was rated by a human listener on how similar the imitation is to the recording it was an imitation of. Vocal Imitation Set also has an average of 10 different original recordings per sound class. Since each sound class has about 19 listener-vetted imitations and 10 original sound files, the data set is suited for training models to do fine-grained vocal imitation-based search within sound classes. We provide an example of using the dataset to measure how well the existing state-of-the-art in QBV search performs on fine-grained search.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Vocal imitation datasets, audio retrieval, queryby-vocal imitation search
      </p>
<p>
<strong>
        Cites:
       </strong>
       26 (
       <a href="https://scholar.google.com/scholar?cites=6981864882963875212" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Kim_135.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://zenodo.org/record/1340763#.W-wmClVfi7A" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        Vocal Imitation Set v1.1.3
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2018label" class="modal fade" id="bibtex-Kim2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2018label">
        Vocal Imitation Set: a dataset of vocally imitated sound events using the AudioSet ontology
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kim2018,
    author = "Kim, Bongjun and Ghei, Madhav and Pardo, Bryan and Duan, Zhiyao",
    title = "{Vocal Imitation Set}: a dataset of vocally imitated sound events using the AudioSet ontology",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "148--152",
    keywords = "Vocal imitation datasets, audio retrieval, queryby-vocal imitation search",
    abstract = "Query-By-Vocal Imitation (QBV) search systems enable searching a collection of audio files using a vocal imitation as a query. This can be useful when sounds do not have commonly agreed-upon textlabels, or many sounds share a label. As deep learning approaches have been successfully applied to QBV systems, datasets to build models have become more important. We present Vocal Imitation Set, a new vocal imitation dataset containing 11; 242 crowd-sourced vocal imitations of 302 sound event classes in the AudioSet sound event ontology. It is the largest publicly-available dataset of vocal imitations as well as the first to adopt the widely-used AudioSet ontology for a vocal imitation dataset. Each imitation recording in Vocal Imitation Set was rated by a human listener on how similar the imitation is to the recording it was an imitation of. Vocal Imitation Set also has an average of 10 different original recordings per sound class. Since each sound class has about 19 listener-vetted imitations and 10 original sound files, the data set is suited for training models to do fine-grained vocal imitation-based search within sound classes. We provide an example of using the dataset to measure how well the existing state-of-the-art in QBV search performs on fine-grained search."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2018" style="box-shadow: none">
<div class="panel-heading" id="headingKong2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE 2018 Challenge Surrey cross-task convolutional neural network baseline
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Qiuqiang Kong, Turab Iqbal, Yong Xu, Wenwu Wang, Mark D. Plumbley
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision, Speech and Signal Processing, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          69 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Kong_152.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Kong2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Kong2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2018" class="panel-collapse collapse" id="collapse-Kong2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) consists of five audio classification and sound event detection tasks: 1) Acoustic scene classification, 2) General-purpose audio tagging of Freesound, 3) Bird audio detection, 4) Weakly-labeled semi-supervised sound event detection and 5) Multi-channel audio classification. In this paper, we create a cross-task baseline system for all five tasks based on a convlutional neural network (CNN): a “CNN Baseline” system. We implemented CNNs with 4 layers and 8 layers originating from AlexNet and VGG from computer vision. We investigated how the performance varies from task to task with the same configuration of neural networks. Experiments show that deeper CNN with 8 layers performs better than CNN with 4 layers on all tasks except Task 1. Using CNN with 8 layers, we achieve an accuracy of 0.680 on Task 1, an accuracy of 0.895 and a mean average precision (MAP) of 0.928 on Task 2, an accuracy of 0.751 and an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8% on Task 4, and an F1 score of 87.75% on Task 5. We released the Python source code of the baseline systems under the MIT license for further research.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2018 challenge, convolutional neural networks, open source
      </p>
<p>
<strong>
        Cites:
       </strong>
       69 (
       <a href="https://scholar.google.com/scholar?cites=10030635746648879515" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Kong_152.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Task1 code">
<i class="fa fa-git">
</i>
        Task1 code
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Task2 code">
<i class="fa fa-git">
</i>
        Task2 code
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task3" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Task3 code">
<i class="fa fa-git">
</i>
        Task3 code
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Task4 code">
<i class="fa fa-git">
</i>
        Task4 code
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task5" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Task5 code">
<i class="fa fa-git">
</i>
        Task5 code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2018label" class="modal fade" id="bibtex-Kong2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2018label">
        DCASE 2018 Challenge Surrey cross-task convolutional neural network baseline
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kong2018,
    author = "Kong, Qiuqiang and Iqbal, Turab and Xu, Yong and Wang, Wenwu and Plumbley, Mark D.",
    title = "{DCASE 2018} Challenge Surrey cross-task convolutional neural network baseline",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "217--221",
    keywords = "DCASE 2018 challenge, convolutional neural networks, open source",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) consists of five audio classification and sound event detection tasks: 1) Acoustic scene classification, 2) General-purpose audio tagging of Freesound, 3) Bird audio detection, 4) Weakly-labeled semi-supervised sound event detection and 5) Multi-channel audio classification. In this paper, we create a cross-task baseline system for all five tasks based on a convlutional neural network (CNN): a “CNN Baseline” system. We implemented CNNs with 4 layers and 8 layers originating from AlexNet and VGG from computer vision. We investigated how the performance varies from task to task with the same configuration of neural networks. Experiments show that deeper CNN with 8 layers performs better than CNN with 4 layers on all tasks except Task 1. Using CNN with 8 layers, we achieve an accuracy of 0.680 on Task 1, an accuracy of 0.895 and a mean average precision (MAP) of 0.928 on Task 2, an accuracy of 0.751 and an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8\% on Task 4, and an F1 score of 87.75\% on Task 5. We released the Python source code of the baseline systems under the MIT license for further research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Koutini2018" style="box-shadow: none">
<div class="panel-heading" id="headingKoutini2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Iterative knowledge distillation in R-CNNs for weakly-labeled semi-supervised sound event detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Khaled Koutini, Hamid Eghbal-zadeh, and Gerhard Widmer
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Johannes Kepler University, Institute of Computational Perception, Linz, Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          22 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Koutini2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Koutini_142.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Koutini2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Koutini2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Koutini2018" class="panel-collapse collapse" id="collapse-Koutini2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present our approach used for the CP-JKU submission in Task 4 of the DCASE-2018 Challenge. We propose a novel iterative knowledge distillation technique for weakly-labeled semi-supervised event detection using neural networks, specifically Recurrent Convolutional Neural Networks (R-CNNs). R-CNNs are used to tag the unlabeled data and predict strong labels. Further, we use the R-CNN strong pseudo-labels on the training datasets and train new models after applying label-smoothing techniques on the strong pseudo-labels. Our proposed approach significantly improved the performance of the baseline, achieving the event-based f-measure of 40.86% compared to 15.11% event-based f-measure of the baseline in the provided test set from the development dataset.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Weakly-labeled, Semi-supervised, Knowledge Distillation, Recurrent Neural Network, Convolutional Neural Network
      </p>
<p>
<strong>
        Cites:
       </strong>
       22 (
       <a href="https://scholar.google.com/scholar?cites=12850713671408730447" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Koutini2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Koutini_142.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Koutini2018label" class="modal fade" id="bibtex-Koutini2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKoutini2018label">
        Iterative knowledge distillation in R-CNNs for weakly-labeled semi-supervised sound event detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Koutini2018,
    author = "Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard",
    title = "Iterative knowledge distillation in {R-CNNs} for weakly-labeled semi-supervised sound event detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "173--177",
    keywords = "Weakly-labeled, Semi-supervised, Knowledge Distillation, Recurrent Neural Network, Convolutional Neural Network",
    abstract = "In this paper, we present our approach used for the CP-JKU submission in Task 4 of the DCASE-2018 Challenge. We propose a novel iterative knowledge distillation technique for weakly-labeled semi-supervised event detection using neural networks, specifically Recurrent Convolutional Neural Networks (R-CNNs). R-CNNs are used to tag the unlabeled data and predict strong labels. Further, we use the R-CNN strong pseudo-labels on the training datasets and train new models after applying label-smoothing techniques on the strong pseudo-labels. Our proposed approach significantly improved the performance of the baseline, achieving the event-based f-measure of 40.86\% compared to 15.11\% event-based f-measure of the baseline in the provided test set from the development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lasseck2018" style="box-shadow: none">
<div class="panel-heading" id="headingLasseck2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic bird detection with deep convolutional neural networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Mario Lasseck
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Museum fuer Naturkunde, Leibniz Institute for Evolution and Biodiversity Science, Berlin, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          61 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lasseck2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Lasseck_134.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lasseck2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lasseck2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lasseck2018" class="panel-collapse collapse" id="collapse-Lasseck2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 \% on the public challenge leaderboard.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Bird Detection, Deep Learning, Deep Convolutional Neural Networks, Data Augmentation
      </p>
<p>
<strong>
        Cites:
       </strong>
       61 (
       <a href="https://scholar.google.com/scholar?cites=9050595568643444487" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lasseck2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Lasseck_134.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lasseck2018label" class="modal fade" id="bibtex-Lasseck2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLasseck2018label">
        Acoustic bird detection with deep convolutional neural networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lasseck2018,
    author = "Lasseck, Mario",
    title = "Acoustic bird detection with deep convolutional neural networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "143--147",
    keywords = "Bird Detection, Deep Learning, Deep Convolutional Neural Networks, Data Augmentation",
    abstract = "This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 \\% on the public challenge leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lederle2018" style="box-shadow: none">
<div class="panel-heading" id="headingLederle2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Combining high-level features of raw audio waves and mel-spectrograms for audio tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Marcel Lederle and Benjamin Wilhelm
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Konstanz, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          7 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lederle2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Lederle_150.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lederle2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lederle2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lederle2018" class="panel-collapse collapse" id="collapse-Lederle2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe our contribution to Task 2 of the DCASE 2018 Audio Challenge [1]. While it has become ubiquitous to utilize an ensemble of machine learning methods for classification tasks to obtain better predictive performance, the majority of ensemble methods combine predictions rather than learned features. We propose a single-model method that combines learned highlevel features computed from log-scaled mel-spectrograms and raw audio data. These features are learned separately by two Convolutional Neural Networks, one for each input type, and then combined by densely connected layers within a single network. This relatively simple approach along with data augmentation ranks among the best two percent in the Freesound General-Purpose Audio Tagging Challenge on Kaggle.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio-tagging, convolutional neural network, raw audio, mel-spectrogram
      </p>
<p>
<strong>
        Cites:
       </strong>
       7 (
       <a href="https://scholar.google.com/scholar?cites=5349311940706203773" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lederle2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Lederle_150.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lederle2018label" class="modal fade" id="bibtex-Lederle2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLederle2018label">
        Combining high-level features of raw audio waves and mel-spectrograms for audio tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lederle2018,
    author = "Lederle, Marcel and Wilhelm, Benjamin",
    title = "Combining high-level features of raw audio waves and mel-spectrograms for audio tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "207--211",
    keywords = "audio-tagging, convolutional neural network, raw audio, mel-spectrogram",
    abstract = "In this paper, we describe our contribution to Task 2 of the DCASE 2018 Audio Challenge [1]. While it has become ubiquitous to utilize an ensemble of machine learning methods for classification tasks to obtain better predictive performance, the majority of ensemble methods combine predictions rather than learned features. We propose a single-model method that combines learned highlevel features computed from log-scaled mel-spectrograms and raw audio data. These features are learned separately by two Convolutional Neural Networks, one for each input type, and then combined by densely connected layers within a single network. This relatively simple approach along with data augmentation ranks among the best two percent in the Freesound General-Purpose Audio Tagging Challenge on Kaggle."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2018" style="box-shadow: none">
<div class="panel-heading" id="headingLi2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Fast mosquito acoustic detection with field cup recordings: an initial investigation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yunpeng Li<sup>1</sup>, Ivan Kiskin<sup>1</sup>, Marianne Sinka<sup>2</sup>, Davide Zilli<sup>1,3</sup>, Henry Chan<sup>1</sup>, Eva Herreros-Moya<sup>2</sup>, Theeraphap Chareonviriyaphap<sup>4</sup>, Rungarun Tisgratog<sup>4</sup>, Kathy Willis<sup>2,5</sup>, and Stephen Roberts<sup>1,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>University of Oxford, Machine Learning Research Group, Department of Engineering Science, UK, <sup>2</sup>University of Oxford, Department of Zoology, UK, <sup>3</sup>Mind Foundry Ltd., UK, <sup>4</sup>Kasetsart University, Department of Entomology, Faculty of Agriculture, Bangkok, Thailand, <sup>5</sup>Royal Botanic Gardens, Kew, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          7 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Li_136.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Li2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2018" class="panel-collapse collapse" id="collapse-Li2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In terms of vectoring disease, mosquitoes are the world’s deadliest. A fast and efficient mosquito survey tool is crucial for vectored disease intervention programmes to reduce mosquito-induced deaths. Standard mosquito sampling techniques, such as human landing catches, are time consuming, expensive and can put the collectors at risk of diseases. Mosquito acoustic detection aims to provide a cost-effective automated detection tool, based on mosquitoes’ characteristic flight tones. We propose a simple, yet highly effective, classification pipeline based on the mel-frequency spectrum allied with convolutional neural networks. This detection pipeline is computationally efficient in not only detecting mosquitoes, but also in classifying species. Many previous assessments of mosquito acoustic detection techniques have relied only upon lab recordings of mosquito colonies. We illustrate in this paper our proposed algorithm’s performance over an extensive dataset, consisting of cup recordings of more than 1000 mosquito individuals from 6 species captured in field studies in Thailand.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Mosquito detection, acoustic signal processing, multi-species classification, convolutional neural networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       7 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Li_136.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2018label" class="modal fade" id="bibtex-Li2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2018label">
        Fast mosquito acoustic detection with field cup recordings: an initial investigation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Li2018,
    author = "Li, Yunpeng and Kiskin, Ivan and Sinka, Marianne and Zilli, Davide and Chan, Henry and Herreros-Moya, Eva and Chareonviriyaphap, Theeraphap and Tisgratog, Rungarun and Willis, Kathy and Roberts, Stephen",
    title = "Fast mosquito acoustic detection with field cup recordings: an initial investigation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "153--157",
    keywords = "Mosquito detection, acoustic signal processing, multi-species classification, convolutional neural networks",
    abstract = "In terms of vectoring disease, mosquitoes are the world’s deadliest. A fast and efficient mosquito survey tool is crucial for vectored disease intervention programmes to reduce mosquito-induced deaths. Standard mosquito sampling techniques, such as human landing catches, are time consuming, expensive and can put the collectors at risk of diseases. Mosquito acoustic detection aims to provide a cost-effective automated detection tool, based on mosquitoes’ characteristic flight tones. We propose a simple, yet highly effective, classification pipeline based on the mel-frequency spectrum allied with convolutional neural networks. This detection pipeline is computationally efficient in not only detecting mosquitoes, but also in classifying species. Many previous assessments of mosquito acoustic detection techniques have relied only upon lab recordings of mosquito colonies. We illustrate in this paper our proposed algorithm’s performance over an extensive dataset, consisting of cup recordings of more than 1000 mosquito individuals from 6 species captured in field studies in Thailand."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liaqat2018" style="box-shadow: none">
<div class="panel-heading" id="headingLiaqat2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Domain tuning methods for bird audio detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sidrah Liaqat, Narjes Bozorg, Neenu Jose, Patrick Conrey, Antony Tamasi and Michael T. Johnson
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Kentucky, Speech and Signal Processing Lab, Electrical Engineering Department, Lexington, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liaqat2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Liaqat_138.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Liaqat2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liaqat2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liaqat2018" class="panel-collapse collapse" id="collapse-Liaqat2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents several feature extraction and normalization methods implemented for the DCASE 2018 Bird Audio Detection challenge, a binary audio classification task, to identify whether a ten second audio segment from a specified dataset contains one or more bird vocalizations. Our baseline system is adapted from the Convolutional Neural Network system of last year’s challenge winner bulbul [1]. We introduce one feature modification, an increase in temporal resolution of the Mel-spectrogram feature matrix, tailored to the fast-changing temporal structure of many song-bird vocalizations. Additionally, we introduce two feature normalization approaches, a front-end signal enhancement method to reduce differences in dataset noise characteristics and an explicit domain adaptation method based on covariance normalization. Results show that none of these approaches gave significant benefit individually, but that combining the methods lead to overall improvement. Despite the modest improvement, this system won the award for “Highestscoring open-source/reproducible method” for this task.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio classification, convolutional neural network, bioacoustic vocalization analysis, domain adaptation
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liaqat2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Liaqat_138.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liaqat2018label" class="modal fade" id="bibtex-Liaqat2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiaqat2018label">
        Domain tuning methods for bird audio detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Liaqat2018,
    author = "Liaqat, Sidrah and Bozorg, Narjes and Jose, Neenu and Conrey, Patrick and Tamasi, Anthony and Johnson, Michael T.",
    title = "Domain tuning methods for bird audio detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "163--167",
    keywords = "audio classification, convolutional neural network, bioacoustic vocalization analysis, domain adaptation",
    abstract = "This paper presents several feature extraction and normalization methods implemented for the DCASE 2018 Bird Audio Detection challenge, a binary audio classification task, to identify whether a ten second audio segment from a specified dataset contains one or more bird vocalizations. Our baseline system is adapted from the Convolutional Neural Network system of last year’s challenge winner bulbul [1]. We introduce one feature modification, an increase in temporal resolution of the Mel-spectrogram feature matrix, tailored to the fast-changing temporal structure of many song-bird vocalizations. Additionally, we introduce two feature normalization approaches, a front-end signal enhancement method to reduce differences in dataset noise characteristics and an explicit domain adaptation method based on covariance normalization. Results show that none of these approaches gave significant benefit individually, but that combining the methods lead to overall improvement. Despite the modest improvement, this system won the award for “Highestscoring open-source/reproducible method” for this task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lim2018" style="box-shadow: none">
<div class="panel-heading" id="headingLim2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Weakly labeled semi-supervised sound event detection using CRNN with inception module
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Wootaek Lim, Sangwon Suh, and Youngho Jeong
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Realistic AV Research Group, Electronics and Telecommunications Research Institute, Daejeon, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          16 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Lim_115.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lim2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lim2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lim2018" class="panel-collapse collapse" id="collapse-Lim2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present a method for large-scale detection of sound events using small weakly labeled data proposed in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 challenge Task 4. To perform this task, we adopted the convolutional neural network (CNN) and gated recurrent unit (GRU) based bidirectional recurrent neural network (RNN) as our proposed system. In addition, we proposed the Inception module for handling various receptive fields at once in each CNN layer. We also applied the data augmentation method to solve the labeled data shortage problem and applied the event activity detection method for strong label learning. By applying the proposed method to a weakly labeled semi-supervised sound event detection, it was verified that the proposed system provides better performance compared to the DCASE 2018 baseline system.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2018, Sound event detection, Weakly labeled semi-supervised learning, Deep learning, Inception module
      </p>
<p>
<strong>
        Cites:
       </strong>
       16 (
       <a href="https://scholar.google.com/scholar?cites=4361295654627945793" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Lim_115.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lim2018label" class="modal fade" id="bibtex-Lim2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLim2018label">
        Weakly labeled semi-supervised sound event detection using CRNN with inception module
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lim2018,
    author = "Lim, Wootaek and Suh, Sangwon and Jeong, Youngho",
    title = "Weakly labeled semi-supervised sound event detection using CRNN with inception module",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "74--77",
    keywords = "DCASE 2018, Sound event detection, Weakly labeled semi-supervised learning, Deep learning, Inception module",
    abstract = "In this paper, we present a method for large-scale detection of sound events using small weakly labeled data proposed in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 challenge Task 4. To perform this task, we adopted the convolutional neural network (CNN) and gated recurrent unit (GRU) based bidirectional recurrent neural network (RNN) as our proposed system. In addition, we proposed the Inception module for handling various receptive fields at once in each CNN layer. We also applied the data augmentation method to solve the labeled data shortage problem and applied the event activity detection method for strong label learning. By applying the proposed method to a weakly labeled semi-supervised sound event detection, it was verified that the proposed system provides better performance compared to the DCASE 2018 baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liping2018" style="box-shadow: none">
<div class="panel-heading" id="headingLiping2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic scene classification using multi-scale features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Liping Yang , Xinxing Chen, and Lianjie Tao
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Chongqing University, Key Laboratory of Optoelectronic Technology and System, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          48 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liping2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Liping_37.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Liping2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liping2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liping2018" class="panel-collapse collapse" id="collapse-Liping2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Convolutional neural networks(CNNs) has shown tremendous ability in many classification problems, because it could improve classification performance by extracting abstract features. In this paper, we use CNNs to calculate features layer by layer. With the layers deepen, the extracted features become more abstract, but the shallow features are also very useful for classification. So we propose a method that fuses features of different layers(it’s called multi-scale features), which can improve performance of acoustic scene classification. In our method, the logMel features of audio signal are used as the input of CNNs. In order to reduce the parameters’ number, we use Xception as the foundation network, which is a CNNs with depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). And we modify Xception to fuse multi-scale features. We also introduce the focal loss, to further improve classification performance. This method can achieve commendable result, whether the audio recordings are collected by same device(subtask A) or by different devices (subtask B).
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Multi-scale features, acoustic scene classification, convolutional neural network, Xception, log Mel features
      </p>
<p>
<strong>
        Cites:
       </strong>
       48 (
       <a href="https://scholar.google.com/scholar?cites=14630394828767467308" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liping2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Liping_37.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liping2018label" class="modal fade" id="bibtex-Liping2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiping2018label">
        Acoustic scene classification using multi-scale features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Liping2018,
    author = "Liping, Yang and Xinxing, Chen and Lianjie, Tao",
    title = "Acoustic scene classification using multi-scale features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "29--33",
    keywords = "Multi-scale features, acoustic scene classification, convolutional neural network, Xception, log Mel features",
    abstract = "Convolutional neural networks(CNNs) has shown tremendous ability in many classification problems, because it could improve classification performance by extracting abstract features. In this paper, we use CNNs to calculate features layer by layer. With the layers deepen, the extracted features become more abstract, but the shallow features are also very useful for classification. So we propose a method that fuses features of different layers(it’s called multi-scale features), which can improve performance of acoustic scene classification. In our method, the logMel features of audio signal are used as the input of CNNs. In order to reduce the parameters’ number, we use Xception as the foundation network, which is a CNNs with depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). And we modify Xception to fuse multi-scale features. We also introduce the focal loss, to further improve classification performance. This method can achieve commendable result, whether the audio recordings are collected by same device(subtask A) or by different devices (subtask B)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Maka2018" style="box-shadow: none">
<div class="panel-heading" id="headingMaka2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Audio feature space analysis for acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Tomasz Maka
        </em>
</p>
<p class="text-muted">
<small>
<em>
          West Pomeranian University of Technology, Faculty of Computer Science and Information Technology, Szczecin, Poland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Maka2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Maka_127.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Maka2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Maka2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Maka2018" class="panel-collapse collapse" id="collapse-Maka2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The paper presents a study of audio features analysis for acoustic scene classification. Various feature sets and many classifiers were employed to build a system for scene classification by determining compact feature space and using an ensemble learning. The input feature space containing different sets and representations were reduced to 223 attributes using the importance of individual features computed by gradient boosting trees algorithm. The resulting set of features was split into distinct groups partly reflected auditory cues, and then their contribution to discriminative power was analysed. Also, to determine the influence of the pattern recognition system on the final efficacy, accuracy tests were performed using several classifiers. Finally, conducted experiments show that proposed solution with a dedicated feature set outperformed baseline system by 6%.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio features, auditory scene analysis, ensemble learning, majority voting
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="https://scholar.google.com/scholar?cites=10069355063744178989" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Maka2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Maka_127.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" href="http://quefrency.org/dcase2018/" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="ASA Browser">
<i class="fa fa-external-link-square">
</i>
        ASA Browser
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Maka2018label" class="modal fade" id="bibtex-Maka2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMaka2018label">
        Audio feature space analysis for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Maka2018,
    author = "Maka, Tomasz",
    title = "Audio feature space analysis for acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "113--117",
    keywords = "audio features, auditory scene analysis, ensemble learning, majority voting",
    abstract = "The paper presents a study of audio features analysis for acoustic scene classification. Various feature sets and many classifiers were employed to build a system for scene classification by determining compact feature space and using an ensemble learning. The input feature space containing different sets and representations were reduced to 223 attributes using the importance of individual features computed by gradient boosting trees algorithm. The resulting set of features was split into distinct groups partly reflected auditory cues, and then their contribution to discriminative power was analysed. Also, to determine the influence of the pattern recognition system on the final efficacy, accuracy tests were performed using several classifiers. Finally, conducted experiments show that proposed solution with a dedicated feature set outperformed baseline system by 6\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Margolis2018" style="box-shadow: none">
<div class="panel-heading" id="headingMargolis2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Applying triplet loss to siamese-style networks for audio similarity ranking
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Brian Margolis, Madhav Ghei, and Bryan Pardo
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Northwestern University, Electrical Engineering and Computer Science, Evanston, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Margolis2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Margolis_130.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Margolis2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Margolis2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Margolis2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Margolis2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Margolis2018" class="panel-collapse collapse" id="collapse-Margolis2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Query by vocal imitation (QBV) systems let users search a library of general non-speech audio files using a vocal imitation of the desired sound as the query. The best existing system for QBV uses a similarity measure between vocal imitations and general audio files that is learned by a two-tower semi-Siamese deep neural network architecture. This approach typically uses pairwise training examples and error measurement. In this work, we show that this pairwise error signal does not correlate well with improved search rankings and instead describe how triplet loss can be used to train a two-tower network designed to work with pairwise loss, resulting in better correlation with search rankings. This approach can be used to train any two-tower architecture using triplet loss. Empirical results on a dataset of vocal imitations and general audio files show that low triplet loss is much better correlated with improved search ranking than low pairwise loss.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       vocal imitation, information retrieval, convolutional Siamese-style networks, triplet loss
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Margolis2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Margolis_130.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/interactiveaudiolab/Siamese-Vocal-Imitations" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Code">
<i class="fa fa-git">
</i>
        Code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Margolis2018label" class="modal fade" id="bibtex-Margolis2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMargolis2018label">
        Applying triplet loss to siamese-style networks for audio similarity ranking
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Margolis2018,
    author = "Margolis, Brian and Ghei, Madhav and Pardo, Bryan",
    title = "Applying triplet loss to siamese-style networks for audio similarity ranking",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "128--132",
    keywords = "vocal imitation, information retrieval, convolutional Siamese-style networks, triplet loss",
    abstract = "Query by vocal imitation (QBV) systems let users search a library of general non-speech audio files using a vocal imitation of the desired sound as the query. The best existing system for QBV uses a similarity measure between vocal imitations and general audio files that is learned by a two-tower semi-Siamese deep neural network architecture. This approach typically uses pairwise training examples and error measurement. In this work, we show that this pairwise error signal does not correlate well with improved search rankings and instead describe how triplet loss can be used to train a two-tower network designed to work with pairwise loss, resulting in better correlation with search rankings. This approach can be used to train any two-tower architecture using triplet loss. Empirical results on a dataset of vocal imitations and general audio files show that low triplet loss is much better correlated with improved search ranking than low pairwise loss."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mariotti2018" style="box-shadow: none">
<div class="panel-heading" id="headingMariotti2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Exploring deep vision models for acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Octave Mariotti, Matthieu Cord, and Olivier Schwander
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Sorbonne Université, CNRS, Laboratoire d’Informatique de Paris 6, Paris, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          25 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mariotti2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Mariotti_123.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Mariotti2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mariotti2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mariotti2018" class="panel-collapse collapse" id="collapse-Mariotti2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report evaluates the application of deep vision models, namely VGG and Resnet, to general audio recognition. In the context of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events 2018, we trained several of these architectures on the task 1 dataset to perform acoustic scene classification. Then, in order to produce more robust predictions, we explored two ensemble methods to aggregate the different model outputs. Our results show a final accuracy of 79% on the development dataset for subtask A, outperforming the baseline by almost 20%.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, DCASE 2018, Vision, VGG, Residual networks, Ensemble
      </p>
<p>
<strong>
        Cites:
       </strong>
       25 (
       <a href="https://scholar.google.com/scholar?cites=11872934308094658501" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mariotti2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Mariotti_123.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mariotti2018label" class="modal fade" id="bibtex-Mariotti2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMariotti2018label">
        Exploring deep vision models for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mariotti2018,
    author = "Mariotti, Octave and Cord, Matthieu and Schwander, Olivier",
    title = "Exploring deep vision models for acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "103--107",
    keywords = "Acoustic scene classification, DCASE 2018, Vision, VGG, Residual networks, Ensemble",
    abstract = "This report evaluates the application of deep vision models, namely VGG and Resnet, to general audio recognition. In the context of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events 2018, we trained several of these architectures on the task 1 dataset to perform acoustic scene classification. Then, in order to produce more robust predictions, we explored two ensemble methods to aggregate the different model outputs. Our results show a final accuracy of 79\% on the development dataset for subtask A, outperforming the baseline by almost 20\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mesaros2018" style="box-shadow: none">
<div class="panel-heading" id="headingMesaros2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A multi-device dataset for urban acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Tampere University of Technology, Laboratory of Signal Processing, Tampere, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          409 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mesaros2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Mesaros2018').collapse('show');window.location.hash='#Mesaros2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Mesaros2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mesaros2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mesaros2018" class="panel-collapse collapse" id="collapse-Mesaros2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, DCASE challenge, public datasets, multi-device data
      </p>
<p>
<strong>
        Cites:
       </strong>
       409 (
       <a href="https://scholar.google.com/scholar?cites=10716039270407677408" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mesaros2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://zenodo.org/record/1228142" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        TUT Urban Acoustic Scenes 2018
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://zenodo.org/record/1228235" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        TUT Urban Acoustic Scenes 2018 Mobile
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mesaros2018label" class="modal fade" id="bibtex-Mesaros2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMesaros2018label">
        A multi-device dataset for urban acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2018,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "A multi-device dataset for urban acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "9--13",
    keywords = "Acoustic scene classification, DCASE challenge, public datasets, multi-device data",
    abstract = "This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Morfi2018" style="box-shadow: none">
<div class="panel-heading" id="headingMorfi2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Data-efficient weakly supervised learning for low-resource audio event detection using deep learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Veronica Morfi and Dan Stowell
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Queen Mary University of London, Machine Listening Lab, Centre for Digital Music (C4DM), UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          20 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Morfi2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Morfi_129.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Morfi2018').collapse('show');window.location.hash='#Morfi2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Morfi2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Morfi2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Morfi2018" class="panel-collapse collapse" id="collapse-Morfi2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a method to perform audio event detection under the common constraint that only limited training data are available. In training a deep learning system to perform audio event detection, two practical problems arise. Firstly, most datasets are “weakly labelled” having only a list of events present in each recording without any temporal information for training. Secondly, deep neural networks need a very large amount of labelled training data to achieve good quality performance, yet in practice it is difficult to collect enough samples for most classes of interest. In this paper, we propose a data-efficient training of a stacked convolutional and recurrent neural network. This neural network is trained in a multi instance learning setting for which we introduce a new loss function that leads to improved training compared to the usual approaches for weakly supervised learning. We successfully test our approach on two low-resource datasets that lack temporal labels.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Multi instance learning, deep learning, weak labels, audio event detection
      </p>
<p>
<strong>
        Cites:
       </strong>
       20 (
       <a href="https://scholar.google.com/scholar?cites=10969444816382011486" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Morfi2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Morfi_129.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://figshare.com/articles/Transcriptions_of_NIPS4B_2013_Bird_Challenge_Training_Dataset/6798548" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        Annotations
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Morfi2018label" class="modal fade" id="bibtex-Morfi2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMorfi2018label">
        Data-efficient weakly supervised learning for low-resource audio event detection using deep learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Morfi2018,
    author = "Morfi, Veronica and Stowell, Dan",
    title = "Data-efficient weakly supervised learning for low-resource audio event detection using deep learning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "123--127",
    keywords = "Multi instance learning, deep learning, weak labels, audio event detection",
    abstract = "We propose a method to perform audio event detection under the common constraint that only limited training data are available. In training a deep learning system to perform audio event detection, two practical problems arise. Firstly, most datasets are “weakly labelled” having only a list of events present in each recording without any temporal information for training. Secondly, deep neural networks need a very large amount of labelled training data to achieve good quality performance, yet in practice it is difficult to collect enough samples for most classes of interest. In this paper, we propose a data-efficient training of a stacked convolutional and recurrent neural network. This neural network is trained in a multi instance learning setting for which we introduce a new loss function that leads to improved training compared to the usual approaches for weakly supervised learning. We successfully test our approach on two low-resource datasets that lack temporal labels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2018a" style="box-shadow: none">
<div class="panel-heading" id="headingNguyen2018a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Truc Nguyen and Franz Pernkopf
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Graz University of Technology, Signal Processing and Speech Communication Lab., Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          59 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Nguyen_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Nguyen2018a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2018a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2018a" class="panel-collapse collapse" id="collapse-Nguyen2018a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes Convolutional Neural Network (CNN) ensembles for acoustic scene classification of tasks 1A and 1B of the DCASE 2018 challenge. We introduce a nearest neighbor filter applied on spectrograms, which allows to emphasize and smooth similar patterns of sound events in a scene. We also propose a variety of CNN models for single-input (SI) and multi-input (MI) channels and three different methods for building a network ensemble. The experimental results show that for task 1A the combination of the MI-CNN structures using both of log-mel features and their nearest neighbor filtering is slightly more effective than the single-input channel CNN models using log-mel features only. This statement is opposite for task 1B. In addition, the ensemble methods improve the accuracy of the system significantly, the best ensemble method is ensemble selection, which achieves 69.3% for task 1A and 63.6% for task 1B. This improves the baseline system by 8.9% and 14.4% for task 1A and 1B, respectively.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2018, acoustic scene classification, convolution neural network, nearest neighbor filter
      </p>
<p>
<strong>
        Cites:
       </strong>
       59 (
       <a href="https://scholar.google.com/scholar?cites=2411947584031109944" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Nguyen_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2018alabel" class="modal fade" id="bibtex-Nguyen2018a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2018alabel">
        Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nguyen2018a,
    author = "Nguyen, Truc and Pernkopf, Franz",
    title = "Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "34--38",
    keywords = "DCASE 2018, acoustic scene classification, convolution neural network, nearest neighbor filter",
    abstract = "This paper proposes Convolutional Neural Network (CNN) ensembles for acoustic scene classification of tasks 1A and 1B of the DCASE 2018 challenge. We introduce a nearest neighbor filter applied on spectrograms, which allows to emphasize and smooth similar patterns of sound events in a scene. We also propose a variety of CNN models for single-input (SI) and multi-input (MI) channels and three different methods for building a network ensemble. The experimental results show that for task 1A the combination of the MI-CNN structures using both of log-mel features and their nearest neighbor filtering is slightly more effective than the single-input channel CNN models using log-mel features only. This statement is opposite for task 1B. In addition, the ensemble methods improve the accuracy of the system significantly, the best ensemble method is ensemble selection, which achieves 69.3\% for task 1A and 63.6\% for task 1B. This improves the baseline system by 8.9\% and 14.4\% for task 1A and 1B, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2018b" style="box-shadow: none">
<div class="panel-heading" id="headingNguyen2018b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE 2018 task 2: iterative training, label smoothing, and background noise normalization for audio event tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Thi Ngoc Tho Nguyen<sup>1</sup>, Ngoc Khanh Nguyen<sup>2</sup>, Douglas L. Jones<sup>3</sup>, and Woon Seng Gan<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nanyang Technological University, Electrical and Electronic Engineering Dept., Singapore, <sup>2</sup>SWAT, Singapore, <sup>3</sup>University of Illinois at Urbana-Champaign, Dept. of Electrical and Computer Engineering, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          6 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Nguyen_105.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Nguyen2018b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2018b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2018b" class="panel-collapse collapse" id="collapse-Nguyen2018b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes an approach from our submissions for DCASE 2018 Task 2: general-purpose audio tagging of Freesound content with AudioSet labels. To tackle the problem of diverse recording environments, we propose to use background noise normalization. To tackle the problem of noisy labels, we propose to use pseudolabel for automatic label verification and label smoothing to reduce the over-fitting. We train several convolutional neural networks with data augmentation and different input sizes for the automatic label verification process. The label verification procedure is promising to improve the quality of datasets for audio classification. Our ensemble model ranked fifth on the private leaderboard of the competition with an mAP@3 score of 0:9496.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio event tagging, Background noise normalization, Convolutional neural networks, DCASE 2018, Label smoothing, Pseudo-label
      </p>
<p>
<strong>
        Cites:
       </strong>
       6 (
       <a href="https://scholar.google.com/scholar?cites=6368871621854457091" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Nguyen_105.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2018blabel" class="modal fade" id="bibtex-Nguyen2018b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2018blabel">
        DCASE 2018 task 2: iterative training, label smoothing, and background noise normalization for audio event tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nguyen2018b,
    author = "Nguyen, Thi Ngoc Tho and Nguyen, Ngoc Khanh and Jones, Douglas L. and Gan, Woon Seng",
    title = "{DCASE 2018} task 2: iterative training, label smoothing, and background noise normalization for audio event tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "54--58",
    keywords = "Audio event tagging, Background noise normalization, Convolutional neural networks, DCASE 2018, Label smoothing, Pseudo-label",
    abstract = "This paper describes an approach from our submissions for DCASE 2018 Task 2: general-purpose audio tagging of Freesound content with AudioSet labels. To tackle the problem of diverse recording environments, we propose to use background noise normalization. To tackle the problem of noisy labels, we propose to use pseudolabel for automatic label verification and label smoothing to reduce the over-fitting. We train several convolutional neural networks with data augmentation and different input sizes for the automatic label verification process. The label verification procedure is promising to improve the quality of datasets for audio classification. Our ensemble model ranked fifth on the private leaderboard of the competition with an mAP@3 score of 0:9496."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nolasco2018" style="box-shadow: none">
<div class="panel-heading" id="headingNolasco2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         To bee or not to bee: Investigating machine learning approaches for beehive sound recognition
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Inês Nolasco and Emmanouil Benetos
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Queen Mary University of London, School of Electronic Engineering and Computer Science, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          46 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nolasco2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Nolasco_131.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Nolasco2018').collapse('show');window.location.hash='#Nolasco2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Nolasco2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nolasco2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Nolasco2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nolasco2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nolasco2018" class="panel-collapse collapse" id="collapse-Nolasco2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this work, we aim to explore the potential of machine learning methods to the problem of beehive sound recognition. A major contribution of this work is the creation and release of annotations for a selection of beehive recordings. By experimenting with both support vector machines and convolutional neural networks, we explore important aspects to be considered in the development of beehive sound recognition systems using machine learning approaches.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Computational bioacoustic scene analysis, ecoacoustics, beehive sound recognition
      </p>
<p>
<strong>
        Cites:
       </strong>
       46 (
       <a href="https://scholar.google.com/scholar?cites=6349261330573316644" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nolasco2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Nolasco_131.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://zenodo.org/record/1321278#.W-12qlVfiEJ" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        Dataset
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/madzimia/Audio_based_identification_beehive_states" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Code">
<i class="fa fa-git">
</i>
        Code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nolasco2018label" class="modal fade" id="bibtex-Nolasco2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNolasco2018label">
        To bee or not to bee: Investigating machine learning approaches for beehive sound recognition
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nolasco2018,
    author = "Nolasco, Ines and Benetos, Emmanouil",
    title = "To bee or not to bee: Investigating machine learning approaches for beehive sound recognition",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "133--137",
    keywords = "Computational bioacoustic scene analysis, ecoacoustics, beehive sound recognition",
    abstract = "In this work, we aim to explore the potential of machine learning methods to the problem of beehive sound recognition. A major contribution of this work is the creation and release of annotations for a selection of beehive recordings. By experimenting with both support vector machines and convolutional neural networks, we explore important aspects to be considered in the development of beehive sound recognition systems using machine learning approaches."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pantic2018" style="box-shadow: none">
<div class="panel-heading" id="headingPantic2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Ensemble of convolutional neural networks for general-purpose audio tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Bogdan Pantic
        </em>
</p>
<p class="text-muted">
<small>
<em>
          School of Electrical Engineering, Signals and Systems Department, Belgrade, Serbia
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pantic2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Pantic_119.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Pantic2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pantic2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pantic2018" class="panel-collapse collapse" id="collapse-Pantic2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This work describes our solution for the general-purpose audio tagging task of DCASE 2018 challenge. We propose the ensemble of several Convolutional Neural Networks (CNNs) with different properties. Logistic regression is used as a meta-classifier to produce final predictions. Experiments demonstrate that the ensemble outperforms each CNN individually. Finally, the proposed system achieves Mean Average Precision (MAP) score of 0.945 on test set, which is a significant improvement compared to the baseline.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio tagging, DCASE 2018, convolutional neural networks, ensembling
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pantic2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Pantic_119.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pantic2018label" class="modal fade" id="bibtex-Pantic2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPantic2018label">
        Ensemble of convolutional neural networks for general-purpose audio tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Pantic2018,
    author = "Pantic, Bogdan",
    title = "Ensemble of convolutional neural networks for general-purpose audio tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "88--92",
    keywords = "audio tagging, DCASE 2018, convolutional neural networks, ensembling",
    abstract = "This work describes our solution for the general-purpose audio tagging task of DCASE 2018 challenge. We propose the ensemble of several Convolutional Neural Networks (CNNs) with different properties. Logistic regression is used as a meta-classifier to produce final predictions. Experiments demonstrate that the ensemble outperforms each CNN individually. Finally, the proposed system achieves Mean Average Precision (MAP) score of 0.945 on test set, which is a significant improvement compared to the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ren2018" style="box-shadow: none">
<div class="panel-heading" id="headingRen2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Attention-based convolutional neural networks for acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Zhao Ren<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Kun Qian<sup>1</sup>, Mark D. Plumbley<sup>2</sup>, and Björn W. Schuller <sup>1,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, <sup>2</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK, <sup>3</sup>GLAM – Group on Language, Audio &amp; Music, Imperial College London, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          77 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ren2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Ren_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Ren2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ren2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ren2018" class="panel-collapse collapse" id="collapse-Ren2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a convolutional neural network (CNN) model based on an attention pooling method to classify ten different acoustic scenes, participating in the acoustic scene classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2018), which includes data from one device (subtask A) and data from three different devices (subtask B). The log mel spectrogram images of the audio waves are first forwarded to convolutional layers, and then fed into an attention pooling layer to reduce the feature dimension and achieve classification. From attention perspective, we build a weighted evaluation of the features, instead of simple max pooling or average pooling. On the official development set of the challenge, the best accuracy of subtask A is 72.6 \%, which is an improvement of 12.9 \% when compared with the official baseline (p &lt; :001 in a one-tailed z-test). For subtask B, the best result of our attention-based CNN is a significant improvement of the baseline as well, in which the accuracies are 71.8 \%, 58.3 \%, and 58.3 \% for the three devices A to C (p &lt; :001 for device A, p &lt; :01 for device B, and p &lt; :05 for device C).
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic Scene Classification, Convolutional Neural Network, Attention Pooling, Log Mel Spectrogram
      </p>
<p>
<strong>
        Cites:
       </strong>
       77 (
       <a href="https://scholar.google.com/scholar?cites=14117993367401563583" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ren2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Ren_67.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ren2018label" class="modal fade" id="bibtex-Ren2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRen2018label">
        Attention-based convolutional neural networks for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ren2018,
    author = "Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark and Schuller, Björn",
    title = "Attention-based convolutional neural networks for acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "39--43",
    keywords = "Acoustic Scene Classification, Convolutional Neural Network, Attention Pooling, Log Mel Spectrogram",
    abstract = "We propose a convolutional neural network (CNN) model based on an attention pooling method to classify ten different acoustic scenes, participating in the acoustic scene classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2018), which includes data from one device (subtask A) and data from three different devices (subtask B). The log mel spectrogram images of the audio waves are first forwarded to convolutional layers, and then fed into an attention pooling layer to reduce the feature dimension and achieve classification. From attention perspective, we build a weighted evaluation of the features, instead of simple max pooling or average pooling. On the official development set of the challenge, the best accuracy of subtask A is 72.6 \\%, which is an improvement of 12.9 \\% when compared with the official baseline (p &lt; :001 in a one-tailed z-test). For subtask B, the best result of our attention-based CNN is a significant improvement of the baseline as well, in which the accuracies are 71.8 \\%, 58.3 \\%, and 58.3 \\% for the three devices A to C (p &lt; :001 for device A, p &lt; :01 for device B, and p &lt; :05 for device C)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Roletscheck2018" style="box-shadow: none">
<div class="panel-heading" id="headingRoletscheck2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Using an evolutionary approach to explore convolutional neural networks for acoustic scene classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Christian Roletscheck, Tobias Watzka, Andreas Seiderer, Dominik Schiller, Elisabeth André
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Augsburg University, Human Centered Multimedia, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          15 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Roletscheck2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Roletscheck_137.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Roletscheck2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Roletscheck2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Roletscheck2018" class="panel-collapse collapse" id="collapse-Roletscheck2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The successful application of modern deep neural networks is heavily reliant on the chosen architecture and the selection of the appropriate hyperparameters. Due to the large number of parameters and the complex inner workings of a neural network, finding a suitable configuration for a respective problem turns out to be a rather complex task for a human. In this paper we, propose an evolutionary approach to automatically generate a suitable neural network architecture and hyperparameters for any given classification problem. A genetic algorithm is used to generate and evaluate a variety of deep convolutional networks. We take the DCASE 2018 Challenge as an opportunity to evaluate our algorithm on the task of acoustic scene classification. The best accuracy achieved by our approach was 74.7% on the development dataset.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Evolutionary algorithm, genetic algorithm, convolutional neural networks, acoustic scene classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       15 (
       <a href="https://scholar.google.com/scholar?cites=6010463547983023439" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Roletscheck2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Roletscheck_137.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Roletscheck2018label" class="modal fade" id="bibtex-Roletscheck2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRoletscheck2018label">
        Using an evolutionary approach to explore convolutional neural networks for acoustic scene classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Roletscheck2018,
    author = "Roletscheck, Christian and Watzka, Tobias and Seiderer, Andreas and Schiller, Dominik and André, Elisabeth",
    title = "Using an evolutionary approach to explore convolutional neural networks for acoustic scene classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "158--162",
    keywords = "Evolutionary algorithm, genetic algorithm, convolutional neural networks, acoustic scene classification",
    abstract = "The successful application of modern deep neural networks is heavily reliant on the chosen architecture and the selection of the appropriate hyperparameters. Due to the large number of parameters and the complex inner workings of a neural network, finding a suitable configuration for a respective problem turns out to be a rather complex task for a human. In this paper we, propose an evolutionary approach to automatically generate a suitable neural network architecture and hyperparameters for any given classification problem. A genetic algorithm is used to generate and evaluate a variety of deep convolutional networks. We take the DCASE 2018 Challenge as an opportunity to evaluate our algorithm on the task of acoustic scene classification. The best accuracy achieved by our approach was 74.7\% on the development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Serizel2018" style="box-shadow: none">
<div class="panel-heading" id="headingSerizel2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Large-scale weakly labeled semi-supervised sound event detection in domestic environments
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Romain Serizel<sup>1</sup>, Nicolas Turpault<sup>1</sup>, Hamid Eghbal-Zadeh<sup>2</sup>, and Ankit Parag Shah<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universit de Lorraine, CNRS, Inria, Loria, Nancy, France, <sup>2</sup>Institute of Computational Perception, Johannes Kepler University of Linz, Austria, <sup>3</sup>Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA, United States
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          158 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Serizel2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Serizel_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Serizel2018').collapse('show');window.location.hash='#Serizel2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Serizel2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Serizel2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Serizel2018" class="panel-collapse collapse" id="collapse-Serizel2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events. . . ) and potential applications.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, Large scale, Weakly labeled data, Semi-supervised learning
      </p>
<p>
<strong>
        Cites:
       </strong>
       158 (
       <a href="https://scholar.google.com/scholar?cites=11895582268876503844" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Serizel2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Serizel_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/dataset" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        DCASE2018 Task 4 development dataset
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Serizel2018label" class="modal fade" id="bibtex-Serizel2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSerizel2018label">
        Large-scale weakly labeled semi-supervised sound event detection in domestic environments
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Serizel2018,
    author = "Serizel, Romain and Turpault, Nicolas and Eghbal-Zadeh, Hamid and Shah, Ankit Parag",
    title = "Large-scale weakly labeled semi-supervised sound event detection in domestic environments",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "19--23",
    keywords = "Sound event detection, Large scale, Weakly labeled data, Semi-supervised learning",
    abstract = "This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events. . . ) and potential applications."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wei2018a" style="box-shadow: none">
<div class="panel-heading" id="headingWei2018a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A report on audio tagging with deeper CNN, 1D-ConvNet and 2D-ConvNet
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Qingkai Wei, Yanfang Liu, and Xiaohui Ruan
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Beijing Kuaiyu Electronics Co. Ltd., Beijing, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wei2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Wei_100.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wei2018a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wei2018a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wei2018a" class="panel-collapse collapse" id="collapse-Wei2018a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       General-purpose audio tagging is a newly proposed task in DCASE 2018, which can provide insight towards broadly-applicable sound event classifiers. In this paper, two systems (named as 1D-ConvNet and 2D-ConvNet in this paper) with small kernel sizes, multiple functional modules, deeper CNN (convolutional neural networks) are developed to improve performance in this task. In detail, different audio features are used, i.e. raw waveforms are for 1D-ConvNet, while frequency domain features, such as mfcc, log-mel spectrogram, multi-resolution log-mel spectrogram and spectrogram, are utilized as the 2D-ConvNet input. Using DCASE 2018 Challenge task 2 dataset to train and evaluate, the best single model with 1DConvNet and 2D-ConvNet are chosen, whose kaggle public leaderboard score are 0.877 and 0.961 respectively. In addition, a better ensemble rank averaging prediction get a score 0.967 on the public leaderboard, ranking 5/558, while score 0.942 on the private leaderboard ranking 11/558.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       DCASE 2018, Audio tagging, Convolutional neural networks, 1D-ConvNet, 2D-ConvNet, Model ensemble
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="https://scholar.google.com/scholar?cites=17563190051876528528" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wei2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Wei_100.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wei2018alabel" class="modal fade" id="bibtex-Wei2018a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWei2018alabel">
        A report on audio tagging with deeper CNN, 1D-ConvNet and 2D-ConvNet
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wei2018a,
    author = "Wei, Qingkai and Liu, Yanfang and Ruan, Xiaohui",
    title = "A report on audio tagging with deeper {CNN}, {1D-ConvNet} and {2D-ConvNet}",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "49--53",
    keywords = "DCASE 2018, Audio tagging, Convolutional neural networks, 1D-ConvNet, 2D-ConvNet, Model ensemble",
    abstract = "General-purpose audio tagging is a newly proposed task in DCASE 2018, which can provide insight towards broadly-applicable sound event classifiers. In this paper, two systems (named as 1D-ConvNet and 2D-ConvNet in this paper) with small kernel sizes, multiple functional modules, deeper CNN (convolutional neural networks) are developed to improve performance in this task. In detail, different audio features are used, i.e. raw waveforms are for 1D-ConvNet, while frequency domain features, such as mfcc, log-mel spectrogram, multi-resolution log-mel spectrogram and spectrogram, are utilized as the 2D-ConvNet input. Using DCASE 2018 Challenge task 2 dataset to train and evaluate, the best single model with 1DConvNet and 2D-ConvNet are chosen, whose kaggle public leaderboard score are 0.877 and 0.961 respectively. In addition, a better ensemble rank averaging prediction get a score 0.967 on the public leaderboard, ranking 5/558, while score 0.942 on the private leaderboard ranking 11/558."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wei2018b" style="box-shadow: none">
<div class="panel-heading" id="headingWei2018b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sample mixed-based data augmentation for domestic audio tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Shengyun Wei<sup>1</sup>, Kele Xu<sup>1,2</sup>, Dezhi Wang<sup>3</sup>, Feifan Liao<sup>1</sup>, Huaimin Wang<sup>2</sup>, and Qiuqiang Kong<sup>4</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>National University of Defense Technology, Information and Communication Dept., Wuhan, China, <sup>2</sup>National University of Defense Technology, Computer Dept., Changsha, China, <sup>3</sup>National University of Defense Technology, College of Meteorology and Oceanography, Changsha, China, <sup>4</sup>University of Surrey, Center for Vision, Speech and Signal Processing, Guildford, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          40 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wei2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Wei_120.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wei2018b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wei2018b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wei2018b" class="panel-collapse collapse" id="collapse-Wei2018b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Audio tagging has attracted increasing attention since last decade and has various potential applications in many fields. The objective of audio tagging is to predict the labels of an audio clip. Recently deep learning methods have been applied to audio tagging and have achieved state-of-the-art performance, which provides a poor generalization ability on new data. However due to the limited size of audio tagging data such as DCASE data, the trained models tend to result in overfitting of the network. Previous data augmentation methods such as pitch shifting, time stretching and adding background noise do not show much improvement in audio tagging. In this paper, we explore the sample mixed data augmentation for the domestic audio tagging task, including mixup, SamplePairing and extrapolation. We apply a convolutional recurrent neural network (CRNN) with attention module with log-scaled mel spectrum as a baseline system. In our experiments, we achieve an state-of-the-art of equal error rate (EER) of 0.10 on DCASE 2016 task4 dataset with mixup approach, outperforming the baseline system without data augmentation.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio tagging, data augmentation, sample mixed, convolutional recurrent neural network
      </p>
<p>
<strong>
        Cites:
       </strong>
       40 (
       <a href="https://scholar.google.com/scholar?cites=16617630022366500986" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wei2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Wei_120.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wei2018blabel" class="modal fade" id="bibtex-Wei2018b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWei2018blabel">
        Sample mixed-based data augmentation for domestic audio tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wei2018b,
    author = "Wei, Shengyun and Xu, Kele and Wang, Dezhi and Liao, Feifan and Wang, Huaimin and Kong, Qiuqiang",
    title = "Sample mixed-based data augmentation for domestic audio tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "93--97",
    keywords = "Audio tagging, data augmentation, sample mixed, convolutional recurrent neural network",
    abstract = "Audio tagging has attracted increasing attention since last decade and has various potential applications in many fields. The objective of audio tagging is to predict the labels of an audio clip. Recently deep learning methods have been applied to audio tagging and have achieved state-of-the-art performance, which provides a poor generalization ability on new data. However due to the limited size of audio tagging data such as DCASE data, the trained models tend to result in overfitting of the network. Previous data augmentation methods such as pitch shifting, time stretching and adding background noise do not show much improvement in audio tagging. In this paper, we explore the sample mixed data augmentation for the domestic audio tagging task, including mixup, SamplePairing and extrapolation. We apply a convolutional recurrent neural network (CRNN) with attention module with log-scaled mel spectrum as a baseline system. In our experiments, we achieve an state-of-the-art of equal error rate (EER) of 0.10 on DCASE 2016 task4 dataset with mixup approach, outperforming the baseline system without data augmentation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wilkinghoff2018" style="box-shadow: none">
<div class="panel-heading" id="headingWilkinghoff2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         General-purpose audio tagging by ensembling convolutional neural networks based on multiple features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kevin Wilkinghoff
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Fraunhofer Institute for Communication, Information Processing and Ergonomics FKIE, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          3 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wilkinghoff2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Wilkinghoff_81.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wilkinghoff2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wilkinghoff2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wilkinghoff2018" class="panel-collapse collapse" id="collapse-Wilkinghoff2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes an audio tagging system that participated in Task 2 “General-purpose audio tagging of Freesound content with AudioSet labels” of the “Detection and Classification of Acoustic Scenes and Events (DCASE)” Challenge 2018. The system is an ensemble consisting of five convolutional neural networks based on Mel-frequency Cepstral Coefficients, Perceptual Linear Prediction features, Mel-spectrograms and the raw audio data. For ensembling all models, score-based fusion via Logistic Regression is performed with another neural network. In experimental evaluations, it is shown that ensembling the models significantly improves upon the performances obtained with the individual models. As a final result, the system achieved a Mean Average Precision with Cutoff 3 of 0:9414 on the private leaderboard of the challenge.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio tagging, acoustic event classification, convolutional neural network, score-based fusion
      </p>
<p>
<strong>
        Cites:
       </strong>
       3 (
       <a href="https://scholar.google.com/scholar?cites=16635369836801187128" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wilkinghoff2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Wilkinghoff_81.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wilkinghoff2018label" class="modal fade" id="bibtex-Wilkinghoff2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWilkinghoff2018label">
        General-purpose audio tagging by ensembling convolutional neural networks based on multiple features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wilkinghoff2018,
    author = "Wilkinghoff, Kevin",
    title = "General-purpose audio tagging by ensembling convolutional neural networks based on multiple features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "44--48",
    keywords = "audio tagging, acoustic event classification, convolutional neural network, score-based fusion",
    abstract = "This paper describes an audio tagging system that participated in Task 2 “General-purpose audio tagging of Freesound content with AudioSet labels” of the “Detection and Classification of Acoustic Scenes and Events (DCASE)” Challenge 2018. The system is an ensemble consisting of five convolutional neural networks based on Mel-frequency Cepstral Coefficients, Perceptual Linear Prediction features, Mel-spectrograms and the raw audio data. For ensembling all models, score-based fusion via Logistic Regression is performed with another neural network. In experimental evaluations, it is shown that ensembling the models significantly improves upon the performances obtained with the individual models. As a final result, the system achieved a Mean Average Precision with Cutoff 3 of 0:9414 on the private leaderboard of the challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2018a" style="box-shadow: none">
<div class="panel-heading" id="headingXu2018a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         The Aalto system based on fine-tuned AudioSet features for DCASE2018 task2 - general purpose audio tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Zhicun Xu, Peter Smit, and Mikko Kurimo
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Aalto University, Department of Signal Processing and Acoustics, Espoo, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          2 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Xu_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Xu2018a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2018a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2018a" class="panel-collapse collapse" id="collapse-Xu2018a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we presented a neural network system for DCASE 2018 task 2, general purpose audio tagging. We fine-tuned the Google AudioSet feature generation model with different settings for the given 41 classes on top of a fully connected layer with 100 units. Then we used the fine-tuned models to generate 128 dimensional features for each 0.960s audio. We tried different neural network structures including LSTM and multi-level attention models. In our experiments, the multi-level attention model has shown its superiority over others. Truncating the silence parts, repeating and splitting the audio into the fixed length, pitch shifting augmentation, and mixup techniques are all used in our experiments. The proposed system achieved a result with MAP@3 score at 0.936, which outperforms the baseline result of 0.704 and achieves top 8% in the public leaderboard.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio tagging, AudioSet, multi-level attention model
      </p>
<p>
<strong>
        Cites:
       </strong>
       2 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Xu_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2018alabel" class="modal fade" id="bibtex-Xu2018a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2018alabel">
        The Aalto system based on fine-tuned AudioSet features for DCASE2018 task2 - general purpose audio tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xu2018a,
    author = "Xu, Zhicun and Smit, Peter and Kurimo, Mikko",
    title = "The Aalto system based on fine-tuned AudioSet features for {DCASE2018} task2 - general purpose audio tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "24--28",
    keywords = "audio tagging, AudioSet, multi-level attention model",
    abstract = "In this paper, we presented a neural network system for DCASE 2018 task 2, general purpose audio tagging. We fine-tuned the Google AudioSet feature generation model with different settings for the given 41 classes on top of a fully connected layer with 100 units. Then we used the fine-tuned models to generate 128 dimensional features for each 0.960s audio. We tried different neural network structures including LSTM and multi-level attention models. In our experiments, the multi-level attention model has shown its superiority over others. Truncating the silence parts, repeating and splitting the audio into the fixed length, pitch shifting augmentation, and mixup techniques are all used in our experiments. The proposed system achieved a result with MAP@3 score at 0.936, which outperforms the baseline result of 0.704 and achieves top 8\% in the public leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2018b" style="box-shadow: none">
<div class="panel-heading" id="headingXu2018b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Meta learning based audio tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kele Xu<sup>1</sup>, Boqing Zhu<sup>1</sup>, Dezhi Wang<sup>2</sup>, Yuxing Peng<sup>1</sup>, Huaimin Wang<sup>1</sup>, Lilun Zhang<sup>2</sup>, Bo Li<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>National University of Defense Technology, Computer Department, Changsha, China, <sup>2</sup>National University of Defense Technology, Computer Department, Changsha, China, <sup>3</sup>Beijing University of Posts and Telecommunications, Automation Department, Beijing, China
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          4 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Xu_147.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Xu2018b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2018b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2018b" class="panel-collapse collapse" id="collapse-Xu2018b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe our solution for the general-purpose audio tagging task, which belongs to one of the subtasks in the DCASE 2018 challenge. For the solution, we employed both deep learning methods and statistic features-based shallow architecture learners. For single model, different deep convolutional neural network architectures are tested with different kinds of input, which ranges from the raw-signal, log-scaled Mel-spectrograms (log Mel) to Mel Frequency Cepstral Coefficients (MFCC). For log Mel and MFCC, the delta and delta-delta information are also used to formulate three-channel features, while mixup is used for the data augmentation. Using ResNeXt, our best single convolutional neural network architecture provides a mAP@3 of 0.967 on the public Kaggle leaderboard, 0.939 on the private leaderboard. Moreover, to improve the accuracy further, we also propose a meta learning-based ensemble method. By employing the diversities between different architectures, the meta learning-based model can provide higher prediction accuracy and robustness with comparison to the single model. Our solution achieves a mAP@3 of 0.977 on the public leaderboard and 0.951 as our best on the private leaderboard, while the baseline gives a mAP@3 of 0.704.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio tagging, convolutional neural networks, meta-learning, mixup
      </p>
<p>
<strong>
        Cites:
       </strong>
       4 (
       <a href="https://scholar.google.com/scholar?cites=816734404214805199" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Xu_147.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2018blabel" class="modal fade" id="bibtex-Xu2018b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2018blabel">
        Meta learning based audio tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xu2018b,
    author = "Xu, Kele and Zhu, Boqing and Wang, Dezhi and Peng, Yuxing and Wang, Huaimin and Zhang, Lilun and Li, Bo",
    title = "Meta learning based audio tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "193--196",
    keywords = "Audio tagging, convolutional neural networks, meta-learning, mixup",
    abstract = "In this paper, we describe our solution for the general-purpose audio tagging task, which belongs to one of the subtasks in the DCASE 2018 challenge. For the solution, we employed both deep learning methods and statistic features-based shallow architecture learners. For single model, different deep convolutional neural network architectures are tested with different kinds of input, which ranges from the raw-signal, log-scaled Mel-spectrograms (log Mel) to Mel Frequency Cepstral Coefficients (MFCC). For log Mel and MFCC, the delta and delta-delta information are also used to formulate three-channel features, while mixup is used for the data augmentation. Using ResNeXt, our best single convolutional neural network architecture provides a mAP@3 of 0.967 on the public Kaggle leaderboard, 0.939 on the private leaderboard. Moreover, to improve the accuracy further, we also propose a meta learning-based ensemble method. By employing the diversities between different architectures, the meta learning-based model can provide higher prediction accuracy and robustness with comparison to the single model. Our solution achieves a mAP@3 of 0.977 on the public leaderboard and 0.951 as our best on the private leaderboard, while the baseline gives a mAP@3 of 0.704."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yu2018" style="box-shadow: none">
<div class="panel-heading" id="headingYu2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Multi-level attention model for weakly supervised audio classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Changsong Yu<sup>1</sup>, Karim Said Barsim<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Bin Yang<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>University of Stuttgart, Institute of Signal Processing and System Theory, Germany, <sup>2</sup>University of Surrey, Center for Vision, Speech and Signal Processing, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          79 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Yu_146.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Yu2018" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yu2018" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Yu2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yu2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yu2018" class="panel-collapse collapse" id="collapse-Yu2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose a multi-level attention model for the weakly labelled audio classification problem. The objective of audio classification is to predict the presence or the absence of sound events in an audio clip. Recently, Google published a large scale weakly labelled AudioSet dataset containing 2 million audio clips with only the presence or the absence labels of the sound events, without the onset and offset time of the sound events. Previously proposed attention models only applied a single attention module on the last layer of a neural network which limited the capacity of the attention model. In this paper, we propose a multi-level attention model which consists of multiple attention modules applied on the intermediate neural network layers. The outputs of these attention modules are concatenated to a vector followed by a fully connected layer to obtain the final prediction of each class. Experiments show that the proposed multi-attention attention model achieves a stateof-the-art mean average precision (mAP) of 0.360, outperforming the single attention model and the Google baseline system of 0.327 and 0.314, respectively.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       AudioSet, audio classification, attention model
      </p>
<p>
<strong>
        Cites:
       </strong>
       79 (
       <a href="https://scholar.google.com/scholar?cites=9065212988538146657" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Yu_146.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/ChangsongYu/Eusipco2018_Google_AudioSet" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Code">
<i class="fa fa-git">
</i>
        Code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yu2018label" class="modal fade" id="bibtex-Yu2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYu2018label">
        Multi-level attention model for weakly supervised audio classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Yu2018,
    author = "Yu, Changsong and Barsim, Karim Said and Kong, Qiuqiang and Yang, Bin",
    title = "Multi-level attention model for weakly supervised audio classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "188--192",
    keywords = "AudioSet, audio classification, attention model",
    abstract = "In this paper, we propose a multi-level attention model for the weakly labelled audio classification problem. The objective of audio classification is to predict the presence or the absence of sound events in an audio clip. Recently, Google published a large scale weakly labelled AudioSet dataset containing 2 million audio clips with only the presence or the absence labels of the sound events, without the onset and offset time of the sound events. Previously proposed attention models only applied a single attention module on the last layer of a neural network which limited the capacity of the attention model. In this paper, we propose a multi-level attention model which consists of multiple attention modules applied on the intermediate neural network layers. The outputs of these attention modules are concatenated to a vector followed by a fully connected layer to obtain the final prediction of each class. Experiments show that the proposed multi-attention attention model achieves a stateof-the-art mean average precision (mAP) of 0.360, outperforming the single attention model and the Google baseline system of 0.327 and 0.314, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zeinali2018" style="box-shadow: none">
<div class="panel-heading" id="headingZeinali2018" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Convolutional neural networks and x-vector embedding for DCASE2018 Acoustic Scene Classification challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Hossein Zeinali, Lukáš Burget and Jan Honza Černocký
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Brno University of Technology, Speech@FIT and IT4I Center of Excellence, Czech Republic
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          72 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zeinali2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Zeinali_149.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Zeinali2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zeinali2018" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zeinali2018" class="panel-collapse collapse" id="collapse-Zeinali2018" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, the Brno University of Technology (BUT) team submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge are described. Also, the analysis of different methods on the leaderboard set is provided. The proposed approach is a fusion of two different Convolutional Neural Network (CNN) topologies. The first one is the common two-dimensional CNNs which is mainly used in image classification. The second one is a one-dimensional CNN for extracting fixed-length audio segment embeddings, so called x-vectors, which has also been used in speech processing, especially for speaker recognition. In addition to the different topologies, two types of features were tested: log mel-spectrogram and CQT features. Finally, the outputs of different systems are fused using a simple output averaging in the best performing system. Our submissions ranked third among 24 teams in the ASC sub-task A (task1a).
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio scene classification, Convolutional neural networks, Deep learning, x-vectors, Regularized LDA
      </p>
<p>
<strong>
        Cites:
       </strong>
       72 (
       <a href="https://scholar.google.com/scholar?cites=14644340893808719463" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zeinali2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2018/proceedings/DCASE2018Workshop_Zeinali_149.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zeinali2018label" class="modal fade" id="bibtex-Zeinali2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZeinali2018label">
        Convolutional neural networks and x-vector embedding for DCASE2018 Acoustic Scene Classification challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zeinali2018,
    author = "Zeinali, Hossein and Burget, Lukas and Cernocky, Jan Honza",
    title = "Convolutional neural networks and x-vector embedding for {DCASE2018} Acoustic Scene Classification challenge",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    year = "2018",
    month = "November",
    pages = "202--206",
    keywords = "Audio scene classification, Convolutional neural networks, Deep learning, x-vectors, Regularized LDA",
    abstract = "In this paper, the Brno University of Technology (BUT) team submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge are described. Also, the analysis of different methods on the leaderboard set is provided. The proposed approach is a fusion of two different Convolutional Neural Network (CNN) topologies. The first one is the common two-dimensional CNNs which is mainly used in image classification. The second one is a one-dimensional CNN for extracting fixed-length audio segment embeddings, so called x-vectors, which has also been used in speech processing, especially for speaker recognition. In addition to the different topologies, two types of features were tested: log mel-spectrogram and CQT features. Finally, the outputs of different systems are fused using a simple output averaging in the best performing system. Our submissions ranked third among 24 teams in the ASC sub-task A (task1a)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>