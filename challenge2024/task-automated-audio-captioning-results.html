<!DOCTYPE html><html lang="en">
<head>
    <title>Automated Audio Captioning - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2024/task-automated-audio-captioning-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description Automated audio captioning is the task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. Given the novelty of …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right navbar-tighter" id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2024</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2024/"><i class="fa fa-home"></i>&nbsp;Intro</a>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-t1"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-t2"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-t3"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-t4"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-t5"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2024/task-automated-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-t6"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2024/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-scene-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-t7"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-retrieval text-t8"></i>&nbsp;Task8&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-queried-audio-source-separation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-separation text-t9"></i>&nbsp;Task9&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-traffic text-t10"></i>&nbsp;Task10&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2024/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2024/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wall-09.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-t6"></i><i class="fa dc-captioning fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Caption</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 6</span></span><img src="../images/logos/dcase/dcase2024_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Automated Audio Captioning</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#systems-ranking">Systems ranking</a>
<ul>
<li><a href="#systems-ranking-challenge-metrics">Systems ranking, challenge metrics</a></li>
<li><a href="#systems-ranking-additional-metrics">Systems ranking, additional metrics</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#overview-of-characteristics">Overview of characteristics</a></li>
<li><a href="#detailed-characteristics">Detailed characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>Automated audio captioning is the task of general audio content
description using free text. It is an intermodal translation task
(not speech-to-text), where a system accepts as an input an audio
signal and outputs the textual description (i.e. the caption) of
that signal. Given the novelty of the task of audio captioning,
current focus is on exploring and developing different methods
that can provide some kind of captions for a general audio recording.
To this aim, the Clotho dataset is used, which provides
good quality captions, without speech transcription, named entities,
and hapax legomena (i.e. words that appear once in a split).</p>
<p>The developed systems are evaluated on their generated captions,
using the evaluation split of Clotho, which does not provide the corresponding
captions for the audio.</p>
<!-- The ranking of the submitted systems is based on the achieved FENSE
metric. -->
<p>More information about Task 6: Automated Audio Captioning can be found at the <a class="btn btn-primary" href="/challenge2024/task-automated-audio-captioning" style="">task description page.</a></p>
<!-- The ranking of the submitted systems is based on the achieved FENSE
metric. Though, in this page is provided a more thorough presentation,
grouping the metrics into those that are originated from machine translation
and to those that originated from captioning. -->
<h1 id="teams-ranking">Teams ranking</h1>
<p>Here are listed the best systems from all teams. <strong>The ranking is based on the
FENSE.</strong> For more elaborated exploration of the performance of the
different systems, at the same table are listed the values achieved for
all the metrics employed in the task. The values for the metrics are for
the Clotho evaluation split and the Clotho testing split. The values for the
Clotho testing split are provided in order to allow further comparison
with systems and methods developed outside of this task, since captions for the Clotho
testing split are freely available. This year, we asked participants to exclude a list of Freesound IDs to prevent a data leakage between the training and evaluation subsets. We mark "True" in the "Data leak" column for participants who have used Freesound data without taking into account the forbidden IDs.</p>
<p>All confidence intervals are computed using a bootstrap method on the evaluation set, with a 95% confidence level.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="submission_code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="results_evaluation_fense_bootstrap" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected<br/> metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="5">Submission Information</th>
<th class="sep-left-cell text-center" colspan="6">Clotho evaluation split</th>
<th class="sep-left-cell text-center" colspan="6">Clotho development-testing split</th>
</tr>
<tr>
<th class="text-center" data-chartable="true" data-field="team_rank" data-sortable="true" data-value-type="int">
              Team<br/>rank
            </th>
<th data-field="submission_code" data-sortable="true">
              Submission code
            </th>
<th data-field="system_bias" data-sortable="true">
              Data<br/>leak
            </th>
<th data-field="corresponding_author_name" data-sortable="false">
              Corresponding author
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_evaluation_meteor_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_cider_d_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              CIDEr-D
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_spice_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_spider_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_spider_fl_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              SPIDEr-FL
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_fense_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              FENSE
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_development_testing_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_cider_d" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr-D
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_spider_fl" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_fense" data-reversed="true" data-sortable="true" data-value-type="float3">
              FENSE
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>1</td>
<td>Jung_CMU_t6_4</td>
<td>False</td>
<td>Jee-weon Jung</td>
<td>jung_cmu_t6_2024</td>
<td>0.172 (0.174 - 0.182)</td>
<td>0.344 (0.324 - 0.366)</td>
<td>0.140 (0.134 - 0.146)</td>
<td>0.242 (0.230 - 0.255)</td>
<td>0.241 (0.229 - 0.254)</td>
<td>0.554 (0.543 - 0.563)</td>
<td>0.174</td>
<td>0.327</td>
<td>0.136</td>
<td>0.230</td>
<td>0.230</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>2</td>
<td>Kim_SNU_t6_2</td>
<td>False</td>
<td>Jaeyeon Kim</td>
<td>kim_snu_t6_2024</td>
<td>0.199 (0.200 - 0.210)</td>
<td>0.480 (0.453 - 0.508)</td>
<td>0.148 (0.142 - 0.154)</td>
<td>0.314 (0.299 - 0.330)</td>
<td>0.314 (0.299 - 0.330)</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.196</td>
<td>0.477</td>
<td>0.142</td>
<td>0.310</td>
<td>0.310</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>3</td>
<td>Chen_SJTU_t6_4</td>
<td>True</td>
<td>Wenxi Chen</td>
<td>chen_sjtu_t6_2024</td>
<td>0.194 (0.196 - 0.207)</td>
<td>0.509 (0.479 - 0.541)</td>
<td>0.145 (0.138 - 0.151)</td>
<td>0.327 (0.310 - 0.345)</td>
<td>0.322 (0.306 - 0.341)</td>
<td>0.541 (0.530 - 0.552)</td>
<td>0.193</td>
<td>0.522</td>
<td>0.148</td>
<td>0.335</td>
<td>0.333</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>Li_ALXC_t6_4</td>
<td>False</td>
<td>Gang Li</td>
<td>li_alxc_t6_2024</td>
<td>0.195 (0.197 - 0.208)</td>
<td>0.493 (0.464 - 0.525)</td>
<td>0.145 (0.139 - 0.151)</td>
<td>0.319 (0.302 - 0.337)</td>
<td>0.317 (0.300 - 0.335)</td>
<td>0.533 (0.522 - 0.543)</td>
<td>0.194</td>
<td>0.503</td>
<td>0.145</td>
<td>0.324</td>
<td>0.323</td>
<td>0.532</td>
</tr>
<tr>
<td></td>
<td>5</td>
<td>Kyogu_SNU_t6_2</td>
<td>False</td>
<td>Lee Kyogu</td>
<td>kyogu_snu_t6_2024</td>
<td>0.189 (0.190 - 0.201)</td>
<td>0.409 (0.383 - 0.437)</td>
<td>0.135 (0.129 - 0.141)</td>
<td>0.272 (0.257 - 0.288)</td>
<td>0.272 (0.257 - 0.288)</td>
<td>0.526 (0.515 - 0.537)</td>
<td>0.187</td>
<td>0.412</td>
<td>0.134</td>
<td>0.273</td>
<td>0.273</td>
<td>0.518</td>
</tr>
<tr>
<td></td>
<td>6</td>
<td>Kong_CUHK_t6_1</td>
<td>True</td>
<td>Qiuqiang Kong</td>
<td>kong_cuhk_t6_2024</td>
<td>0.192 (0.195 - 0.206)</td>
<td>0.495 (0.467 - 0.526)</td>
<td>0.141 (0.135 - 0.147)</td>
<td>0.318 (0.301 - 0.336)</td>
<td>0.315 (0.299 - 0.333)</td>
<td>0.525 (0.514 - 0.536)</td>
<td>0.196</td>
<td>0.529</td>
<td>0.138</td>
<td>0.334</td>
<td>0.332</td>
<td>0.528</td>
</tr>
<tr>
<td></td>
<td>7</td>
<td>Choi_KAIST_t6_1</td>
<td>False</td>
<td>Inhan Choi</td>
<td>choi_kaist_t6_2024</td>
<td>0.187 (0.188 - 0.199)</td>
<td>0.465 (0.438 - 0.494)</td>
<td>0.135 (0.129 - 0.142)</td>
<td>0.300 (0.284 - 0.317)</td>
<td>0.299 (0.284 - 0.316)</td>
<td>0.520 (0.509 - 0.531)</td>
<td>0.189</td>
<td>0.464</td>
<td>0.134</td>
<td>0.299</td>
<td>0.299</td>
<td>0.521</td>
</tr>
<tr>
<td></td>
<td>8</td>
<td>Li_SCUT_t6_4</td>
<td>False</td>
<td>Qianqian Li</td>
<td>li_scut_t6_2024</td>
<td>0.188 (0.190 - 0.201)</td>
<td>0.468 (0.440 - 0.497)</td>
<td>0.138 (0.132 - 0.145)</td>
<td>0.303 (0.287 - 0.320)</td>
<td>0.302 (0.286 - 0.319)</td>
<td>0.520 (0.508 - 0.531)</td>
<td>0.189</td>
<td>0.469</td>
<td>0.134</td>
<td>0.301</td>
<td>0.301</td>
<td>0.513</td>
</tr>
<tr>
<td></td>
<td>9</td>
<td>Silva_JKUICP_t6_2</td>
<td>False</td>
<td>Jakob De Jesus Silva</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>0.188 (0.190 - 0.201)</td>
<td>0.456 (0.430 - 0.484)</td>
<td>0.138 (0.132 - 0.144)</td>
<td>0.297 (0.282 - 0.313)</td>
<td>0.296 (0.281 - 0.313)</td>
<td>0.516 (0.505 - 0.527)</td>
<td>0.192</td>
<td>0.479</td>
<td>0.138</td>
<td>0.309</td>
<td>0.308</td>
<td>0.508</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>Epshtein_ARC_t6_1</td>
<td>False</td>
<td>Dan Epshtein</td>
<td>epshtein_arc_t6_2024</td>
<td>0.188 (0.190 - 0.200)</td>
<td>0.462 (0.437 - 0.491)</td>
<td>0.137 (0.131 - 0.143)</td>
<td>0.300 (0.285 - 0.316)</td>
<td>0.298 (0.283 - 0.315)</td>
<td>0.514 (0.503 - 0.525)</td>
<td>0.189</td>
<td>0.473</td>
<td>0.135</td>
<td>0.304</td>
<td>0.302</td>
<td>0.504</td>
</tr>
<tr>
<td></td>
<td>11</td>
<td>Hong_CAU_t6_1</td>
<td>False</td>
<td>Hyunhee Hong</td>
<td>hong_cau_t6_2024</td>
<td>0.184 (0.185 - 0.195)</td>
<td>0.427 (0.402 - 0.454)</td>
<td>0.134 (0.128 - 0.140)</td>
<td>0.280 (0.266 - 0.296)</td>
<td>0.279 (0.265 - 0.295)</td>
<td>0.513 (0.502 - 0.524)</td>
<td>0.188</td>
<td>0.458</td>
<td>0.133</td>
<td>0.295</td>
<td>0.294</td>
<td>0.509</td>
</tr>
<tr class="info">
<td></td>
<td>12</td>
<td>Baseline</td>
<td>False</td>
<td>Étienne Labbé</td>
<td>labbé_irit_t6_2024</td>
<td>0.186 (0.187 - 0.198)</td>
<td>0.442 (0.417 - 0.468)</td>
<td>0.135 (0.129 - 0.141)</td>
<td>0.288 (0.274 - 0.304)</td>
<td>0.287 (0.273 - 0.303)</td>
<td>0.510 (0.499 - 0.521)</td>
<td>0.190</td>
<td>0.462</td>
<td>0.134</td>
<td>0.298</td>
<td>0.296</td>
<td>0.504</td>
</tr>
</tbody>
</table>
<h1 id="systems-ranking">Systems ranking</h1>
<p>Here are listed all submitted systems and their ranking according to the different
metrics and grouping of metrics. The first table shows all challenge metrics and
all systems, and the second table shows all systems but with contrastive metrics.</p>
<p>Detailed information for each system is provided in the next section.</p>
<h2 id="systems-ranking-challenge-metrics">Systems ranking, challenge metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="submission_code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="results_evaluation_fense_bootstrap" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected<br/> metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="4">Submission Information</th>
<th class="sep-left-cell text-center" colspan="6">Clotho evaluation split</th>
<th class="sep-left-cell text-center" colspan="6">Clotho development testing split</th>
</tr>
<tr>
<th class="text-center" data-chartable="true" data-field="entry_rank" data-sortable="true" data-value-type="int">
              Submission<br/>rank
            </th>
<th data-field="submission_code" data-sortable="true">
              Submission code
            </th>
<th data-field="system_bias" data-sortable="true">
              Data<br/>leak
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_evaluation_meteor_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_cider_d_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              CIDEr-D
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_spice_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_spider_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_spider_fl_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              SPIDEr-FL
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_fense_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
              FENSE
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_development_testing_meteor" data-reversed="true" data-sortable="true" data-value-type="float3">
              METEOR
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_cider_d" data-reversed="true" data-sortable="true" data-value-type="float3">
              CIDEr-D
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_spice" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPICE
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_spider" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_spider_fl" data-reversed="true" data-sortable="true" data-value-type="float3">
              SPIDEr-FL
            </th>
<th class="text-center" data-chartable="true" data-field="results_development_testing_fense" data-reversed="true" data-sortable="true" data-value-type="float3">
              FENSE
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>1</td>
<td>Jung_CMU_t6_4</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.172 (0.174 - 0.182)</td>
<td>0.344 (0.324 - 0.366)</td>
<td>0.140 (0.134 - 0.146)</td>
<td>0.242 (0.230 - 0.255)</td>
<td>0.241 (0.229 - 0.254)</td>
<td>0.554 (0.543 - 0.563)</td>
<td>0.174</td>
<td>0.327</td>
<td>0.136</td>
<td>0.230</td>
<td>0.230</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>2</td>
<td>Jung_CMU_t6_2</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.176 (0.177 - 0.186)</td>
<td>0.359 (0.338 - 0.382)</td>
<td>0.142 (0.137 - 0.148)</td>
<td>0.251 (0.238 - 0.264)</td>
<td>0.249 (0.236 - 0.262)</td>
<td>0.549 (0.538 - 0.559)</td>
<td>0.177</td>
<td>0.341</td>
<td>0.140</td>
<td>0.240</td>
<td>0.239</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>3</td>
<td>Jung_CMU_t6_3</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.172 (0.173 - 0.182)</td>
<td>0.345 (0.325 - 0.368)</td>
<td>0.141 (0.135 - 0.146)</td>
<td>0.243 (0.231 - 0.256)</td>
<td>0.239 (0.227 - 0.252)</td>
<td>0.547 (0.536 - 0.557)</td>
<td>0.174</td>
<td>0.333</td>
<td>0.132</td>
<td>0.232</td>
<td>0.232</td>
<td>0.544</td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>Jung_CMU_t6_1</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.181 (0.182 - 0.192)</td>
<td>0.387 (0.365 - 0.412)</td>
<td>0.135 (0.130 - 0.141)</td>
<td>0.261 (0.248 - 0.276)</td>
<td>0.260 (0.247 - 0.275)</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.182</td>
<td>0.366</td>
<td>0.133</td>
<td>0.250</td>
<td>0.249</td>
<td>0.541</td>
</tr>
<tr>
<td></td>
<td>5</td>
<td>Kim_SNU_t6_2</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.199 (0.200 - 0.210)</td>
<td>0.480 (0.453 - 0.508)</td>
<td>0.148 (0.142 - 0.154)</td>
<td>0.314 (0.299 - 0.330)</td>
<td>0.314 (0.299 - 0.330)</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.196</td>
<td>0.477</td>
<td>0.142</td>
<td>0.310</td>
<td>0.310</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>6</td>
<td>Kim_SNU_t6_4</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.199 (0.200 - 0.211)</td>
<td>0.487 (0.460 - 0.516)</td>
<td>0.151 (0.145 - 0.158)</td>
<td>0.319 (0.303 - 0.336)</td>
<td>0.319 (0.303 - 0.336)</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.199</td>
<td>0.478</td>
<td>0.149</td>
<td>0.313</td>
<td>0.313</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>7</td>
<td>Kim_SNU_t6_3</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.197 (0.198 - 0.209)</td>
<td>0.472 (0.446 - 0.501)</td>
<td>0.148 (0.142 - 0.154)</td>
<td>0.310 (0.295 - 0.326)</td>
<td>0.310 (0.295 - 0.326)</td>
<td>0.542 (0.532 - 0.552)</td>
<td>0.200</td>
<td>0.478</td>
<td>0.149</td>
<td>0.313</td>
<td>0.313</td>
<td>0.539</td>
</tr>
<tr>
<td></td>
<td>8</td>
<td>Chen_SJTU_t6_4</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.194 (0.196 - 0.207)</td>
<td>0.509 (0.479 - 0.541)</td>
<td>0.145 (0.138 - 0.151)</td>
<td>0.327 (0.310 - 0.345)</td>
<td>0.322 (0.306 - 0.341)</td>
<td>0.541 (0.530 - 0.552)</td>
<td>0.193</td>
<td>0.522</td>
<td>0.148</td>
<td>0.335</td>
<td>0.333</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>9</td>
<td>Chen_SJTU_t6_3</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.194 (0.196 - 0.207)</td>
<td>0.510 (0.480 - 0.542)</td>
<td>0.145 (0.139 - 0.152)</td>
<td>0.327 (0.310 - 0.346)</td>
<td>0.323 (0.306 - 0.342)</td>
<td>0.541 (0.530 - 0.552)</td>
<td>0.193</td>
<td>0.518</td>
<td>0.148</td>
<td>0.333</td>
<td>0.331</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>Chen_SJTU_t6_1</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.195 (0.197 - 0.208)</td>
<td>0.497 (0.468 - 0.528)</td>
<td>0.144 (0.138 - 0.151)</td>
<td>0.321 (0.304 - 0.339)</td>
<td>0.317 (0.301 - 0.335)</td>
<td>0.540 (0.529 - 0.551)</td>
<td>0.195</td>
<td>0.512</td>
<td>0.147</td>
<td>0.329</td>
<td>0.329</td>
<td>0.543</td>
</tr>
<tr>
<td></td>
<td>11</td>
<td>Kim_SNU_t6_1</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.195 (0.197 - 0.207)</td>
<td>0.470 (0.443 - 0.499)</td>
<td>0.145 (0.139 - 0.151)</td>
<td>0.307 (0.292 - 0.324)</td>
<td>0.307 (0.292 - 0.324)</td>
<td>0.540 (0.530 - 0.550)</td>
<td>0.199</td>
<td>0.483</td>
<td>0.148</td>
<td>0.316</td>
<td>0.316</td>
<td>0.539</td>
</tr>
<tr>
<td></td>
<td>12</td>
<td>Chen_SJTU_t6_2</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.195 (0.197 - 0.208)</td>
<td>0.518 (0.489 - 0.551)</td>
<td>0.146 (0.140 - 0.153)</td>
<td>0.332 (0.315 - 0.351)</td>
<td>0.329 (0.312 - 0.348)</td>
<td>0.538 (0.527 - 0.550)</td>
<td>0.196</td>
<td>0.537</td>
<td>0.150</td>
<td>0.343</td>
<td>0.342</td>
<td>0.540</td>
</tr>
<tr>
<td></td>
<td>13</td>
<td>Li_ALXC_t6_4</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.195 (0.197 - 0.208)</td>
<td>0.493 (0.464 - 0.525)</td>
<td>0.145 (0.139 - 0.151)</td>
<td>0.319 (0.302 - 0.337)</td>
<td>0.317 (0.300 - 0.335)</td>
<td>0.533 (0.522 - 0.543)</td>
<td>0.194</td>
<td>0.503</td>
<td>0.145</td>
<td>0.324</td>
<td>0.323</td>
<td>0.532</td>
</tr>
<tr>
<td></td>
<td>14</td>
<td>Li_ALXC_t6_3</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.177 (0.179 - 0.189)</td>
<td>0.441 (0.415 - 0.470)</td>
<td>0.128 (0.123 - 0.134)</td>
<td>0.285 (0.270 - 0.301)</td>
<td>0.284 (0.270 - 0.301)</td>
<td>0.528 (0.517 - 0.538)</td>
<td>0.178</td>
<td>0.447</td>
<td>0.127</td>
<td>0.287</td>
<td>0.287</td>
<td>0.521</td>
</tr>
<tr>
<td></td>
<td>15</td>
<td>Kyogu_SNU_t6_2</td>
<td>False</td>
<td>kyogu_snu_t6_2024</td>
<td>0.189 (0.190 - 0.201)</td>
<td>0.409 (0.383 - 0.437)</td>
<td>0.135 (0.129 - 0.141)</td>
<td>0.272 (0.257 - 0.288)</td>
<td>0.272 (0.257 - 0.288)</td>
<td>0.526 (0.515 - 0.537)</td>
<td>0.187</td>
<td>0.412</td>
<td>0.134</td>
<td>0.273</td>
<td>0.273</td>
<td>0.518</td>
</tr>
<tr>
<td></td>
<td>16</td>
<td>Kong_CUHK_t6_1</td>
<td>True</td>
<td>kong_cuhk_t6_2024</td>
<td>0.192 (0.195 - 0.206)</td>
<td>0.495 (0.467 - 0.526)</td>
<td>0.141 (0.135 - 0.147)</td>
<td>0.318 (0.301 - 0.336)</td>
<td>0.315 (0.299 - 0.333)</td>
<td>0.525 (0.514 - 0.536)</td>
<td>0.196</td>
<td>0.529</td>
<td>0.138</td>
<td>0.334</td>
<td>0.332</td>
<td>0.528</td>
</tr>
<tr>
<td></td>
<td>17</td>
<td>Kong_CUHK_t6_2</td>
<td>False</td>
<td>kong_cuhk_t6_2024</td>
<td>0.193 (0.195 - 0.206)</td>
<td>0.478 (0.451 - 0.507)</td>
<td>0.145 (0.138 - 0.151)</td>
<td>0.311 (0.296 - 0.328)</td>
<td>0.307 (0.292 - 0.325)</td>
<td>0.525 (0.514 - 0.536)</td>
<td>0.193</td>
<td>0.495</td>
<td>0.140</td>
<td>0.317</td>
<td>0.314</td>
<td>0.523</td>
</tr>
<tr>
<td></td>
<td>18</td>
<td>Choi_KAIST_t6_1</td>
<td>False</td>
<td>choi_kaist_t6_2024</td>
<td>0.187 (0.188 - 0.199)</td>
<td>0.465 (0.438 - 0.494)</td>
<td>0.135 (0.129 - 0.142)</td>
<td>0.300 (0.284 - 0.317)</td>
<td>0.299 (0.284 - 0.316)</td>
<td>0.520 (0.509 - 0.531)</td>
<td>0.189</td>
<td>0.464</td>
<td>0.134</td>
<td>0.299</td>
<td>0.299</td>
<td>0.521</td>
</tr>
<tr>
<td></td>
<td>19</td>
<td>Li_ALXC_t6_1</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.190 (0.191 - 0.202)</td>
<td>0.474 (0.446 - 0.506)</td>
<td>0.141 (0.135 - 0.148)</td>
<td>0.308 (0.291 - 0.326)</td>
<td>0.307 (0.290 - 0.325)</td>
<td>0.520 (0.509 - 0.532)</td>
<td>0.191</td>
<td>0.499</td>
<td>0.139</td>
<td>0.319</td>
<td>0.318</td>
<td>0.522</td>
</tr>
<tr>
<td></td>
<td>20</td>
<td>Li_SCUT_t6_4</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.188 (0.190 - 0.201)</td>
<td>0.468 (0.440 - 0.497)</td>
<td>0.138 (0.132 - 0.145)</td>
<td>0.303 (0.287 - 0.320)</td>
<td>0.302 (0.286 - 0.319)</td>
<td>0.520 (0.508 - 0.531)</td>
<td>0.189</td>
<td>0.469</td>
<td>0.134</td>
<td>0.301</td>
<td>0.301</td>
<td>0.513</td>
</tr>
<tr>
<td></td>
<td>21</td>
<td>Li_SCUT_t6_3</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.189 (0.191 - 0.202)</td>
<td>0.471 (0.443 - 0.502)</td>
<td>0.138 (0.132 - 0.145)</td>
<td>0.305 (0.288 - 0.322)</td>
<td>0.304 (0.288 - 0.322)</td>
<td>0.519 (0.508 - 0.530)</td>
<td>0.187</td>
<td>0.467</td>
<td>0.133</td>
<td>0.134</td>
<td>0.300</td>
<td>0.512</td>
</tr>
<tr>
<td></td>
<td>22</td>
<td>Choi_KAIST_t6_2</td>
<td>False</td>
<td>choi_kaist_t6_2024</td>
<td>0.184 (0.185 - 0.196)</td>
<td>0.429 (0.403 - 0.457)</td>
<td>0.133 (0.127 - 0.139)</td>
<td>0.281 (0.266 - 0.297)</td>
<td>0.279 (0.264 - 0.296)</td>
<td>0.518 (0.507 - 0.529)</td>
<td>0.182</td>
<td>0.414</td>
<td>0.130</td>
<td>0.272</td>
<td>0.272</td>
<td>0.515</td>
</tr>
<tr>
<td></td>
<td>23</td>
<td>Li_ALXC_t6_2</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.187 (0.188 - 0.199)</td>
<td>0.462 (0.433 - 0.492)</td>
<td>0.135 (0.129 - 0.141)</td>
<td>0.298 (0.282 - 0.316)</td>
<td>0.298 (0.281 - 0.315)</td>
<td>0.518 (0.506 - 0.529)</td>
<td>0.187</td>
<td>0.458</td>
<td>0.137</td>
<td>0.298</td>
<td>0.297</td>
<td>0.520</td>
</tr>
<tr>
<td></td>
<td>24</td>
<td>Silva_JKUICP_t6_2</td>
<td>False</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>0.188 (0.190 - 0.201)</td>
<td>0.456 (0.430 - 0.484)</td>
<td>0.138 (0.132 - 0.144)</td>
<td>0.297 (0.282 - 0.313)</td>
<td>0.296 (0.281 - 0.313)</td>
<td>0.516 (0.505 - 0.527)</td>
<td>0.192</td>
<td>0.479</td>
<td>0.138</td>
<td>0.309</td>
<td>0.308</td>
<td>0.508</td>
</tr>
<tr>
<td></td>
<td>25</td>
<td>Li_SCUT_t6_2</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.189 (0.191 - 0.202)</td>
<td>0.467 (0.441 - 0.497)</td>
<td>0.139 (0.133 - 0.145)</td>
<td>0.303 (0.287 - 0.320)</td>
<td>0.301 (0.286 - 0.318)</td>
<td>0.516 (0.505 - 0.527)</td>
<td>0.186</td>
<td>0.460</td>
<td>0.133</td>
<td>0.296</td>
<td>0.295</td>
<td>0.505</td>
</tr>
<tr>
<td></td>
<td>26</td>
<td>Silva_JKUICP_t6_1</td>
<td>False</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>0.187 (0.188 - 0.199)</td>
<td>0.450 (0.424 - 0.478)</td>
<td>0.135 (0.129 - 0.141)</td>
<td>0.292 (0.277 - 0.308)</td>
<td>0.291 (0.276 - 0.307)</td>
<td>0.515 (0.504 - 0.526)</td>
<td>0.186</td>
<td>0.451</td>
<td>0.134</td>
<td>0.292</td>
<td>0.290</td>
<td>0.506</td>
</tr>
<tr>
<td></td>
<td>27</td>
<td>Epshtein_ARC_t6_1</td>
<td>False</td>
<td>epshtein_arc_t6_2024</td>
<td>0.188 (0.190 - 0.200)</td>
<td>0.462 (0.437 - 0.491)</td>
<td>0.137 (0.131 - 0.143)</td>
<td>0.300 (0.285 - 0.316)</td>
<td>0.298 (0.283 - 0.315)</td>
<td>0.514 (0.503 - 0.525)</td>
<td>0.189</td>
<td>0.473</td>
<td>0.135</td>
<td>0.304</td>
<td>0.302</td>
<td>0.504</td>
</tr>
<tr>
<td></td>
<td>28</td>
<td>Hong_CAU_t6_1</td>
<td>False</td>
<td>hong_cau_t6_2024</td>
<td>0.184 (0.185 - 0.195)</td>
<td>0.427 (0.402 - 0.454)</td>
<td>0.134 (0.128 - 0.140)</td>
<td>0.280 (0.266 - 0.296)</td>
<td>0.279 (0.265 - 0.295)</td>
<td>0.513 (0.502 - 0.524)</td>
<td>0.188</td>
<td>0.458</td>
<td>0.133</td>
<td>0.295</td>
<td>0.294</td>
<td>0.509</td>
</tr>
<tr>
<td></td>
<td>29</td>
<td>Kyogu_SNU_t6_1</td>
<td>False</td>
<td>kyogu_snu_t6_2024</td>
<td>0.186 (0.189 - 0.200)</td>
<td>0.441 (0.414 - 0.469)</td>
<td>0.134 (0.128 - 0.140)</td>
<td>0.288 (0.272 - 0.304)</td>
<td>0.287 (0.271 - 0.303)</td>
<td>0.512 (0.501 - 0.524)</td>
<td>0.185</td>
<td>0.444</td>
<td>0.133</td>
<td>0.288</td>
<td>0.287</td>
<td>0.507</td>
</tr>
<tr class="info">
<td></td>
<td>30</td>
<td>Baseline</td>
<td>False</td>
<td>labbé_irit_t6_2024</td>
<td>0.186 (0.187 - 0.198)</td>
<td>0.442 (0.417 - 0.468)</td>
<td>0.135 (0.129 - 0.141)</td>
<td>0.288 (0.274 - 0.304)</td>
<td>0.287 (0.273 - 0.303)</td>
<td>0.510 (0.499 - 0.521)</td>
<td>0.190</td>
<td>0.462</td>
<td>0.134</td>
<td>0.298</td>
<td>0.296</td>
<td>0.504</td>
</tr>
<tr>
<td></td>
<td>31</td>
<td>Li_SCUT_t6_1</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.187 (0.189 - 0.200)</td>
<td>0.459 (0.432 - 0.488)</td>
<td>0.137 (0.131 - 0.143)</td>
<td>0.298 (0.283 - 0.315)</td>
<td>0.296 (0.281 - 0.314)</td>
<td>0.508 (0.496 - 0.519)</td>
<td>0.187</td>
<td>0.470</td>
<td>0.131</td>
<td>0.301</td>
<td>0.300</td>
<td>0.507</td>
</tr>
</tbody>
</table>
<h2 id="systems-ranking-additional-metrics">Systems ranking, additional metrics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="submission_code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="results_evaluation_fense_bootstrap" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Selected<br/> metric<br/>rank</th>
<th class="sep-left-cell text-center" colspan="4">Submission Information</th>
<th class="sep-left-cell text-center" colspan="4">Clotho evaluation split</th>
</tr>
<tr>
<th class="text-center" data-chartable="true" data-field="entry_rank" data-sortable="true" data-value-type="int">
              Submission<br/>rank
            </th>
<th data-field="submission_code" data-sortable="true">
              Submission code
            </th>
<th data-field="system_bias" data-sortable="true">
              Data<br/>leak
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="results_evaluation_fense_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
                FENSE
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_sbert_sim_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
                Sentence-BERT
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_fer_bootstrap" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
                Fluency Error Rate
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_vocab_cands" data-reversed="true" data-sortable="true" data-value-type="int">
                Vocabulary
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>1</td>
<td>Jung_CMU_t6_4</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.554 (0.543 - 0.563)</td>
<td>0.556 (0.546 - 0.566)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>915.0</td>
</tr>
<tr>
<td></td>
<td>2</td>
<td>Jung_CMU_t6_2</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.549 (0.538 - 0.559)</td>
<td>0.553 (0.543 - 0.563)</td>
<td>0.008 (0.004 - 0.014)</td>
<td>920.0</td>
</tr>
<tr>
<td></td>
<td>3</td>
<td>Jung_CMU_t6_3</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.547 (0.536 - 0.557)</td>
<td>0.554 (0.544 - 0.564)</td>
<td>0.012 (0.007 - 0.020)</td>
<td>888.0</td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>Jung_CMU_t6_1</td>
<td>False</td>
<td>jung_cmu_t6_2024</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.548 (0.537 - 0.558)</td>
<td>0.007 (0.003 - 0.013)</td>
<td>896.0</td>
</tr>
<tr>
<td></td>
<td>5</td>
<td>Kim_SNU_t6_2</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.000 (0.000 - 0.000)</td>
<td>836.0</td>
</tr>
<tr>
<td></td>
<td>6</td>
<td>Kim_SNU_t6_4</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.544 (0.534 - 0.555)</td>
<td>0.000 (0.000 - 0.000)</td>
<td>799.0</td>
</tr>
<tr>
<td></td>
<td>7</td>
<td>Kim_SNU_t6_3</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.542 (0.532 - 0.552)</td>
<td>0.542 (0.532 - 0.552)</td>
<td>0.000 (0.000 - 0.000)</td>
<td>840.0</td>
</tr>
<tr>
<td></td>
<td>8</td>
<td>Chen_SJTU_t6_4</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.541 (0.530 - 0.552)</td>
<td>0.546 (0.536 - 0.557)</td>
<td>0.009 (0.004 - 0.015)</td>
<td>783.0</td>
</tr>
<tr>
<td></td>
<td>9</td>
<td>Chen_SJTU_t6_3</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.541 (0.530 - 0.552)</td>
<td>0.546 (0.535 - 0.557)</td>
<td>0.009 (0.004 - 0.015)</td>
<td>787.0</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>Chen_SJTU_t6_1</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.540 (0.529 - 0.551)</td>
<td>0.546 (0.534 - 0.556)</td>
<td>0.010 (0.005 - 0.017)</td>
<td>835.0</td>
</tr>
<tr>
<td></td>
<td>11</td>
<td>Kim_SNU_t6_1</td>
<td>False</td>
<td>kim_snu_t6_2024</td>
<td>0.540 (0.530 - 0.550)</td>
<td>0.540 (0.530 - 0.550)</td>
<td>0.000 (0.000 - 0.000)</td>
<td>832.0</td>
</tr>
<tr>
<td></td>
<td>12</td>
<td>Chen_SJTU_t6_2</td>
<td>True</td>
<td>chen_sjtu_t6_2024</td>
<td>0.538 (0.527 - 0.550)</td>
<td>0.543 (0.532 - 0.554)</td>
<td>0.010 (0.005 - 0.017)</td>
<td>800.0</td>
</tr>
<tr>
<td></td>
<td>13</td>
<td>Li_ALXC_t6_4</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.533 (0.522 - 0.543)</td>
<td>0.535 (0.524 - 0.545)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>786.0</td>
</tr>
<tr>
<td></td>
<td>14</td>
<td>Li_ALXC_t6_3</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.528 (0.517 - 0.538)</td>
<td>0.528 (0.518 - 0.539)</td>
<td>0.001 (0.000 - 0.006)</td>
<td>612.0</td>
</tr>
<tr>
<td></td>
<td>15</td>
<td>Kyogu_SNU_t6_2</td>
<td>False</td>
<td>kyogu_snu_t6_2024</td>
<td>0.526 (0.515 - 0.537)</td>
<td>0.526 (0.516 - 0.537)</td>
<td>0.001 (0.000 - 0.006)</td>
<td>954.0</td>
</tr>
<tr>
<td></td>
<td>16</td>
<td>Kong_CUHK_t6_1</td>
<td>True</td>
<td>kong_cuhk_t6_2024</td>
<td>0.525 (0.514 - 0.536)</td>
<td>0.529 (0.518 - 0.539)</td>
<td>0.006 (0.002 - 0.012)</td>
<td>606.0</td>
</tr>
<tr>
<td></td>
<td>17</td>
<td>Kong_CUHK_t6_2</td>
<td>False</td>
<td>kong_cuhk_t6_2024</td>
<td>0.525 (0.514 - 0.536)</td>
<td>0.531 (0.520 - 0.541)</td>
<td>0.011 (0.006 - 0.018)</td>
<td>565.0</td>
</tr>
<tr>
<td></td>
<td>18</td>
<td>Choi_KAIST_t6_1</td>
<td>False</td>
<td>choi_kaist_t6_2024</td>
<td>0.520 (0.509 - 0.531)</td>
<td>0.521 (0.510 - 0.532)</td>
<td>0.003 (0.001 - 0.009)</td>
<td>609.0</td>
</tr>
<tr>
<td></td>
<td>19</td>
<td>Li_ALXC_t6_1</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.520 (0.509 - 0.532)</td>
<td>0.522 (0.511 - 0.533)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>751.0</td>
</tr>
<tr>
<td></td>
<td>20</td>
<td>Li_SCUT_t6_4</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.520 (0.508 - 0.531)</td>
<td>0.521 (0.510 - 0.532)</td>
<td>0.002 (0.000 - 0.007)</td>
<td>498.0</td>
</tr>
<tr>
<td></td>
<td>21</td>
<td>Li_SCUT_t6_3</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.519 (0.508 - 0.530)</td>
<td>0.520 (0.509 - 0.531)</td>
<td>0.002 (0.000 - 0.007)</td>
<td>513.0</td>
</tr>
<tr>
<td></td>
<td>22</td>
<td>Choi_KAIST_t6_2</td>
<td>False</td>
<td>choi_kaist_t6_2024</td>
<td>0.518 (0.507 - 0.529)</td>
<td>0.520 (0.509 - 0.531)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>866.0</td>
</tr>
<tr>
<td></td>
<td>23</td>
<td>Li_ALXC_t6_2</td>
<td>False</td>
<td>li_alxc_t6_2024</td>
<td>0.518 (0.506 - 0.529)</td>
<td>0.520 (0.508 - 0.531)</td>
<td>0.003 (0.001 - 0.008)</td>
<td>773.0</td>
</tr>
<tr>
<td></td>
<td>24</td>
<td>Silva_JKUICP_t6_2</td>
<td>False</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>0.516 (0.505 - 0.527)</td>
<td>0.517 (0.505 - 0.528)</td>
<td>0.001 (0.000 - 0.005)</td>
<td>606.0</td>
</tr>
<tr>
<td></td>
<td>25</td>
<td>Li_SCUT_t6_2</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.516 (0.505 - 0.527)</td>
<td>0.517 (0.506 - 0.528)</td>
<td>0.002 (0.000 - 0.007)</td>
<td>517.0</td>
</tr>
<tr>
<td></td>
<td>26</td>
<td>Silva_JKUICP_t6_1</td>
<td>False</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>0.515 (0.504 - 0.526)</td>
<td>0.517 (0.506 - 0.528)</td>
<td>0.003 (0.001 - 0.008)</td>
<td>610.0</td>
</tr>
<tr>
<td></td>
<td>27</td>
<td>Epshtein_ARC_t6_1</td>
<td>False</td>
<td>epshtein_arc_t6_2024</td>
<td>0.514 (0.503 - 0.525)</td>
<td>0.516 (0.505 - 0.527)</td>
<td>0.005 (0.002 - 0.011)</td>
<td>563.0</td>
</tr>
<tr>
<td></td>
<td>28</td>
<td>Hong_CAU_t6_1</td>
<td>False</td>
<td>hong_cau_t6_2024</td>
<td>0.513 (0.502 - 0.524)</td>
<td>0.515 (0.504 - 0.526)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>604.0</td>
</tr>
<tr>
<td></td>
<td>29</td>
<td>Kyogu_SNU_t6_1</td>
<td>False</td>
<td>kyogu_snu_t6_2024</td>
<td>0.512 (0.501 - 0.524)</td>
<td>0.515 (0.503 - 0.526)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>822.0</td>
</tr>
<tr class="info">
<td></td>
<td>30</td>
<td>Baseline</td>
<td>False</td>
<td>labbé_irit_t6_2024</td>
<td>0.510 (0.499 - 0.521)</td>
<td>0.512 (0.501 - 0.523)</td>
<td>0.004 (0.001 - 0.010)</td>
<td>532.0</td>
</tr>
<tr>
<td></td>
<td>31</td>
<td>Li_SCUT_t6_1</td>
<td>False</td>
<td>li_scut_t6_2024</td>
<td>0.508 (0.496 - 0.519)</td>
<td>0.511 (0.499 - 0.522)</td>
<td>0.006 (0.002 - 0.012)</td>
<td>539.0</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<p>In this section you can find the characteristics of the submitted systems. There are two tables
for easy reference, in the corresponding subsections. The first table has an overview of the systems
and the second has a detailed presentation of each system.</p>
<h2 id="overview-of-characteristics">Overview of characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-default-mode="scatter" data-chart-modes="scatter" data-chart-tooltip-fields="submission_code" data-filter-control="true" data-filter-show-clear="true" data-id-field="submission_code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="system_complexity_total_parameters" data-scatter-y="results_evaluation_fense_bootstrap" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="entry_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="text-center" data-chartable="true" data-field="entry_rank" data-sortable="true" data-value-type="int">
              Submission<br/>rank
            </th>
<th class="sm-cell" data-field="submission_code" data-sortable="true">
              Submission<br/>code
            </th>
<th data-field="system_bias" data-sortable="true">
              Data<br/>leak
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_fense_bootstrap" data-reversed="false" data-sortable="true" data-value-type="float3-interval-muted">
              FENSE
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Method<br/>scheme/architecture
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of<br/>parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Audio modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Data<br/>augmentation
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Jung_CMU_t6_4</td>
<td>False</td>
<td>0.554 (0.543 - 0.563)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>7857055850</td>
<td>Conformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>2</td>
<td>Jung_CMU_t6_2</td>
<td>False</td>
<td>0.549 (0.538 - 0.559)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>1571411170</td>
<td>Conformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>3</td>
<td>Jung_CMU_t6_3</td>
<td>False</td>
<td>0.547 (0.536 - 0.557)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>5612182750</td>
<td>Conformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>4</td>
<td>Jung_CMU_t6_1</td>
<td>False</td>
<td>0.544 (0.534 - 0.555)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>224487310</td>
<td>Conformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>5</td>
<td>Kim_SNU_t6_2</td>
<td>False</td>
<td>0.544 (0.534 - 0.555)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>754328981</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup</td>
</tr>
<tr>
<td>6</td>
<td>Kim_SNU_t6_4</td>
<td>False</td>
<td>0.544 (0.534 - 0.555)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>4575364501</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup</td>
</tr>
<tr>
<td>7</td>
<td>Kim_SNU_t6_3</td>
<td>False</td>
<td>0.542 (0.532 - 0.552)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>3620105621</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup</td>
</tr>
<tr>
<td>8</td>
<td>Chen_SJTU_t6_4</td>
<td>True</td>
<td>0.541 (0.530 - 0.552)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>6840335631</td>
<td>transformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>9</td>
<td>Chen_SJTU_t6_3</td>
<td>True</td>
<td>0.541 (0.530 - 0.552)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>6840335631</td>
<td>transformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>10</td>
<td>Chen_SJTU_t6_1</td>
<td>True</td>
<td>0.540 (0.529 - 0.551)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>6840335631</td>
<td>transformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>11</td>
<td>Kim_SNU_t6_1</td>
<td>False</td>
<td>0.540 (0.530 - 0.550)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>754328981</td>
<td>cnn</td>
<td>transformer</td>
<td>mixup</td>
</tr>
<tr>
<td>12</td>
<td>Chen_SJTU_t6_2</td>
<td>True</td>
<td>0.538 (0.527 - 0.550)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>6840335631</td>
<td>transformer</td>
<td>transformer</td>
<td>SpecAugment, mixup</td>
</tr>
<tr>
<td>13</td>
<td>Li_ALXC_t6_4</td>
<td>False</td>
<td>0.533 (0.522 - 0.543)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>6850672271</td>
<td>ced</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>14</td>
<td>Li_ALXC_t6_3</td>
<td>False</td>
<td>0.528 (0.517 - 0.538)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>245365903</td>
<td>ced</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>15</td>
<td>Kyogu_SNU_t6_2</td>
<td>False</td>
<td>0.526 (0.515 - 0.537)</td>
<td>kyogu_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>8131137200</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>16</td>
<td>Kong_CUHK_t6_1</td>
<td>True</td>
<td>0.525 (0.514 - 0.536)</td>
<td>kong_cuhk_t6_2024</td>
<td>encoder-decoder</td>
<td>146403855</td>
<td>cnn</td>
<td>transformer</td>
<td>spec-based mixup, label smoothing</td>
</tr>
<tr>
<td>17</td>
<td>Kong_CUHK_t6_2</td>
<td>False</td>
<td>0.525 (0.514 - 0.536)</td>
<td>kong_cuhk_t6_2024</td>
<td>encoder-decoder</td>
<td>126355215</td>
<td>cnn</td>
<td>transformer</td>
<td>spec-based mixup, label smoothing</td>
</tr>
<tr>
<td>18</td>
<td>Choi_KAIST_t6_1</td>
<td>False</td>
<td>0.520 (0.509 - 0.531)</td>
<td>choi_kaist_t6_2024</td>
<td>encoder-decoder</td>
<td>42038209</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing, ChatGPT paraphrasing</td>
</tr>
<tr>
<td>19</td>
<td>Li_ALXC_t6_1</td>
<td>False</td>
<td>0.520 (0.509 - 0.532)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>6850408320</td>
<td>Dasheng</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>20</td>
<td>Li_SCUT_t6_4</td>
<td>False</td>
<td>0.520 (0.508 - 0.531)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>41303080</td>
<td>ConvNeXt</td>
<td>transformer</td>
<td>mixup, SpecAugment</td>
</tr>
<tr>
<td>21</td>
<td>Li_SCUT_t6_3</td>
<td>False</td>
<td>0.519 (0.508 - 0.530)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>41303080</td>
<td>ConvNeXt</td>
<td>transformer</td>
<td>mixup, SpecAugment</td>
</tr>
<tr>
<td>22</td>
<td>Choi_KAIST_t6_2</td>
<td>False</td>
<td>0.518 (0.507 - 0.529)</td>
<td>choi_kaist_t6_2024</td>
<td>encoder-decoder</td>
<td>42038209</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing, ChatGPT paraphrasing</td>
</tr>
<tr>
<td>23</td>
<td>Li_ALXC_t6_2</td>
<td>False</td>
<td>0.518 (0.506 - 0.529)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>7397882752</td>
<td>Dasheng</td>
<td>transformer</td>
<td></td>
</tr>
<tr>
<td>24</td>
<td>Silva_JKUICP_t6_2</td>
<td>False</td>
<td>0.516 (0.505 - 0.527)</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>encoder-decoder</td>
<td>59486498</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing</td>
</tr>
<tr>
<td>25</td>
<td>Li_SCUT_t6_2</td>
<td>False</td>
<td>0.516 (0.505 - 0.527)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>41303080</td>
<td>ConvNeXt</td>
<td>transformer</td>
<td>mixup, SpecAugment</td>
</tr>
<tr>
<td>26</td>
<td>Silva_JKUICP_t6_1</td>
<td>False</td>
<td>0.515 (0.504 - 0.526)</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>encoder-decoder</td>
<td>59486498</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing</td>
</tr>
<tr>
<td>27</td>
<td>Epshtein_ARC_t6_1</td>
<td>False</td>
<td>0.514 (0.503 - 0.525)</td>
<td>epshtein_arc_t6_2024</td>
<td>encoder-decoder</td>
<td>48014000</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing</td>
</tr>
<tr>
<td>28</td>
<td>Hong_CAU_t6_1</td>
<td>False</td>
<td>0.513 (0.502 - 0.524)</td>
<td>hong_cau_t6_2024</td>
<td>encoder-decoder</td>
<td>41303080</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing</td>
</tr>
<tr>
<td>29</td>
<td>Kyogu_SNU_t6_1</td>
<td>False</td>
<td>0.512 (0.501 - 0.524)</td>
<td>kyogu_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>8131137200</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="info">
<td>30</td>
<td>Baseline</td>
<td>False</td>
<td>0.510 (0.499 - 0.521)</td>
<td>labbé_irit_t6_2024</td>
<td>encoder-decoder</td>
<td>41303080</td>
<td></td>
<td>transformer</td>
<td>mixup, label smoothing</td>
</tr>
<tr>
<td>31</td>
<td>Li_SCUT_t6_1</td>
<td>False</td>
<td>0.508 (0.496 - 0.519)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>41303080</td>
<td>ConvNeXt</td>
<td>transformer</td>
<td>mixup, SpecAugment</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="detailed-characteristics">Detailed characteristics</h2>
<table class="datatable table table-hover table-condensed" data-chart-default-mode="scatter" data-chart-modes="scatter" data-chart-tooltip-fields="submission_code" data-filter-control="true" data-filter-show-clear="true" data-id-field="submission_code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="system_complexity_total_parameters" data-scatter-y="results_evaluation_fense_bootstrap" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="entry_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="text-center" data-chartable="true" data-field="entry_rank" data-sortable="true" data-value-type="int">
              Submission<br/>rank
            </th>
<th class="sm-cell" data-field="submission_code" data-sortable="true">
              Submission<br/>code
            </th>
<th data-field="system_bias" data-sortable="true">
              Data<br/>leak
            </th>
<th class="text-center" data-chartable="true" data-field="results_evaluation_fense_bootstrap" data-reversed="false" data-sortable="true" data-value-type="float3-interval-muted">
              FENSE
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
              Technical<br/>Report
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_machine_learning_method" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Method<br/>scheme/architecture
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_learnable_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of<br/>learnable<br/>parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_frozen_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of<br/>frozen<br/>parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_inference_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of<br/>inference<br/>parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total_parameters" data-sortable="true" data-value-type="numeric-unit">
              Amount of<br/> total <br/>parameters
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_inference_macs" data-sortable="true" data-value-type="numeric-unit">
              Amount of<br/>inference<br/>MACs
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_audio_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Audio<br/>modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_acoustic_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Acoustic<br/>features
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_word_modelling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word<br/>modelling
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_word_embeddings" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Word<br/>embeddings
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Data<br/>augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_input_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Sampling<br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_learning_scheme" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning<br/>set-up
            </th>
<th class="sep-left-cell text-center narrow-col" data-chartable="true" data-field="system_description_ensemble_num_systems" data-reversed="false" data-sortable="true" data-value-type="numeric-unit">
              Ensemble<br/>number of<br/>systems
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_loss_function" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Loss function
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_optimizer" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Optimizer
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_learning_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Learning rate
            </th>
<th class="text-center" data-chartable="true" data-field="system_description_weight_decay" data-reversed="false" data-sortable="true" data-value-type="float3">
              Weight decay
            </th>
<th class="text-center" data-chartable="true" data-field="system_description_gradient_clipping" data-reversed="false" data-sortable="true" data-value-type="float1">
              Gradient<br/>clipping
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_gradient_norm" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Gradient norm<br/>for clipping
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_description_metric_monitored" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Metric monitored<br/>for training
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_train_datasets_used" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              Dataset(s) used<br/>for training
            </th>
<th class="sep-left-cell text-center narrow-col" data-chartable="true" data-field="system_complexity_gpu_count" data-reversed="false" data-sortable="true" data-value-type="numeric-unit">
              Number of<br/>GPUs used<br/>for training
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_complexity_gpu_model" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
              GPU model<br/>used<br/>for training
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Jung_CMU_t6_4</td>
<td>False</td>
<td>0.554 (0.543 - 0.563)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>3653368320</td>
<td>4203687530</td>
<td>7857055850</td>
<td>7857055850</td>
<td></td>
<td>Conformer</td>
<td>BEATs, ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART</td>
<td>SpecAugment, mixup</td>
<td>32kHz, 16kHz</td>
<td>supervised</td>
<td>5</td>
<td>cross_entropy, infonce</td>
<td>AdamW</td>
<td>2e-5</td>
<td>0.001</td>
<td>0.0</td>
<td></td>
<td>validation_accuracy</td>
<td>Clotho, AudioCaps</td>
<td>4</td>
<td>NVIDIA A5000</td>
</tr>
<tr>
<td>2</td>
<td>Jung_CMU_t6_2</td>
<td>False</td>
<td>0.549 (0.538 - 0.559)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>730673664</td>
<td>840737506</td>
<td>1571411170</td>
<td>1571411170</td>
<td></td>
<td>Conformer</td>
<td>BEATs, ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART</td>
<td>SpecAugment, mixup</td>
<td>32kHz, 16kHz</td>
<td>supervised</td>
<td>5</td>
<td>cross_entropy, infonce</td>
<td>AdamW</td>
<td>2e-5</td>
<td>0.001</td>
<td>0.0</td>
<td></td>
<td>validation_accuracy</td>
<td>Clotho, AudioCaps</td>
<td>4</td>
<td>NVIDIA A5000</td>
</tr>
<tr>
<td>3</td>
<td>Jung_CMU_t6_3</td>
<td>False</td>
<td>0.547 (0.536 - 0.557)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>2609548800</td>
<td>3002633950</td>
<td>5612182750</td>
<td>5612182750</td>
<td></td>
<td>Conformer</td>
<td>BEATs, ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART</td>
<td>SpecAugment, mixup</td>
<td>32kHz, 16kHz</td>
<td>supervised</td>
<td>5</td>
<td>cross_entropy, infonce</td>
<td>AdamW</td>
<td>2e-5</td>
<td>0.001</td>
<td>0.0</td>
<td></td>
<td>validation_accuracy</td>
<td>Clotho, AudioCaps</td>
<td>4</td>
<td>NVIDIA A5000</td>
</tr>
<tr>
<td>4</td>
<td>Jung_CMU_t6_1</td>
<td>False</td>
<td>0.544 (0.534 - 0.555)</td>
<td>jung_cmu_t6_2024</td>
<td>encoder-decoder</td>
<td>104381952</td>
<td>120105358</td>
<td>224487310</td>
<td>224487310</td>
<td></td>
<td>Conformer</td>
<td>BEATs, ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART</td>
<td>SpecAugment, mixup</td>
<td>32kHz, 16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy, infonce</td>
<td>AdamW</td>
<td>2e-5</td>
<td>0.001</td>
<td>0.0</td>
<td></td>
<td>validation_accuracy</td>
<td>Clotho, AudioCaps</td>
<td>4</td>
<td>NVIDIA A5000</td>
</tr>
<tr>
<td>5</td>
<td>Kim_SNU_t6_2</td>
<td>False</td>
<td>0.544 (0.534 - 0.555)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>477629440</td>
<td>276699541</td>
<td>754328981</td>
<td>754328981</td>
<td></td>
<td>cnn</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART-large</td>
<td>mixup</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-5</td>
<td>0.010</td>
<td>1.0</td>
<td>L2</td>
<td>FENSE</td>
<td>Clotho, Clotho-ChatGPT-mixup, AudioCaps, WavCaps</td>
<td>8</td>
<td>NVIDIA A100 80GB</td>
</tr>
<tr>
<td>6</td>
<td>Kim_SNU_t6_4</td>
<td>False</td>
<td>0.544 (0.534 - 0.555)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>4298664960</td>
<td>276699541</td>
<td>4575364501</td>
<td>4575364501</td>
<td></td>
<td>cnn</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART-large</td>
<td>mixup</td>
<td>32kHz</td>
<td>supervised</td>
<td>9</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-5</td>
<td>0.010</td>
<td>1.0</td>
<td>L2</td>
<td>FENSE</td>
<td>Clotho, Clotho-ChatGPT-mixup, AudioCaps, WavCaps</td>
<td>8</td>
<td>NVIDIA A100 80GB</td>
</tr>
<tr>
<td>7</td>
<td>Kim_SNU_t6_3</td>
<td>False</td>
<td>0.542 (0.532 - 0.552)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>3343406080</td>
<td>276699541</td>
<td>3620105621</td>
<td>3620105621</td>
<td></td>
<td>cnn</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART-large</td>
<td>mixup</td>
<td>32kHz</td>
<td>supervised</td>
<td>7</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-5</td>
<td>0.010</td>
<td>1.0</td>
<td>L2</td>
<td>FENSE</td>
<td>Clotho, Clotho-ChatGPT-mixup, AudioCaps, WavCaps</td>
<td>8</td>
<td>NVIDIA A100 80GB</td>
</tr>
<tr>
<td>8</td>
<td>Chen_SJTU_t6_4</td>
<td>True</td>
<td>0.541 (0.530 - 0.552)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>20453376</td>
<td>6819882255</td>
<td>6840335631</td>
<td>6840335631</td>
<td>6990830300000</td>
<td>transformer</td>
<td>EAT</td>
<td>transformer</td>
<td>vicuna-7b-v1.5</td>
<td>SpecAugment, mixup</td>
<td>16kHz</td>
<td>supervised</td>
<td>10</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>8e-6</td>
<td>0.000</td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, MACS, WavCaps</td>
<td>1</td>
<td>NVIDIA A800-SXM4-80GB</td>
</tr>
<tr>
<td>9</td>
<td>Chen_SJTU_t6_3</td>
<td>True</td>
<td>0.541 (0.530 - 0.552)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>20453376</td>
<td>6819882255</td>
<td>6840335631</td>
<td>6840335631</td>
<td>6990830300000</td>
<td>transformer</td>
<td>EAT</td>
<td>transformer</td>
<td>vicuna-7b-v1.5</td>
<td>SpecAugment, mixup</td>
<td>16kHz</td>
<td>supervised</td>
<td>10</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>8e-6</td>
<td>0.000</td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, MACS, WavCaps</td>
<td>1</td>
<td>NVIDIA A800-SXM4-80GB</td>
</tr>
<tr>
<td>10</td>
<td>Chen_SJTU_t6_1</td>
<td>True</td>
<td>0.540 (0.529 - 0.551)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>20453376</td>
<td>6819882255</td>
<td>6840335631</td>
<td>6840335631</td>
<td>6990830300000</td>
<td>transformer</td>
<td>EAT</td>
<td>transformer</td>
<td>vicuna-7b-v1.5</td>
<td>SpecAugment, mixup</td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>8e-6</td>
<td>0.000</td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, MACS, WavCaps</td>
<td>1</td>
<td>NVIDIA A800-SXM4-80GB</td>
</tr>
<tr>
<td>11</td>
<td>Kim_SNU_t6_1</td>
<td>False</td>
<td>0.540 (0.530 - 0.550)</td>
<td>kim_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>477629440</td>
<td>276699541</td>
<td>754328981</td>
<td>754328981</td>
<td></td>
<td>cnn</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>BART-large</td>
<td>mixup</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-5</td>
<td>0.010</td>
<td>1.0</td>
<td>L2</td>
<td>FENSE</td>
<td>Clotho, Clotho-ChatGPT-mixup, AudioCaps, WavCaps</td>
<td>8</td>
<td>NVIDIA A100 80GB</td>
</tr>
<tr>
<td>12</td>
<td>Chen_SJTU_t6_2</td>
<td>True</td>
<td>0.538 (0.527 - 0.550)</td>
<td>chen_sjtu_t6_2024</td>
<td>encoder-decoder</td>
<td>20453376</td>
<td>6819882255</td>
<td>6840335631</td>
<td>6840335631</td>
<td>6990830300000</td>
<td>transformer</td>
<td>EAT</td>
<td>transformer</td>
<td>vicuna-7b-v1.5</td>
<td>SpecAugment, mixup</td>
<td>16kHz</td>
<td>supervised</td>
<td>5</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>8e-6</td>
<td>0.000</td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho, AudioCaps, MACS, WavCaps</td>
<td>1</td>
<td>NVIDIA A800-SXM4-80GB</td>
</tr>
<tr>
<td>13</td>
<td>Li_ALXC_t6_4</td>
<td>False</td>
<td>0.533 (0.522 - 0.543)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>26544896</td>
<td>6824127375</td>
<td>6850672271</td>
<td>6850672271</td>
<td>none</td>
<td>ced</td>
<td>CED</td>
<td>transformer</td>
<td>llama2_7b</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-5</td>
<td></td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>2</td>
<td>NVIDIA A100</td>
</tr>
<tr>
<td>14</td>
<td>Li_ALXC_t6_3</td>
<td>False</td>
<td>0.528 (0.517 - 0.538)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>20233728</td>
<td>225132175</td>
<td>245365903</td>
<td>245365903</td>
<td>none</td>
<td>ced</td>
<td>CED</td>
<td>transformer</td>
<td>bart</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-5</td>
<td></td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>2</td>
<td>NVIDIA A100</td>
</tr>
<tr>
<td>15</td>
<td>Kyogu_SNU_t6_2</td>
<td>False</td>
<td>0.526 (0.515 - 0.537)</td>
<td>kyogu_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>9965568</td>
<td>8121171632</td>
<td>8124321456</td>
<td>8131137200</td>
<td></td>
<td></td>
<td>BEATs</td>
<td></td>
<td>LLaMa</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-4</td>
<td></td>
<td>5.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho, AudioCaps</td>
<td>1</td>
<td>NVIDIA GeForce RTX 3090</td>
</tr>
<tr>
<td>16</td>
<td>Kong_CUHK_t6_1</td>
<td>True</td>
<td>0.525 (0.514 - 0.536)</td>
<td>kong_cuhk_t6_2024</td>
<td>encoder-decoder</td>
<td>117015552</td>
<td>29388303</td>
<td>146403855</td>
<td>146403855</td>
<td>60483202884</td>
<td>cnn</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>spec-based mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-5</td>
<td></td>
<td>0.0</td>
<td></td>
<td>the SPIDEr metric</td>
<td>Clotho, AudioCaps, WavCaps</td>
<td>5</td>
<td>NVIDIA GeForce RTX 4090</td>
</tr>
<tr>
<td>17</td>
<td>Kong_CUHK_t6_2</td>
<td>False</td>
<td>0.525 (0.514 - 0.536)</td>
<td>kong_cuhk_t6_2024</td>
<td>encoder-decoder</td>
<td>96966912</td>
<td>29388303</td>
<td>126355215</td>
<td>126355215</td>
<td>53459049669</td>
<td>cnn</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>spec-based mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-5</td>
<td></td>
<td>0.0</td>
<td></td>
<td>the SPIDEr metric</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce RTX 4090</td>
</tr>
<tr>
<td>18</td>
<td>Choi_KAIST_t6_1</td>
<td>False</td>
<td>0.520 (0.509 - 0.531)</td>
<td>choi_kaist_t6_2024</td>
<td>encoder-decoder</td>
<td>12649906</td>
<td>29388303</td>
<td>42038209</td>
<td>42038209</td>
<td>49888899616</td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing, ChatGPT paraphrasing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>train_loss</td>
<td>Clotho</td>
<td>3</td>
<td>NVIDIA GeForce RTX 2080 Ti</td>
</tr>
<tr>
<td>19</td>
<td>Li_ALXC_t6_1</td>
<td>False</td>
<td>0.520 (0.509 - 0.532)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>26544896</td>
<td>6823863424</td>
<td>6850408320</td>
<td>6850408320</td>
<td>none</td>
<td>Dasheng</td>
<td>Dasheng</td>
<td>transformer</td>
<td>llama2_7b</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-5</td>
<td></td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>2</td>
<td>NVIDIA A100</td>
</tr>
<tr>
<td>20</td>
<td>Li_SCUT_t6_4</td>
<td>False</td>
<td>0.520 (0.508 - 0.531)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>11914777</td>
<td>29388303</td>
<td>41303080</td>
<td>41303080</td>
<td></td>
<td>ConvNeXt</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td></td>
<td>mixup, SpecAugment</td>
<td>32kHz</td>
<td>supervised</td>
<td>4</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce RTX 4090 Ti</td>
</tr>
<tr>
<td>21</td>
<td>Li_SCUT_t6_3</td>
<td>False</td>
<td>0.519 (0.508 - 0.530)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>11914777</td>
<td>29388303</td>
<td>41303080</td>
<td>41303080</td>
<td></td>
<td>ConvNeXt</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td></td>
<td>mixup, SpecAugment</td>
<td>32kHz</td>
<td>supervised</td>
<td>4</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce RTX 4090 Ti</td>
</tr>
<tr>
<td>22</td>
<td>Choi_KAIST_t6_2</td>
<td>False</td>
<td>0.518 (0.507 - 0.529)</td>
<td>choi_kaist_t6_2024</td>
<td>encoder-decoder</td>
<td>12649906</td>
<td>29388303</td>
<td>42038209</td>
<td>42038209</td>
<td>50768107552</td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing, ChatGPT paraphrasing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>train_loss</td>
<td>Clotho</td>
<td>3</td>
<td>NVIDIA GeForce RTX 2080 Ti</td>
</tr>
<tr>
<td>23</td>
<td>Li_ALXC_t6_2</td>
<td>False</td>
<td>0.518 (0.506 - 0.529)</td>
<td>li_alxc_t6_2024</td>
<td>encoder-decoder</td>
<td>29133568</td>
<td>7368749184</td>
<td>7397882752</td>
<td>7397882752</td>
<td>none</td>
<td>Dasheng</td>
<td>Dasheng</td>
<td>transformer</td>
<td>llama2_7b</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-5</td>
<td></td>
<td>0.0</td>
<td></td>
<td>validation_loss</td>
<td>Clotho</td>
<td>2</td>
<td>NVIDIA A100</td>
</tr>
<tr>
<td>24</td>
<td>Silva_JKUICP_t6_2</td>
<td>False</td>
<td>0.516 (0.505 - 0.527)</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>encoder-decoder</td>
<td>30098195</td>
<td>29388303</td>
<td>59486498</td>
<td>59486498</td>
<td>14715294720</td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>4e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho, Clotho</td>
<td>1</td>
<td>NVIDIA GeForce GTX 1060 6GB</td>
</tr>
<tr>
<td>25</td>
<td>Li_SCUT_t6_2</td>
<td>False</td>
<td>0.516 (0.505 - 0.527)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>11914777</td>
<td>29388303</td>
<td>41303080</td>
<td>41303080</td>
<td></td>
<td>ConvNeXt</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td></td>
<td>mixup, SpecAugment</td>
<td>32kHz</td>
<td>supervised</td>
<td>4</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce RTX 4090 Ti</td>
</tr>
<tr>
<td>26</td>
<td>Silva_JKUICP_t6_1</td>
<td>False</td>
<td>0.515 (0.504 - 0.526)</td>
<td>de_jesus_silva_jkuicp_t6_2024</td>
<td>encoder-decoder</td>
<td>30098195</td>
<td>29388303</td>
<td>59486498</td>
<td>59486498</td>
<td>15301713408</td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>4e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce GTX 1060 6GB</td>
</tr>
<tr>
<td>27</td>
<td>Epshtein_ARC_t6_1</td>
<td>False</td>
<td>0.514 (0.503 - 0.525)</td>
<td>epshtein_arc_t6_2024</td>
<td>encoder-decoder</td>
<td>12003511</td>
<td>36010489</td>
<td>48014000</td>
<td>48014000</td>
<td>4821624576</td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy, NTXent</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA T1200 Laptop GPU</td>
</tr>
<tr>
<td>28</td>
<td>Hong_CAU_t6_1</td>
<td>False</td>
<td>0.513 (0.502 - 0.524)</td>
<td>hong_cau_t6_2024</td>
<td>encoder-decoder</td>
<td>11914777</td>
<td>29388303</td>
<td>41303080</td>
<td>41303080</td>
<td></td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA 20TF-V100</td>
</tr>
<tr>
<td>29</td>
<td>Kyogu_SNU_t6_1</td>
<td>False</td>
<td>0.512 (0.501 - 0.524)</td>
<td>kyogu_snu_t6_2024</td>
<td>encoder-decoder</td>
<td>9965568</td>
<td>8121171632</td>
<td>8124321456</td>
<td>8131137200</td>
<td></td>
<td></td>
<td>BEATs</td>
<td></td>
<td>LLaMa</td>
<td></td>
<td>16kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>3e-4</td>
<td></td>
<td>5.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho, AudioCaps</td>
<td>1</td>
<td>NVIDIA GeForce RTX 3090</td>
</tr>
<tr class="info">
<td>30</td>
<td>Baseline</td>
<td>False</td>
<td>0.510 (0.499 - 0.521)</td>
<td>labbé_irit_t6_2024</td>
<td>encoder-decoder</td>
<td>11914777</td>
<td>29388303</td>
<td>41303080</td>
<td>41303080</td>
<td>48762319200</td>
<td></td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td>learned</td>
<td>mixup, label smoothing</td>
<td>32kHz</td>
<td>supervised</td>
<td>1</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce RTX 2080 Ti</td>
</tr>
<tr>
<td>31</td>
<td>Li_SCUT_t6_1</td>
<td>False</td>
<td>0.508 (0.496 - 0.519)</td>
<td>li_scut_t6_2024</td>
<td>ConvNeXt-Trans</td>
<td>11914777</td>
<td>29388303</td>
<td>41303080</td>
<td>41303080</td>
<td></td>
<td>ConvNeXt</td>
<td>ConvNeXt-Tiny</td>
<td>transformer</td>
<td></td>
<td>mixup, SpecAugment</td>
<td>32kHz</td>
<td>supervised</td>
<td>4</td>
<td>cross_entropy</td>
<td>AdamW</td>
<td>5e-4</td>
<td>2.000</td>
<td>1.0</td>
<td>L2</td>
<td>validation_loss</td>
<td>Clotho</td>
<td>1</td>
<td>NVIDIA GeForce RTX 4090 Ti</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2024/technical_reports_task6.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="jung_cmu_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-jung_cmu_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUTOMATIC AUDIO CAPTIONING WITH ENCODER FUSION, MULTI-LAYER AGGREGATION, AND LARGE LANGUAGE MODEL ENRICHED SUMMARIZATION
       </h4>
<p style="text-align:left">
        Jee-weon Jung<sup>1</sup>, Dong Zhang<sup>2</sup>, Huck C.-H. Yang<sup>3</sup>, Shih-Lun Wu<sup>1</sup>, David M. Chan<sup>4</sup>, Zhifeng Kong<sup>5</sup>, Deng Ruifan<sup>2</sup>, Zhou Yaqian<sup>2</sup>, Valle Rafael<sup>5</sup>, Shinji Watanabe<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Carnegie Mellon University, USA, <sup>2</sup>Fudan University, China, <sup>3</sup>NVIDIA Research, USA, <sup>4</sup>University of California, Berkeley, USA, <sup>5</sup>NVIDIA Applied Deep Learning Research, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jung_CMU_t6_1</span> <span class="label label-primary">Jung_CMU_t6_2</span> <span class="label label-primary">Jung_CMU_t6_4</span> <span class="label label-primary">Jung_CMU_t6_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-jung_cmu_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-jung_cmu_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-jung_cmu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Jung_74_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-jung_cmu_t6_2024" class="panel-collapse collapse" id="collapse-jung_cmu_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUTOMATIC AUDIO CAPTIONING WITH ENCODER FUSION, MULTI-LAYER AGGREGATION, AND LARGE LANGUAGE MODEL ENRICHED SUMMARIZATION
      </h4>
<p style="text-align:left">
<small>
        Jee-weon Jung<sup>1</sup>, Dong Zhang<sup>2</sup>, Huck C.-H. Yang<sup>3</sup>, Shih-Lun Wu<sup>1</sup>, David M. Chan<sup>4</sup>, Zhifeng Kong<sup>5</sup>, Deng Ruifan<sup>2</sup>, Zhou Yaqian<sup>2</sup>, Valle Rafael<sup>5</sup>, Shinji Watanabe<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Carnegie Mellon University, USA, <sup>2</sup>Fudan University, China, <sup>3</sup>NVIDIA Research, USA, <sup>4</sup>University of California, Berkeley, USA, <sup>5</sup>NVIDIA Applied Deep Learning Research, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we describe our submission to Track 6 of the DCASE 2024 challenge for the task of Automated Audio Captioning (AAC). The submitted models utilize an encoder-decoder architecture using pre-trained and frozen audio encoders, a Conformer post-encoder, and a BART decoder. We introduce five different architectures, employing diverse fusion strategies to leverage multiple audio encoders and a multi-layer aggregation technique, thus exploiting the complementary information from various representations. For inference, we propose a novel scheme incorporating nucleus sampling, CLAP-based filtering, hybrid re-ranking, and large language model summarization. Combining these approaches, our top-performing single and ensemble systems achieve Fluency Enhanced Sentence-BERT Evaluation (FENSE) scores of 0.5410 and 0.5442, respectively, on the Clotho (V2) evaluation partition.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Jung_CMU_t6_4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         Conformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         5
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho, AudioCaps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         7857055850
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5536877719555068
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-jung_cmu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Jung_74_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-jung_cmu_t6_2024label" class="modal fade" id="bibtex-jung_cmu_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexjung_cmu_t6_2024label">
        AUTOMATIC AUDIO CAPTIONING WITH ENCODER FUSION, MULTI-LAYER AGGREGATION, AND LARGE LANGUAGE MODEL ENRICHED SUMMARIZATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{jung_cmu_t6_2024,
    author = "Jung, Jee-weon and Zhang, Dong and Yang, Huck C.-H. and Wu, Shih-Lun and Chan, David M. and Kong, Zhifeng and Ruifan, Deng and Yaqian, Zhou and Rafael, Valle and Watanabe, Shinji",
    title = "AUTOMATIC AUDIO CAPTIONING WITH ENCODER FUSION, MULTI-LAYER AGGREGATION, AND LARGE LANGUAGE MODEL ENRICHED SUMMARIZATION",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "In this report, we describe our submission to Track 6 of the DCASE 2024 challenge for the task of Automated Audio Captioning (AAC). The submitted models utilize an encoder-decoder architecture using pre-trained and frozen audio encoders, a Conformer post-encoder, and a BART decoder. We introduce five different architectures, employing diverse fusion strategies to leverage multiple audio encoders and a multi-layer aggregation technique, thus exploiting the complementary information from various representations. For inference, we propose a novel scheme incorporating nucleus sampling, CLAP-based filtering, hybrid re-ranking, and large language model summarization. Combining these approaches, our top-performing single and ensemble systems achieve Fluency Enhanced Sentence-BERT Evaluation (FENSE) scores of 0.5410 and 0.5442, respectively, on the Clotho (V2) evaluation partition.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Jung\_74\_t6.pdf",
    number = "74",
    note = "Ranked 1/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.554."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="kim_snu_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-kim_snu_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning
       </h4>
<p style="text-align:left">
        Jaeyeon Kim<sup>1</sup>, Jaeyoon Jung<sup>2</sup>, Minjeong Jeon<sup>3</sup>, Sang Hoon Woo<sup>4</sup>, Jinjoo Lee<sup>5</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Seoul National Unversity, Seoul, Republic of Korea, <sup>2</sup>Soongsil University, Seoul, Republic of Korea, <sup>3</sup>MAUM AI Inc., Seongnam, Republic of Korea, <sup>4</sup>Independent Researcher, Everywhere, <sup>5</sup>MAUM AI Inc.,, Seongnam, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_SNU_t6_3</span> <span class="label label-primary">Kim_SNU_t6_4</span> <span class="label label-primary">Kim_SNU_t6_1</span> <span class="label label-primary">Kim_SNU_t6_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-kim_snu_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-kim_snu_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-kim_snu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kim_108_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-kim_snu_t6_2024" class="panel-collapse collapse" id="collapse-kim_snu_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning
      </h4>
<p style="text-align:left">
<small>
        Jaeyeon Kim<sup>1</sup>, Jaeyoon Jung<sup>2</sup>, Minjeong Jeon<sup>3</sup>, Sang Hoon Woo<sup>4</sup>, Jinjoo Lee<sup>5</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Seoul National Unversity, Seoul, Republic of Korea, <sup>2</sup>Soongsil University, Seoul, Republic of Korea, <sup>3</sup>MAUM AI Inc., Seongnam, Republic of Korea, <sup>4</sup>Independent Researcher, Everywhere, <sup>5</sup>MAUM AI Inc.,, Seongnam, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our submission to DCASE2024 Challenge Task6 (Automated Audio Captioning) and Task8 (Language-based Audio Retrieval). We develop our approach building upon the EnCLAP audio captioning framework and optimizing it for Task 6 of the challenge. Notably, we outline the changes in the underlying components and the incorporation of the reranking process. Additionally, we submit a supplementary retriever model, a byproduct of our modified framework, to Task8. Our proposed systems achieve FENSE score of 0.542 on Task6 and mAP@10 score of 0.386 on Task8, significantly outperforming the baseline models.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Kim_SNU_t6_2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         cnn
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho, Clotho-ChatGPT-mixup, AudioCaps, WavCaps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         754328981
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5441769132406691
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-kim_snu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kim_108_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-kim_snu_t6_2024label" class="modal fade" id="bibtex-kim_snu_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexkim_snu_t6_2024label">
        Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{kim_snu_t6_2024,
    author = "Kim, Jaeyeon and Jung, Jaeyoon and Jeon, Minjeong and Woo, Sang Hoon and Lee, Jinjoo",
    title = "Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "In this technical report, we describe our submission to DCASE2024 Challenge Task6 (Automated Audio Captioning) and Task8 (Language-based Audio Retrieval). We develop our approach building upon the EnCLAP audio captioning framework and optimizing it for Task 6 of the challenge. Notably, we outline the changes in the underlying components and the incorporation of the reranking process. Additionally, we submit a supplementary retriever model, a byproduct of our modified framework, to Task8. Our proposed systems achieve FENSE score of 0.542 on Task6 and mAP@10 score of 0.386 on Task8, significantly outperforming the baseline models.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Kim\_108\_t6.pdf",
    number = "108",
    note = "Ranked 2/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.544."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="chen_sjtu_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-chen_sjtu_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SJTU-THU Automated Audio Captioning System for DCASE 2024
       </h4>
<p style="text-align:left">
        Wenxi Chen<sup>1</sup>, Xiquan Li<sup>1</sup>, Ziyang Ma<sup>1</sup>, Yuzhe Liang<sup>1</sup>, Anbai Jiang<sup>2</sup>, Zhisheng Zheng<sup>1</sup>, Yanmin Qian<sup>1</sup>, Pingyi Fan<sup>2</sup>, Wei-Qiang Zhang<sup>2</sup>, Cheng Lu<sup>3</sup>, Jia Liu<sup>2</sup>, Xie Chen<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Shanghai Jiao Tong University, Shanghai, China, <sup>2</sup>Tsinghua University, Beijing, China, <sup>3</sup>North China Electric Power University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chen_SJTU_t6_1</span> <span class="label label-primary">Chen_SJTU_t6_4</span> <span class="label label-primary">Chen_SJTU_t6_2</span> <span class="label label-primary">Chen_SJTU_t6_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-chen_sjtu_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-chen_sjtu_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-chen_sjtu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Chen_2_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-chen_sjtu_t6_2024').collapse('show');window.location.hash='#chen_sjtu_t6_2024';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-chen_sjtu_t6_2024" class="panel-collapse collapse" id="collapse-chen_sjtu_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SJTU-THU Automated Audio Captioning System for DCASE 2024
      </h4>
<p style="text-align:left">
<small>
        Wenxi Chen<sup>1</sup>, Xiquan Li<sup>1</sup>, Ziyang Ma<sup>1</sup>, Yuzhe Liang<sup>1</sup>, Anbai Jiang<sup>2</sup>, Zhisheng Zheng<sup>1</sup>, Yanmin Qian<sup>1</sup>, Pingyi Fan<sup>2</sup>, Wei-Qiang Zhang<sup>2</sup>, Cheng Lu<sup>3</sup>, Jia Liu<sup>2</sup>, Xie Chen<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Shanghai Jiao Tong University, Shanghai, China, <sup>2</sup>Tsinghua University, Beijing, China, <sup>3</sup>North China Electric Power University, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Task 6 (Automated Audio Captioning) of the DCASE 2024 Challenge requires the automatic creation of textual descriptions for general audio signals. This technical report presents a novel model that integrates a self-supervised model with a large language model (LLM) for audio captioning. For audio feature extraction, we utilize the efficient self-supervised pre-trained model, EAT, to achieve more effective audio representation extraction. The language model component is based on Vicuna, a large language model, which we fine-tune using LoRA to fully harness its robust reasoning capabilities. During training, linear layers function as projectors to align audio and textual representations. Our model is pre-trained using the Clotho, WavCaps, AudioCaps, and MACS datasets, and fine-tuned on Clotho. For decoding, we employ a filtering strategy based on the CLAP model. By leveraging the text-audio alignment capabilities of the CLAP model, we filter out the beam search decoding results to retain only the textual description that best matches the input audio. Evaluation on the testing subset of Clotho demonstrates that our model achieves a FENSE score of 0.5431 in the single-system setting and 0.5429 in the multi-system setting, while the multi-systems outperform the single-system in other metrics. Our project code is based on the SLAM-LLM toolkit.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Chen_SJTU_t6_4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         3
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         10
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho, AudioCaps, MACS, WavCaps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         6840335631
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5412474964331918
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-chen_sjtu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Chen_2_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/X-LANCE/SLAM-LLM" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-chen_sjtu_t6_2024label" class="modal fade" id="bibtex-chen_sjtu_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexchen_sjtu_t6_2024label">
        SJTU-THU Automated Audio Captioning System for DCASE 2024
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{chen_sjtu_t6_2024,
    author = "Chen, Wenxi and Li, Xiquan and Ma, Ziyang and Liang, Yuzhe and Jiang, Anbai and Zheng, Zhisheng and Qian, Yanmin and Fan, Pingyi and Zhang, Wei-Qiang and Lu, Cheng and Liu, Jia and Chen, Xie",
    title = "SJTU-THU Automated Audio Captioning System for DCASE 2024",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "Task 6 (Automated Audio Captioning) of the DCASE 2024 Challenge requires the automatic creation of textual descriptions for general audio signals. This technical report presents a novel model that integrates a self-supervised model with a large language model (LLM) for audio captioning. For audio feature extraction, we utilize the efficient self-supervised pre-trained model, EAT, to achieve more effective audio representation extraction. The language model component is based on Vicuna, a large language model, which we fine-tune using LoRA to fully harness its robust reasoning capabilities. During training, linear layers function as projectors to align audio and textual representations. Our model is pre-trained using the Clotho, WavCaps, AudioCaps, and MACS datasets, and fine-tuned on Clotho. For decoding, we employ a filtering strategy based on the CLAP model. By leveraging the text-audio alignment capabilities of the CLAP model, we filter out the beam search decoding results to retain only the textual description that best matches the input audio. Evaluation on the testing subset of Clotho demonstrates that our model achieves a FENSE score of 0.5431 in the single-system setting and 0.5429 in the multi-system setting, while the multi-systems outperform the single-system in other metrics. Our project code is based on the SLAM-LLM toolkit.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Chen\_2\_t6.pdf",
    number = "2",
    note = "Ranked 3/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.541."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="li_alxc_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-li_alxc_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Leveraging CED Encoder and Large Language Models for Automated Audio Captioning
       </h4>
<p style="text-align:left">
        Jizhong Liu<sup>1</sup>, Gang Li<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>AI Lab, Xiaomi Corporation, Wuhan, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_ALXC_t6_2</span> <span class="label label-primary">Li_ALXC_t6_4</span> <span class="label label-primary">Li_ALXC_t6_1</span> <span class="label label-primary">Li_ALXC_t6_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-li_alxc_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-li_alxc_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-li_alxc_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Li_42_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-li_alxc_t6_2024" class="panel-collapse collapse" id="collapse-li_alxc_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Leveraging CED Encoder and Large Language Models for Automated Audio Captioning
      </h4>
<p style="text-align:left">
<small>
        Jizhong Liu<sup>1</sup>, Gang Li<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>AI Lab, Xiaomi Corporation, Wuhan, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents an automated audio captioning (AAC) method participating in the DCASE 2024 Challenge Task 6. The method builds upon our previous work.Recent advancements in large language models (LLMs), coupled with improved training approaches for audio encoders, have opened up possibilities for enhancing AAC. Thus, we optimize AAC from three points: 1) a pre-trained audio encoder named consistent ensemble distillation (CED) improves the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we introduce a Llama 2 with 7B parameters as the decoder; 3) a frozen Llama 3 Instruct with 8B parameters corrects text errors caused by insufficient training data and annotation ambiguities. Both the encoder and text decoder are optimized by low-rank adaptation (LoRA). Our method obtains a 53.2 FENSE score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Li_ALXC_t6_4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         ced
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         6850672271
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5327607233845204
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-li_alxc_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Li_42_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-li_alxc_t6_2024label" class="modal fade" id="bibtex-li_alxc_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexli_alxc_t6_2024label">
        Leveraging CED Encoder and Large Language Models for Automated Audio Captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{li_alxc_t6_2024,
    author = "Liu, Jizhong and Li, Gang",
    title = "Leveraging CED Encoder and Large Language Models for Automated Audio Captioning",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "This technical report presents an automated audio captioning (AAC) method participating in the DCASE 2024 Challenge Task 6. The method builds upon our previous work.Recent advancements in large language models (LLMs), coupled with improved training approaches for audio encoders, have opened up possibilities for enhancing AAC. Thus, we optimize AAC from three points: 1) a pre-trained audio encoder named consistent ensemble distillation (CED) improves the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we introduce a Llama 2 with 7B parameters as the decoder; 3) a frozen Llama 3 Instruct with 8B parameters corrects text errors caused by insufficient training data and annotation ambiguities. Both the encoder and text decoder are optimized by low-rank adaptation (LoRA). Our method obtains a 53.2 FENSE score.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Li\_42\_t6.pdf",
    number = "42",
    note = "Ranked 4/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.533."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="kyogu_snu_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-kyogu_snu_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUTOMATED AUDIO CAPTIONING USING PARAMETER EFFICIENT FINE-TUNING AND MERGING OF LLMS
       </h4>
<p style="text-align:left">
        Kim Eungbeom<sup>1</sup>, Sim Jaeheon<sup>1</sup>, Lee Jin Woo<sup>1</sup>, Lee Kyogu<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Seoul National University, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kyogu_SNU_t6_2</span> <span class="label label-primary">Kyogu_SNU_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-kyogu_snu_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-kyogu_snu_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-kyogu_snu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kyogu_111_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-kyogu_snu_t6_2024" class="panel-collapse collapse" id="collapse-kyogu_snu_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUTOMATED AUDIO CAPTIONING USING PARAMETER EFFICIENT FINE-TUNING AND MERGING OF LLMS
      </h4>
<p style="text-align:left">
<small>
        Kim Eungbeom<sup>1</sup>, Sim Jaeheon<sup>1</sup>, Lee Jin Woo<sup>1</sup>, Lee Kyogu<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Seoul National University, Seoul, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report introduces an audio captioning system, which is designed to tackle the task of Automated Audio Captioning (AAC) in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 challenge. Our approach employs BEATs for robust audio representation learning and Llama 3 for high-quality text generation. To address the limitations of small datasets like Clotho, we fix the pre-trained weights of the BEATs and train a small linear model to map audio encoder dimensions to the LLM input. We further fine-tune the LLM using parameter-efficient finetuning method, LoRA, to train the model. We also explore the concatenation based LoRA merging method, achieving notable results on standard benchmarks. Experimental results show that our proposed system achieves a FENSE [1] score of 0.5180 on the evaluation dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Kyogu_SNU_t6_2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         5
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         None
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         None
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho, AudioCaps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         8131137200
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5262071474093661
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-kyogu_snu_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kyogu_111_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-kyogu_snu_t6_2024label" class="modal fade" id="bibtex-kyogu_snu_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexkyogu_snu_t6_2024label">
        AUTOMATED AUDIO CAPTIONING USING PARAMETER EFFICIENT FINE-TUNING AND MERGING OF LLMS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{kyogu_snu_t6_2024,
    author = "Eungbeom, Kim and Jaeheon, Sim and Woo, Lee Jin and Kyogu, Lee",
    title = "AUTOMATED AUDIO CAPTIONING USING PARAMETER EFFICIENT FINE-TUNING AND MERGING OF LLMS",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "This technical report introduces an audio captioning system, which is designed to tackle the task of Automated Audio Captioning (AAC) in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 challenge. Our approach employs BEATs for robust audio representation learning and Llama 3 for high-quality text generation. To address the limitations of small datasets like Clotho, we fix the pre-trained weights of the BEATs and train a small linear model to map audio encoder dimensions to the LLM input. We further fine-tune the LLM using parameter-efficient finetuning method, LoRA, to train the model. We also explore the concatenation based LoRA merging method, achieving notable results on standard benchmarks. Experimental results show that our proposed system achieves a FENSE [1] score of 0.5180 on the evaluation dataset.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Kyogu\_111\_t6.pdf",
    number = "111",
    note = "Ranked 5/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.526."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="kong_cuhk_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-kong_cuhk_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Semantic Enhancement Encoder for Audio Captioning and Spectrogram-based data augmentation
       </h4>
<p style="text-align:left">
        Qianhang Feng<sup>1</sup>, Qiuqiang Kong<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>The Chinese University of Hong Kong, New Territories, HOng Kong
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kong_CUHK_t6_2</span> <span class="label label-primary">Kong_CUHK_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-kong_cuhk_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-kong_cuhk_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-kong_cuhk_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kong_10_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-kong_cuhk_t6_2024').collapse('show');window.location.hash='#kong_cuhk_t6_2024';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-kong_cuhk_t6_2024" class="panel-collapse collapse" id="collapse-kong_cuhk_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Semantic Enhancement Encoder for Audio Captioning and Spectrogram-based data augmentation
      </h4>
<p style="text-align:left">
<small>
        Qianhang Feng<sup>1</sup>, Qiuqiang Kong<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>The Chinese University of Hong Kong, New Territories, HOng Kong
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Automatic Audio Captioning (AAC) is a process that transforms audio signals into descriptive narratives. This paper introduces an innovative automated audio captioning model developed for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 6A. The model architecture presented here is meticulously designed to adeptly manage the intricacies of AAC tasks. Additionally, this project introduces a novel data enhancement technique, which, with minimal model adjustments, significantly boosts performance. Exclusively trained and fine-tuned on the Clotho dataset, this project achieved a final SPIDEr-FL score of 0.3318, demonstrating its effectiveness.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Kong_CUHK_t6_1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         6
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         cnn
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         spec-based mixup, label smoothing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho, AudioCaps, WavCaps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         146403855
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5254088402978455
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-kong_cuhk_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Kong_10_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/QianhangFeng/ASE-SDA" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-kong_cuhk_t6_2024label" class="modal fade" id="bibtex-kong_cuhk_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexkong_cuhk_t6_2024label">
        Semantic Enhancement Encoder for Audio Captioning and Spectrogram-based data augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{kong_cuhk_t6_2024,
    author = "Feng, Qianhang and Kong, Qiuqiang",
    title = "Semantic Enhancement Encoder for Audio Captioning and Spectrogram-based data augmentation",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "Automatic Audio Captioning (AAC) is a process that transforms audio signals into descriptive narratives. This paper introduces an innovative automated audio captioning model developed for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 6A. The model architecture presented here is meticulously designed to adeptly manage the intricacies of AAC tasks. Additionally, this project introduces a novel data enhancement technique, which, with minimal model adjustments, significantly boosts performance. Exclusively trained and fine-tuned on the Clotho dataset, this project achieved a final SPIDEr-FL score of 0.3318, demonstrating its effectiveness.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Kong\_10\_t6.pdf",
    number = "10",
    note = "Ranked 6/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.525."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="choi_kaist_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-choi_kaist_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CHATGPT CAPTION PARAPHRASING AND FENSE-BASED CAPTION FILTERING FOR AUTOMATED AUDIO CAPTIONING
       </h4>
<p style="text-align:left">
        Inhan Choi<sup>1</sup>, Hyeonuk Nam<sup>1</sup>, Deokki Min<sup>1</sup>, Seung-Deok Choi<sup>1</sup>, Yong-Hwa Park<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Korea Advanced Institute of Science and Technology, 291, Daehak-ro, Yuseong-gu, Daejeon 34141, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Choi_KAIST_t6_1</span> <span class="label label-primary">Choi_KAIST_t6_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-choi_kaist_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-choi_kaist_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-choi_kaist_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Choi_38_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-choi_kaist_t6_2024" class="panel-collapse collapse" id="collapse-choi_kaist_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CHATGPT CAPTION PARAPHRASING AND FENSE-BASED CAPTION FILTERING FOR AUTOMATED AUDIO CAPTIONING
      </h4>
<p style="text-align:left">
<small>
        Inhan Choi<sup>1</sup>, Hyeonuk Nam<sup>1</sup>, Deokki Min<sup>1</sup>, Seung-Deok Choi<sup>1</sup>, Yong-Hwa Park<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Korea Advanced Institute of Science and Technology, 291, Daehak-ro, Yuseong-gu, Daejeon 34141, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper presents an Automated Audio Captioning (AAC) model developed for the DCASE2024 Task 6. To address the scarcity of audio captioning datasets, we generate paraphrases of captions from the Clotho dataset as a data augmentation strategy. We utilize the ChatGPT-API to produce captions. To ensure the selection of paraphrases with high semantic relevance, we employed FENSE, the metric adopted for this AAC task. By integrating ChatGPT paraphrasing into the AAC baseline model, our submitted model achieves 0.521 FENSE score.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Choi_KAIST_t6_1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         7
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         None
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, label smoothing, ChatGPT paraphrasing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         42038209
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5203327059152886
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-choi_kaist_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Choi_38_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-choi_kaist_t6_2024label" class="modal fade" id="bibtex-choi_kaist_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexchoi_kaist_t6_2024label">
        CHATGPT CAPTION PARAPHRASING AND FENSE-BASED CAPTION FILTERING FOR AUTOMATED AUDIO CAPTIONING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{choi_kaist_t6_2024,
    author = "Choi, Inhan and Nam, Hyeonuk and Min, Deokki and Choi, Seung-Deok and Park, Yong-Hwa",
    title = "CHATGPT CAPTION PARAPHRASING AND FENSE-BASED CAPTION FILTERING FOR AUTOMATED AUDIO CAPTIONING",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "This paper presents an Automated Audio Captioning (AAC) model developed for the DCASE2024 Task 6. To address the scarcity of audio captioning datasets, we generate paraphrases of captions from the Clotho dataset as a data augmentation strategy. We utilize the ChatGPT-API to produce captions. To ensure the selection of paraphrases with high semantic relevance, we employed FENSE, the metric adopted for this AAC task. By integrating ChatGPT paraphrasing into the AAC baseline model, our submitted model achieves 0.521 FENSE score.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Choi\_38\_t6.pdf",
    number = "38",
    note = "Ranked 7/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.520."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="li_scut_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-li_scut_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SCUT SUBMISSION FOR AUTOMATED AUDIO CAPTIONING USING GRAPH ATTENTION AND CROSS-ATTENTION MECHANISMS
       </h4>
<p style="text-align:left">
        Qianqian Li<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>South China University of Technology, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_SCUT_t6_2</span> <span class="label label-primary">Li_SCUT_t6_4</span> <span class="label label-primary">Li_SCUT_t6_1</span> <span class="label label-primary">Li_SCUT_t6_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-li_scut_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-li_scut_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-li_scut_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Li_54_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-li_scut_t6_2024" class="panel-collapse collapse" id="collapse-li_scut_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SCUT SUBMISSION FOR AUTOMATED AUDIO CAPTIONING USING GRAPH ATTENTION AND CROSS-ATTENTION MECHANISMS
      </h4>
<p style="text-align:left">
<small>
        Qianqian Li<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>South China University of Technology, Guangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report presents our work for automated audio caption-ing which is the Task 6A of DCASE 2024. Our system is an encoder-decoder framework. The encoder uses a pre-trained ConvNeXt network and the decoder employs a standard Transformer structure. Among the encoders, we include a graph attention module to enhance the module's ability to extract audio features. In the decoder, in addition to the Transformer's multi-head self-attention mechanism, a cross-attention mechanism is added to improve the association between output subtitles and audio features. Finally, our system achieves FENSE score of 0.5131 which is higher than the baseline system's FENSE score of 0.5040.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Li_SCUT_t6_4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         8
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         ConvNeXt
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         41303080
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5196854597534395
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-li_scut_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Li_54_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-li_scut_t6_2024label" class="modal fade" id="bibtex-li_scut_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexli_scut_t6_2024label">
        SCUT SUBMISSION FOR AUTOMATED AUDIO CAPTIONING USING GRAPH ATTENTION AND CROSS-ATTENTION MECHANISMS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{li_scut_t6_2024,
    author = "Li, Qianqian",
    title = "SCUT SUBMISSION FOR AUTOMATED AUDIO CAPTIONING USING GRAPH ATTENTION AND CROSS-ATTENTION MECHANISMS",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "This report presents our work for automated audio caption-ing which is the Task 6A of DCASE 2024. Our system is an encoder-decoder framework. The encoder uses a pre-trained ConvNeXt network and the decoder employs a standard Transformer structure. Among the encoders, we include a graph attention module to enhance the module's ability to extract audio features. In the decoder, in addition to the Transformer's multi-head self-attention mechanism, a cross-attention mechanism is added to improve the association between output subtitles and audio features. Finally, our system achieves FENSE score of 0.5131 which is higher than the baseline system's FENSE score of 0.5040.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Li\_54\_t6.pdf",
    number = "54",
    note = "Ranked 8/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.520."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="de_jesus_silva_jkuicp_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-de_jesus_silva_jkuicp_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        HYPERPARAMETER TUNING OF THE CONETTE AUDIO CAPTIONING SYSTEM
       </h4>
<p style="text-align:left">
        Jakob De Jesus Silva<sup>1</sup>, Justus Tobias<sup>1</sup>, Sebastian Sonderegger<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute for Computational Perception, JKU Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Silva_JKUICP_t6_1</span> <span class="label label-primary">Silva_JKUICP_t6_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-de_jesus_silva_jkuicp_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-de_jesus_silva_jkuicp_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-de_jesus_silva_jkuicp_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_De_Jesus_Silva_60_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-de_jesus_silva_jkuicp_t6_2024').collapse('show');window.location.hash='#de_jesus_silva_jkuicp_t6_2024';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-de_jesus_silva_jkuicp_t6_2024" class="panel-collapse collapse" id="collapse-de_jesus_silva_jkuicp_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       HYPERPARAMETER TUNING OF THE CONETTE AUDIO CAPTIONING SYSTEM
      </h4>
<p style="text-align:left">
<small>
        Jakob De Jesus Silva<sup>1</sup>, Justus Tobias<sup>1</sup>, Sebastian Sonderegger<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Institute for Computational Perception, JKU Linz, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In the course of this challenge, we explored various methods to achieve a state-of-the-art audio captioning model. Initially, we worked with the baseline provided by the challenge organizers, then we also constructed several models from scratch, using diverse architectures. The best outcome we could achieve, was by tuning the hyperparameters of the baseline model CoNeTTE[1]. Our systematic approach involved finding hyperparameters that had the most effect on performance and their best combination. Although our enhanced baseline model demonstrated some performance gains, it still did not achieve a significant breakthrough over the original baseline. This is a student project in course of the lecture ”Machine-learning and Audio: A Challenge” at JKU.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Silva_JKUICP_t6_2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         9
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         None
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, label smoothing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho, Clotho
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         59486498
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5161157423087457
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-de_jesus_silva_jkuicp_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_De_Jesus_Silva_60_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Labbeti/dcase2024-task6-baseline" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-de_jesus_silva_jkuicp_t6_2024label" class="modal fade" id="bibtex-de_jesus_silva_jkuicp_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexde_jesus_silva_jkuicp_t6_2024label">
        HYPERPARAMETER TUNING OF THE CONETTE AUDIO CAPTIONING SYSTEM
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{de_jesus_silva_jkuicp_t6_2024,
    author = "Silva, Jakob De Jesus and Tobias, Justus and Sonderegger, Sebastian",
    title = "HYPERPARAMETER TUNING OF THE CONETTE AUDIO CAPTIONING SYSTEM",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "In the course of this challenge, we explored various methods to achieve a state-of-the-art audio captioning model. Initially, we worked with the baseline provided by the challenge organizers, then we also constructed several models from scratch, using diverse architectures. The best outcome we could achieve, was by tuning the hyperparameters of the baseline model CoNeTTE[1]. Our systematic approach involved finding hyperparameters that had the most effect on performance and their best combination. Although our enhanced baseline model demonstrated some performance gains, it still did not achieve a significant breakthrough over the original baseline. This is a student project in course of the lecture ”Machine-learning and Audio: A Challenge” at JKU.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_De\_Jesus\_Silva\_60\_t6.pdf",
    number = "60",
    note = "Ranked 9/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.516."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="epshtein_arc_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-epshtein_arc_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2024 TASK6: AUTOMATED AUDIO CAPTIONING USING CONTRASTIVE LEARNING
       </h4>
<p style="text-align:left">
        Dan Epshtein<sup>1</sup>, Yuval Amsalem<sup>1</sup>, Alon Amar<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Acoustics Research Center, Israel
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Epshtein_ARC_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-epshtein_arc_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-epshtein_arc_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-epshtein_arc_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Epshtein_34_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-epshtein_arc_t6_2024" class="panel-collapse collapse" id="collapse-epshtein_arc_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2024 TASK6: AUTOMATED AUDIO CAPTIONING USING CONTRASTIVE LEARNING
      </h4>
<p style="text-align:left">
<small>
        Dan Epshtein<sup>1</sup>, Yuval Amsalem<sup>1</sup>, Alon Amar<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Acoustics Research Center, Israel
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents our proposed enhancements to improving the baseline results of the DCASE2024 challenge Task 6 on Automated Audio Captioning. We introduce an additional loss function for contrastive learning, incorporating the NTXent loss as proposed in [1][3] into the baseline platform.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Epshtein_ARC_t6_1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         10
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         None
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, label smoothing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         48014000
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5140716527189527
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-epshtein_arc_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Epshtein_34_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-epshtein_arc_t6_2024label" class="modal fade" id="bibtex-epshtein_arc_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexepshtein_arc_t6_2024label">
        DCASE 2024 TASK6: AUTOMATED AUDIO CAPTIONING USING CONTRASTIVE LEARNING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{epshtein_arc_t6_2024,
    author = "Epshtein, Dan and Amsalem, Yuval and Amar, Alon",
    title = "DCASE 2024 TASK6: AUTOMATED AUDIO CAPTIONING USING CONTRASTIVE LEARNING",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "This technical report presents our proposed enhancements to improving the baseline results of the DCASE2024 challenge Task 6 on Automated Audio Captioning. We introduce an additional loss function for contrastive learning, incorporating the NTXent loss as proposed in [1][3] into the baseline platform.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Epshtein\_34\_t6.pdf",
    number = "34",
    note = "Ranked 10/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.514."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="hong_cau_t6_2024" style="box-shadow: none">
<div class="panel-heading" id="heading-hong_cau_t6_2024" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2024 task 6 automated audio captioning
       </h4>
<p style="text-align:left">
        Hyunhee Hong<sup>1</sup>, Yunjung Lee<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Chungang University Graduate School, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hong_CAU_t6_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-hong_cau_t6_2024" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-hong_cau_t6_2024" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-hong_cau_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Hong_16_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-hong_cau_t6_2024" class="panel-collapse collapse" id="collapse-hong_cau_t6_2024" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2024 task 6 automated audio captioning
      </h4>
<p style="text-align:left">
<small>
        Hyunhee Hong<sup>1</sup>, Yunjung Lee<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Chungang University Graduate School, Seoul, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This project describes an Automated Audio Captioning model for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge, Task 6. The proposed systems in this submission are based on a supervised language-audio pretraining strategy. Experiments show that our systems can achieve a SPIDEr-FL score of 29.39 on automated audio captioning.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Best submission
        </td>
<td>
         Hong_CAU_t6_1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Team rank
        </td>
<td>
         11
        </td>
</tr>
<tr>
<td class="col-md-3">
         Audio modelling
        </td>
<td>
         None
        </td>
</tr>
<tr>
<td class="col-md-3">
         Word modelling
        </td>
<td>
         transformer
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, label smoothing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble number of systems
        </td>
<td>
         1
        </td>
</tr>
<tr>
<td class="col-md-3">
         Train datasets used
        </td>
<td>
         Clotho
        </td>
</tr>
<tr>
<td class="col-md-3">
         Total number of parameters
        </td>
<td>
         41303080
        </td>
</tr>
<tr>
<td class="col-md-3">
         FENSE score
        </td>
<td>
         0.5131689575665977
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-hong_cau_t6_2024" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2024/technical_reports/DCASE2024_Hong_16_t6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-hong_cau_t6_2024label" class="modal fade" id="bibtex-hong_cau_t6_2024" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexhong_cau_t6_2024label">
        DCASE 2024 task 6 automated audio captioning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{hong_cau_t6_2024,
    author = "Hong, Hyunhee and Lee, Yunjung",
    title = "DCASE 2024 task 6 automated audio captioning",
    institution = "DCASE2024 Challenge",
    year = "2024",
    month = "May",
    abstract = "This project describes an Automated Audio Captioning model for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge, Task 6. The proposed systems in this submission are based on a supervised language-audio pretraining strategy. Experiments show that our systems can achieve a SPIDEr-FL score of 29.39 on automated audio captioning.",
    url = "https://dcase.community/documents/challenge2024/technical\_reports/DCASE2024\_Hong\_16\_t6.pdf",
    number = "16",
    note = "Ranked 11/12 in the DCASE2024 Challenge Task 6 with FENSE score of 0.513."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>