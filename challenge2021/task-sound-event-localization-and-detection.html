<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Localization and Detection with Directional Interference - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2021/task-sound-event-localization-and-detection">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of this task is to recognize individual sound events of specific classes, detect their temporal activity, and estimate their location during it, in the presence of interfering directional events not belonging to the target classes and spatial ambient noise. Challenge has ended. Full results for this task can â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2021</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2021/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2021/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2021/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2021/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2021/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wall-26.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-warning"></i><i class="fa dc-localization fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Localization</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 3</span></span><img src="../images/logos/dcase/dcase2021_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Localization and Detection with Directional Interference</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Archontis Politis" class="img img-circle" src="/images/person/archontis_politis.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Archontis Politis</strong>
<a class="icon" href="mailto:archontis.politis@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Antoine Deleforge" class="img img-circle" src="/images/person/antoine_deleforge.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Antoine Deleforge</strong>
<a class="icon" href="mailto:antoine.deleforge@inria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.inria.fr/">
                                Inria Nancy Grand-Est
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Sharath Adavanne" class="img img-circle" src="/images/person/sharath_adavanne.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Sharath Adavanne</strong>
<a class="icon" href="mailto:sharath.adavanne@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Prerak Srivastava" class="img img-circle" src="/images/person/prerak_srivastava.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Prerak Srivastava</strong>
<a class="icon" href="mailto:prerak.srivastava@inria.fr"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://www.inria.fr/">
                                Inria Nancy Grand-Est
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Daniel Krause" class="img img-circle" src="/images/person/daniel_krause.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Daniel Krause</strong>
<a class="icon" href="mailto:daniel.krause@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Tuomas Virtanen" class="img img-circle" src="/images/person/tuomas_virtanen.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Tuomas Virtanen</strong>
<a class="icon" href="mailto:tuomas.virtanen@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#recording-procedure">Recording procedure</a></li>
<li><a href="#recording-formats">Recording formats</a></li>
<li><a href="#sound-event-classes">Sound event classes</a></li>
<li><a href="#dataset-specifications">Dataset specifications</a></li>
<li><a href="#reference-labels-and-directions-of-arrival">Reference labels and directions-of-arrival</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#submission">Submission</a></li>
<li><a href="#evaluation">Evaluation</a>
<ul>
<li><a href="#metrics">Metrics</a></li>
<li><a href="#ranking">Ranking</a></li>
</ul>
</li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#baseline-changes">Baseline changes</a></li>
<li><a href="#repository">Repository</a></li>
<li><a href="#baseline-results-development-dataset">Baseline results (development dataset)</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of this task is to recognize individual sound events of specific classes, detect their temporal activity, and estimate their location during it, in the presence of interfering directional events not belonging to the target classes and spatial ambient noise.</p>
<p class="alert alert-info">
<strong>Challenge has ended. Full results for this task can be found in the <a class="btn btn-default btn-xs" href="/challenge2021/task-sound-event-localization-and-detection-results">Results <i class="fa fa-caret-right"></i></a> page.</strong>
</p>
<h1 id="description">Description</h1>
<p>Given multichannel audio input, a sound event detection and localization (SELD) system outputs a temporal activation track for each of the target sound classes, along with one or more corresponding spatial trajectories when the track indicates activity. This results in a spatio-temporal characterization of the acoustic scene that can be used in a wide range of machine cognition tasks, such as inference on the type of environment, self-localization, navigation without visual input or with occluded targets, tracking of specific types of sound sources, smart-home applications, scene visualization systems, and audio surveillance, among others.</p>
<p><strong>The task setup remains mostly unchanged with the previous year's DCASE2020 Challenge. The main difference is the emulation of scene recordings wth a more natural temporal distribution of target events and, more importantly, the inclusion of directional interferences, meaning sound events out of the target classes that are also point-like in nature. For each reverberant environment and every emulated recording, Interferences are spatialized in the same way as the target events, resulting in recordings that are more challenging and closer to real-life conditions.</strong> </p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2020/task3_sound_event_localization_and_detection.png"/>
<figcaption>Figure 1: Overview of sound event localization and detection system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<h1 id="audio-dataset">Audio dataset</h1>
<p>The <strong>TAU-NIGENS Spatial Sound Events 2021</strong> dataset contains multiple spatial sound-scene recordings, consisting of sound events of distinct categories integrated into a variety of acoustical spaces, and from multiple source directions and distances as seen from the recording position. <strong>Apart from the spatialized sound events of the target classes, diverse sound events not belonging to any of the target classes are also included in the scene.</strong> The spatialization of all sound events is based on filtering through real spatial room impulse responses (RIRs), captured in multiple rooms of various shapes, sizes, and acoustical absorption properties. Furthermore, each scene recording is delivered in two spatial recording formats, a microphone array one (<strong>MIC</strong>), and first-order Ambisonics one (<strong>FOA</strong>). The sound events are spatialized as either stationary sound sources in the room, or moving sound sources, in which case time-variant RIRs are used. Each sound event in the sound scene is associated with a trajectory of its direction-of-arrival (DoA) to the recording point, and a temporal onset and offset time. The isolated sound event recordings used for the synthesis of the sound scenes are obtained from the <a href="https://doi.org/10.5281/zenodo.2535878">NIGENS general sound events database</a>. These recordings serve as the development dataset for the <a href="http://dcase.community/challenge2021/task-sound-event-localization-and-detection">DCASE 2021 Sound Event Localization and Detection Task</a> of the <a href="http://dcase.community/challenge2021/">DCASE 2021 Challenge</a>.</p>
<p>The RIRs were collected in Finland by staff of Tampere University between 12/2017 - 06/2018, and between 11/2019 - 1/2020. The RIRs or subsets of them, have been also used in the datasets associated with the earlier two iterations of the challenge, the <strong>TAU Spatial Sound Events 2019</strong> <a href="https://doi.org/10.5281/zenodo.2580091">development</a> and <a href="https://doi.org/10.5281/zenodo.3066124">evaluation</a> datasets, with RIRs from 5 rooms, and the <strong>TAU-NIGENS Spatial Sound Events 2020</strong> <a href="https://doi.org/10.5281/zenodo.4064792">dataset</a>  with RIRs from 13 rooms. The data collection received funding from the European Research Council, grant agreement <a href="https://cordis.europa.eu/project/id/637422">637422 EVERYSOUND</a>.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="https://erc.europa.eu/sites/default/files/content/erc_banner-horizontal.jpg" title="ERC"/></a></p>
<p>More details on the dataset can be found in:</p>
<div class="btex-item" data-item="politis2021dataset" data-source="content/data/challenge2021/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="politis2021dataset"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Archontis Politis, Sharath Adavanne, Daniel Krause, Antoine Deleforge, Prerak Srivastava, and Tuomas Virtanen.
<em>A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection.</em>
In Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021), 125â€“129. Barcelona, Spain, November 2021.
URL: <a href="https://dcase.community/workshop2021/proceedings">https://dcase.community/workshop2021/proceedings</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexpolitis2021dataset0348724408a8431791f3e568bef75368" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Politis_43.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsepolitis2021dataset0348724408a8431791f3e568bef75368" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsepolitis2021dataset0348724408a8431791f3e568bef75368" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingpolitis2021dataset0348724408a8431791f3e568bef75368" class="panel-collapse collapse" id="collapsepolitis2021dataset0348724408a8431791f3e568bef75368" role="tabpanel">
<h4>A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation of SELD problems accompanies the dataset and it is shown to outperform the previous one. The new dataset is shown to be significantly more challenging for both baselines according to all considered metrics. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexpolitis2021dataset0348724408a8431791f3e568bef75368" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Politis_43.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexpolitis2021dataset0348724408a8431791f3e568bef75368label" class="modal fade" id="bibtexpolitis2021dataset0348724408a8431791f3e568bef75368" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexpolitis2021dataset0348724408a8431791f3e568bef75368label">A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{politis2021dataset,
    author = "Politis, Archontis and Adavanne, Sharath and Krause, Daniel and Deleforge, Antoine and Srivastava, Prerak and Virtanen, Tuomas",
    title = "A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection",
    booktitle = "Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)",
    address = "Barcelona, Spain",
    month = "November",
    year = "2021",
    pages = "125--129",
    abstract = "This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation of SELD problems accompanies the dataset and it is shown to outperform the previous one. The new dataset is shown to be significantly more challenging for both baselines according to all considered metrics. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers.",
    isbn = "978-84-09-36072-7",
    doi. = "10.5281/zenodo.5770113",
    url = "https://dcase.community/workshop2021/proceedings"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="recording-procedure">Recording procedure</h2>
<p>To construct a realistic dataset, real-life IR recordings were collected using an <a href="https://mhacoustics.com/products">Eigenmike</a> spherical microphone array. A <a href="https://www.genelec.com/g-three">Genelec G Three loudspeaker</a> was used to playback a maximum length sequence (MLS) around the Eigenmike. The IRs were obtained in the STFT domain using a least-squares regression between the known measurement signal (MLS) and far-field recording independently at each frequency.</p>
<p>The IRs were recorded at fifteen different indoor locations inside the Tampere University campus at Hervanta, Finland. Additionally, 30 minutes of ambient noise recordings were collected at the same locations with the IR recording setup unchanged. For each space IRs were collected with the source placed along specified trajectories, at various heights. The IR trajectories, source directions and distances differ with the space. Possible azimuths span the whole range of <span class="math">\(\phi\in[-180,180)\)</span>, while the elevations span approximately a range between <span class="math">\(\theta\in[-45,45]\)</span> degrees. A summary of the measured spaces is as follows:</p>
<hr/>
<ol>
<li>Large common area with multiple seating tables and carpet flooring. People chatting and working.</li>
<li>Large cafeteria with multiple seating tables and carpet flooring. People chatting and having food.</li>
<li>High ceiling corridor with hard flooring. People walking around and chatting.</li>
<li>Corridor with classrooms around and hard flooring. People walking around and chatting.</li>
<li>Large corridor with multiple sofas and tables, hard and carpet flooring at different parts. People walking around and chatting.</li>
<li>(2x) Large lecture halls with inclined floor. Ventilation noise.</li>
<li>(2x) Modern classrooms with multiple seating tables and carpet flooring. Ventilation noise.</li>
<li>(2x) Meeting rooms with hard floor and partially glass walls. Ventilation noise.</li>
<li>(2x) Old-style large classrooms with hard floor and rows of desks. Ventilation noise.</li>
<li>Large open space in underground bomb shelter, with plastic floor and rock walls. Ventilation noise.</li>
<li>Large open gym space. People using weights and gym equipment.</li>
</ol>
<h2 id="recording-formats">Recording formats</h2>
<p>The array response of the two recording formats can be considered known. The following theoretical spatial responses (steering vectors) modeling the two formats describe the directional response of each channel to a source incident from direction-of-arrival (DOA) given by azimuth angle <span class="math">\(\phi\)</span> and elevation angle <span class="math">\(\theta\)</span>.</p>
<p><strong>For the first-order ambisonics (FOA):</strong></p>
<div class="math">\begin{eqnarray}
H_1(\phi, \theta, f) &amp;=&amp; 1 \\
H_2(\phi, \theta, f) &amp;=&amp; \sin(\phi) * \cos(\theta) \\
H_3(\phi, \theta, f) &amp;=&amp; \sin(\theta) \\
H_4(\phi, \theta, f) &amp;=&amp; \cos(\phi) * \cos(\theta)
\end{eqnarray}</div>
<p>
The (FOA) format is obtained by converting the 32-channel microphone array signals by means of encoding filters based on anechoic measurements of the Eigenmike array response. Note that in the formulas above the encoding format is assumed frequency-independent, something that holds true up to around 9kHz with the specific microphone array, while the actual encoded responses start to deviate gradually at higher frequencies from the ideal ones provided above. </p>
<p><strong>For the tetrahedral microphone array (MIC):</strong></p>
<p>The four microphone have the following positions, in spherical coordinates <span class="math">\((\phi, \theta, r)\)</span>:</p>
<div class="math">\begin{eqnarray} 
M1: &amp;\quad(&amp;45^\circ, &amp;&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber\\
M2: &amp;\quad(&amp;-45^\circ, &amp;-&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber\\
M3: &amp;\quad(&amp;135^\circ, &amp;-&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber\\
M4: &amp;\quad(&amp;-135^\circ, &amp;&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber
\end{eqnarray}</div>
<p>Since the microphones are mounted on an acoustically-hard spherical baffle, an analytical expression for the directional array response is given by the expansion:
</p>
<div class="math">\begin{equation}
H_m(\phi_m, \theta_m, \phi, \theta, \omega) = \frac{1}{(\omega R/c)^2}\sum_{n=0}^{30} \frac{i^{n-1}}{h_n'^{(2)}(\omega R/c)}(2n+1)P_n(\cos(\gamma_m))
\end{equation}</div>
<p>where <span class="math">\(m\)</span> is the channel number, <span class="math">\((\phi_m, \theta_m)\)</span> are the specific microphone's azimuth and elevation position, <span class="math">\(\omega = 2\pi f\)</span> is the angular frequency, <span class="math">\(R = 0.042\)</span>m is the array radius, <span class="math">\(c = 343\)</span>m/s is the speed of sound, <span class="math">\(\cos(\gamma_m)\)</span> is the cosine angle between the microphone and the DOA, and <span class="math">\(P_n\)</span> is the unnormalized Legendre polynomial of degree <span class="math">\(n\)</span>, and <span class="math">\(h_n'^{(2)}\)</span> is the derivative with respect to the argument of a spherical Hankel function of the second kind. The expansion is limited to 30 terms which provides negligible modeling error up to 20kHz. Example routines that can generate directional frequency and impulse array responses based on the above formula can be found <a href="https://github.com/polarch/Array-Response-Simulator">here</a>.</p>
<h2 id="sound-event-classes">Sound event classes</h2>
<p>To generate the spatial sound scenes the measured room IRs are convolved with dry recordings of sound samples belonging to distinct sound classes. The sound event database of sound samples used for that purpose is the recent <a href="https://doi.org/10.5281/zenodo.2535878">NIGENS general sound events database</a>:</p>
<p>The 12 target sound classes of the spatialized events are:</p>
<ol start="0">
<li>alarm </li>
<li>crying baby</li>
<li>crash</li>
<li>barking dog</li>
<li>female scream</li>
<li>female speech</li>
<li>footsteps</li>
<li>knocking on door</li>
<li>male scream</li>
<li>male speech</li>
<li>ringing phone</li>
<li>piano</li>
</ol>
<p>Additionally, dry recordings of disparate sounds not belonging to any of those classes are also spatialized in the same way to serve as directional interference. The sounds are sourced from the <em>running engine</em>, <em>burning fire</em>, and <em>general</em> classes of NIGENS database. </p>
<h2 id="dataset-specifications">Dataset specifications</h2>
<p>The specifications of the dataset can be summarized in the following:</p>
<ul>
<li>600 one-minute long sound scene recordings with metadata (development dataset).</li>
<li>200 one-minute long sound scene recordings without metadata (evaluation dataset).</li>
<li>Sampling rate 24kHz.</li>
<li>About 500 sound event samples distributed over the 12 target classes (see <a href="http://doi.org/10.5281/zenodo.2535878">here</a> for more details).</li>
<li>About 400 sound event samples used as interference events (see <a href="http://doi.org/10.5281/zenodo.2535878">here</a> for more details).</li>
<li>Two 4-channel 3-dimensional recording formats: first-order Ambisonics (FOA) and tetrahedral microphone array.</li>
<li>Realistic spatialization and reverberation through multichannel RIRs collected in 13 different enclosures.</li>
<li>From 1184 to 6480 possible RIR positions across the different rooms.</li>
<li>Both static reverberant and moving reverberant sound events.</li>
<li>Three possible angular speeds for moving sources of approximately 10, 20, or 40deg/sec.</li>
<li>Up to three overlapping sound events possible, temporally and spatially.</li>
<li>SImultaneous directional interfering sound events with their own temporal activities, static or moving.</li>
<li>Realistic spatial ambient noise collected from each room is added to the spatialized sound events, at varying signal-to-noise ratios (SNR) ranging from noiseless (30dB) to noisy (6dB) conditions.</li>
</ul>
<p>Each recording corresponds to a single room, and allowed overlap of two simulatenous sources, or no overlap. Each event spatialized in the recording has equal probability of being either static or moving, and is asigned randomly one of the room RIR positions, or motion along one of the densely measured RIR trajectories. The moving sound events are synthesized with a slow (10deg/sec), moderate (20deg/sec), or fast (40deg/sec) angular speed. A partitioned time-frequency interpolation scheme of the RIRs extracted from the measurements at regular intervals is used to approximate the time-variant room response corresponding to the target motion.</p>
<p>A schematic example of a scene recording, with 4 target classes, 8 target events, 2 interference events, of which 2 targets and 1 interferer are moving, is depicted below. The maximum co-occurences of target events (polyphony) in this case is 3.  </p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2021/task3_SELD_recording_sketch.png"/>
<figcaption>Figure 2: Sketch of the spatial distribution and temporal distribution of events in an example scene recording.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<p>In the development dataset, eleven out of the thirteen rooms along with the NIGENS event samples are assigned to 6 disjoint sets, and their combinations form 6 distinct splits of 100 recordings each. The splits permits testing and validation across different acoustic conditions. The remaining two rooms are used for the evaluation dataset.</p>
<h2 id="reference-labels-and-directions-of-arrival">Reference labels and directions-of-arrival</h2>
<p>For each recording in the development dataset, the labels and DoAs are provided in a plain text CSV file of the same filename as the recording, in the following format:</p>
<div class="highlight"><pre><span></span><code>[frame number (int)], [active class index (int)], [event number index (int)], [azimuth (int)], [elevation (int)]
</code></pre></div>
<p>Frame, class, and track enumeration begins at 0. Frames correspond to a temporal resolution of 100msec. Azimuth and elevation angles are given in degrees, rounded to the closest integer value, with azimuth and elevation being zero at the front, azimuth <span class="math">\(\phi \in [-180^{\circ}, 180^{\circ}]\)</span>, and elevation <span class="math">\(\theta \in [-90^{\circ}, 90^{\circ}]\)</span>. Note that the azimuth angle is increasing counter-clockwise (<span class="math">\(\phi = 90^{\circ}\)</span> at the left). </p>
<p>The event number index is a unique integer for each event in the recording, enumerating them in the order of appearance. This event identifiers are useful to disentangle directions of co-occuring events through time in the metadata file. The interferers are considered unknown and no activity or direction labels of them are provided with the training datasets.</p>
<p>Overlapping sound events are indicated with duplicate frame numbers, and can belong to a different or the same class. An example sequence could be as:</p>
<div class="highlight"><pre><span></span><code><span class="mf">10</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">0</span><span class="p">,</span><span class="w">  </span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="w">  </span><span class="mf">30</span>
<span class="mf">11</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">0</span><span class="p">,</span><span class="w">  </span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="w">  </span><span class="mf">30</span>
<span class="mf">11</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">1</span><span class="p">,</span><span class="w">   </span><span class="mf">10</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">20</span>
<span class="mf">12</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">1</span><span class="p">,</span><span class="w">   </span><span class="mf">10</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">20</span>
<span class="mf">13</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">1</span><span class="p">,</span><span class="w">   </span><span class="mf">10</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">20</span>
<span class="mf">13</span><span class="p">,</span><span class="w">     </span><span class="mf">4</span><span class="p">,</span><span class="w">  </span><span class="mf">2</span><span class="p">,</span><span class="w">  </span><span class="o">-</span><span class="mf">40</span><span class="p">,</span><span class="w">   </span><span class="mf">0</span>
</code></pre></div>
<p>which describes that in frame 10-13, the first instance (<em>event 0</em>) of class <em>crying baby</em> (<em>class 1</em>) is active, however at frame 11 a second instance (<em>event 1</em>) of the same class appears simultaneously at a different direction, while at frame 13 an additional event of class 4 appears. Frames that contain no sound events are not included in the sequence. Note that event identifier information is only included in the development metadata and is not required to be provided by the participants in their results.</p>
<p>In the scenario that a participant would like to use a higher temporal resolution in their estimation than 100msec, we would recommend for an integer number of sub-frames to be used, to simplify processing of the metadata. A simple example routine performing (linear) spherical interpolation of directions is provided <a href="https://github.com/sharathadavanne/seld-dcase2020/blob/master/utils/interpolateDirections.py">here</a> (where e.g. for a sub-frame of 20msec, four interpolated directions are returned between the two input directions spaced at 100msec).</p>
<h2 id="download">Download</h2>
<p>The development and evaluation version of the dataset can be downloaded at:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.4844825" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.4844825" target="_blank">
<span style="font-size:20px;">TAU-NIGENS Spatial Sound Events 2021, Development &amp; Evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(15.2 GB)</span>
<br/>
<a href="10.5281/zenodo.4844825">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.4844825.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="task-setup">Task setup</h1>
<p>In order to allow a fair comparison of methods on the development dataset, participants are required to report results using the following split:</p>
<div class="table-responsive col-md-8">
<table class="table table-striped">
<thead>
<tr>
<th>Training folds</th>
<th>Validation fold</th>
<th>Testing fold</th>
</tr>
</thead>
<tbody>
<tr>
<td>1, 2, 3, 4</td>
<td>5</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p>The evaluation dataset is released a few weeks before the final submission deadline. This dataset consists of only audio recordings without any metadata/labels. Participants can decide the training procedure, i.e. the amount of training and validation files in the development dataset, the number of ensemble models etc., and submit the results of the SELD performance on the evaluation dataset.</p>
<h2 id="development-dataset">Development dataset</h2>
<p>The recordings in the development dataset follow the naming convention:</p>
<div class="highlight"><pre><span></span><code><span class="n">fold</span><span class="p">[</span><span class="n">fold</span><span class="w"> </span><span class="n">number</span><span class="p">]</span><span class="n">_room</span><span class="p">[</span><span class="n">room</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">fold</span><span class="p">]</span><span class="n">_mix</span><span class="p">[</span><span class="n">recording</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">split</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<p>Note that the room number only distinguishes different rooms used inside a split. For example, <code>room1</code> in the first split is not the same as <code>room1</code> in the second split. The room information is provided for the user of the dataset to understand the performance of their method with respect to different conditions.</p>
<p>Compared to the <strong>TAU-NIGENS Spatial Sound Events 2020</strong> dataset of the previous iteration, there is no indication of polyphony in each recording, as all recordings now allow conditions with the maximum polyphony of 3. However, all lower polyphonies and sliences can occur in each recording. </p>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>The evaluation dataset consists of 200 one-minute recordings without any information on the location in the naming convention, as below:</p>
<div class="highlight"><pre><span></span><code><span class="n">mix</span><span class="p">[</span><span class="n">recording</span><span class="w"> </span><span class="n">number</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<h1 id="task-rules">Task rules</h1>
<ul>
<li>Use of external data is <strong>not allowed</strong>.</li>
<li>Manipulation of the provided cross-validation split in the development dataset is <strong>not allowed</strong>.</li>
<li>Participants are <strong>not allowed</strong> to make subjective judgments of the evaluation data, nor to annotate it. The evaluation dataset cannot be used to train the submitted system.</li>
<li>Use of the dry event samples from the NIGENS database for additional data generation or augmentation is <strong>not allowed</strong>. </li>
<li>The development dataset can be augmented without the use of external data e.g. using techniques such as pitch shifting or time stretching, respatialization or re-reverberation of parts, etc.</li>
</ul>
<h1 id="submission">Submission</h1>
<p>The results for each of the 200 recordings in the evaluation dataset should be collected in individual CSV files. Each result file should have the same name as the file name of the respective audio recording, but with the <code>.csv</code> extension, and should contain the same information at each row as the reference labels, excluding the <em>event id</em>:</p>
<div class="highlight"><pre><span></span><code>[frame number (int)],[active class index (int)],[azimuth (int)],[elevation (int)]
</code></pre></div>
<p>Enumeration of frame and class indices begins at zero. The class indices are as ordered in the class descriptions mentioned above. The evaluation will be performed at a temporal resolution of 100msec. In case the participants use a different frame or hop length for their study, we expect them to use a suitable method to resample the information at the specified resolution before submitting the evaluation results.</p>
<p>In addition to the CSV files, the participants are asked to update the information of their method in the provided file and submit a technical report describing the method. We allow upto 4 system output submissions per participant/team. For each system, meta-information should be provided in a separate file, containing the task specific information. All files should be packaged into a zip file for submission. The detailed information regarding the challenge information can be found in the submission page.</p>
<p>General information for all DCASE submissions can be found on the <a href="/challenge2020/submission">Submission page</a>.</p>
<h1 id="evaluation">Evaluation</h1>
<p>The evaluation will be similar to the one employed in DCASE2020. Metrics evaluating jointly detection and localization performance are used. </p>
<h2 id="metrics">Metrics</h2>
<p>The first two metrics are the classic sound event detection (SED) metrics of F-score (<span class="math">\(F_{\leq T^\circ}\)</span>) and Error Rate (<span class="math">\(ER_{\leq T^\circ}\)</span>), but are now location-dependent, considering true positives predicted only under a distance threshold <span class="math">\(T^\circ\)</span> (angular in our case) from the reference. For the evaluation of this challenge we take this threshold to be <span class="math">\(T = 20^\circ\)</span>. </p>
<p>The next two metrics are focused on the localization part, but are now classification-dependent, meaning that they are computed only across each class only, instead of across all outputs. The first is the localization error <span class="math">\(LE_{\mathrm{CD}}\)</span> expressing average angular distance between predictions and references of the same class. The second is a simple localization recall metric <span class="math">\(LR_{\mathrm{CD}}\)</span> expressing the true positive rate of how many of these localization estimates were detected in a class, out of the total class instances. Unlike the location-dependent detection, these localization metrics do not use any threshold.</p>
<p>All metrics are computed in one-second non-overlapping frames. For a more thorough analysis on the joint SELD metrics please refer to:</p>
<div class="btex-item" data-item="politis2020overview" data-source="content/data/challenge2021/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="politis2020overview"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Archontis Politis, Annamaria Mesaros, Sharath Adavanne, Toni Heittola, and Tuomas Virtanen.
<em>Overview and evaluation of sound event localization and detection in dcase 2019.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:684â€“698, 2020.
URL: <a href="https://ieeexplore.ieee.org/abstract/document/9306885">https://ieeexplore.ieee.org/abstract/document/9306885</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexpolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9306885" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsepolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsepolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingpolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" class="panel-collapse collapse" id="collapsepolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" role="tabpanel">
<h4>Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019</h4>
<h5>Abstract</h5>
<p class="text-justify">Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of the DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a reevaluation of submissions using these new metrics. The new analysis reveals submissions that performed better on the joint task of detecting the correct type of event close to its original location than some of the submissions that were ranked higher in the challenge. Consequently, ranking of submissions which performed strongly when evaluated separately on detection or localization, but not jointly on both, was affected negatively.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexpolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9306885" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexpolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9label" class="modal fade" id="bibtexpolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexpolitis2020overviewfb005b8d8d2b47e3ab50948ba3ebcac9label">Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019</h4>
</div>
<div class="modal-body">
<pre>@article{politis2020overview,
    author = "Politis, Archontis and Mesaros, Annamaria and Adavanne, Sharath and Heittola, Toni and Virtanen, Tuomas",
    title = "Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    volume = "29",
    pages = "684--698",
    year = "2020",
    publisher = "IEEE",
    abstract = "Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of the DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a reevaluation of submissions using these new metrics. The new analysis reveals submissions that performed better on the joint task of detecting the correct type of event close to its original location than some of the submissions that were ranked higher in the challenge. Consequently, ranking of submissions which performed strongly when evaluated separately on detection or localization, but not jointly on both, was affected negatively.",
    url = "https://ieeexplore.ieee.org/abstract/document/9306885"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="ranking">Ranking</h2>
<p>Overall ranking will be based on the cumulative rank of the four metrics mentioned above, sorted in ascending order. By cumulative rank we mean the following: if system A was ranked individually for each metric as <span class="math">\(ER:1, F1:1, LE:3, LR:1\)</span>, then its cumulative rank is <span class="math">\(1+1+3+1=5\)</span>. Then if system B has <span class="math">\(ER:3, F1:2, LE:2, LR:3\)</span> (10), and system C has <span class="math">\(ER:2, F1:3, LE:1, LR:2\)</span> (8), then the overall rank of the systems is A,C,B. If two systems end up with the same cumulative rank, then they are assumed to have equal place in the challenge, even though they will be listed alphabetically in the ranking tables. </p>
<h1 id="results">Results</h1>
<p>The SELD task received 37 submissions in total from 13 teams across the world. Results from one of the teams were retracted after request from their side. Main results for these submissions are as following (the table below includes only the best performing system per submitting team):</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="anchor" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_er20" data-scatter-y="eval_le" data-show-chart="false" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="team_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="3">Submission Information</th>
<th class="sep-left-cell" colspan="4">Evaluation dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
Submission
</th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
Corresponding<br/> author
</th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
Affiliation
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="er_20" data-reversed="true" data-sortable="true" data-value-type="float2">
Error Rate<br/> (20Â°)
</th>
<th class="text-center" data-chartable="true" data-field="f_20" data-sortable="true" data-value-type="float1-percentage">
F-score<br/> (20Â°)
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="le" data-reversed="true" data-sortable="true" data-value-type="float1">
Localization <br/>error (Â°)
</th>
<th class="text-center" data-chartable="true" data-field="lr" data-sortable="true" data-value-type="float1-percentage">
Localization <br/>recall
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Shimada_SONY_task3_3</td>
<td>Kazuki Shimada</td>
<td>Sony Group Corporation</td>
<td>0.32</td>
<td>79.1</td>
<td>8.5</td>
<td>82.8</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Nanyang Technological University</td>
<td>0.32</td>
<td>78.3</td>
<td>10.0</td>
<td>78.3</td>
</tr>
<tr>
<td></td>
<td>Parrish_JHU_task3_2</td>
<td>Nathan Parrish</td>
<td>Johns Hopkins University</td>
<td>0.39</td>
<td>73.8</td>
<td>12.8</td>
<td>76.8</td>
</tr>
<tr>
<td></td>
<td>Lee_SGU_task3_1</td>
<td>Sang-Hoon Lee</td>
<td>Sogang University</td>
<td>0.40</td>
<td>72.9</td>
<td>13.2</td>
<td>76.5</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_4</td>
<td>Sooyoung Park</td>
<td>Electronics and Telecommunications Research Institute</td>
<td>0.46</td>
<td>67.8</td>
<td>12.8</td>
<td>72.3</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task3_1</td>
<td>Zihao Li</td>
<td>University of Chinese Academy of Sciences</td>
<td>0.46</td>
<td>64.7</td>
<td>12.8</td>
<td>61.9</td>
</tr>
<tr>
<td></td>
<td>Ko_SKKU_task3_4</td>
<td>Jonghwan Ko</td>
<td>Sungkyunkwan University</td>
<td>0.58</td>
<td>60.3</td>
<td>15.1</td>
<td>70.7</td>
</tr>
<tr>
<td></td>
<td>Huang_Aalto_task3_1</td>
<td>Huang Daolang</td>
<td>Aalto University</td>
<td>0.57</td>
<td>52.3</td>
<td>18.5</td>
<td>58.5</td>
</tr>
<tr>
<td></td>
<td>Yalta_HIT_task3_1</td>
<td>Nelson Yalta</td>
<td>Hitachi, Ltd.</td>
<td>0.72</td>
<td>52.5</td>
<td>20.1</td>
<td>71.1</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task3_2</td>
<td>Javier Naranjo-Alcazar</td>
<td>Instituto TecnolÃ³gico de InformÃ¡tica</td>
<td>0.68</td>
<td>37.7</td>
<td>25.3</td>
<td>53.9</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Politis_TAU_task3_foa</td>
<td>Archontis Politis</td>
<td>Tampere University</td>
<td>0.67</td>
<td>37.2</td>
<td>23.9</td>
<td>45.8</td>
</tr>
<tr>
<td></td>
<td>Bai_NWPU_task3_2</td>
<td>Jisheng Bai</td>
<td>LianFeng Acoustic Technologies Co., Ltd.</td>
<td>0.79</td>
<td>16.4</td>
<td>66.5</td>
<td>35.5</td>
</tr>
<tr>
<td></td>
<td>Sun_AIAL-XJU_task3_2</td>
<td>Xinghao Sun</td>
<td>Xinjiang University</td>
<td>0.95</td>
<td>2.7</td>
<td>84.5</td>
<td>17.4</td>
</tr>
</tbody>
</table>
<p>Complete results and technical reports can be found in the <a class="btn btn-primary" href="/challenge2021/task-sound-event-localization-and-detection-results">results page</a></p>
<h1 id="baseline-system">Baseline system</h1>
<p>Similarly to the previous iterations of the challenge, as the baseline we use a straightforward convolutional recurrent neural netowrk (CRNN) based on SELDnet, but with a few important modifications. </p>
<div class="btex-item" data-item="Adavanne2018_JSTSP" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Adavanne2018_JSTSP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Sharath Adavanne, Archontis Politis, Joonas Nikunen, and Tuomas Virtanen.
<em>Sound event localization and detection of overlapping sources using convolutional recurrent neural networks.</em>
<em>IEEE Journal of Selected Topics in Signal Processing</em>, 13(1):34â€“48, March 2018.
URL: <a href="https://ieeexplore.ieee.org/abstract/document/8567942">https://ieeexplore.ieee.org/abstract/document/8567942</a>, <a href="https://doi.org/10.1109/JSTSP.2018.2885636">doi:10.1109/JSTSP.2018.2885636</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/1807.00129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" class="panel-collapse collapse" id="collapseAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" role="tabpanel">
<h4>Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h4>
<h5>Abstract</h5>
<p class="text-justify">In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.</p>
<h5>Keywords</h5>
<p class="text-justify">Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/1807.00129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352blabel" class="modal fade" id="bibtexAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAdavanne2018_JSTSP35da19de8e1342948c09ac66c588352blabel">Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h4>
</div>
<div class="modal-body">
<pre>@article{Adavanne2018_JSTSP,
    author = "Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    title = "Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks",
    year = "2018",
    volume = "13",
    number = "1",
    pages = "34-48",
    keywords = "Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network",
    abstract = "In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.",
    doi = "10.1109/JSTSP.2018.2885636",
    issn = "1932-4553",
    month = "March",
    url = "https://ieeexplore.ieee.org/abstract/document/8567942"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><strong>Note:</strong> The baseline only supports detection of one instance of a sound class in a given time frame. However, the recordings and the training labels can consist of multiple instances of the same sound class in a given time frame. On the other hand, the provided metric code - <em>SELD_evaluation_metrics.py</em> and the class wrapper for it <em>cls_compute_seld_results.py</em> both evaluate succesful detection and localization of simultaneous multiple instances of the same class.</p>
<h2 id="baseline-changes">Baseline changes</h2>
<p>**Compared to the DCASE2020 and the associated published <a href="https://github.com/sharathadavanne/seld-dcase2020">SELDnet version</a>, a few modifications have been integrated in the model, in order to take into account some of the simplest effective improvements demonstrated by the participants in the previous year. </p>
<p>The most important one is the elimination of the dedicated event classification output branch, by adopting the ACCDOA training target which unifies the localization and classification losses in a homogenous regression vector loss, pioneered by Shimada et al. and the third best performing team in DCASE2020. More details can be found in their report:</p>
<div class="btex-item" data-item="Shimada2021" data-source="content/data/external_publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Shimada2021"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Kazuki Shimada, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, and Yuki Mitsufuji.
<em>Accdoa: activity-coupled cartesian direction of arrival representation for sound event localization and detection.</em>
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Toronto, Ontario, Canada, June 2021.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexShimada2021eace28f691f5400aa5d0e1177a9e67af" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2010.15306.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseShimada2021eace28f691f5400aa5d0e1177a9e67af" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseShimada2021eace28f691f5400aa5d0e1177a9e67af" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingShimada2021eace28f691f5400aa5d0e1177a9e67af" class="panel-collapse collapse" id="collapseShimada2021eace28f691f5400aa5d0e1177a9e67af" role="tabpanel">
<h4>ACCDOA: Activity-Coupled Cartesian Direction of Arrival Representation for Sound Event Localization and Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">Neural-network (NN)-based methods show high performance in sound event localization and detection (SELD). Conventional NN-based methods use two branches for a sound event detection (SED) target and a direction-of-arrival (DOA) target. The two-branch representation with a single network has to decide how to balance the two objectives during optimization. Using two networks dedicated to each task increases system complexity and network size. To address these problems, we propose an activity-coupled Cartesian DOA (ACCDOA) representation, which assigns a sound event activity to the length of a corresponding Cartesian DOA vector. The ACCDOA representation enables us to solve a SELD task with a single target and has two advantages: avoiding the necessity of balancing the objectives and model size increase. In experimental evaluations with the DCASE 2020 Task 3 dataset, the ACCDOA representation outperformed the two-branch representation in SELD metrics with a smaller network size. The ACCDOA-based SELD system also performed better than state-of-the-art SELD systems in terms of localization and location-dependent detection.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexShimada2021eace28f691f5400aa5d0e1177a9e67af" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2010.15306.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexShimada2021eace28f691f5400aa5d0e1177a9e67aflabel" class="modal fade" id="bibtexShimada2021eace28f691f5400aa5d0e1177a9e67af" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexShimada2021eace28f691f5400aa5d0e1177a9e67aflabel">ACCDOA: Activity-Coupled Cartesian Direction of Arrival Representation for Sound Event Localization and Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Shimada2021,
    Author = "Shimada, Kazuki and Koyama, Yuichiro and Takahashi, Naoya and Takahashi, Shusuke and Mitsufuji, Yuki",
    title = "ACCDOA: Activity-Coupled Cartesian Direction of Arrival Representation for Sound Event Localization and Detection",
    abstract = "Neural-network (NN)-based methods show high performance in sound event localization and detection (SELD). Conventional NN-based methods use two branches for a sound event detection (SED) target and a direction-of-arrival (DOA) target. The two-branch representation with a single network has to decide how to balance the two objectives during optimization. Using two networks dedicated to each task increases system complexity and network size. To address these problems, we propose an activity-coupled Cartesian DOA (ACCDOA) representation, which assigns a sound event activity to the length of a corresponding Cartesian DOA vector. The ACCDOA representation enables us to solve a SELD task with a single target and has two advantages: avoiding the necessity of balancing the objectives and model size increase. In experimental evaluations with the DCASE 2020 Task 3 dataset, the ACCDOA representation outperformed the two-branch representation in SELD metrics with a smaller network size. The ACCDOA-based SELD system also performed better than state-of-the-art SELD systems in terms of localization and location-dependent detection.",
    month = "June",
    year = "2021",
    address = "Toronto, Ontario, Canada",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="repository">Repository</h2>
<p>The baseline, along with more details on it, can be found in:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/sharathadavanne/seld-dcase2021" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/sharathadavanne/seld-dcase2021" target="_blank">
<span style="font-size:20px;">DCASE 2021 SELD task baseline repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="baseline-results-development-dataset">Baseline results (development dataset)</h2>
<p>The evaluation metric scores for the test split of the development dataset are given below. The location-dependent detection metrics are computed within a 20Â° threshold from the reference.</p>
<div class="table-responsive col-md-12">
<table class="table table-striped">
<thead>
<tr>
<th>Dataset</th>
<th>ER<sub>20Â°</sub></th>
<th>F<sub>20Â°</sub></th>
<th>LE<sub>CD</sub></th>
<th>LR<sub>CD</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td>Ambisonic</td>
<td>0.69</td>
<td>33.9 %</td>
<td>24.1Â°</td>
<td>43.9 %</td>
</tr>
<tr>
<td>Microphone array</td>
<td>0.74</td>
<td>24.7%</td>
<td>30.9Â°</td>
<td>38.2 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p><strong>Note:</strong> The reported baseline system performance is not exactly reproducible due to varying setups. However, you should be able to obtain very similar results.</p>
<h1 id="citation">Citation</h1>
<p>If you are participating in this task or using the dataset and code please consider citing the following papers:</p>
<div class="btex-item" data-item="politis2021dataset" data-source="content/data/challenge2021/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="politis2021dataset"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Archontis Politis, Sharath Adavanne, Daniel Krause, Antoine Deleforge, Prerak Srivastava, and Tuomas Virtanen.
<em>A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection.</em>
In Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021), 125â€“129. Barcelona, Spain, November 2021.
URL: <a href="https://dcase.community/workshop2021/proceedings">https://dcase.community/workshop2021/proceedings</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexpolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Politis_43.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsepolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsepolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingpolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" class="panel-collapse collapse" id="collapsepolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" role="tabpanel">
<h4>A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation of SELD problems accompanies the dataset and it is shown to outperform the previous one. The new dataset is shown to be significantly more challenging for both baselines according to all considered metrics. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexpolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Politis_43.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexpolitis2021datasetaea2065495974b5b9102cda6ded4b7bdlabel" class="modal fade" id="bibtexpolitis2021datasetaea2065495974b5b9102cda6ded4b7bd" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexpolitis2021datasetaea2065495974b5b9102cda6ded4b7bdlabel">A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{politis2021dataset,
    author = "Politis, Archontis and Adavanne, Sharath and Krause, Daniel and Deleforge, Antoine and Srivastava, Prerak and Virtanen, Tuomas",
    title = "A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection",
    booktitle = "Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)",
    address = "Barcelona, Spain",
    month = "November",
    year = "2021",
    pages = "125--129",
    abstract = "This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation of SELD problems accompanies the dataset and it is shown to outperform the previous one. The new dataset is shown to be significantly more challenging for both baselines according to all considered metrics. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers.",
    isbn = "978-84-09-36072-7",
    doi. = "10.5281/zenodo.5770113",
    url = "https://dcase.community/workshop2021/proceedings"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="politis2020overview" data-source="content/data/challenge2021/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="politis2020overview"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Archontis Politis, Annamaria Mesaros, Sharath Adavanne, Toni Heittola, and Tuomas Virtanen.
<em>Overview and evaluation of sound event localization and detection in dcase 2019.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:684â€“698, 2020.
URL: <a href="https://ieeexplore.ieee.org/abstract/document/9306885">https://ieeexplore.ieee.org/abstract/document/9306885</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexpolitis2020overview2c3ba42f02e7414989567d643ad71f0f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9306885" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsepolitis2020overview2c3ba42f02e7414989567d643ad71f0f" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsepolitis2020overview2c3ba42f02e7414989567d643ad71f0f" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingpolitis2020overview2c3ba42f02e7414989567d643ad71f0f" class="panel-collapse collapse" id="collapsepolitis2020overview2c3ba42f02e7414989567d643ad71f0f" role="tabpanel">
<h4>Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019</h4>
<h5>Abstract</h5>
<p class="text-justify">Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of the DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a reevaluation of submissions using these new metrics. The new analysis reveals submissions that performed better on the joint task of detecting the correct type of event close to its original location than some of the submissions that were ranked higher in the challenge. Consequently, ranking of submissions which performed strongly when evaluated separately on detection or localization, but not jointly on both, was affected negatively.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexpolitis2020overview2c3ba42f02e7414989567d643ad71f0f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9306885" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexpolitis2020overview2c3ba42f02e7414989567d643ad71f0flabel" class="modal fade" id="bibtexpolitis2020overview2c3ba42f02e7414989567d643ad71f0f" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexpolitis2020overview2c3ba42f02e7414989567d643ad71f0flabel">Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019</h4>
</div>
<div class="modal-body">
<pre>@article{politis2020overview,
    author = "Politis, Archontis and Mesaros, Annamaria and Adavanne, Sharath and Heittola, Toni and Virtanen, Tuomas",
    title = "Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    volume = "29",
    pages = "684--698",
    year = "2020",
    publisher = "IEEE",
    abstract = "Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of the DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a reevaluation of submissions using these new metrics. The new analysis reveals submissions that performed better on the joint task of detecting the correct type of event close to its original location than some of the submissions that were ranked higher in the challenge. Consequently, ranking of submissions which performed strongly when evaluated separately on detection or localization, but not jointly on both, was affected negatively.",
    url = "https://ieeexplore.ieee.org/abstract/document/9306885"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>