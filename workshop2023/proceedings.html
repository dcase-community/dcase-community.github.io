<!DOCTYPE html><html lang="en">
<head>
    <title>Proceedings - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2023/proceedings">
        <meta name="author" content="DCASE2023" />
        <meta name="description" content="Magdalena Fuentes, Toni Heittola, Keisuke Imoto, Annamaria Mesaros, Archontis Politis, Romain Serizel, Tuomas Virtanen (eds.), Proceedings of the 8th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2023), September 2023. ISBN (Electronic): 978-952-03-3171-9 Link PDF Bibtex ×Close Proceedings of the 7th Workshop on Detection and Classification …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2023</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2023/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-list fa-fw"></i>&nbsp;Program&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2023/schedule"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule</a>
    </li>
            <li class="">
        <a href="/workshop2023/keynotes"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Keynotes</a>
    </li>
            <li class="">
        <a href="/workshop2023/tutorials"><i class="fa fa-comments-o fa-fw"></i>&nbsp;Tutorials</a>
    </li>
        </ul>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2023/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="">
        <a href="/workshop2023/registration"><i class="fa fa-key fa-fw"></i>&nbsp;Registration</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-compass fa-fw"></i>&nbsp;Venue & Travel&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Travel & Accommodation">
        <a href="/workshop2023/travel"><i class="fa fa-info fa-fw"></i>&nbsp;Travel & Accommodation</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Sauna Rules">
        <a href="/workshop2023/sauna-rules"><i class="fa fa-th-list fa-fw"></i>&nbsp;Sauna Rules</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2023/submission"><i class="fa fa-upload fa-fw"></i>&nbsp;Submission</a>
    </li>
            <li class="">
        <a href="/workshop2023/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2023/presenter-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Presenters</a>
    </li>
            <li class="">
        <a href="/workshop2023/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
            <li class="">
        <a href="/workshop2023/call-for-tutorials"><i class="fa fa-hand-paper-o fa-fw"></i>&nbsp;Call for tutorials</a>
    </li>
        </ul>
    </li><li class="">
        <a href="/workshop2023/reviewer-guidelines"><i class="fa fa-sticky-note-o fa-fw"></i>&nbsp;Reviewers</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2023/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metropol-sevilla-03.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><h1 class="bold">Proceedings</h1><span class="subheading">Workshop on Detection and Classification of Acoustic Scenes and Events</span><hr class="small right bold">
                        <span class="subheading subheading-secondary">20-22 September 2023, Tampere, Finland</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <div class="row" style="display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;">
<div class="col-xs-2">
<a data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/152310/978-952-03-3171-9.pdf?sequence=2&amp;isAllowed=y" rel="tooltip" target="_blank" title="PDF"><img class="img-responsive img-thumbnail" src="../images/covers/DCASE2023Workshop_proceedings_cover.png"/></a>
</div>
<div class="col-xs-10 bg-light-gray">
<p>Magdalena Fuentes, Toni Heittola, Keisuke Imoto, Annamaria Mesaros, Archontis Politis, Romain Serizel, Tuomas Virtanen (eds.), Proceedings of the 8th Workshop on
Detection and Classification of Acoustic Scenes and Events (DCASE 2023),
September 2023.</p>
<p>ISBN (Electronic): 978-952-03-3171-9<br/>
<div class="btn-group">
<a class="btn btn-xs btn-primary" data-placement="bottom" href="https://trepo.tuni.fi/handle/10024/152310" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" target="_blank" title="Permanent link"><i class="fa fa-link fa-1x"></i> Link</a>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/152310/978-952-03-3171-9.pdf?sequence=2&amp;isAllowed=y" rel="tooltip" style="text-decoration:none;border:0;padding-bottom:3px" target="_blank" title="PDF"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button class="btn btn-xs btn-danger" data-target="#bibtex_proceedings" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
</p></div>
</div>
<div aria-hidden="true" aria-labelledby="bibtex_proceedingslabel" class="modal fade" id="bibtex_proceedings" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtex_proceedingslabel">Proceedings of the 7th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2022)</h4>
</div>
<div class="modal-body">
<pre>
@book{DCASE2023Workshop,
    title = "Proceedings of the 8th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2023)",
    author = "Magdalena Fuentes, Toni Heittola, Keisuke Imoto, Annamaria Mesaros, Archontis Politis, Romain Serizel, and Tuomas Virtanen",
    year = "2023",
    publisher = "Tampere University",
    isbn = "978-952-03-3171-9",
    month = "September",
    address = "Tampere, Finland"
}
                </pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
<div class="clearfix"></div>
<div class="btex" data-scholar-cite-counts="true" data-source="content/data/workshop2023/proceedings.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Afolaranmi2023" style="box-shadow: none">
<div class="panel-heading" id="headingAfolaranmi2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Sound Event Classification with Object-Based Labels
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         James Afolaranmi, Irene Martín-Morató and Annamaria Mesaros
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Computing Science, Tampere University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Afolaranmi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Afolaranmi_71.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Afolaranmi2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Afolaranmi2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-37">
        P4-37
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Afolaranmi2023" class="panel-collapse collapse" id="collapse-Afolaranmi2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Availability of audio-visual datasets and increase of computational resources have made possible the use of deep learning techniques that exploit the relationship between audio and video. In this paper, we present an approach that makes use of pretrained models for object detection to label audio clips based on objects that are expected to make sound. The study consists of performing object detection for four target classes belonging to vehicle category and training sound classifiers in supervised way using the resulting labels. We conclude that object detection is a useful alternative for labeling audio-visual material for audio classification, with substantial improvements in different datasets. Results show that even for data provided with reference audio labels, labeling through video object detection can identify additional, non-annotated acoustic events, thus improving the quality of the labels in existing datasets. This promotes exploitation of video content not only as an alternative, but also to complement the available label information.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Afolaranmi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Afolaranmi_71.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Afolaranmi2023label" class="modal fade" id="bibtex-Afolaranmi2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAfolaranmi2023label">
        Sound Event Classification with Object-Based Labels
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Afolaranmi2023,
    author = "Afolaranmi, James and Martín-Morató, Irene and Mesaros, Annamaria",
    title = "Sound Event Classification with Object-Based Labels",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "1--5",
    abstract = "Availability of audio-visual datasets and increase of computational resources have made possible the use of deep learning techniques that exploit the relationship between audio and video. In this paper, we present an approach that makes use of pretrained models for object detection to label audio clips based on objects that are expected to make sound. The study consists of performing object detection for four target classes belonging to vehicle category and training sound classifiers in supervised way using the resulting labels. We conclude that object detection is a useful alternative for labeling audio-visual material for audio classification, with substantial improvements in different datasets. Results show that even for data provided with reference audio labels, labeling through video object detection can identify additional, non-annotated acoustic events, thus improving the quality of the labels in existing datasets. This promotes exploitation of video content not only as an alternative, but also to complement the available label information."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Aguado2023" style="box-shadow: none">
<div class="panel-heading" id="headingAguado2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Learning in the Wild: Bioacoustics Few Shot Learning Without Using a Training Set
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Víctor Aguado, Joan Navarro and Ester Vidaña-Vila
        </em>
</p>
<p class="text-muted">
<small>
<em>
          La Salle Campus Barcelona, University Ramon Llull
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Aguado2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Aguado_58.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Aguado2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Aguado2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-24">
        P3-24
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Aguado2023" class="panel-collapse collapse" id="collapse-Aguado2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Few-shot learning is a machine learning approach in which a pre-trained model is re-trained for new categories with just a few examples. This strategy results very convenient for problems with a dynamic number of categories as typically happens in acoustic data. The purpose of this paper is to explore the possibility of skipping this pre-training process and using as training data only the five first shots of an audio file together with the silence between them. For the experimental evaluation, data belonging to the Validation set of Task 5 DCASE Challenge 2023 is used, purposely neglecting the Training set. This challenge consists of detecting animal species using only five positive examples. In this exploratory work, three learning methods have been compared: a ResNet architecture with a prototypical loss, a ProtoNet and an XGBoost classifier. In all cases, spectrograms with different transformations are used as inputs. Obtained results are evaluated per audio file, enabling the obtention of particular conclusions about different animal species. While the detection for some species presents encouraging results using only these first 5-shots as training data, all the tested algorithms are unable to successfully learn how to properly detect the blackbird sounds of the validation dataset.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Aguado2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Aguado_58.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Aguado2023label" class="modal fade" id="bibtex-Aguado2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAguado2023label">
        Learning in the Wild: Bioacoustics Few Shot Learning Without Using a Training Set
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Aguado2023,
    author = "Aguado, Víctor and Navarro, Joan and Vidaña-Vila, Ester",
    title = "Learning in the Wild: Bioacoustics Few Shot Learning Without Using a Training Set",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "6--10",
    abstract = "Few-shot learning is a machine learning approach in which a pre-trained model is re-trained for new categories with just a few examples. This strategy results very convenient for problems with a dynamic number of categories as typically happens in acoustic data. The purpose of this paper is to explore the possibility of skipping this pre-training process and using as training data only the five first shots of an audio file together with the silence between them. For the experimental evaluation, data belonging to the Validation set of Task 5 DCASE Challenge 2023 is used, purposely neglecting the Training set. This challenge consists of detecting animal species using only five positive examples. In this exploratory work, three learning methods have been compared: a ResNet architecture with a prototypical loss, a ProtoNet and an XGBoost classifier. In all cases, spectrograms with different transformations are used as inputs. Obtained results are evaluated per audio file, enabling the obtention of particular conclusions about different animal species. While the detection for some species presents encouraging results using only these first 5-shots as training data, all the tested algorithms are unable to successfully learn how to properly detect the blackbird sounds of the validation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Barahona2023" style="box-shadow: none">
<div class="panel-heading" id="headingBarahona2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Multi-Resolution Conformer for Sound Event Detection: Analysis and Optimization
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sara Barahona, Diego de Benito-Gorron, Sergio Segovia, Daniel Ramos and Doroteo T. Toledano
        </em>
</p>
<p class="text-muted">
<small>
<em>
          AUDIAS Research Group, Universidad Autóonoma de Madrid
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Barahona2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Barahona_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Barahona2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Barahona2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-39">
        P4-39
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Barahona2023" class="panel-collapse collapse" id="collapse-Barahona2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The Conformer architecture has achieved state-of-the-art results in several tasks, including automatic speech recognition and automatic speaker verification. However, its utilization in sound event detection and in particular in the DCASE Challenge Task 4 has been limited despite winning the 2020 edition. Although the Conformer architecture may not excel in accurately localizing sound events, it shows promising potential in minimizing confusion between different classes. Therefore, in this paper we propose a Conformer optimization to enhance the second Polyphonic Sound Detection Score (PSDS) scenario defined for the DCASE 2023 Task 4A. With the aim of maximizing its classification properties, we have employed recently proposed methods such as Frequency Dynamic Convolutions in addition to our multi-resolution approach, which allow us to analyse its behaviour over different time-frequency resolution points. Furthermore, our Conformer systems are compared with multi-resolution models based on Convolutional Recurrent Neural Networks (CRNNs) to evaluate the respective benefits of each architecture in relation to the two proposed scenarios for the PSDS and the different time-frequency resolution points defined. These systems were submitted as our participation in the DCASE 2023 Task 4A, in which our Conformer system obtained a PSDS2 value of 0.728, achieving one of the highest scores for this scenario among systems trained without external resources.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Barahona2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Barahona_69.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Barahona2023label" class="modal fade" id="bibtex-Barahona2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBarahona2023label">
        Multi-Resolution Conformer for Sound Event Detection: Analysis and Optimization
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Barahona2023,
    author = "Barahona, Sara and de Benito-Gorron, Diego and Segovia, Sergio and Ramos, Daniel and Toledano, Doroteo T.",
    title = "Multi-Resolution Conformer for Sound Event Detection: Analysis and Optimization",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "11--15",
    abstract = "The Conformer architecture has achieved state-of-the-art results in several tasks, including automatic speech recognition and automatic speaker verification. However, its utilization in sound event detection and in particular in the DCASE Challenge Task 4 has been limited despite winning the 2020 edition. Although the Conformer architecture may not excel in accurately localizing sound events, it shows promising potential in minimizing confusion between different classes. Therefore, in this paper we propose a Conformer optimization to enhance the second Polyphonic Sound Detection Score (PSDS) scenario defined for the DCASE 2023 Task 4A. With the aim of maximizing its classification properties, we have employed recently proposed methods such as Frequency Dynamic Convolutions in addition to our multi-resolution approach, which allow us to analyse its behaviour over different time-frequency resolution points. Furthermore, our Conformer systems are compared with multi-resolution models based on Convolutional Recurrent Neural Networks (CRNNs) to evaluate the respective benefits of each architecture in relation to the two proposed scenarios for the PSDS and the different time-frequency resolution points defined. These systems were submitted as our participation in the DCASE 2023 Task 4A, in which our Conformer system obtained a PSDS2 value of 0.728, achieving one of the highest scores for this scenario among systems trained without external resources."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Choi2023" style="box-shadow: none">
<div class="panel-heading" id="headingChoi2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Foley Sound Synthesis at the DCASE 2023 Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Keunwoo Choi<sup>1</sup>, Jaekwon Im<sup>1,2</sup>, Laurie M. Heller<sup>3</sup>, Brian McFee<sup>4</sup>, Keisuke Imoto<sup>5</sup>, Yuki Okamoto<sup>6</sup>, Mathieu Lagrange<sup>7</sup> and Shinnosuke Takamichi<sup>8</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Gaudio Lab, Inc., <sup>2</sup>KAIST, <sup>3</sup>Carnegie Mellon University, <sup>4</sup>New York University, <sup>5</sup>Doshisha University, <sup>6</sup>Ritsumeikan University, <sup>7</sup>Ecole Centrale Nantes, CNRS, <sup>8</sup>The University of Tokyo
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Choi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Choi_4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Choi2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Choi2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-18">
        P2-18
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Choi2023" class="panel-collapse collapse" id="collapse-Choi2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The addition of Foley sound effects during post-production is a common technique used to enhance the perceived acoustic properties of multimedia content. Traditionally, Foley sound has been produced by human Foley artists, which involves manual recording and mixing of sound. However, recent advances in sound synthesis and generative models have generated interest in machine-assisted or automatic Foley synthesis techniques. To promote further research in this area, we have organized a challenge in DCASE 2023: Task 7 - Foley Sound Synthesis. Our challenge aims to provide a standardized evaluation framework that is both rigorous and efficient, allowing for the evaluation of different Foley synthesis systems. We received 17 submissions, and performed both objective and subjective evaluation to rank them according to three criteria: audio quality, fit-to-category, and diversity. Through this challenge, we hope to encourage active participation from the research community and advance the state-of-the-art in automatic Foley synthesis. In this paper, we provide a detailed overview of the Foley sound synthesis challenge, including task definition, dataset, baseline, evaluation scheme and criteria, challenge result, and discussion.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Choi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Choi_4.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Choi2023label" class="modal fade" id="bibtex-Choi2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChoi2023label">
        Foley Sound Synthesis at the DCASE 2023 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Choi2023,
    author = "Choi, Keunwoo and Im, Jaekwon and Heller, Laurie M. and McFee, Brian and Imoto, Keisuke and Okamoto, Yuki and Lagrange, Mathieu and Takamichi, Shinnosuke",
    title = "Foley Sound Synthesis at the {DCASE} 2023 Challenge",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "16--20",
    abstract = "The addition of Foley sound effects during post-production is a common technique used to enhance the perceived acoustic properties of multimedia content. Traditionally, Foley sound has been produced by human Foley artists, which involves manual recording and mixing of sound. However, recent advances in sound synthesis and generative models have generated interest in machine-assisted or automatic Foley synthesis techniques. To promote further research in this area, we have organized a challenge in DCASE 2023: Task 7 - Foley Sound Synthesis. Our challenge aims to provide a standardized evaluation framework that is both rigorous and efficient, allowing for the evaluation of different Foley synthesis systems. We received 17 submissions, and performed both objective and subjective evaluation to rank them according to three criteria: audio quality, fit-to-category, and diversity. Through this challenge, we hope to encourage active participation from the research community and advance the state-of-the-art in automatic Foley synthesis. In this paper, we provide a detailed overview of the Foley sound synthesis challenge, including task definition, dataset, baseline, evaluation scheme and criteria, challenge result, and discussion."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chunarkar2023" style="box-shadow: none">
<div class="panel-heading" id="headingChunarkar2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         STELIN-US: A Spatio-Temporally Linked Neighborhood Urban Sound Database
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Snehit Chunarkar, Bo-Hao Su and Chi-Chun Lee
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Department of Electrical Engineering, National Tsing Hua University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chunarkar2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Chunarkar_49.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Chunarkar2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chunarkar2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-44">
        P4-44
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chunarkar2023" class="panel-collapse collapse" id="collapse-Chunarkar2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automated acoustic understanding, e.g., sound event detection and acoustic scene recognition, is an important research direction enabling numerous modern technologies. Although there is a wealth of corpora, most, if not all, include acoustic samples of scenes/events in isolation without considering their inter-connectivity with locations nearby in a neighborhood. Within a connected neighborhood, the temporal continuity and regional limitation (sound-location dependency) at distinct locations creates non-iid acoustics samples at each site across spatial-temporal dimensions. To our best knowledge, none of the previous data sources takes on this particular angle. In this work, we present a novel dataset, Spatio-temporally Linked Neighborhood Urban Sound (STeLiN-US) database. The dataset is semi-synthesized, that is, each sample is generated by leveraging diverse sets of real urban sounds with crawled information of real-world user behaviors over time. This method helps create realistic large-scale dataset, and we further evaluate through perceptual listening test. This neighborhood-based data generation opens up novel opportunities to advance user-centered applications with automated acoustic understanding. For example, to develop real-world technology to model a user's speech data over a day, one can imagine utilizing this dataset as user's speech samples would modulate by diverse sources of acoustics surrounding linked across sites and temporally by natural behavior dynamics at each location over time.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chunarkar2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Chunarkar_49.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chunarkar2023label" class="modal fade" id="bibtex-Chunarkar2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChunarkar2023label">
        STELIN-US: A Spatio-Temporally Linked Neighborhood Urban Sound Database
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Chunarkar2023,
    author = "Chunarkar, Snehit and Su, Bo-Hao and Lee, Chi-Chun",
    title = "{STELIN-US}: A Spatio-Temporally Linked Neighborhood Urban Sound Database",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "21--25",
    abstract = "Automated acoustic understanding, e.g., sound event detection and acoustic scene recognition, is an important research direction enabling numerous modern technologies. Although there is a wealth of corpora, most, if not all, include acoustic samples of scenes/events in isolation without considering their inter-connectivity with locations nearby in a neighborhood. Within a connected neighborhood, the temporal continuity and regional limitation (sound-location dependency) at distinct locations creates non-iid acoustics samples at each site across spatial-temporal dimensions. To our best knowledge, none of the previous data sources takes on this particular angle. In this work, we present a novel dataset, Spatio-temporally Linked Neighborhood Urban Sound (STeLiN-US) database. The dataset is semi-synthesized, that is, each sample is generated by leveraging diverse sets of real urban sounds with crawled information of real-world user behaviors over time. This method helps create realistic large-scale dataset, and we further evaluate through perceptual listening test. This neighborhood-based data generation opens up novel opportunities to advance user-centered applications with automated acoustic understanding. For example, to develop real-world technology to model a user's speech data over a day, one can imagine utilizing this dataset as user's speech samples would modulate by diverse sources of acoustics surrounding linked across sites and temporally by natural behavior dynamics at each location over time."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chung2023" style="box-shadow: none">
<div class="panel-heading" id="headingChung2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Foley Sound Synthesis Based on Generative Adversarial Networks Using Oneself-Conditioned Contrastive Learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         HaeChun Chung, Yuna Lee and JaeHoon Jung
        </em>
</p>
<p class="text-muted">
<small>
<em>
          AI2XL Lab., KT Corporation, Institute of Convergence Technology
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chung2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Chung_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Chung2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chung2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-5">
        P1-5
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chung2023" class="panel-collapse collapse" id="collapse-Chung2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The creation of sound effects, such as foley sounds, for radio or film has traditionally relied on the expertise of skilled professionals. However, synthesizing these sounds automatically without expert intervention presents significant challenge. Particularly, when the available data is limited, this challenge becomes even more compounded. This often leads to a lack of diversity in the generated data. In this paper, we propose effective GAN frameworks, O2C-GAN and OC-SupConGAN for foley sound synthesis in this situation. The proposed frameworks use a new learning method, oneself-conditioned contrastive learning (OCC learning), to solve problems encountered in small dataset. The OCC learning is a method that aims to expand the diversity of data while preserving the inherent attributes of each class within the data. Experiments show that the proposed framework outperforms baseline schemes, ranking 2nd in DCASE2023-T7 Track B with a FAD score of 5.023 on the evaluation set.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chung2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Chung_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chung2023label" class="modal fade" id="bibtex-Chung2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChung2023label">
        Foley Sound Synthesis Based on Generative Adversarial Networks Using Oneself-Conditioned Contrastive Learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Chung2023,
    author = "Chung, HaeChun and Lee, Yuna and Jung, JaeHoon",
    title = "Foley Sound Synthesis Based on Generative Adversarial Networks Using Oneself-Conditioned Contrastive Learning",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "26--30",
    abstract = "The creation of sound effects, such as foley sounds, for radio or film has traditionally relied on the expertise of skilled professionals. However, synthesizing these sounds automatically without expert intervention presents significant challenge. Particularly, when the available data is limited, this challenge becomes even more compounded. This often leads to a lack of diversity in the generated data. In this paper, we propose effective GAN frameworks, O2C-GAN and OC-SupConGAN for foley sound synthesis in this situation. The proposed frameworks use a new learning method, oneself-conditioned contrastive learning (OCC learning), to solve problems encountered in small dataset. The OCC learning is a method that aims to expand the diversity of data while preserving the inherent attributes of each class within the data. Experiments show that the proposed framework outperforms baseline schemes, ranking 2nd in DCASE2023-T7 Track B with a FAD score of 5.023 on the evaluation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dohi2023" style="box-shadow: none">
<div class="panel-heading" id="headingDohi2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Kota Dohi<sup>1</sup>, Keisuke Imoto<sup>2</sup>, Noboru Harada<sup>3</sup>, Daisuke Niizumi<sup>3</sup>, Yuma Koizumi<sup>4</sup>, Tomoya Nishida<sup>1</sup>, Harsh Purohit<sup>1</sup>, Ryo Tanabe<sup>1</sup>, Takashi Endo<sup>1</sup> and Yohei Kawaguchi<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Hitachi, Ltd., <sup>2</sup>Doshisha University, <sup>3</sup>NTT Corporation, <sup>4</sup>Google
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dohi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Dohi_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Dohi2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dohi2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-11">
        P2-11
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dohi2023" class="panel-collapse collapse" id="collapse-Dohi2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 2: “First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring”. The main goal is to enable rapid deployment of ASD systems for new kinds of machines without the need for hyperparameter tuning. In the past ASD tasks, developed methods tuned hyperparameters for each machine type, as the development and evaluation datasets had the same machine types. However, collecting normal and anomalous data as the development dataset can be infeasible in practice. In 2023 Task 2, we focus on solving first-shot problem, which is the challenge of training a model on a completely novel machine type. Specifically, (i) each machine type has only one section (a subset of machine type) and (ii) machine types in the development and evaluation datasets are completely different. Analysis of 86 submissions from 23 teams revealed that keys to outperform baselines were: 1) sampling techniques for dealing with class imbalances across different domains and attributes, 2) generation of synthetic samples for robust detection, and 3) use of multiple large pre-trained models to extract meaningful embeddings for the anomaly detector.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dohi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Dohi_70.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dohi2023label" class="modal fade" id="bibtex-Dohi2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDohi2023label">
        Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dohi2023,
    author = "Dohi, Kota and Imoto, Keisuke and Harada, Noboru and Niizumi, Daisuke and Koizumi, Yuma and Nishida, Tomoya and Purohit, Harsh and Tanabe, Ryo and Endo, Takashi and Kawaguchi, Yohei",
    title = "Description and Discussion on {DCASE} 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "31--35",
    abstract = "We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 2: “First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring”. The main goal is to enable rapid deployment of ASD systems for new kinds of machines without the need for hyperparameter tuning. In the past ASD tasks, developed methods tuned hyperparameters for each machine type, as the development and evaluation datasets had the same machine types. However, collecting normal and anomalous data as the development dataset can be infeasible in practice. In 2023 Task 2, we focus on solving first-shot problem, which is the challenge of training a model on a completely novel machine type. Specifically, (i) each machine type has only one section (a subset of machine type) and (ii) machine types in the development and evaluation datasets are completely different. Analysis of 86 submissions from 23 teams revealed that keys to outperform baselines were: 1) sampling techniques for dealing with class imbalances across different domains and attributes, 2) generation of synthetic samples for robust detection, and 3) use of multiple large pre-trained models to extract meaningful embeddings for the anomaly detector."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ebbers2023" style="box-shadow: none">
<div class="panel-heading" id="headingEbbers2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Post-Processing Independent Evaluation of Sound Event Detection Systems
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Janek Ebbers<sup>1</sup>, Reinhold Haeb-Umbach<sup>1</sup> and Romain Serizel<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Paderborn University, <sup>2</sup>Université de Lorraine
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ebbers2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Ebbers_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Ebbers2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ebbers2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-38">
        P4-38
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ebbers2023" class="panel-collapse collapse" id="collapse-Ebbers2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Due to the high variation in the application requirements of sound event detection (SED) systems, it is not sufficient to evaluate systems only in a single operating mode. Therefore, the community recently adopted the polyphonic sound detection score (PSDS) as an evaluation metric, which is the normalized area under the PSD receiver operating characteristic (PSD-ROC). It summarizes the system performance over a range of operating modes resulting from varying the decision threshold that is used to translate the system output scores into a binary detection output. Hence, it provides a more complete picture of the overall system behavior and is less biased by specific threshold tuning. However, besides the decision threshold there is also the post-processing that can be changed to enter another operating mode. In this paper we propose the post-processing independent PSDS (piPSDS) as a generalization of the PSDS. Here, the post-processing independent PSD-ROC includes operating points from varying post-processings with varying decision thresholds. Thus, it summarizes even more operating modes of an SED system and allows for system comparison without the need of implementing a post-processing and without a bias due to different post-processings. While piPSDS can in principle combine different types of post-processing, we here, as a first step, present median filter independent PSDS (miPSDS) results for this year’s DCASE Challenge Task4a systems. Source code is publicly available in our sed_scores_eval package (https://github.com/fgnt/sed_scores_eval).
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ebbers2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Ebbers_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ebbers2023label" class="modal fade" id="bibtex-Ebbers2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexEbbers2023label">
        Post-Processing Independent Evaluation of Sound Event Detection Systems
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ebbers2023,
    author = "Ebbers, Janek and Haeb-Umbach, Reinhold and Serizel, Romain",
    title = "Post-Processing Independent Evaluation of Sound Event Detection Systems",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "36--40",
    abstract = "Due to the high variation in the application requirements of sound event detection (SED) systems, it is not sufficient to evaluate systems only in a single operating mode. Therefore, the community recently adopted the polyphonic sound detection score (PSDS) as an evaluation metric, which is the normalized area under the PSD receiver operating characteristic (PSD-ROC). It summarizes the system performance over a range of operating modes resulting from varying the decision threshold that is used to translate the system output scores into a binary detection output. Hence, it provides a more complete picture of the overall system behavior and is less biased by specific threshold tuning. However, besides the decision threshold there is also the post-processing that can be changed to enter another operating mode. In this paper we propose the post-processing independent PSDS (piPSDS) as a generalization of the PSDS. Here, the post-processing independent PSD-ROC includes operating points from varying post-processings with varying decision thresholds. Thus, it summarizes even more operating modes of an SED system and allows for system comparison without the need of implementing a post-processing and without a bias due to different post-processings. While piPSDS can in principle combine different types of post-processing, we here, as a first step, present median filter independent PSDS (miPSDS) results for this year’s DCASE Challenge Task4a systems. Source code is publicly available in our sed\_scores\_eval package (https://github.com/fgnt/sed\_scores\_eval)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Harada2023" style="box-shadow: none">
<div class="panel-heading" id="headingHarada2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         ToyADMOS2+: New Toyadmos Data and Benchmark Results of the First-Shot Anomalous Sound Event Detection Baseline
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Noboru Harada, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi and Masahiro Yasuda
        </em>
</p>
<p class="text-muted">
<small>
<em>
          NTT Corporation
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Harada2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Harada_74.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Harada2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Harada2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-53">
        P5-53
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Harada2023" class="panel-collapse collapse" id="collapse-Harada2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper introduces the newly recorded ToyADMOS dataset for the DCASE 2023 Challenge Task 2, First-shot anomalous sound detection for machine condition monitoring (DCASE2023T2). New machine types, such as ToyDrone, ToyNscale, Vacuum, and ToyTank, were newly recorded as a part of the Additional training and Evaluation datasets. This paper also shows benchmark results of the First-shot baseline implementation (with simple autoencoder and selective Mahalanobis modes) on the DCASE2023T2 Evaluation dataset and the previous DCASE Challenge Task 2 datasets in 2020, 2021, and 2022, compared with the baselines of those years.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Harada2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Harada_74.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Harada2023label" class="modal fade" id="bibtex-Harada2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHarada2023label">
        ToyADMOS2+: New Toyadmos Data and Benchmark Results of the First-Shot Anomalous Sound Event Detection Baseline
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Harada2023,
    author = "Harada, Noboru and Niizumi, Daisuke and Takeuchi, Daiki and Ohishi, Yasunori and Yasuda, Masahiro",
    title = "{ToyADMOS2+}: New Toyadmos Data and Benchmark Results of the First-Shot Anomalous Sound Event Detection Baseline",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "41--45",
    abstract = "This paper introduces the newly recorded ToyADMOS dataset for the DCASE 2023 Challenge Task 2, First-shot anomalous sound detection for machine condition monitoring (DCASE2023T2). New machine types, such as ToyDrone, ToyNscale, Vacuum, and ToyTank, were newly recorded as a part of the Additional training and Evaluation datasets. This paper also shows benchmark results of the First-shot baseline implementation (with simple autoencoder and selective Mahalanobis modes) on the DCASE2023T2 Evaluation dataset and the previous DCASE Challenge Task 2 datasets in 2020, 2021, and 2022, compared with the baselines of those years."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Harju2023" style="box-shadow: none">
<div class="panel-heading" id="headingHarju2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Manu Harju and Annamaria Mesaros
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Tampere University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Harju2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Harju_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Harju2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Harju2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-8">
        P1-8
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Harju2023" class="panel-collapse collapse" id="collapse-Harju2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Classification systems are normally trained by minimizing the cross-entropy between system outputs and reference labels, which makes the Kullback-Leibler divergence a natural choice for measuring how closely the system can follow the data. Precision and recall provide another perspective for measuring the performance of a classification system. Non-binary references can arise from various sources, and it is often beneficial to use the soft labels for training instead of the binarized data. However, the existing definitions for precision and recall require binary reference labels, and binarizing the data can cause erroneous interpretations. We present a novel method to calculate precision, recall and F-score without quantizing the data. The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels. To understand the behavior of the metrics we show simple example cases and an evaluation of different sound event detection models trained on real data with soft labels.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Harju2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Harju_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Harju2023label" class="modal fade" id="bibtex-Harju2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHarju2023label">
        Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Harju2023,
    author = "Harju, Manu and Mesaros, Annamaria",
    title = "Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "46--50",
    abstract = "Classification systems are normally trained by minimizing the cross-entropy between system outputs and reference labels, which makes the Kullback-Leibler divergence a natural choice for measuring how closely the system can follow the data. Precision and recall provide another perspective for measuring the performance of a classification system. Non-binary references can arise from various sources, and it is often beneficial to use the soft labels for training instead of the binarized data. However, the existing definitions for precision and recall require binary reference labels, and binarizing the data can cause erroneous interpretations. We present a novel method to calculate precision, recall and F-score without quantizing the data. The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels. To understand the behavior of the metrics we show simple example cases and an evaluation of different sound event detection models trained on real data with soft labels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hu2023" style="box-shadow: none">
<div class="panel-heading" id="headingHu2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         META-SELD: Meta-Learning for Fast Adaptation to the New Environment in Sound Event Localization and Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jinbo Hu<sup>1,2</sup>, Yin Cao<sup>3</sup>, Ming Wu<sup>1</sup>, Feiran Yang<sup>1</sup>, Ziying Yu<sup>1</sup>, Wenwu Wang<sup>4</sup>, Mark D. Plumbley<sup>4</sup> and Jun Yang<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Key Laboratory of Noise and Vibration Research, Institute of Acoustics, <sup>2</sup>University of Chinese Academy of Sciences, <sup>3</sup>Department of Intelligent Science, Xi’an Jiaotong Liverpool University, <sup>4</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hu2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Hu_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Hu2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hu2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-6">
        P1-6
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hu2023" class="panel-collapse collapse" id="collapse-Hu2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       For learning-based sound event localization and detection (SELD) methods, different acoustic environments in training and test sets may result in large performance differences in validation and evaluation stages. Different environments, such as different sizes of rooms, different reverberation times, and different background noise, may be the reason for a learning-based system to fail. On the other hand, acquiring annotated spatial sound event samples, which include onset and offset time stamps, class types of sound events, and direction-of-arrival (DOA) of sound sources is very expensive. In addition, deploying a SELD system in a new environment often poses challenges due to time-consuming training and fine-tuning processes. To address these issues, we propose Meta-SELD, which applies meta-learning methods to achieve fast adaptation to new environments. More specifically, based on Model Agnostic Meta-Learning (MAML), the proposed Meta-SELD aims at finding good meta-initialized parameters to adapt to new environments only with a small number of samples and parameter updating iterations. We can then adapt the meta-trained SELD model to unseen environments quickly. Our experiments compare fine-tuning methods from pre-trained SELD models with our Meta-SELD on Sony-TAU Realistic Spatial Soundscapes 2023 (STARSSS23) dataset. The evaluation results demonstrate the effectiveness of Meta-SELD adapting to new environments.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hu2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Hu_19.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hu2023label" class="modal fade" id="bibtex-Hu2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHu2023label">
        META-SELD: Meta-Learning for Fast Adaptation to the New Environment in Sound Event Localization and Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Hu2023,
    author = "Hu, Jinbo and Cao, Yin and Wu, Ming and Yang, Feiran and Yu, Ziying and Wang, Wenwu and Plumbley, Mark D. and Yang, Jun",
    title = "{META-SELD}: Meta-Learning for Fast Adaptation to the New Environment in Sound Event Localization and Detection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "51--55",
    abstract = "For learning-based sound event localization and detection (SELD) methods, different acoustic environments in training and test sets may result in large performance differences in validation and evaluation stages. Different environments, such as different sizes of rooms, different reverberation times, and different background noise, may be the reason for a learning-based system to fail. On the other hand, acquiring annotated spatial sound event samples, which include onset and offset time stamps, class types of sound events, and direction-of-arrival (DOA) of sound sources is very expensive. In addition, deploying a SELD system in a new environment often poses challenges due to time-consuming training and fine-tuning processes. To address these issues, we propose Meta-SELD, which applies meta-learning methods to achieve fast adaptation to new environments. More specifically, based on Model Agnostic Meta-Learning (MAML), the proposed Meta-SELD aims at finding good meta-initialized parameters to adapt to new environments only with a small number of samples and parameter updating iterations. We can then adapt the meta-trained SELD model to unseen environments quickly. Our experiments compare fine-tuning methods from pre-trained SELD models with our Meta-SELD on Sony-TAU Realistic Spatial Soundscapes 2023 (STARSSS23) dataset. The evaluation results demonstrate the effectiveness of Meta-SELD adapting to new environments."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ick2023" style="box-shadow: none">
<div class="panel-heading" id="headingIck2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Christopher Ick and Brian McFee
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Music and Audio Research Laboratory, New York University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ick2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Ick_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Ick2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ick2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-35">
        P4-35
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ick2023" class="panel-collapse collapse" id="collapse-Ick2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       As deeper and more complex models are developed for the task of sound event localization and detection (SELD), the demand for annotated spatial audio data continues to increase. Annotating field recordings with 360^{\circ} video takes many hours from trained annotators, while recording events within motion-tracked laboratories are bounded by cost and expertise. Because of this, localization models rely on a relatively limited amount of spatial audio data in the form of spatial room impulse response (SRIR) datasets, which limits the progress of increasingly deep neural network based approaches. In this work, we demonstrate that simulated geometrical acoustics can provide an appealing solution to this problem. We use simulated geometrical acoustics to generate a novel SRIR dataset that can train a SELD model to provide similar performance to that of a real SRIR dataset. Furthermore, we demonstrate using simulated data to augment existing datasets, improving on benchmarks set by state of the art SELD models. We explore the potential and limitations of geometric acoustic simulation for localization and event detection. We also propose further studies to verify the limitations of this method, as well as further methods to generate synthetic data for SELD tasks without the need to record more data.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ick2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Ick_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ick2023label" class="modal fade" id="bibtex-Ick2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexIck2023label">
        Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ick2023,
    author = "Ick, Christopher and McFee, Brian",
    title = "Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "56--60",
    abstract = "As deeper and more complex models are developed for the task of sound event localization and detection (SELD), the demand for annotated spatial audio data continues to increase. Annotating field recordings with 360^{\circ} video takes many hours from trained annotators, while recording events within motion-tracked laboratories are bounded by cost and expertise. Because of this, localization models rely on a relatively limited amount of spatial audio data in the form of spatial room impulse response (SRIR) datasets, which limits the progress of increasingly deep neural network based approaches. In this work, we demonstrate that simulated geometrical acoustics can provide an appealing solution to this problem. We use simulated geometrical acoustics to generate a novel SRIR dataset that can train a SELD model to provide similar performance to that of a real SRIR dataset. Furthermore, we demonstrate using simulated data to augment existing datasets, improving on benchmarks set by state of the art SELD models. We explore the potential and limitations of geometric acoustic simulation for localization and event detection. We also propose further studies to verify the limitations of this method, as well as further methods to generate synthetic data for SELD tasks without the need to record more data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jacobs2023" style="box-shadow: none">
<div class="panel-heading" id="headingJacobs2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Speech Obfuscation in Mel Spectra That Allows for Centralised Annotation and Classification of Sound Events
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Michiel Jacobs<sup>1,2,3</sup>, Lode Vuegen<sup>1,2,3</sup>, Suraj Khan<sup>1,2,3</sup> and Peter Karsmakers<sup>1,2,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Deptartment of Computer Science, KU Leuven, <sup>2</sup>Flanders Make @ KU Leuven, <sup>3</sup>Leuven.AI - KU Leuven Institute for AI
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jacobs2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Jacobs_42.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Jacobs2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jacobs2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-36">
        P4-36
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jacobs2023" class="panel-collapse collapse" id="collapse-Jacobs2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Nowadays, computerised Sound Event Classification (SEC) aids in several applications, e.g. monitoring domestic events in smart homes. SEC model development typically requires data collected from a diverse set of remote locations. However, this data could disclose sensitive information about uttered speech that might have been present during the acquisition. This makes automated data acquisition at remote locations difficult in practice. In this work, three data preprocessing techniques are investigated that obstruct recognising semantics in speech, but retain the required information in the data for annotating sound events and SEC model development. At the remote location, the data are first preprocessed before transferring to a central place. At the central location, speech should not be interpretable anymore, while still having the opportunity to annotate data with relevant sound event labels. For this purpose, starting from a log-mel representation of the sound signals, three speech obfuscation techniques are assessed: 1) calculating a moving average of the log-mel spectra, 2) sampling a few of the most energetic log-mel spectra and 3) shredding the log-mel spectra. Both intelligibility and SEC experiments were carried out.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jacobs2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Jacobs_42.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jacobs2023label" class="modal fade" id="bibtex-Jacobs2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJacobs2023label">
        Speech Obfuscation in Mel Spectra That Allows for Centralised Annotation and Classification of Sound Events
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jacobs2023,
    author = "Jacobs, Michiel and Vuegen, Lode and Khan, Suraj and Karsmakers, Peter",
    title = "Speech Obfuscation in Mel Spectra That Allows for Centralised Annotation and Classification of Sound Events",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "61--65",
    abstract = "Nowadays, computerised Sound Event Classification (SEC) aids in several applications, e.g. monitoring domestic events in smart homes. SEC model development typically requires data collected from a diverse set of remote locations. However, this data could disclose sensitive information about uttered speech that might have been present during the acquisition. This makes automated data acquisition at remote locations difficult in practice. In this work, three data preprocessing techniques are investigated that obstruct recognising semantics in speech, but retain the required information in the data for annotating sound events and SEC model development. At the remote location, the data are first preprocessed before transferring to a central place. At the central location, speech should not be interpretable anymore, while still having the opportunity to annotate data with relevant sound event labels. For this purpose, starting from a log-mel representation of the sound signals, three speech obfuscation techniques are assessed: 1) calculating a moving average of the log-mel spectra, 2) sampling a few of the most energetic log-mel spectra and 3) shredding the log-mel spectra. Both intelligibility and SEC experiments were carried out."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kang2023" style="box-shadow: none">
<div class="panel-heading" id="headingKang2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         FALL-E: A Foley Sound Synthesis Model and Strategies
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Minsung Kang, Sangshin Oh, Hyeongi Moon, Kyungyun Lee and Ben Sangbae Chon
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Gaudio Lab, Inc.
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kang2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kang_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Kang2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kang2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-2">
        P1-2
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kang2023" class="panel-collapse collapse" id="collapse-Kang2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper introduces FALL-E, a foley synthesis system and its training/inference strategies. The FALL-E model employs a cascaded approach comprising low-resolution spectrogram generation, spectrogram super-resolution, and a vocoder. We trained every sound-related model from scratch using our extensive datasets, and utilized a pre-trained language model. We conditioned the model with dataset-specific texts, enabling it to learn sound quality and recording environment based on text input. Moreover, we leveraged external language models to improve text descriptions of our datasets and performed prompt engineering for quality, coherence, and diversity. FALL-E was evaluated by an objective measure as well as listening tests in the DCASE 2023 challenge Task 7. The submission achieved the second place on average, while achieving the best score for diversity, second place for audio quality, and third place for class fitness.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kang2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kang_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kang2023label" class="modal fade" id="bibtex-Kang2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKang2023label">
        FALL-E: A Foley Sound Synthesis Model and Strategies
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kang2023,
    author = "Kang, Minsung and Oh, Sangshin and Moon, Hyeongi and Lee, Kyungyun and Chon, Ben Sangbae",
    title = "{FALL-E}: A Foley Sound Synthesis Model and Strategies",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "66--70",
    abstract = "This paper introduces FALL-E, a foley synthesis system and its training/inference strategies. The FALL-E model employs a cascaded approach comprising low-resolution spectrogram generation, spectrogram super-resolution, and a vocoder. We trained every sound-related model from scratch using our extensive datasets, and utilized a pre-trained language model. We conditioned the model with dataset-specific texts, enabling it to learn sound quality and recording environment based on text input. Moreover, we leveraged external language models to improve text descriptions of our datasets and performed prompt engineering for quality, coherence, and diversity. FALL-E was evaluated by an objective measure as well as listening tests in the DCASE 2023 challenge Task 7. The submission achieved the second place on average, while achieving the best score for diversity, second place for audio quality, and third place for class fitness."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2023" style="box-shadow: none">
<div class="panel-heading" id="headingKim2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Label Filtering-Based Self-Learning for Sound Event Detection Using Frequency Dynamic Convolution with Large Kernel Attention
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ji Won Kim<sup>1</sup>, Sang Won Son<sup>1</sup>, Yoonah Song<sup>1</sup>, Hong Kook Kim<sup>1,2</sup>, Il Hoon Song<sup>3</sup> and Jeong Eun Lim<sup>3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>AI Graduate School, Gwangju Institute of Science and Technology, <sup>2</sup>School of EECS, Gwangju Institute of Science and Technology, <sup>3</sup>AI Lab., Hanwha Vision
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kim_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Kim2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-30">
        P3-30
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2023" class="panel-collapse collapse" id="collapse-Kim2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes a convolutional recurrent neural network (CRNN)-based sound event detection (SED) model. The proposed model utilizes frequency dynamic convolution (FDY) with a large kernel attention (LKA) for convolution operations within the CRNN. This is designed to effectively capture time-frequency patterns and long-term dependencies for non-stationary audio events. In addition, we concatenate a pre-trained bidirectional encoder representation from audio transformers (BEATs) embedding with the output of FDY–LKA. This provides the FDY-based feature maps with semantic information. Given the limited labeled data condition of the DCASE Challenge dataset, we first employ the mean-teacher-based semi-supervised learning. Then, we propose label filtering-based self-learning for audio event data selection, when their pseudo labels predicted from the mean-teacher model are strong correlated with given weakly labels. This strategy applies weakly labeled and unlabeled data, and then extends to the AudioSet. We evaluate its performance of the proposed SED model on DCASE 2023 Challenge Task 4A, measuring the F1-score and polyphonic sound detection scores, namely PSDS1 and PSDS2. The results indicate that the proposed CRNN-based model with FDY–LKA improves the F1-score, PSDS1, and PSDS2 in comparison to the baseline for DCASE 2023 Challenge Task 4A. When we apply the BEATs embedding via average pooling to both the baseline and the proposed model, we find that the performance of the proposed model significantly outperforms the baseline, with an F1-score of 6.2%, a PSDS1 score of 0.055, and a PSDS2 score of 0.021. Consequently, our model is ranked first in the DCASE 2023 Challenge Task 4A evaluation for a single model track, and second for an ensemble model.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kim_34.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2023label" class="modal fade" id="bibtex-Kim2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2023label">
        Label Filtering-Based Self-Learning for Sound Event Detection Using Frequency Dynamic Convolution with Large Kernel Attention
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kim2023,
    author = "Kim, Ji Won and Son, Sang Won and Song, Yoonah and Kim, Hong Kook and Song, Il Hoon and Lim, Jeong Eun",
    title = "Label Filtering-Based Self-Learning for Sound Event Detection Using Frequency Dynamic Convolution with Large Kernel Attention",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "71--75",
    abstract = "This paper proposes a convolutional recurrent neural network (CRNN)-based sound event detection (SED) model. The proposed model utilizes frequency dynamic convolution (FDY) with a large kernel attention (LKA) for convolution operations within the CRNN. This is designed to effectively capture time-frequency patterns and long-term dependencies for non-stationary audio events. In addition, we concatenate a pre-trained bidirectional encoder representation from audio transformers (BEATs) embedding with the output of FDY–LKA. This provides the FDY-based feature maps with semantic information. Given the limited labeled data condition of the DCASE Challenge dataset, we first employ the mean-teacher-based semi-supervised learning. Then, we propose label filtering-based self-learning for audio event data selection, when their pseudo labels predicted from the mean-teacher model are strong correlated with given weakly labels. This strategy applies weakly labeled and unlabeled data, and then extends to the AudioSet. We evaluate its performance of the proposed SED model on DCASE 2023 Challenge Task 4A, measuring the F1-score and polyphonic sound detection scores, namely PSDS1 and PSDS2. The results indicate that the proposed CRNN-based model with FDY–LKA improves the F1-score, PSDS1, and PSDS2 in comparison to the baseline for DCASE 2023 Challenge Task 4A. When we apply the BEATs embedding via average pooling to both the baseline and the proposed model, we find that the performance of the proposed model significantly outperforms the baseline, with an F1-score of 6.2\%, a PSDS1 score of 0.055, and a PSDS2 score of 0.021. Consequently, our model is ranked first in the DCASE 2023 Challenge Task 4A evaluation for a single model track, and second for an ensemble model."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2023a" style="box-shadow: none">
<div class="panel-heading" id="headingKim2023a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Improving Automated Audio Captioning Fluency Through Data Augmentation and Ensemble Selection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jaewon Kim, Yoon-Ah Park, Jae-Heung Cho and Joon-Hyuk Chang
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Department of Electronic Engineering, Hanyang University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2023a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kim_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Kim2023a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2023a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-47">
        P5-47
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2023a" class="panel-collapse collapse" id="collapse-Kim2023a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automated audio captioning is a task of generating descriptions corresponding to audio clips. The model typically consists of a pre-training and fine-tuning process, with additional utilization of reinforcement learning techniques. While reinforcement learning enhances the evaluation metrics for captions, it has the drawback of potentially lowering the quality of the captions, such as incomplete sentence structures or repetitive words. In this study, we propose an ensemble selection technique that combines models before and after reinforcement learning to improve evaluation metrics while maintaining caption quality. Furthermore, we apply several data augmentation techniques to complement the characteristics of WavCaps, which predominantly consists of single events, and enhance the model's generalization abilities. In particular, proposed approaches can reach impressive scores both an existing metric SPIDE_{r}, and a new fluency metric SPIDE_{r}-FL, 0.344 and 0.315, respectively. This resulted in a 2nd place ranking in DCASE 2023 task 6a, while the baseline system achieved SPIDE_{r} of 0.271 and SPIDE_{r}-FL of 0.264.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2023a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kim_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2023alabel" class="modal fade" id="bibtex-Kim2023a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2023alabel">
        Improving Automated Audio Captioning Fluency Through Data Augmentation and Ensemble Selection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kim2023a,
    author = "Kim, Jaewon and Park, Yoon-Ah and Cho, Jae-Heung and Chang, Joon-Hyuk",
    title = "Improving Automated Audio Captioning Fluency Through Data Augmentation and Ensemble Selection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "76--80",
    abstract = "Automated audio captioning is a task of generating descriptions corresponding to audio clips. The model typically consists of a pre-training and fine-tuning process, with additional utilization of reinforcement learning techniques. While reinforcement learning enhances the evaluation metrics for captions, it has the drawback of potentially lowering the quality of the captions, such as incomplete sentence structures or repetitive words. In this study, we propose an ensemble selection technique that combines models before and after reinforcement learning to improve evaluation metrics while maintaining caption quality. Furthermore, we apply several data augmentation techniques to complement the characteristics of WavCaps, which predominantly consists of single events, and enhance the model's generalization abilities. In particular, proposed approaches can reach impressive scores both an existing metric SPIDE\_{r}, and a new fluency metric SPIDE\_{r}-FL, 0.344 and 0.315, respectively. This resulted in a 2nd place ranking in DCASE 2023 task 6a, while the baseline system achieved SPIDE\_{r} of 0.271 and SPIDE\_{r}-FL of 0.264."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kouzelis2023" style="box-shadow: none">
<div class="panel-heading" id="headingKouzelis2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Weakly-Supervised Automated Audio Captioning via Text Only Training
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Theodoros Kouzelis and Vassilis Katsouros
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Institute for Language and Speech Processing, Athena Research Center
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kouzelis2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kouzelis_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Kouzelis2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kouzelis2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-31">
        P3-31
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kouzelis2023" class="panel-collapse collapse" id="collapse-Kouzelis2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to achieve up to ~83% of the performance attained by fully supervised approaches trained on paired target data.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kouzelis2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Kouzelis_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kouzelis2023label" class="modal fade" id="bibtex-Kouzelis2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKouzelis2023label">
        Weakly-Supervised Automated Audio Captioning via Text Only Training
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kouzelis2023,
    author = "Kouzelis, Theodoros and Katsouros, Vassilis",
    title = "Weakly-Supervised Automated Audio Captioning via Text Only Training",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "81--85",
    abstract = "In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to achieve up to \textasciitilde 83\% of the performance attained by fully supervised approaches trained on paired target data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Labba2023" style="box-shadow: none">
<div class="panel-heading" id="headingLabba2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Killing Two Birds with One Stone: Can an Audio Captioning System Also Be Used for Audio-Text Retrieval?
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Étienne Labbé<sup>1</sup>, Thomas Pellegrini<sup>1,2</sup> and Julien Pinquier<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IRIT, Université Paul Sabatier, <sup>2</sup>Artificial and Natural Intelligence Toulouse Institute (ANITI)
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Labba2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Labbé_43.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Labba2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Labba2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-40">
        P4-40
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Labba2023" class="panel-collapse collapse" id="collapse-Labba2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Automated Audio Captioning (AAC) aims to develop systems capable of describing an audio recording using a textual sentence. In contrast, Audio-Text Retrieval (ATR) systems seek to find the best matching audio recording(s) for a given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks require different types of systems: AAC employs a sequence-to-sequence model, while ATR utilizes a ranking model that compares audio and text representations within a shared projection subspace. However, this work investigates the relationship between AAC and ATR by exploring the ATR capabilities of an unmodified AAC system, without fine-tuning for the new task. Our AAC system consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio tagging, and a transformer decoder responsible for generating sentences. For AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss values obtained for any audio/caption pair. Experimental results on the Clotho and AudioCaps datasets demonstrate decent recall values using this simple approach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for AudioCaps, which is above the current state-of-the-art method without external data. Interestingly, we observe that normalizing the loss values was necessary for Audio-to-Text retrieval.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Labba2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Labbé_43.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Labba2023label" class="modal fade" id="bibtex-Labba2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLabba2023label">
        Killing Two Birds with One Stone: Can an Audio Captioning System Also Be Used for Audio-Text Retrieval?
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Labba2023,
    author = "Labbé, Étienne and Pellegrini, Thomas and Pinquier, Julien",
    title = "Killing Two Birds with One Stone: Can an Audio Captioning System Also Be Used for Audio-Text Retrieval?",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "86--90",
    abstract = "Automated Audio Captioning (AAC) aims to develop systems capable of describing an audio recording using a textual sentence. In contrast, Audio-Text Retrieval (ATR) systems seek to find the best matching audio recording(s) for a given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks require different types of systems: AAC employs a sequence-to-sequence model, while ATR utilizes a ranking model that compares audio and text representations within a shared projection subspace. However, this work investigates the relationship between AAC and ATR by exploring the ATR capabilities of an unmodified AAC system, without fine-tuning for the new task. Our AAC system consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio tagging, and a transformer decoder responsible for generating sentences. For AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss values obtained for any audio/caption pair. Experimental results on the Clotho and AudioCaps datasets demonstrate decent recall values using this simple approach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for AudioCaps, which is above the current state-of-the-art method without external data. Interestingly, we observe that normalizing the loss values was necessary for Audio-to-Text retrieval."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2023" style="box-shadow: none">
<div class="panel-heading" id="headingLee2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Few Shot Bioacoustic Detection Boosting with Finetuning Strategy Using Negative-Based Prototypical Learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yuna Lee, HaeChun Chung and JaeHoon Jung
        </em>
</p>
<p class="text-muted">
<small>
<em>
          AI2XL Lab., KT Corporation, Institute of Convergence Technology
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Lee_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lee2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-21">
        P2-21
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2023" class="panel-collapse collapse" id="collapse-Lee2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event detection involves the identification and temporal localization of sound events within audio recordings. Bioacoustic sound event detection specifically targets animal vocalizations, which necessitate substantial time and resources for manual annotation of temporal boundaries. This paper aims to address the challenges associated with bioacoustic sound event detection by proposing a novel prototypical learning framework. Our approach fuses contrastive learning and prototypical learning to use the limited amount of dataset at its utmost. Further, our framework leverages finetuning strategy with a novel loss function to develop a robust framework. Experimental results on a benchmark dataset demonstrate the effectiveness of our proposed method in accurately detecting and localizing bioacoustic sound events, improving the F1 score from 29.59% to 83.08%.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Lee_60.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2023label" class="modal fade" id="bibtex-Lee2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2023label">
        Few Shot Bioacoustic Detection Boosting with Finetuning Strategy Using Negative-Based Prototypical Learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2023,
    author = "Lee, Yuna and Chung, HaeChun and Jung, JaeHoon",
    title = "Few Shot Bioacoustic Detection Boosting with Finetuning Strategy Using Negative-Based Prototypical Learning",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "91--95",
    abstract = "Sound event detection involves the identification and temporal localization of sound events within audio recordings. Bioacoustic sound event detection specifically targets animal vocalizations, which necessitate substantial time and resources for manual annotation of temporal boundaries. This paper aims to address the challenges associated with bioacoustic sound event detection by proposing a novel prototypical learning framework. Our approach fuses contrastive learning and prototypical learning to use the limited amount of dataset at its utmost. Further, our framework leverages finetuning strategy with a novel loss function to develop a robust framework. Experimental results on a benchmark dataset demonstrate the effectiveness of our proposed method in accurately detecting and localizing bioacoustic sound events, improving the F1 score from 29.59\% to 83.08\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liu2023" style="box-shadow: none">
<div class="panel-heading" id="headingLiu2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Masked Modeling Duo Vision Transformer with Multi-Layer Feature Fusion on Respiratory Sound Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Boxin Liu<sup>1</sup>, Shiqi Zhang<sup>1</sup>, Daiki Takeuchi<sup>2</sup>, Noboru Harada<sup>2</sup> and Shoji Makino<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Waseda University, <sup>2</sup>NTT Corporation
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liu2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Liu_17.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Liu2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liu2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-41">
        P4-41
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liu2023" class="panel-collapse collapse" id="collapse-Liu2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Respiratory sounds are significant relevant indicators for respi- ratory health and conditions. Classifying the respiratory sounds of patients can assist doctors’ diagnosis of lung diseases. For this pur- pose, many deep learning-based automatic analysis methods have been developed. However, it is still challenging due to the lim- ited medical sound datasets. In this study, we apply a pre-trained Vision Transformer (ViT) based model from the Masked Model- ing Duo (M2D) framework for this task. While the M2D ViT pre- trained model provides effective features, we think combining fea- tures from different layers can improve the performance in this task. We propose a multi-layer feature fusion method using learnable layer-wise weights and validate its effectiveness in experiments and an analysis of pre-trained model layers. Our approach achieves the best ICBHI score of 60.68, 2.39 higher than the previous state-of- the-art method.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liu2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Liu_17.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liu2023label" class="modal fade" id="bibtex-Liu2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiu2023label">
        Masked Modeling Duo Vision Transformer with Multi-Layer Feature Fusion on Respiratory Sound Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Liu2023,
    author = "Liu, Boxin and Zhang, Shiqi and Takeuchi, Daiki and Harada, Noboru and Makino, Shoji",
    title = "Masked Modeling Duo Vision Transformer with Multi-Layer Feature Fusion on Respiratory Sound Classification",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "96--100",
    abstract = "Respiratory sounds are significant relevant indicators for respi- ratory health and conditions. Classifying the respiratory sounds of patients can assist doctors’ diagnosis of lung diseases. For this pur- pose, many deep learning-based automatic analysis methods have been developed. However, it is still challenging due to the lim- ited medical sound datasets. In this study, we apply a pre-trained Vision Transformer (ViT) based model from the Masked Model- ing Duo (M2D) framework for this task. While the M2D ViT pre- trained model provides effective features, we think combining fea- tures from different layers can improve the performance in this task. We propose a multi-layer feature fusion method using learnable layer-wise weights and validate its effectiveness in experiments and an analysis of pre-trained model layers. Our approach achieves the best ICBHI score of 60.68, 2.39 higher than the previous state-of- the-art method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lostanlen2023" style="box-shadow: none">
<div class="panel-heading" id="headingLostanlen2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Efficient Evaluation Algorithms for Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Vincent Lostanlen<sup>1</sup> and Brian McFee<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Ecole Centrale Nantes, Nantes Université, <sup>2</sup>Music and Audio Research Lab &amp; Center for Data Science, New York University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lostanlen2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Lostanlen_28.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lostanlen2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lostanlen2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-45">
        P5-45
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lostanlen2023" class="panel-collapse collapse" id="collapse-Lostanlen2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The prediction of a sound event detection (SED) system may be represented on a timeline by intervals whose bounds correspond to onset and offset respectively. In this context, SED evaluation requires to find all non-empty intersections between predicted and reference intervals. Denoting by M and N the number of predicted events and reference events, the time complexity of exhaustive search is O(MN). This is particularly inefficient when the acoustic scene of interest contains many events (typically above 10^3) or when the detection threshold is low. Our article presents an algorithm for pairwise intersection of intervals by performing binary search within sorted onset and offset times. Computational benchmarks on the BirdVox-full-night dataset confirms that our algorithm is significantly faster than exhaustive search. Moreover, we explain how to use this list of intersecting prediction--reference pairs for the purpose of SED evaluation: the Hopcroft-Karp algorithm guarantees an optimal bipartite matching in time O((M+N)^{3/2}) in the best case (all events are pairwise disjoint) and O((M+N)^{5/2}) in the worst case (all events overlap with each other). The solution found by Hopcroft-Karp unambiguously defines a number of true positives, false positives, and false negatives; and ultimately, information retrieval metrics such as precision, recall, and F-score.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lostanlen2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Lostanlen_28.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lostanlen2023label" class="modal fade" id="bibtex-Lostanlen2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLostanlen2023label">
        Efficient Evaluation Algorithms for Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lostanlen2023,
    author = "Lostanlen, Vincent and McFee, Brian",
    title = "Efficient Evaluation Algorithms for Sound Event Detection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "101--105",
    abstract = "The prediction of a sound event detection (SED) system may be represented on a timeline by intervals whose bounds correspond to onset and offset respectively. In this context, SED evaluation requires to find all non-empty intersections between predicted and reference intervals. Denoting by M and N the number of predicted events and reference events, the time complexity of exhaustive search is O(MN). This is particularly inefficient when the acoustic scene of interest contains many events (typically above 10^3) or when the detection threshold is low. Our article presents an algorithm for pairwise intersection of intervals by performing binary search within sorted onset and offset times. Computational benchmarks on the BirdVox-full-night dataset confirms that our algorithm is significantly faster than exhaustive search. Moreover, we explain how to use this list of intersecting prediction--reference pairs for the purpose of SED evaluation: the Hopcroft-Karp algorithm guarantees an optimal bipartite matching in time O((M+N)^{3/2}) in the best case (all events are pairwise disjoint) and O((M+N)^{5/2}) in the worst case (all events overlap with each other). The solution found by Hopcroft-Karp unambiguously defines a number of true positives, false positives, and false negatives; and ultimately, information retrieval metrics such as precision, recall, and F-score."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Martin-Morato2023" style="box-shadow: none">
<div class="panel-heading" id="headingMartin-Morato2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Aggregate or Separate: Learning From Multi-Annotator Noisy Labels for Best Classification Performance
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Irene Martín-Morató, Paul Ahokas and Annamaria Mesaros
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Computing Science, Tampere University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Martin-Morato2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Martín-Morató_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Martin-Morato2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Martin-Morato2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-28">
        P3-28
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Martin-Morato2023" class="panel-collapse collapse" id="collapse-Martin-Morato2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       While there is the saying of two heads are better than one, having multiple opinions brings the problem of finding a common ground. For data, multiple annotator opinions are usually aggregated into a single set of labels, regarded as the ground truth. With this ground truth, classification models can be trained in a supervised way to learn the annotated data categories. Finding a suitable aggregation for multiple annotator opinions is the topic of research in many domains. In this work we investigate the use of raw data obtained from multiple annotators with various levels of reliability, to train a model for audio classification. The model sees all the individual annotator opinions and learns the categories without the need of aggregating the information. The results show that using a fully-connected layer that models individual annotators, it is possible to leverage the data distribution and learn to classify sounds without the need for aggregation of labels.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Martin-Morato2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Martín-Morató_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Martin-Morato2023label" class="modal fade" id="bibtex-Martin-Morato2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMartin-Morato2023label">
        Aggregate or Separate: Learning From Multi-Annotator Noisy Labels for Best Classification Performance
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Martin-Morato2023,
    author = "Martín-Morató, Irene and Ahokas, Paul and Mesaros, Annamaria",
    title = "Aggregate or Separate: Learning From Multi-Annotator Noisy Labels for Best Classification Performance",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "106--110",
    abstract = "While there is the saying of two heads are better than one, having multiple opinions brings the problem of finding a common ground. For data, multiple annotator opinions are usually aggregated into a single set of labels, regarded as the ground truth. With this ground truth, classification models can be trained in a supervised way to learn the annotated data categories. Finding a suitable aggregation for multiple annotator opinions is the topic of research in many domains. In this work we investigate the use of raw data obtained from multiple annotators with various levels of reliability, to train a model for audio classification. The model sees all the individual annotator opinions and learns the categories without the need of aggregating the information. The results show that using a fully-connected layer that models individual annotators, it is possible to leverage the data distribution and learn to classify sounds without the need for aggregation of labels."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Meire2023" style="box-shadow: none">
<div class="panel-heading" id="headingMeire2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Active Learning in Sound-Based Bearing Fault Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Maarten Meire<sup>1,2,3</sup>, Jeroen Zegers<sup>4</sup> and Peter Karsmakers<sup>1,2,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Department of Computer Science, ADVISE-DTAI, KU Leuven, <sup>2</sup>Leuven.AI - KU Leuven institute for AI, <sup>3</sup>Flanders Make @ KU Leuven, <sup>4</sup>Flanders Make vzw, CoreLab MotionS
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Meire2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Meire_37.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Meire2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Meire2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-46">
        P5-46
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Meire2023" class="panel-collapse collapse" id="collapse-Meire2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       An important aspect in machine monitoring, specifically bearings in rotating machinery, is being able to provide an accurate Sound Event Classification (SEC) model for fault detection. Various approaches have shown good results. However, a lot of these approaches are based on deep learning (DL), which requires large amounts of data. In real-life cases acquiring vibration data from bearings can be difficult since making direct contact to the bearing is not trivial. When using a microphone, which does not need direct contact, the capturing of data can be made easier. However, to supervise the learning process annotation of data is required which is a time (and cost) intensive process. Active learning (AL) methods have been developed to reduce this time by only annotating examples that would be of most use during the learning process. In this work a novel data set, that contains acoustic data from accelerated life time tests for bearings, is used to investigate the performance of two AL methods in terms of classification accuracy and number of additionally selected and annotated examples.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Meire2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Meire_37.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Meire2023label" class="modal fade" id="bibtex-Meire2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMeire2023label">
        Active Learning in Sound-Based Bearing Fault Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Meire2023,
    author = "Meire, Maarten and Zegers, Jeroen and Karsmakers, Peter",
    title = "Active Learning in Sound-Based Bearing Fault Detection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "111--115",
    abstract = "An important aspect in machine monitoring, specifically bearings in rotating machinery, is being able to provide an accurate Sound Event Classification (SEC) model for fault detection. Various approaches have shown good results. However, a lot of these approaches are based on deep learning (DL), which requires large amounts of data. In real-life cases acquiring vibration data from bearings can be difficult since making direct contact to the bearing is not trivial. When using a microphone, which does not need direct contact, the capturing of data can be made easier. However, to supervise the learning process annotation of data is required which is a time (and cost) intensive process. Active learning (AL) methods have been developed to reduce this time by only annotating examples that would be of most use during the learning process. In this work a novel data set, that contains acoustic data from accelerated life time tests for bearings, is used to investigate the performance of two AL methods in terms of classification accuracy and number of additionally selected and annotated examples."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Min2023" style="box-shadow: none">
<div class="panel-heading" id="headingMin2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Auditory Neural Response Inspired Sound Event Detection Based on Spectro-Temporal Receptive Field
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Deokki Min, Hyeonuk Nam and Yong-Hwa Park
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Korea Advanced Institute of Science and Technology (KAIST)
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Min2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Min_41.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Min2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Min2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-32">
        P3-32
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Min2023" class="panel-collapse collapse" id="collapse-Min2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event detection (SED) is one of tasks to automate function by human auditory system which listens and understands auditory scenes. Therefore, we were inspired to make SED recognize sound events in the way human auditory system does. Spectro-temporal receptive field (STRF), an approach to describe the relationship between perceived sound at ear and transformed neural response in the auditory cortex, is closely related to recognition of sound. In this work, we utilized STRF as a kernel of the first convolutional layer in SED model to extract neural response from input sound to make SED model similar to human auditory system. In addition, we constructed two-branched SED model named as Two Branch STRFNet (TB-STRFNet) composed of STRF branch and baseline branch. While STRF branch extracts sound event information from auditory neural response, baseline branch extracts sound event information directly from the mel spectrogram just as conventional SED models do. TB-STRFNet outperformed the DCASE baseline by 4.3% in terms of threshold-independent macro F1 score, achieving 4th rank in DCASE Challenge 2023 Task 4b. We further improved TB-STRFNet by applying frequency dynamic convolution (FDY-Conv) which also leveraged domain knowledge on acoustics. As a result, two branch model applied with FDYConv on both branches outperformed the DCASE baseline by 6.2% in terms of the same metric.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Min2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Min_41.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Min2023label" class="modal fade" id="bibtex-Min2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMin2023label">
        Auditory Neural Response Inspired Sound Event Detection Based on Spectro-Temporal Receptive Field
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Min2023,
    author = "Min, Deokki and Nam, Hyeonuk and Park, Yong-Hwa",
    title = "Auditory Neural Response Inspired Sound Event Detection Based on Spectro-Temporal Receptive Field",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "116--120",
    abstract = "Sound event detection (SED) is one of tasks to automate function by human auditory system which listens and understands auditory scenes. Therefore, we were inspired to make SED recognize sound events in the way human auditory system does. Spectro-temporal receptive field (STRF), an approach to describe the relationship between perceived sound at ear and transformed neural response in the auditory cortex, is closely related to recognition of sound. In this work, we utilized STRF as a kernel of the first convolutional layer in SED model to extract neural response from input sound to make SED model similar to human auditory system. In addition, we constructed two-branched SED model named as Two Branch STRFNet (TB-STRFNet) composed of STRF branch and baseline branch. While STRF branch extracts sound event information from auditory neural response, baseline branch extracts sound event information directly from the mel spectrogram just as conventional SED models do. TB-STRFNet outperformed the DCASE baseline by 4.3\% in terms of threshold-independent macro F1 score, achieving 4th rank in DCASE Challenge 2023 Task 4b. We further improved TB-STRFNet by applying frequency dynamic convolution (FDY-Conv) which also leveraged domain knowledge on acoustics. As a result, two branch model applied with FDYConv on both branches outperformed the DCASE baseline by 6.2\% in terms of the same metric."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Morocutti2023" style="box-shadow: none">
<div class="panel-heading" id="headingMorocutti2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Tobias Morocutti<sup>1</sup>, Florian Schmid<sup>2</sup>, Khaled Koutini<sup>1</sup> and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>LIT Artificial Intelligence Lab, Johannes Kepler University Linz, <sup>2</sup>Institute of Computational Perception (CP-JKU)
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Morocutti2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Morocutti_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Morocutti2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Morocutti2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-51">
        P5-51
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Morocutti2023" class="panel-collapse collapse" id="collapse-Morocutti2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Knowledge Distillation (KD) is a widespread technique for compressing the knowledge of large models into more compact and efficient models. KD has proved to be highly effective in building well-performing low-complexity Acoustic Scene Classification (ASC) systems and was used in all the top-ranked submissions to this task of the annual DCASE challenge in the past three years. There is extensive research available on establishing the KD process, designing efficient student models, and forming well-performing teacher ensembles. However, less research has been conducted on investigating which teacher model attributes are beneficial for low-complexity students. In this work, we try to close this gap by studying the effects on the student's performance when using different teacher network architectures, varying the teacher model size, training them with different data augmentation methods, and applying different ensembling strategies. The results show that teacher model sizes, data augmentation methods, the ensembling strategy and the ensemble size are key factors for a well-performing student network.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Morocutti2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Morocutti_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Morocutti2023label" class="modal fade" id="bibtex-Morocutti2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMorocutti2023label">
        Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Morocutti2023,
    author = "Morocutti, Tobias and Schmid, Florian and Koutini, Khaled and Widmer, Gerhard",
    title = "Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "121--125",
    abstract = "Knowledge Distillation (KD) is a widespread technique for compressing the knowledge of large models into more compact and efficient models. KD has proved to be highly effective in building well-performing low-complexity Acoustic Scene Classification (ASC) systems and was used in all the top-ranked submissions to this task of the annual DCASE challenge in the past three years. There is extensive research available on establishing the KD process, designing efficient student models, and forming well-performing teacher ensembles. However, less research has been conducted on investigating which teacher model attributes are beneficial for low-complexity students. In this work, we try to close this gap by studying the effects on the student's performance when using different teacher network architectures, varying the teacher model size, training them with different data augmentation methods, and applying different ensembling strategies. The results show that teacher model sizes, data augmentation methods, the ensembling strategy and the ensemble size are key factors for a well-performing student network."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Moummad2023" style="box-shadow: none">
<div class="panel-heading" id="headingMoummad2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Pretraining Representations for Bioacoustic Few-Shot Detection Using Supervised Contrastive Learning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ilyass Moummad<sup>1</sup>, Romain Serizel<sup>2</sup> and Nicolas Farrugia<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>IMT Atlantique, Lab-STICC, <sup>2</sup>University of Lorraine
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Moummad2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Moummad_63.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Moummad2023').collapse('show');window.location.hash='#Moummad2023';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
<button aria-controls="collapse-Moummad2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Moummad2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-4">
        P1-4
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Moummad2023" class="panel-collapse collapse" id="collapse-Moummad2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46% on the validation set and 42.7% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Moummad2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Moummad_63.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/ilyassmoummad/dcase23_task5_scl" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Moummad2023label" class="modal fade" id="bibtex-Moummad2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMoummad2023label">
        Pretraining Representations for Bioacoustic Few-Shot Detection Using Supervised Contrastive Learning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Moummad2023,
    author = "Moummad, Ilyass and Serizel, Romain and Farrugia, Nicolas",
    title = "Pretraining Representations for Bioacoustic Few-Shot Detection Using Supervised Contrastive Learning",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "126--130",
    abstract = "Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46\% on the validation set and 42.7\% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mulimani2023" style="box-shadow: none">
<div class="panel-heading" id="headingMulimani2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Incremental Learning of Acoustic Scenes and Sound Events
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Manjunath Mulimani and Annamaria Mesaros
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Computing Science, Tampere University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mulimani2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Mulimani_50.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Mulimani2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mulimani2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-50">
        P5-50
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mulimani2023" class="panel-collapse collapse" id="collapse-Mulimani2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose a method for incremental learning of two distinct tasks over time: acoustic scene classification (ASC) and audio tagging (AT). We use a simple convolutional neural network (CNN) model as an incremental learner to solve the tasks. Generally, incremental learning methods catastrophically forget the previous task when sequentially trained on a new task. To alleviate this problem, we propose independent learning and knowledge distillation (KD) between the timesteps in learning. Experiments are performed on TUT 2016/2017 dataset, containing 4 acoustic scene classes and 25 sound event classes. The proposed incremental learner first solves the ASC task with an accuracy of 94.0%. Next, it learns to solve the AT task with an F1 score of 54.4%. At the same time, its performance on the previous ASC task decreases only by 5.1 percentage points due to the additional learning of the AT task.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mulimani2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Mulimani_50.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mulimani2023label" class="modal fade" id="bibtex-Mulimani2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMulimani2023label">
        Incremental Learning of Acoustic Scenes and Sound Events
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mulimani2023,
    author = "Mulimani, Manjunath and Mesaros, Annamaria",
    title = "Incremental Learning of Acoustic Scenes and Sound Events",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "131--135",
    abstract = "In this paper, we propose a method for incremental learning of two distinct tasks over time: acoustic scene classification (ASC) and audio tagging (AT). We use a simple convolutional neural network (CNN) model as an incremental learner to solve the tasks. Generally, incremental learning methods catastrophically forget the previous task when sequentially trained on a new task. To alleviate this problem, we propose independent learning and knowledge distillation (KD) between the timesteps in learning. Experiments are performed on TUT 2016/2017 dataset, containing 4 acoustic scene classes and 25 sound event classes. The proposed incremental learner first solves the ASC task with an accuracy of 94.0\%. Next, it learns to solve the AT task with an F1 score of 54.4\%. At the same time, its performance on the previous ASC task decreases only by 5.1 percentage points due to the additional learning of the AT task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nam2023" style="box-shadow: none">
<div class="panel-heading" id="headingNam2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Frequency &amp; Channel Attention for Computationally Efficient Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Hyeonuk Nam, Seong-Hu Kim, Deokki Min and Yong-Hwa Park
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Korea Advanced Institute of Science and Technology (KAIST)
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nam2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Nam_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Nam2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nam2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-52">
        P5-52
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nam2023" class="panel-collapse collapse" id="collapse-Nam2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We explore on various attention methods on frequency and channel dimensions for sound event detection (SED) in order to enhance performance with minimal increase in computational cost while leveraging domain knowledge to address the frequency dimension of audio data. We have introduced frequency dynamic convolution (FDY conv) in a previous work to release the translational equivariance issue associated with 2D convolution on the frequency dimension of 2D audio data. Although this approach demonstrated state-of-the-art SED performance, it resulted in a model with 150% more trainable parameters. To achieve comparable SED performance with computationally efficient methods for practicality, we explore on lighter alternative attention methods. In addition, we focus on attention methods applied to frequency and channel dimensions. Joint application Squeeze-and-excitation (SE) module and time-frame frequency-wise SE (tfwSE) to apply attention on both frequency and channel dimensions shows comparable performance to SED model with FDY conv with only 2.7% more trainable parameters compared to the baseline model. In addition, we performed class-wise comparison of various attention methods to further discuss various attention methods' characteristics.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nam2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Nam_32.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nam2023label" class="modal fade" id="bibtex-Nam2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNam2023label">
        Frequency &amp; Channel Attention for Computationally Efficient Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nam2023,
    author = "Nam, Hyeonuk and Kim, Seong-Hu and Min, Deokki and Park, Yong-Hwa",
    title = "Frequency \&amp; Channel Attention for Computationally Efficient Sound Event Detection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "136--140",
    abstract = "We explore on various attention methods on frequency and channel dimensions for sound event detection (SED) in order to enhance performance with minimal increase in computational cost while leveraging domain knowledge to address the frequency dimension of audio data. We have introduced frequency dynamic convolution (FDY conv) in a previous work to release the translational equivariance issue associated with 2D convolution on the frequency dimension of 2D audio data. Although this approach demonstrated state-of-the-art SED performance, it resulted in a model with 150\% more trainable parameters. To achieve comparable SED performance with computationally efficient methods for practicality, we explore on lighter alternative attention methods. In addition, we focus on attention methods applied to frequency and channel dimensions. Joint application Squeeze-and-excitation (SE) module and time-frame frequency-wise SE (tfwSE) to apply attention on both frequency and channel dimensions shows comparable performance to SED model with FDY conv with only 2.7\% more trainable parameters compared to the baseline model. In addition, we performed class-wise comparison of various attention methods to further discuss various attention methods' characteristics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Napoli2023" style="box-shadow: none">
<div class="panel-heading" id="headingNapoli2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Unsupervised Domain Adaptation for the Cross-Dataset Detection of Humpback Whale Calls
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Andrea Napoli and Paul R. White
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Institute of Sound and Vibration Research, University of Southampton
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Napoli2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Napoli_21.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Napoli2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Napoli2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-9">
        P1-9
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Napoli2023" class="panel-collapse collapse" id="collapse-Napoli2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Machine learning methods, and deep networks in particular, often underperform on data which lies outside the training distribution. Changes to the data distributions (known as domain shift) are particularly prevalent in bioacoustics, where many external factors can vary between datasets, although the effects of this are often not properly considered. This paper presents a benchmark for out of distribution (OOD) performance based on the detection of humpback whales in underwater acoustic data. Several humpback whale detectors from the literature are implemented as baselines, along with our own detector based on a convolutional neural network (CNN). Then, a set of unsupervised domain adaptation (UDA) algorithms are compared. Results show that UDA can significantly improve OOD performance when few distinct sources of training data are available. However, this is not a substitute for better data, as negative transfer (where the adapted models actually perform worse) is commonly observed. On the other hand, we find that training on a variety of distinct sources of data (at least 6) is sufficient to allow models to generalise OOD, without the need for advanced UDA algorithms. This allows our model to outperform all the baseline detectors we test, despite having 10,000 times fewer parameters and 100,000 times less training data than the next-best model.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Napoli2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Napoli_21.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Napoli2023label" class="modal fade" id="bibtex-Napoli2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNapoli2023label">
        Unsupervised Domain Adaptation for the Cross-Dataset Detection of Humpback Whale Calls
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Napoli2023,
    author = "Napoli, Andrea and White, Paul R.",
    title = "Unsupervised Domain Adaptation for the Cross-Dataset Detection of Humpback Whale Calls",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "141--145",
    abstract = "Machine learning methods, and deep networks in particular, often underperform on data which lies outside the training distribution. Changes to the data distributions (known as domain shift) are particularly prevalent in bioacoustics, where many external factors can vary between datasets, although the effects of this are often not properly considered. This paper presents a benchmark for out of distribution (OOD) performance based on the detection of humpback whales in underwater acoustic data. Several humpback whale detectors from the literature are implemented as baselines, along with our own detector based on a convolutional neural network (CNN). Then, a set of unsupervised domain adaptation (UDA) algorithms are compared. Results show that UDA can significantly improve OOD performance when few distinct sources of training data are available. However, this is not a substitute for better data, as negative transfer (where the adapted models actually perform worse) is commonly observed. On the other hand, we find that training on a variety of distinct sources of data (at least 6) is sufficient to allow models to generalise OOD, without the need for advanced UDA algorithms. This allows our model to outperform all the baseline detectors we test, despite having 10,000 times fewer parameters and 100,000 times less training data than the next-best model."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nolasco2023" style="box-shadow: none">
<div class="panel-heading" id="headingNolasco2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Few-Shot Bioacoustic Event Detection at the DCASE 2023 Challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ines Nolasco<sup>1</sup>, Burooj Ghani<sup>2</sup>, Shubhr Singh<sup>1</sup>, Ester Vidaña-Vila<sup>3</sup>, Helen Whitehead<sup>4</sup>, Emily Grout<sup>5,6</sup>, Michael Emmerson<sup>7</sup>, Ivan Kiskin<sup>8</sup>, Frants Jensen<sup>9</sup>, Joe Morford<sup>10</sup>, Ariana Strandburg-Peshkin<sup>5,6</sup>, Lisa Gill<sup>11</sup>, Hanna Pamuła<sup>12</sup>, Vincent Lostanlen<sup>13</sup> and Dan Stowell<sup>14</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Centre for Digital Music (C4DM), Queen Mary University of London, <sup>2</sup>Naturalis Biodiversity Centre, <sup>3</sup>La Salle Campus Barcelona, Ramon Llull University, <sup>4</sup>School of Science, Engineering and Environment, University of Salford, <sup>5</sup>Department of Biology &amp; Centre for the Advanced Study of Collective Behaviour, University of Konstanz, <sup>6</sup>Department for the Ecology of Animal Societies, Max Planck Institute of Animal Behavior, <sup>7</sup>School of Biological and Behavioural Sciences, Queen Mary University of London, <sup>8</sup>Institute for People-Centred AI, FHMS, University of Surrey, <sup>9</sup>Biology Department, Syracuse University, <sup>10</sup>The Oxford Navigation group, Department of Zoology, Oxford University, <sup>11</sup>Landesbund f¨ur Vogel- und Naturschutz, Naturkundemuseum Bayern/BIOTOPIA Lab, <sup>12</sup>AGH University of Science and Technology, <sup>13</sup>École Centrale Nantes, Nantes Université, <sup>14</sup>Tilburg University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nolasco2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Nolasco_45.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Nolasco2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nolasco2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-15">
        P2-15
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nolasco2023" class="panel-collapse collapse" id="collapse-Nolasco2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Few-shot bioacoustic event detection consists in detecting sound events of specified types, in varying soundscapes, while having access to only a few examples of the class of interest. This task ran as part of the DCASE challenge for the third time this year with an evaluation set expanded to include new animal species, and a new rule: ensemble models were no longer allowed. The 2023 few-shot task received submissions from 6 different teams with F-scores reaching as high as 63% on the evaluation set. Here we describe the task, focusing on describing the elements that differed from previous years. We also take a look back at past editions to describe how the task has evolved. Not only have the F-score results steadily improved (40% to 60% to 63%), but the type of systems proposed have also become more complex. Sound event detection systems are no longer simple variations of the baselines provided: multiple few-shot learning methodologies are still strong contenders for the task.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nolasco2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Nolasco_45.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nolasco2023label" class="modal fade" id="bibtex-Nolasco2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNolasco2023label">
        Few-Shot Bioacoustic Event Detection at the DCASE 2023 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nolasco2023,
    author = "Nolasco, Ines and Ghani, Burooj and Singh, Shubhr and Vidaña-Vila, Ester and Whitehead, Helen and Grout, Emily and Emmerson, Michael and Kiskin, Ivan and Jensen, Frants and Morford, Joe and Strandburg-Peshkin, Ariana and Gill, Lisa and Pamuła, Hanna and Lostanlen, Vincent and Stowell, Dan",
    title = "Few-Shot Bioacoustic Event Detection at the {DCASE} 2023 Challenge",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "146--150",
    abstract = "Few-shot bioacoustic event detection consists in detecting sound events of specified types, in varying soundscapes, while having access to only a few examples of the class of interest. This task ran as part of the DCASE challenge for the third time this year with an evaluation set expanded to include new animal species, and a new rule: ensemble models were no longer allowed. The 2023 few-shot task received submissions from 6 different teams with F-scores reaching as high as 63\% on the evaluation set. Here we describe the task, focusing on describing the elements that differed from previous years. We also take a look back at past editions to describe how the task has evolved. Not only have the F-score results steadily improved (40\% to 60\% to 63\%), but the type of systems proposed have also become more complex. Sound event detection systems are no longer simple variations of the baselines provided: multiple few-shot learning methodologies are still strong contenders for the task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Primus2023" style="box-shadow: none">
<div class="panel-heading" id="headingPrimus2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Advancing Natural-Language Based Audio Retrieval with Passt and Large Audio-Caption Data Sets
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Paul Primus<sup>1</sup>, Khaled Koutini<sup>1,2</sup> and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Computational Perception (CP-JKU), <sup>2</sup>LIT Artificial Intelligence Lab, Johannes Kepler University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Primus2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Primus_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Primus2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Primus2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-22">
        P2-22
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Primus2023" class="panel-collapse collapse" id="collapse-Primus2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Primus2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Primus_22.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Primus2023label" class="modal fade" id="bibtex-Primus2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPrimus2023label">
        Advancing Natural-Language Based Audio Retrieval with Passt and Large Audio-Caption Data Sets
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Primus2023,
    author = "Primus, Paul and Koutini, Khaled and Widmer, Gerhard",
    title = "Advancing Natural-Language Based Audio Retrieval with Passt and Large Audio-Caption Data Sets",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "151--155",
    abstract = "This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Scheibler2023" style="box-shadow: none">
<div class="panel-heading" id="headingScheibler2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Foley Sound Synthesis with a Class-Conditioned Latent Diffusion Model
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Robin Scheibler, Takuya Hasumi, Yusuke Fujita, Tatsuya Komatsu, Ryuichi Yamamoto and Kentaro Tachibana
        </em>
</p>
<p class="text-muted">
<small>
<em>
          LINE Corporation
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Scheibler2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Scheibler_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Scheibler2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Scheibler2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-29">
        P3-29
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Scheibler2023" class="panel-collapse collapse" id="collapse-Scheibler2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a competitive Foley sound synthesis system based on available components and fine-tuned on a target dataset. We reuse a text-to-audio pre-trained model composed of a latent diffusion model (LDM), trained on AudioCaps, a variational auto-encoder (VAE), and a vocoder. We fine-tune the LDM on the development dataset of the DCASE 2023 Task 7 to output a latent representation conditioned on the target class number. The VAE and vocoder are then used to generate the waveform from the latent representation. To improve the quality of the generated samples, we utilize a postprocessing filter that selects a subset of generated sounds to match a distribution of target class sounds. In experiments, we found that our system achieved an average Fréchet audio distance (FAD) of 4.744, which is significantly better than 9.702 produced by the baseline system of the DCASE 2023 Challenge Task 7. In addition, we perform ablation studies to evaluate the performance of the system before fine-tuning and the effect of sampling rate on the FAD.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Scheibler2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Scheibler_46.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Scheibler2023label" class="modal fade" id="bibtex-Scheibler2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexScheibler2023label">
        Foley Sound Synthesis with a Class-Conditioned Latent Diffusion Model
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Scheibler2023,
    author = "Scheibler, Robin and Hasumi, Takuya and Fujita, Yusuke and Komatsu, Tatsuya and Yamamoto, Ryuichi and Tachibana, Kentaro",
    title = "Foley Sound Synthesis with a Class-Conditioned Latent Diffusion Model",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "156--160",
    abstract = "We propose a competitive Foley sound synthesis system based on available components and fine-tuned on a target dataset. We reuse a text-to-audio pre-trained model composed of a latent diffusion model (LDM), trained on AudioCaps, a variational auto-encoder (VAE), and a vocoder. We fine-tune the LDM on the development dataset of the DCASE 2023 Task 7 to output a latent representation conditioned on the target class number. The VAE and vocoder are then used to generate the waveform from the latent representation. To improve the quality of the generated samples, we utilize a postprocessing filter that selects a subset of generated sounds to match a distribution of target class sounds. In experiments, we found that our system achieved an average Fréchet audio distance (FAD) of 4.744, which is significantly better than 9.702 produced by the baseline system of the DCASE 2023 Challenge Task 7. In addition, we perform ablation studies to evaluate the performance of the system before fine-tuning and the effect of sampling rate on the FAD."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schmid2023" style="box-shadow: none">
<div class="panel-heading" id="headingSchmid2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Distilling the Knowledge of Transformers and CNNs with CP-Mobile
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Florian Schmid<sup>1</sup>, Tobias Morocutti<sup>2</sup>, Shahed Masoudian<sup>1</sup>, Khaled Koutini<sup>2</sup> and Gerhard Widmer<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Institute of Computational Perception (CP-JKU), <sup>2</sup>LIT Artificial Intelligence Lab, Johannes Kepler University Linz
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schmid2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Schmid_1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Schmid2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schmid2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-1">
        P1-1
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schmid2023" class="panel-collapse collapse" id="collapse-Schmid2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Designing lightweight models that require limited computational resources and can operate on edge devices is a major trajectory in deep learning research. In the context of Acoustic Scene Classification (ASC), the DCASE community hosts an annual challenge on low-complexity ASC, contributing to the research on Knowledge Distillation (KD), Model Pruning, Quantization and efficient neural network design. In this work, we propose a system that contributes to the latter by introducing CP-Mobile, a lightweight CNN architecture constructed of residual inverted bottleneck blocks and Global Response Normalization. Furthermore, we improve Knowledge Distillation by showing that ensembling CNNs and Audio Spectrogram Transformers form strong teacher ensembles. Our proposed system improves the results on the TAU Urban Acoustic Scenes 2022 Mobile development dataset by around 5 percentage points in accuracy compared to the top-ranked submission for Task 1 of the DCASE 22 challenge and achieves the top rank in the DCASE 23 challenge.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schmid2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Schmid_1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schmid2023label" class="modal fade" id="bibtex-Schmid2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchmid2023label">
        Distilling the Knowledge of Transformers and CNNs with CP-Mobile
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schmid2023,
    author = "Schmid, Florian and Morocutti, Tobias and Masoudian, Shahed and Koutini, Khaled and Widmer, Gerhard",
    title = "Distilling the Knowledge of Transformers and {CNNs} with {CP}-Mobile",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "161--165",
    abstract = "Designing lightweight models that require limited computational resources and can operate on edge devices is a major trajectory in deep learning research. In the context of Acoustic Scene Classification (ASC), the DCASE community hosts an annual challenge on low-complexity ASC, contributing to the research on Knowledge Distillation (KD), Model Pruning, Quantization and efficient neural network design. In this work, we propose a system that contributes to the latter by introducing CP-Mobile, a lightweight CNN architecture constructed of residual inverted bottleneck blocks and Global Response Normalization. Furthermore, we improve Knowledge Distillation by showing that ensembling CNNs and Audio Spectrogram Transformers form strong teacher ensembles. Our proposed system improves the results on the TAU Urban Acoustic Scenes 2022 Mobile development dataset by around 5 percentage points in accuracy compared to the top-ranked submission for Task 1 of the DCASE 22 challenge and achieves the top rank in the DCASE 23 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schmidt2023" style="box-shadow: none">
<div class="panel-heading" id="headingSchmidt2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Device Generalization with Inverse Contrastive Loss and Impulse Response Augmentation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Lorenz P. Schmidt<sup>1,2</sup> and Nils Peters<sup>1,2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Friedrich-Alexander-Universität Erlangen-Nürnberg, <sup>2</sup>International Audio Laboratories
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schmidt2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Schmidt_66.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Schmidt2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schmidt2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-27">
        P3-27
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schmidt2023" class="panel-collapse collapse" id="collapse-Schmidt2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic Scene Classification poses a significant challenge in the DCASE Task 1 TAU22 dataset with a sample length of only a single second. The best performing model in the 2023 challenge achieves an accuracy of 62.7% with a gap to unseen devices of approximately 10%. In this study, we propose a novel approach using Inverse Contrastive Loss to ensure a device class invariant latent representation and a better generalization to unseen devices. We evaluate the interaction of this contrastive learning approach with impulse response augmentation and show the effectiveness for suppressing device related information in the encoder structure. Results indicates that both, contrastive learning and impulse response augmentation, improves generalization to unseen devices. Further the impulse response dataset should have a balanced frequency response to work effectively. Combining contrastive learning and impulse response augmentation yields embeddings with least device related information, but does not improve scene classification accuracy when compared to augmentation alone.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schmidt2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Schmidt_66.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schmidt2023label" class="modal fade" id="bibtex-Schmidt2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchmidt2023label">
        Device Generalization with Inverse Contrastive Loss and Impulse Response Augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schmidt2023,
    author = "Schmidt, Lorenz P. and Peters, Nils",
    title = "Device Generalization with Inverse Contrastive Loss and Impulse Response Augmentation",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "166--170",
    abstract = "Acoustic Scene Classification poses a significant challenge in the DCASE Task 1 TAU22 dataset with a sample length of only a single second. The best performing model in the 2023 challenge achieves an accuracy of 62.7\% with a gap to unseen devices of approximately 10\%. In this study, we propose a novel approach using Inverse Contrastive Loss to ensure a device class invariant latent representation and a better generalization to unseen devices. We evaluate the interaction of this contrastive learning approach with impulse response augmentation and show the effectiveness for suppressing device related information in the encoder structure. Results indicates that both, contrastive learning and impulse response augmentation, improves generalization to unseen devices. Further the impulse response dataset should have a balanced frequency response to work effectively. Combining contrastive learning and impulse response augmentation yields embeddings with least device related information, but does not improve scene classification accuracy when compared to augmentation alone."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sridhar2023" style="box-shadow: none">
<div class="panel-heading" id="headingSridhar2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Multi-Label Open-Set Audio Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sripathi Sridhar and Mark Cartwright
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Sound Interaction and Computing (SInC) Lab, New Jersey Institute of Technology
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sridhar2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Sridhar_11.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Sridhar2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sridhar2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-26">
        P3-26
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sridhar2023" class="panel-collapse collapse" id="collapse-Sridhar2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Current audio classification models have small class vocabularies relative to the large number of sound event classes of interest in the real world. Thus, they provide a limited view of the world that may miss important yet unexpected or unknown sound events. To ad- dress this issue, open-set audio classification techniques have been developed to detect sound events from unknown classes. Although these methods have been applied to a multi-class context in audio, such as sound scene classification, they have yet to be investigated for polyphonic audio in which sound events overlap, requiring the use of multi-label models. In this study, we establish the problem of multi-label open-set audio classification by creating a dataset with varying unknown class distributions and evaluating baseline approaches built upon existing techniques.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sridhar2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Sridhar_11.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sridhar2023label" class="modal fade" id="bibtex-Sridhar2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSridhar2023label">
        Multi-Label Open-Set Audio Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Sridhar2023,
    author = "Sridhar, Sripathi and Cartwright, Mark",
    title = "Multi-Label Open-Set Audio Classification",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "171--175",
    abstract = "Current audio classification models have small class vocabularies relative to the large number of sound event classes of interest in the real world. Thus, they provide a limited view of the world that may miss important yet unexpected or unknown sound events. To ad- dress this issue, open-set audio classification techniques have been developed to detect sound events from unknown classes. Although these methods have been applied to a multi-class context in audio, such as sound scene classification, they have yet to be investigated for polyphonic audio in which sound events overlap, requiring the use of multi-label models. In this study, we establish the problem of multi-label open-set audio classification by creating a dataset with varying unknown class distributions and evaluating baseline approaches built upon existing techniques."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tailleur2023" style="box-shadow: none">
<div class="panel-heading" id="headingTailleur2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Spectral Transcoder : Using Pretrained Urban Sound Classifiers on Undersampled Spectral Representations
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Modan Tailleur<sup>1</sup>, Mathieu Lagrange<sup>1</sup>, Pierre Aumond<sup>2</sup> and Vincent Tourre<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>École Centrale Nantes, Nantes Université, <sup>2</sup>Centre for Studies on Risks, Mobility, Land Planning and the Environment (CEREMA), Université Gustave Eiffel
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tailleur2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tailleur_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Tailleur2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tailleur2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-42">
        P4-42
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tailleur2023" class="panel-collapse collapse" id="collapse-Tailleur2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Slow or fast third-octave bands representations (with a frame resp. every 1-s and 125-ms) have been a de facto standard for urban acoustics, used for example in long-term monitoring applications. It has the advantages of requiring few storage capabilities and of preserving privacy. As most audio classification algorithms take Mel spectral representations with very fast time weighting (ex. 10-ms) as input, very few studies have tackled classification tasks using other kinds of spectral representations of audio such as slow or fast third-octave spectra. In this paper, we present a convolutional neural network architecture for transcoding fast third-octave spectrograms into Mel spectrograms, so that it could be used as input for robust pre-trained models such as YAMNet or PANN models. Compared to training a model that would take fast third-octave spectrograms as input, this approach is more effective and requires less training effort. Even if a fast third-octave spectrogram is less precise both on time and frequency dimensions, experiments show that the proposed method still allows for classification accuracy of 62.4% on UrbanSound8k and 0.44 macro AUPRC on SONYC-UST.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tailleur2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tailleur_13.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tailleur2023label" class="modal fade" id="bibtex-Tailleur2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTailleur2023label">
        Spectral Transcoder : Using Pretrained Urban Sound Classifiers on Undersampled Spectral Representations
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tailleur2023,
    author = "Tailleur, Modan and Lagrange, Mathieu and Aumond, Pierre and Tourre, Vincent",
    title = "Spectral Transcoder : Using Pretrained Urban Sound Classifiers on Undersampled Spectral Representations",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "176--180",
    abstract = "Slow or fast third-octave bands representations (with a frame resp. every 1-s and 125-ms) have been a de facto standard for urban acoustics, used for example in long-term monitoring applications. It has the advantages of requiring few storage capabilities and of preserving privacy. As most audio classification algorithms take Mel spectral representations with very fast time weighting (ex. 10-ms) as input, very few studies have tackled classification tasks using other kinds of spectral representations of audio such as slow or fast third-octave spectra. In this paper, we present a convolutional neural network architecture for transcoding fast third-octave spectrograms into Mel spectrograms, so that it could be used as input for robust pre-trained models such as YAMNet or PANN models. Compared to training a model that would take fast third-octave spectrograms as input, this approach is more effective and requires less training effort. Even if a fast third-octave spectrogram is less precise both on time and frequency dimensions, experiments show that the proposed method still allows for classification accuracy of 62.4\% on UrbanSound8k and 0.44 macro AUPRC on SONYC-UST."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Takeuchi2023" style="box-shadow: none">
<div class="panel-heading" id="headingTakeuchi2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada and Kunio Kashino
        </em>
</p>
<p class="text-muted">
<small>
<em>
          NTT Corporation
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Takeuchi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Takeuchi_30.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Takeuchi2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Takeuchi2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-7">
        P1-7
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Takeuchi2023" class="panel-collapse collapse" id="collapse-Takeuchi2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Takeuchi2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Takeuchi_30.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Takeuchi2023label" class="modal fade" id="bibtex-Takeuchi2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTakeuchi2023label">
        Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Takeuchi2023,
    author = "Takeuchi, Daiki and Ohishi, Yasunori and Niizumi, Daisuke and Harada, Noboru and Kashino, Kunio",
    title = "Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "181--185",
    abstract = "We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tanmay2023" style="box-shadow: none">
<div class="panel-heading" id="headingTanmay2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Cross-Dimensional Interaction with Inverted Residual Triplet Attention for Low-Complexity Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Khandelwal Tanmay and Rohan Kumar Das
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Fortemedia Singapore
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tanmay2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tanmay_2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Tanmay2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tanmay2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-19">
        P2-19
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tanmay2023" class="panel-collapse collapse" id="collapse-Tanmay2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Attention mechanisms have been widely used in a variety of sound event detection (SED) tasks, owing to their ability to build interdependencies among channels or spatial locations. The existing state-of-the-art (SOTA) architectures and attention modules incorporated in SED have a high computational cost in terms of the number of parameters. To address this issue, we propose a lightweight module utilizing triplet attention on an inverted residual network (IRN) referred to as an inverted residual triplet attention module (IRTAM) for replacing the standard 2D convolutional neural network. The IRTAM captures cross-dimensional interdependencies using the rotation operation followed by residual transformations with a three-branch structure embedded in IRN. On DCASE 2022 Task 4 validation set, the proposed lightweight module improves the performance of the baseline by 34.1% in terms of polyphonic sound event detection score and achieves SOTA results with only 27.6% parameters of the baseline.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tanmay2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tanmay_2.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tanmay2023label" class="modal fade" id="bibtex-Tanmay2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTanmay2023label">
        Cross-Dimensional Interaction with Inverted Residual Triplet Attention for Low-Complexity Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tanmay2023,
    author = "Tanmay, Khandelwal and Das, Rohan Kumar",
    title = "Cross-Dimensional Interaction with Inverted Residual Triplet Attention for Low-Complexity Sound Event Detection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "186--190",
    abstract = "Attention mechanisms have been widely used in a variety of sound event detection (SED) tasks, owing to their ability to build interdependencies among channels or spatial locations. The existing state-of-the-art (SOTA) architectures and attention modules incorporated in SED have a high computational cost in terms of the number of parameters. To address this issue, we propose a lightweight module utilizing triplet attention on an inverted residual network (IRN) referred to as an inverted residual triplet attention module (IRTAM) for replacing the standard 2D convolutional neural network. The IRTAM captures cross-dimensional interdependencies using the rotation operation followed by residual transformations with a three-branch structure embedded in IRN. On DCASE 2022 Task 4 validation set, the proposed lightweight module improves the performance of the baseline by 34.1\% in terms of polyphonic sound event detection score and achieves SOTA results with only 27.6\% parameters of the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tanmay2023a" style="box-shadow: none">
<div class="panel-heading" id="headingTanmay2023a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Exploring Multi-Task Learning with Weighted Soft Label Loss for Sound Event Detection with Soft Labels
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Khandelwal Tanmay and Rohan Kumar Das
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Fortemedia Singapore
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tanmay2023a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tanmay_7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Tanmay2023a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tanmay2023a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-49">
        P5-49
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tanmay2023a" class="panel-collapse collapse" id="collapse-Tanmay2023a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The learning of sound events often depends on data that is manually labeled by human annotators. In this study, we explore the use of soft labels for sound event detection (SED), which takes into account the uncertainty and variability in human annotations. To address the challenges posed by uncertain or noisy labels, we propose a weighted soft label (WSL) loss function. This loss function effectively emphasizes reliable annotations while mitigating the influence of less confident or noisy labels. Additionally, we introduce auxiliary tasks into a multi-task learning (MTL) framework, which helps to leverage the shared information between the tasks and improves the overall performance of the model. Furthermore, we explore the usage of pretrained models and various front-end feature extraction methods. Experimental results on the MAESTRO-Real dataset introduced in the DCASE 2023 Task 4B demonstrate a significant improvement of 14.9% in the macro-average F1 score with optimum threshold per class compared to the challenge baseline model on the validation set, highlighting the effectiveness of our proposed system.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tanmay2023a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tanmay_7.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tanmay2023alabel" class="modal fade" id="bibtex-Tanmay2023a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTanmay2023alabel">
        Exploring Multi-Task Learning with Weighted Soft Label Loss for Sound Event Detection with Soft Labels
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tanmay2023a,
    author = "Tanmay, Khandelwal and Das, Rohan Kumar",
    title = "Exploring Multi-Task Learning with Weighted Soft Label Loss for Sound Event Detection with Soft Labels",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "191--195",
    abstract = "The learning of sound events often depends on data that is manually labeled by human annotators. In this study, we explore the use of soft labels for sound event detection (SED), which takes into account the uncertainty and variability in human annotations. To address the challenges posed by uncertain or noisy labels, we propose a weighted soft label (WSL) loss function. This loss function effectively emphasizes reliable annotations while mitigating the influence of less confident or noisy labels. Additionally, we introduce auxiliary tasks into a multi-task learning (MTL) framework, which helps to leverage the shared information between the tasks and improves the overall performance of the model. Furthermore, we explore the usage of pretrained models and various front-end feature extraction methods. Experimental results on the MAESTRO-Real dataset introduced in the DCASE 2023 Task 4B demonstrate a significant improvement of 14.9\% in the macro-average F1 score with optimum threshold per class compared to the challenge baseline model on the validation set, highlighting the effectiveness of our proposed system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tonami2023" style="box-shadow: none">
<div class="panel-heading" id="headingTonami2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Event Classification with Class-Level Gated Unit Using Large-Scale Pretrained Model for Optical Fiber Sensing
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Noriyuki Tonami<sup>1</sup>, Sakiko Mishima<sup>1</sup>, Reishi Kondo<sup>1</sup>, Keisuke Imoto<sup>2</sup> and Tomoyuki Hino<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>NEC Corporation, <sup>2</sup>Doshisha University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tonami2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tonami_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Tonami2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tonami2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P4-43">
        P4-43
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tonami2023" class="panel-collapse collapse" id="collapse-Tonami2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Optical fiber sensing is a technology in which sounds, vibrations, and temperature are detected using an optical fiber; especially the sounds/vibrations-aware sensing is called distributed acoustic sensing (DAS). DAS has the potential to capture various types of sounds and/or vibrations in wide areas, e.g., the ground, the sea, and a city area, in our everyday life. To precisely recognize the various types of events, e.g., whale calls, car horns, and wind, by DAS, therefore two problems. First, there is little publicly available data and few pretrained models for the various types of events. Second, the signal-to-noise ratio (SNR) of DAS data is lower than that of other sensor data, such as microphone data, because of optical noise and low sensitivity of DAS. To tackle the lack of DAS data, we first demonstrate a DAS simulation method where DAS observations are simulated by exploiting a microphone simulation. We then propose a method of event classification for DAS utilizing a pretrained audio recognition model, where none of the DAS data are used for training. Moreover, we advocate a class-level gated unit with the pretrained model to overcome the poor classification performance caused by the low SNR of the DAS data. In the proposed method, class probabilities, which are the output of the pretrained model, are employed for controlling priors of DAS, such as events of interest or optical noise. Directly controlling the class probabilities, which are non-black-box values, as priors enables us to utilize not only a pretrained model but also powerful human knowledge. To verify the performance of the proposed method, we conduct event classification, where we simulate observed signals by DAS with the ESC-50 dataset. Experimental results show that the accuracy of the proposed method is improved by 36.75 percentage points compared with that of conventional methods.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tonami2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tonami_29.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tonami2023label" class="modal fade" id="bibtex-Tonami2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTonami2023label">
        Event Classification with Class-Level Gated Unit Using Large-Scale Pretrained Model for Optical Fiber Sensing
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tonami2023,
    author = "Tonami, Noriyuki and Mishima, Sakiko and Kondo, Reishi and Imoto, Keisuke and Hino, Tomoyuki",
    title = "Event Classification with Class-Level Gated Unit Using Large-Scale Pretrained Model for Optical Fiber Sensing",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "196--200",
    abstract = "Optical fiber sensing is a technology in which sounds, vibrations, and temperature are detected using an optical fiber; especially the sounds/vibrations-aware sensing is called distributed acoustic sensing (DAS). DAS has the potential to capture various types of sounds and/or vibrations in wide areas, e.g., the ground, the sea, and a city area, in our everyday life. To precisely recognize the various types of events, e.g., whale calls, car horns, and wind, by DAS, therefore two problems. First, there is little publicly available data and few pretrained models for the various types of events. Second, the signal-to-noise ratio (SNR) of DAS data is lower than that of other sensor data, such as microphone data, because of optical noise and low sensitivity of DAS. To tackle the lack of DAS data, we first demonstrate a DAS simulation method where DAS observations are simulated by exploiting a microphone simulation. We then propose a method of event classification for DAS utilizing a pretrained audio recognition model, where none of the DAS data are used for training. Moreover, we advocate a class-level gated unit with the pretrained model to overcome the poor classification performance caused by the low SNR of the DAS data. In the proposed method, class probabilities, which are the output of the pretrained model, are employed for controlling priors of DAS, such as events of interest or optical noise. Directly controlling the class probabilities, which are non-black-box values, as priors enables us to utilize not only a pretrained model but also powerful human knowledge. To verify the performance of the proposed method, we conduct event classification, where we simulate observed signals by DAS with the ESC-50 dataset. Experimental results show that the accuracy of the proposed method is improved by 36.75 percentage points compared with that of conventional methods."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tsubaki2023" style="box-shadow: none">
<div class="panel-heading" id="headingTsubaki2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Audio-Change Captioning to Explain Machine-Sound Anomalies
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Shunsuke Tsubaki<sup>1</sup>, Yohei Kawaguchi<sup>2</sup>, Tomoya Nishida<sup>2</sup>, Keisuke Imoto<sup>1</sup>, Yuki Okamoto<sup>3</sup>, Kota Dohi<sup>2</sup> and Takashi Endo<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Faculty of Science and Engineering, Doshisha University, <sup>2</sup>Hitachi, Ltd., <sup>3</sup>Graduate School of Information Science and Engineering, Ritsumeikan University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tsubaki2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tsubaki_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Tsubaki2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tsubaki2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-34">
        P3-34
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tsubaki2023" class="panel-collapse collapse" id="collapse-Tsubaki2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper defines the new problem of ``audio-change captioning,'' which describes what has changed between two audio samples. Conventional audio-captioning methods cannot be used to explain such change, and conventional image-change-captioning methods cannot explain the differences in audio samples. To address these issues, we propose a neural-network model for generating sentences that explain how a machine’s normal and anomalous sounds changed in relation to each other. We also created a dataset called MIMII-Change by annotating pairs of normal and anomalous samples extracted from MIMII-DG for each type of sound in machine-operation sounds. The experimental results indicate that our model with spatial attention architecture is effective for stationary sounds because it is able to determine changes in global features, while our model with Transformer Encoder architecture is effective for periodic and sudden sounds because it is able to determine temporal dependencies.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tsubaki2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Tsubaki_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tsubaki2023label" class="modal fade" id="bibtex-Tsubaki2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTsubaki2023label">
        Audio-Change Captioning to Explain Machine-Sound Anomalies
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Tsubaki2023,
    author = "Tsubaki, Shunsuke and Kawaguchi, Yohei and Nishida, Tomoya and Imoto, Keisuke and Okamoto, Yuki and Dohi, Kota and Endo, Takashi",
    title = "Audio-Change Captioning to Explain Machine-Sound Anomalies",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "201--205",
    abstract = "This paper defines the new problem of ``audio-change captioning,'' which describes what has changed between two audio samples. Conventional audio-captioning methods cannot be used to explain such change, and conventional image-change-captioning methods cannot explain the differences in audio samples. To address these issues, we propose a neural-network model for generating sentences that explain how a machine’s normal and anomalous sounds changed in relation to each other. We also created a dataset called MIMII-Change by annotating pairs of normal and anomalous samples extracted from MIMII-DG for each type of sound in machine-operation sounds. The experimental results indicate that our model with spatial attention architecture is effective for stationary sounds because it is able to determine changes in global features, while our model with Transformer Encoder architecture is effective for periodic and sudden sounds because it is able to determine temporal dependencies."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vidana-Vila2023" style="box-shadow: none">
<div class="panel-heading" id="headingVidana-Vila2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Ester Vidaña-Vila<sup>1</sup>, Jordi Malé<sup>1</sup>, Marc Freixes<sup>1</sup>, Mireia Solís-Cifré<sup>1</sup>, Miquel Jiménez<sup>1</sup>, Cristian Larrondo<sup>2,3</sup>, Raúl Guevara<sup>1</sup>, Joana Miranda<sup>2</sup>, Leticia Duboc<sup>1</sup>, Eva Mainau<sup>2</sup>, Pol Llonch<sup>2</sup> and Rosa Ma Alsina-Pagès<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>HER – Human-Environment Research, La Salle, Ramon Llull University, <sup>2</sup>AWEC Advisors S.L., Edifici Eureka, Parc de Recerca de la Universitat Autònoma de Barcelona, <sup>3</sup>Center for Applied Research in Veterinary and Agronomic Sciences, Faculty of Veterinary Medicine and Agronomy, Universidad de Las Américas
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vidana-Vila2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Vidaña-Vila_55.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Vidana-Vila2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vidana-Vila2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P5-48">
        P5-48
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vidana-Vila2023" class="panel-collapse collapse" id="collapse-Vidana-Vila2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40% and a recall of 74.05% can be achieved.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vidana-Vila2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Vidaña-Vila_55.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vidana-Vila2023label" class="modal fade" id="bibtex-Vidana-Vila2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVidana-Vila2023label">
        Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Vidana-Vila2023,
    author = "Vidaña-Vila, Ester and Malé, Jordi and Freixes, Marc and Solís-Cifré, Mireia and Jiménez, Miquel and Larrondo, Cristian and Guevara, Raúl and Miranda, Joana and Duboc, Leticia and Mainau, Eva and Llonch, Pol and Alsina-Pagès, Rosa Ma",
    title = "Automatic Detection of Cow Vocalizations Using Convolutional Neural Networks",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "206--210",
    abstract = "The well-being of animals holds significant importance in our society. Apart from the ethical concerns, recent studies have highlighted the correlation of animal growth, reproductive potential, and overall productivity with animal welfare. In this context, the vocalizations of cows have emerged as a valuable indicator of their well-being for veterinary researchers, but gathering and labelling the vocalizations for their in-depth study is time-consuming and labour-intensive. For this reason, in this work, we present an acoustic event detection algorithm that has been trained and validated with different setups using acoustic data collected from two different farms. The experimental set-up consists of a Convolutional Neural Network followed by a post-processing stage for the detection of vocalizations, so veterinary researchers can easily analyze them. The experimental evaluation assesses the importance of selecting the convenient post-processing and overlapping acoustic window for finding new vocalizations. Furthermore, the study evaluates the significance of using data collected specifically from the same farm for acoustic event detection, as opposed to employing data from a different farm. Results show that by merging training data from different farms, including the farm that is being evaluated, an F1 score of 57.40\% and a recall of 74.05\% can be achieved."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Weng2023" style="box-shadow: none">
<div class="panel-heading" id="headingWeng2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Low-Complexity Acoustic Scene Classification Using Deep Mutual Learning and Knowledge Distillation Fine-Tuning
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Shilong Weng<sup>1</sup>, Liu Yang<sup>1</sup>, Binghong Xu<sup>1</sup> and Xing Li<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of Computer Science and Cyber Engineering, Guangzhou University, <sup>2</sup>vivo Mobile Commun co Ltd
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Weng2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Weng_75.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Weng2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Weng2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-23">
        P3-23
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Weng2023" class="panel-collapse collapse" id="collapse-Weng2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, a novel model training framework constituted by deep mutual learning (DML) and knowledge distillation (KD) fine-tuning is proposed for low-complexity acoustic scene classification (ASC). The model training phase consists of two stages. In the first stage, a ResNet38 teacher model pre-trained on AudioSet and three low-complexity BC-Res2Net student models with different widths and depths are involved in DML to enhance the teacher model performance, and attain a well-initialized student model. In the second stage, we utilize KD fine-tuning to teach this student model to learn from the high-performing teacher model while maintaining the predictive performance of the teacher model. Experimental results on TAU Urban Acoustic Scenes 2022 Mobile development dataset demonstrate the effectiveness of the proposed framework as well as its superiority over using KD alone under the same configurations.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Weng2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Weng_75.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Weng2023label" class="modal fade" id="bibtex-Weng2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWeng2023label">
        Low-Complexity Acoustic Scene Classification Using Deep Mutual Learning and Knowledge Distillation Fine-Tuning
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Weng2023,
    author = "Weng, Shilong and Yang, Liu and Xu, Binghong and Li, Xing",
    title = "Low-Complexity Acoustic Scene Classification Using Deep Mutual Learning and Knowledge Distillation Fine-Tuning",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "211--215",
    abstract = "In this paper, a novel model training framework constituted by deep mutual learning (DML) and knowledge distillation (KD) fine-tuning is proposed for low-complexity acoustic scene classification (ASC). The model training phase consists of two stages. In the first stage, a ResNet38 teacher model pre-trained on AudioSet and three low-complexity BC-Res2Net student models with different widths and depths are involved in DML to enhance the teacher model performance, and attain a well-initialized student model. In the second stage, we utilize KD fine-tuning to teach this student model to learn from the high-performing teacher model while maintaining the predictive performance of the teacher model. Experimental results on TAU Urban Acoustic Scenes 2022 Mobile development dataset demonstrate the effectiveness of the proposed framework as well as its superiority over using KD alone under the same configurations."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wilkins2023" style="box-shadow: none">
<div class="panel-heading" id="headingWilkins2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Two vs. Four-Channel Sound Event Localization and Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Julia Wilkins<sup>1</sup>, Magdalena Fuentes<sup>1</sup>, Luca Bondi<sup>2</sup>, Shabnam Ghaffarzadegan<sup>2</sup>, Ali Abavisani<sup>2</sup> and Juan Pablo Bello<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>New York University, <sup>2</sup>Bosch Research
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wilkins2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Wilkins_26.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wilkins2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wilkins2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-33">
        P3-33
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wilkins2023" class="panel-collapse collapse" id="collapse-Wilkins2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound event localization and detection (SELD) systems estimate both the direction-of-arrival (DOA) and class of sound sources over time. In the DCASE 2022 SELD Challenge (Task 3), models are designed to operate in a 4-channel setting. While beneficial to further the development of SELD systems using a multichannel recording setup such as first-order Ambisonics (FOA), most consumer electronics devices rarely are able to record using more than two channels. For this reason, in this work we investigate the performance of the DCASE 2022 SELD baseline model using three audio input representations: FOA, binaural, and stereo. We perform a novel comparative analysis illustrating the effect of these audio input representations on SELD performance. Crucially, we show that binaural and stereo (i.e. 2-channel) audio-based SELD models are still able to localize and detect sound sources laterally quite well, despite overall performance degrading as less audio information is provided. Further, we segment our analysis by scenes containing varying degrees of sound source polyphony to better understand the effect of audio input representation on localization and detection performance as scene conditions become increasingly complex.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wilkins2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Wilkins_26.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wilkins2023label" class="modal fade" id="bibtex-Wilkins2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWilkins2023label">
        Two vs. Four-Channel Sound Event Localization and Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wilkins2023,
    author = "Wilkins, Julia and Fuentes, Magdalena and Bondi, Luca and Ghaffarzadegan, Shabnam and Abavisani, Ali and Bello, Juan Pablo",
    title = "Two vs. Four-Channel Sound Event Localization and Detection",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "216--220",
    abstract = "Sound event localization and detection (SELD) systems estimate both the direction-of-arrival (DOA) and class of sound sources over time. In the DCASE 2022 SELD Challenge (Task 3), models are designed to operate in a 4-channel setting. While beneficial to further the development of SELD systems using a multichannel recording setup such as first-order Ambisonics (FOA), most consumer electronics devices rarely are able to record using more than two channels. For this reason, in this work we investigate the performance of the DCASE 2022 SELD baseline model using three audio input representations: FOA, binaural, and stereo. We perform a novel comparative analysis illustrating the effect of these audio input representations on SELD performance. Crucially, we show that binaural and stereo (i.e. 2-channel) audio-based SELD models are still able to localize and detect sound sources laterally quite well, despite overall performance degrading as less audio information is provided. Further, we segment our analysis by scenes containing varying degrees of sound source polyphony to better understand the effect of audio input representation on localization and detection performance as scene conditions become increasingly complex."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wu2023" style="box-shadow: none">
<div class="panel-heading" id="headingWu2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         PLDISET: Probabilistic Localization and Detection of Independent Sound Events with Transformers
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Peipei Wu<sup>1</sup>, Jinzheng Zhao<sup>1</sup>, Yaru Chen<sup>1</sup>, Berghi Davide<sup>1</sup>, Yi Yuan<sup>1</sup>, Chenfei Zhu<sup>2</sup>, Yin Cao<sup>3</sup>, Yang Liu<sup>4</sup>, Philip J.B. J Jackson<sup>1</sup>, Mark D. Plumbley<sup>1</sup> and Wenwu Wang<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>University of Surrey, <sup>2</sup>Daqian Information, <sup>3</sup>Xi’an Jiaotong-Liverpool University, <sup>4</sup>Meta
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wu2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Wu_38.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Wu2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wu2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P3-25">
        P3-25
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wu2023" class="panel-collapse collapse" id="collapse-Wu2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sound Event Localization and Detection (SELD) is a task that involves detecting different types of sound events along with their temporal and spatial information, specifically, class-level events detection and their corresponding direction of arrivals at each frame. In practice, real-world sound scenes might be with complex conditions. For instance, DCASE challenges task 3, which contains simultaneous occurrences of up to three or even five events. Based on previous works, current distinct methods show that model size becomes bigger and bigger, and some of them are difficult to be deployed on the edge of sensor networks. To reduce the number of parameters in models, in this paper, we described Probabilistic Localization and Detection of Independent Sound Events with Transformers (PLDISET), which is a novel solution for SELD and can be extended for sound target tracking. The solution consists of three stages: first, we generate several tracks from audio features; second, those tracks are input to two transformers for SED and localization, respectively; third, a linear Gaussian system is used to predict possible location for tracks. We show the improvements of our model compared with the baseline system on the DCASE development datasets.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wu2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Wu_38.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wu2023label" class="modal fade" id="bibtex-Wu2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWu2023label">
        PLDISET: Probabilistic Localization and Detection of Independent Sound Events with Transformers
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wu2023,
    author = "Wu, Peipei and Zhao, Jinzheng and Chen, Yaru and Davide, Berghi and Yuan, Yi and Zhu, Chenfei and Cao, Yin and Liu, Yang and Jackson, Philip J.B. J and Plumbley, Mark D. and Wang, Wenwu",
    title = "{PLDISET}: Probabilistic Localization and Detection of Independent Sound Events with Transformers",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "221--225",
    abstract = "Sound Event Localization and Detection (SELD) is a task that involves detecting different types of sound events along with their temporal and spatial information, specifically, class-level events detection and their corresponding direction of arrivals at each frame. In practice, real-world sound scenes might be with complex conditions. For instance, DCASE challenges task 3, which contains simultaneous occurrences of up to three or even five events. Based on previous works, current distinct methods show that model size becomes bigger and bigger, and some of them are difficult to be deployed on the edge of sensor networks. To reduce the number of parameters in models, in this paper, we described Probabilistic Localization and Detection of Independent Sound Events with Transformers (PLDISET), which is a novel solution for SELD and can be extended for sound target tracking. The solution consists of three stages: first, we generate several tracks from audio features; second, those tracks are input to two transformers for SED and localization, respectively; third, a linear Gaussian system is used to predict possible location for tracks. We show the improvements of our model compared with the baseline system on the DCASE development datasets."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xie2023" style="box-shadow: none">
<div class="panel-heading" id="headingXie2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Crowdsourcing and Evaluating Text-Based Audio Retrieval Relevances
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Huang Xie, Khazar Khorrami, Okko Räsänen and Tuomas Virtanen
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Unit of Computing Sciences, Tampere University
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xie2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Xie_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Xie2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xie2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P1-3">
        P1-3
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xie2023" class="panel-collapse collapse" id="collapse-Xie2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper explores grading text-based audio retrieval relevances with crowdsourcing assessments. Given a free-form text (e.g., a caption) as a query, crowdworkers are asked to grade audio clips using numeric scores (between 0 and 100) to indicate their judgements of how much the sound content of an audio clip matches the text, where 0 indicates no content match at all and 100 indicates perfect content match. We integrate the crowdsourced relevances into training and evaluating text-based audio retrieval systems, and evaluate the effect of using them together with binary relevances from audio captioning. Conventionally, these binary relevances are defined by captioning-based audio-caption pairs, where being positive indicates that the caption describes the paired audio, and being negative applies to all other pairs. Experimental results indicate that there is no clear benefit from incorporating crowdsourced relevances alongside binary relevances when the crowdsourced relevances are binarized for contrastive learning. Conversely, the results suggest that using only binary relevances defined by captioning-based audio-caption pairs is sufficient for contrastive learning.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xie2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Xie_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xie2023label" class="modal fade" id="bibtex-Xie2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXie2023label">
        Crowdsourcing and Evaluating Text-Based Audio Retrieval Relevances
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xie2023,
    author = "Xie, Huang and Khorrami, Khazar and Räsänen, Okko and Virtanen, Tuomas",
    title = "Crowdsourcing and Evaluating Text-Based Audio Retrieval Relevances",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "226--230",
    abstract = "This paper explores grading text-based audio retrieval relevances with crowdsourcing assessments. Given a free-form text (e.g., a caption) as a query, crowdworkers are asked to grade audio clips using numeric scores (between 0 and 100) to indicate their judgements of how much the sound content of an audio clip matches the text, where 0 indicates no content match at all and 100 indicates perfect content match. We integrate the crowdsourced relevances into training and evaluating text-based audio retrieval systems, and evaluate the effect of using them together with binary relevances from audio captioning. Conventionally, these binary relevances are defined by captioning-based audio-caption pairs, where being positive indicates that the caption describes the paired audio, and being negative applies to all other pairs. Experimental results indicate that there is no clear benefit from incorporating crowdsourced relevances alongside binary relevances when the crowdsourced relevances are binarized for contrastive learning. Conversely, the results suggest that using only binary relevances defined by captioning-based audio-caption pairs is sufficient for contrastive learning."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yuan2023" style="box-shadow: none">
<div class="panel-heading" id="headingYuan2023" role="tab">
<div class="row">
<div class="col-xs-9">
<h4 style="color:#444;">
<strong>
         Text-Driven Foley Sound Generation with Latent Diffusion Model
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yi Yuan, Haohe Liu, Xiyuan Kang, Peipei Wu, Mark D. Plumbley and Wenwu Wang
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Centre for Vision Speech and Signal Processing, University of Surrey
         </em>
</small>
</p>
<p class="text-left">
</p>
</div>
<div class="col-xs-3 col-buttons">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yuan2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Yuan_9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Yuan2023').collapse('show');window.location.hash='#Yuan2023';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
<button aria-controls="collapse-Yuan2023" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yuan2023" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
<br/>
<div class="label label-warning item-tag" id="P2-20">
        P2-20
       </div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yuan2023" class="panel-collapse collapse" id="collapse-Yuan2023" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion-based generation system for Foley sound generation with text-driven conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the candidate clips and the embedding of the target text label. Using the proposed method, our submitted system ranks 1st in DCASE Challenge 2023 Task 7. The results of the ablation studies illustrate that the proposed techniques significantly improve sound generation performance. The codes for implementing the proposed system are available at https://github.com/yyua8222/Dcase2023_task7.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yuan2023" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2023/proceedings/DCASE2023Workshop_Yuan_9.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/yyua8222/Dcase2023_task7" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yuan2023label" class="modal fade" id="bibtex-Yuan2023" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYuan2023label">
        Text-Driven Foley Sound Generation with Latent Diffusion Model
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Yuan2023,
    author = "Yuan, Yi and Liu, Haohe and Kang, Xiyuan and Wu, Peipei and Plumbley, Mark D. and Wang, Wenwu",
    title = "Text-Driven Foley Sound Generation with Latent Diffusion Model",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)",
    address = "Tampere, Finland",
    month = "September",
    year = "2023",
    pages = "231--235",
    abstract = "Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion-based generation system for Foley sound generation with text-driven conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the candidate clips and the embedding of the target text label. Using the proposed method, our submitted system ranks 1st in DCASE Challenge 2023 Task 7. The results of the ablation studies illustrate that the proposed techniques significantly improve sound generation performance. The codes for implementing the proposed system are available at https://github.com/yyua8222/Dcase2023\_task7."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
<style>
    .item-tag{
        font-size:85%;
        padding: .5em .6em .3em;
        display: inline-block;
        margin-top: 5px;
    }
    .publication-item {
        border-bottom: 1px dotted #7e7e7e;
        border-radius: 0;
    }
    .col-buttons {
        padding-right: 0px;
        padding-left: 0px;
    }
</style>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>