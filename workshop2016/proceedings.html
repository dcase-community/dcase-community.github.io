<!DOCTYPE html><html lang="en">
<head>
    <title>Proceedings - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2016/proceedings">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="The proceedings of the DCASE2016 workshop have been published as electronic publication of Tampere University of Technology series: Virtanen, T., Mesaros, A., Heittola, T., Plumbley, M. D., Foster, P., Benetos, E., &amp; Lagrange, M. (Eds.) (2016). Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016). ISBN …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2016</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2016/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Technical program">
        <a href="/workshop2016/technical-program"><i class="fa fa-list fa-fw"></i>&nbsp;Program</a>
    </li><li class=" active" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2016/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2016/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2016/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2016/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/colors-02.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-right"><object class="img img-responsive sr-header-overlay" type="image/svg+xml" data="..//images/overlays/wave.svg"></object></div><h1 class="bold">Proceedings</h1><span class="subheading">Workshop on Detection and Classification of Acoustic Scenes and Events</span><hr class="small right bold">
                        <span class="subheading subheading-secondary">3rd of September 2016, Budapest, Hungary</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <div class="row">
<div class="col-xs-10">
<p>The proceedings of the DCASE2016 workshop have been published as electronic publication of Tampere University of Technology series:</p>
</div>
<div class="col-xs-2"></div>
</div>
<div class="row" style="display: -webkit-box !important;display: -webkit-flex !important;display: -ms-flexbox !important;display: flex !important;">
<div class="col-xs-2">
<a data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/116763/DCASE_2016_proceedings.pdf" rel="tooltip" title="PDF"><img class="img-responsive img-thumbnail" src="/images/covers/DCASE2016Workshop_proceedings_cover.png"/></a>
</div>
<div class="col-xs-8 bg-light-gray">
<p>Virtanen, T., Mesaros, A., Heittola, T., Plumbley, M. D., Foster, P., Benetos, E., &amp; Lagrange, M. (Eds.) (2016). <em>Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016).</em>
</p>
<p>ISBN (Electronic): 978-952-15-3807-0</p>
<br/>
<div class="btn-group">
<a class="btn btn-xs btn-primary" data-placement="bottom" href="https://urn.fi/URN:ISBN:978-952-15-3807-0" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" target="_blank" title="Permanent link"><i class="fa fa-link fa-1x"></i> Link</a>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://trepo.tuni.fi/bitstream/handle/10024/116763/DCASE_2016_proceedings.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="PDF"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button class="btn btn-xs btn-danger" data-target="#bibtex_proceedings" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
</div>
<dic class="col-xs-2"></dic></div>
<div aria-hidden="true" aria-labelledby="bibtex_proceedingslabel" class="modal fade" id="bibtex_proceedings" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtex_proceedingslabel">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)</h4>
</div>
<div class="modal-body">
<pre>
@book{DCASE2016Workshop,
    title = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    author = "Tuomas Virtanen and Annamaria Mesaros and Toni Heittola and Plumbley, {Mark D.} and Peter Foster and Emmanouil Benetos and Mathieu Lagrange",
    year = "2016",
    publisher = "Tampere University of Technology. Department of Signal Processing",
}
                </pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
<div class="clearfix"></div>
<div class="btex" data-scholar-cite-counts="true" data-source="content/data/workshop2016/proceedings.bib" data-stats="true">
<em>
  Total cites: 1280 (updated 30.11.2023)
 </em>
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Adavanne2016" style="box-shadow: none">
<div class="panel-heading" id="headingAdavanne2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Sound Event Detection in Multichannel Audio Using Spatial and Harmonic Features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Sharath Adavanne, Giambattista Parascandolo, Pasi Pertilä, Toni Heittola and Tuomas Virtanen
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Tampere University of Technology, Laboratory of Signal Processing, Tampere, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          135 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Adavanne2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Adavanne-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Adavanne2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Adavanne2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Adavanne2016" class="panel-collapse collapse" id="collapse-Adavanne2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose the use of spatial and harmonic features in combination with long short term memory (LSTM) recurrent neural network (RNN) for automatic sound event detection (SED) task. Real life sound recordings typically have many overlapping sound events, making it hard to recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database. The proposed method improves the F-score by 3.75% while reducing the error rate by 6%
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, multichannel, time difference of arrival, pitch, recurrent neural networks, long short term memory
      </p>
<p>
<strong>
        Cites:
       </strong>
       135 (
       <a href="https://scholar.google.com/scholar?cites=1984772077136686359" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Adavanne2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Adavanne-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Adavanne2016label" class="modal fade" id="bibtex-Adavanne2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAdavanne2016label">
        Sound Event Detection in Multichannel Audio Using Spatial and Harmonic Features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Adavanne2016,
    author = "Adavanne, Sharath and Parascandolo, Giambattista and Pertila, Pasi and Heittola, Toni and Virtanen, Tuomas",
    title = "Sound Event Detection in Multichannel Audio Using Spatial and Harmonic Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "6--10",
    abstract = "In this paper, we propose the use of spatial and harmonic features in combination with long short term memory (LSTM) recurrent neural network (RNN) for automatic sound event detection (SED) task. Real life sound recordings typically have many overlapping sound events, making it hard to recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database. The proposed method improves the F-score by 3.75\% while reducing the error rate by 6\%",
    keywords = "Sound event detection, multichannel, time difference of arrival, pitch, recurrent neural networks, long short term memory"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bae2016" style="box-shadow: none">
<div class="panel-heading" id="headingBae2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification Using Parallel Combination of LSTM and CNN
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Soo Hyun Bae, Inkyu Choi and Nam Soo Kim
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Seoul National University, Department of Electrical and Computer Engineering and INMC, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          188 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bae2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Bae-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Bae2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bae2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bae2016" class="panel-collapse collapse" id="collapse-Bae2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deep neural networks(DNNs) have recently achieved a great success in various learning task, and have also been used for classification of environmental sounds. While DNNs are showing their potential in the classification task, they cannot fully utilize the temporal information. In this paper, we propose a neural network architecture for the purpose of using sequential information. The proposed structure is composed of two seperated lower networks and one upper network. We refer to these as LSTM layers, CNN layers and connected layers, respectively. The LSTM layers extract the sequential information from consecutive audio features. The CNN layers learn the spectro-temporal locality from spectrogram images. Finally, the connected layers summarize the outputs of two networks to take advangate of the complementary features of the LSTM and CNN by combining them. To compare the proposed method with other neural netowrks, we conducted a number of experiments on the TUT acoustic scenes 2016 dataset which consists of recordings from various acoustic scenes. By using the proposed combination structure, we achieved higher performance compared to the conventional DNN, CNN and LSTM architecture.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Deep learning, sequence learning, combination of LSTM and CNN, acoustic scene classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       188 (
       <a href="https://scholar.google.com/scholar?cites=2384399465334836763" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bae2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Bae-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bae2016label" class="modal fade" id="bibtex-Bae2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBae2016label">
        Acoustic Scene Classification Using Parallel Combination of LSTM and CNN
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Bae2016,
    author = "Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo",
    title = "Acoustic Scene Classification Using Parallel Combination of {LSTM} and {CNN}",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "11--15",
    abstract = "Deep neural networks(DNNs) have recently achieved a great success in various learning task, and have also been used for classification of environmental sounds. While DNNs are showing their potential in the classification task, they cannot fully utilize the temporal information. In this paper, we propose a neural network architecture for the purpose of using sequential information. The proposed structure is composed of two seperated lower networks and one upper network. We refer to these as LSTM layers, CNN layers and connected layers, respectively. The LSTM layers extract the sequential information from consecutive audio features. The CNN layers learn the spectro-temporal locality from spectrogram images. Finally, the connected layers summarize the outputs of two networks to take advangate of the complementary features of the LSTM and CNN by combining them. To compare the proposed method with other neural netowrks, we conducted a number of experiments on the TUT acoustic scenes 2016 dataset which consists of recordings from various acoustic scenes. By using the proposed combination structure, we achieved higher performance compared to the conventional DNN, CNN and LSTM architecture.",
    keywords = "Deep learning, sequence learning, combination of LSTM and CNN, acoustic scene classification"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Choi2016" style="box-shadow: none">
<div class="panel-heading" id="headingChoi2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DNN-Based Sound Event Detection with Exemplar-Based Approach for Noise Reduction
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Inkyu Choi, Kisoo Kwon, Soo Hyun Bae and Nam Soo Kim
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Seoul National University, Department of Electrical and Computer Engineering and INMC, Seoul, Korea
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          42 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Choi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Choi-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Choi2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Choi2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Choi2016" class="panel-collapse collapse" id="collapse-Choi2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present a sound event detection system based on a deep neural network (DNN). Exemplar-based noise reduction approach is proposed for enhancing mel-band energy feature. Multi-label DNN classifier is trained for polyphonic event detection. The system is evaluated on IEEE DCASE 2016 Challenge Task 2 Train/Development Datasets. The result on the development set yields up to 0.9261 and 0.1379 in terms of F-Score and error rate on segment-based metric, respectively.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, deep neural network, exemplar-based noise reduction
      </p>
<p>
<strong>
        Cites:
       </strong>
       42 (
       <a href="https://scholar.google.com/scholar?cites=1750669289723555472" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Choi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Choi-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Choi2016label" class="modal fade" id="bibtex-Choi2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChoi2016label">
        DNN-Based Sound Event Detection with Exemplar-Based Approach for Noise Reduction
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Choi2016,
    author = "Choi, Inkyu and Kwon, Kisoo and Bae, Soo Hyun and Kim, Nam Soo",
    title = "{DNN}-Based Sound Event Detection with Exemplar-Based Approach for Noise Reduction",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "16--19",
    abstract = "In this paper, we present a sound event detection system based on a deep neural network (DNN). Exemplar-based noise reduction approach is proposed for enhancing mel-band energy feature. Multi-label DNN classifier is trained for polyphonic event detection. The system is evaluated on IEEE DCASE 2016 Challenge Task 2 Train/Development Datasets. The result on the development set yields up to 0.9261 and 0.1379 in terms of F-Score and error rate on segment-based metric, respectively.",
    keywords = "Sound event detection, deep neural network, exemplar-based noise reduction"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Elizalde2016" style="box-shadow: none">
<div class="panel-heading" id="headingElizalde2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Experiments on the DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Benjamin Elizalde<sup>1</sup>, Anurag Kumar<sup>1</sup>, Ankit Shah<sup>2</sup>, Rohan Badlani<sup>3</sup>, Emmanuel Vincent<sup>4</sup>, Bhiksha Raj<sup>1</sup>, and Ian Lane<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Carnegie Mellon University, Pittsburgh, USA, <sup>2</sup>NIT Surathkal, India, <sup>3</sup>BITS, Pilani, India, <sup>4</sup>Inria, Villers-les-Nancy, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          47 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Elizalde2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Elizalde-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Elizalde2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Elizalde2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Elizalde2016" class="panel-collapse collapse" id="collapse-Elizalde2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper we present our work on Task 1 Acoustic Scene Classi- fication and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9% compared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based Error Rate of 0.48 compared to the baseline of 0.91.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       audio, scenes, events, features, segmentation, DCASE, bag of audio words, GMMs
      </p>
<p>
<strong>
        Cites:
       </strong>
       47 (
       <a href="https://scholar.google.com/scholar?cites=6636518362988791516" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Elizalde2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Elizalde-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Elizalde2016label" class="modal fade" id="bibtex-Elizalde2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexElizalde2016label">
        Experiments on the DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Elizalde2016,
    Author = "Elizalde, Benjamin and Kumar, Anurag and Shah, Ankit and Badlani, Rohan and Vincent, Emmanuel and Raj, Bhiksha and Lane, Ian",
    title = "Experiments on the {DCASE} Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "20--24",
    abstract = "In this paper we present our work on Task 1 Acoustic Scene Classi- fication and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9\% compared to the baseline of 72.6\% and for Task 3 we achieved a Segment-Based Error Rate of 0.48 compared to the baseline of 0.91.",
    keywords = "audio, scenes, events, features, segmentation, DCASE, bag of audio words, GMMs"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Giannoulis2016" style="box-shadow: none">
<div class="panel-heading" id="headingGiannoulis2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Improved Dictionary Selection and Detection Schemes in Sparse-CNMF-Based Overlapping Acoustic Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Panagiotis Giannoulis<sup>1,3</sup>, Gerasimos Potamianos <sup>2,3</sup>, Petros Maragos <sup>1,3</sup>, and Athanasios Katsamanis <sup>1,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>School of ECE, National Technical University of Athens, Athens, Greece, <sup>2</sup>Department of ECE, University of Thessaly, Volos, Greece, <sup>3</sup>Athena Research and Innovation Center, Maroussi, Greece
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          17 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Giannoulis2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Giannoulis-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Giannoulis2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Giannoulis2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Giannoulis2016" class="panel-collapse collapse" id="collapse-Giannoulis2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we investigate sparse convolutive non-negative matrix factorization (sparse-CNMF) for detecting overlapped acoustic events in single-channel audio, within the experimental framework of Task 2 of the DCASE’16 challenge. In particular, our main focus lies on the efficient creation of the dictionary, as well as on the detection scheme associated with the CNMF approach. Specifically, we propose a shift-invariant dictionary reduction method that outperforms standard CNMF-based dictionary building. Further, we develop a novel detection algorithm that combines information from the CNMF activation matrix and atom-based reconstruction residuals, achieving significant improvement over the conventional approach based on the activations alone. The resulting system, evaluated on the development set of Task 2 of the DCASE’16 Challenge, also achieves large gains over the traditional NMF baseline provided by the Challenge organizers.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Convolutive Non-Negative Matrix Factorization, Dictionary Building, Overlapped Acoustic Event Detection
      </p>
<p>
<strong>
        Cites:
       </strong>
       17 (
       <a href="https://scholar.google.com/scholar?cites=15573837049001322463" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Giannoulis2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Giannoulis-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Giannoulis2016label" class="modal fade" id="bibtex-Giannoulis2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGiannoulis2016label">
        Improved Dictionary Selection and Detection Schemes in Sparse-CNMF-Based Overlapping Acoustic Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Giannoulis2016,
    Author = "Giannoulis, Panagiotis and Potamianos, Gerasimos and Maragos, Petros and Katsamanis, Athanasios",
    title = "Improved Dictionary Selection and Detection Schemes in Sparse-{CNMF}-Based Overlapping Acoustic Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "25--29",
    abstract = "In this paper, we investigate sparse convolutive non-negative matrix factorization (sparse-CNMF) for detecting overlapped acoustic events in single-channel audio, within the experimental framework of Task 2 of the DCASE’16 challenge. In particular, our main focus lies on the efficient creation of the dictionary, as well as on the detection scheme associated with the CNMF approach. Specifically, we propose a shift-invariant dictionary reduction method that outperforms standard CNMF-based dictionary building. Further, we develop a novel detection algorithm that combines information from the CNMF activation matrix and atom-based reconstruction residuals, achieving significant improvement over the conventional approach based on the activations alone. The resulting system, evaluated on the development set of Task 2 of the DCASE’16 Challenge, also achieves large gains over the traditional NMF baseline provided by the Challenge organizers.",
    keywords = "Convolutive Non-Negative Matrix Factorization, Dictionary Building, Overlapped Acoustic Event Detection"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gutierrez-Arriola2016" style="box-shadow: none">
<div class="panel-heading" id="headingGutierrez-Arriola2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Synthetic Sound Event Detection based on MFCC
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Juana M. Gutiérrez-Arriola, Rubén Fraile, Alexander Camacho, Thibaut Durand, Jaime L. Jarrín, and Shirley R. Mendoza
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Escuela Técnica Superior de Ingeniería y Sistemas de Telecomunicacíon, Universidad Politécnica de Madrid, Spain
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gutierrez-Arriola2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Gutierrez-Arriola-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Gutierrez-Arriola2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gutierrez-Arriola2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gutierrez-Arriola2016" class="panel-collapse collapse" id="collapse-Gutierrez-Arriola2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents a sound event detection system based on mel-frequency cepstral coefficients and a non-parametric classifier. System performance is tested using the training and development datasets corresponding to the second task of the DCASE 2016 challenge. Results indicate that the most relevant spectral information for event detection is below 8000 Hz and that the general shape of the spectral envelope is much more relevant than its fine details.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Sound event detection, spectral envelope, cepstral analysis
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="https://scholar.google.com/scholar?cites=889361176314623876" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gutierrez-Arriola2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Gutierrez-Arriola-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gutierrez-Arriola2016label" class="modal fade" id="bibtex-Gutierrez-Arriola2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGutierrez-Arriola2016label">
        Synthetic Sound Event Detection based on MFCC
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Gutierrez-Arriola2016,
    Author = "Gutiérrez-Arriola, Juana M. and Fraile, Rubén and Camacho, Alexander and Durand, Thibaut and Jarrín, Jaime L. and Mendoza, Shirley R.",
    title = "Synthetic Sound Event Detection based on {MFCC}",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "30--34",
    abstract = "This paper presents a sound event detection system based on mel-frequency cepstral coefficients and a non-parametric classifier. System performance is tested using the training and development datasets corresponding to the second task of the DCASE 2016 challenge. Results indicate that the most relevant spectral information for event detection is below 8000 Hz and that the general shape of the spectral envelope is much more relevant than its fine details.",
    keywords = "Sound event detection, spectral envelope, cepstral analysis"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hayashi2016" style="box-shadow: none">
<div class="panel-heading" id="headingHayashi2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Bidirectional LSTM-HMM Hybrid System for Polyphonic Sound Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Tomoki Hayashi<sup>1</sup>, Shinji Watanabe<sup>2</sup>, Tomoki Toda<sup>1</sup>, Takaaki Hori<sup>2</sup>, Jonathan Le Roux<sup>2</sup>, and Kazuya Takeda<sup>1</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Nagoya University, Nagoya, Japan, <sup>2</sup>Mitsubishi Electric Research Laboratories (MERL), Cambridge, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          47 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hayashi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Hayashi-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Hayashi2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hayashi2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hayashi2016" class="panel-collapse collapse" id="collapse-Hayashi2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this study, we propose a new method of polyphonic sound event detection based on a Bidirectional Long Short-Term Memory Hidden Markov Model hybrid system (BLSTM-HMM). We extend the hybrid model of neural network and HMM, which achieved state of-the-art performance in the field of speech recognition, to the multi-label classification problem. This extension provides an explicit duration model for output labels, unlike the straightforward application of BLSTM-RNN. We compare the performance of our proposed method to conventional methods such as non-negative matrix factorization (NMF) and standard BLSTM-RNN, using the DCASE2016 task 2 dataset. Our proposed method outperformed conventional approaches in both monophonic and polyphonic tasks, and finally achieved an average F1 score of 76.63% (error rate of 51.11%) on the event-based evaluation, and an average F1-score 87.16% (error rate of 25.91%) on the segment-based evaluation.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Polyphonic Sound Event Detection, Bidirectional Long Short-Term Memory, Hidden Markov Model, multilabel classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       47 (
       <a href="https://scholar.google.com/scholar?cites=3509251547559534628" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hayashi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Hayashi-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hayashi2016label" class="modal fade" id="bibtex-Hayashi2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHayashi2016label">
        Bidirectional LSTM-HMM Hybrid System for Polyphonic Sound Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Hayashi2016,
    Author = "Hayashi, Tomoki and Watanabe, Shinji and Toda, Tomoki and Hori, Takaaki and Le Roux, Jonathan and Takeda, Kazuya",
    title = "Bidirectional {LSTM}-{HMM} Hybrid System for Polyphonic Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "35--39",
    abstract = "In this study, we propose a new method of polyphonic sound event detection based on a Bidirectional Long Short-Term Memory Hidden Markov Model hybrid system (BLSTM-HMM). We extend the hybrid model of neural network and HMM, which achieved state of-the-art performance in the field of speech recognition, to the multi-label classification problem. This extension provides an explicit duration model for output labels, unlike the straightforward application of BLSTM-RNN. We compare the performance of our proposed method to conventional methods such as non-negative matrix factorization (NMF) and standard BLSTM-RNN, using the DCASE2016 task 2 dataset. Our proposed method outperformed conventional approaches in both monophonic and polyphonic tasks, and finally achieved an average F1 score of 76.63\% (error rate of 51.11\%) on the event-based evaluation, and an average F1-score 87.16\% (error rate of 25.91\%) on the segment-based evaluation.",
    keywords = "Polyphonic Sound Event Detection, Bidirectional Long Short-Term Memory, Hidden Markov Model, multilabel classification"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jean-Remy2016" style="box-shadow: none">
<div class="panel-heading" id="headingJean-Remy2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Estimating Traffic Noise Levels Using Acoustic Monitoring a Preliminary Study
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jean-Remy Gloaguen<sup>1</sup>, Arnaud Can<sup>1</sup>, Mathieu Lagrange<sup>2</sup>, and Jean-François Petiot<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Ifsttar - LAE, Bouguenais, France, <sup>2</sup>IRCCyN, École Centrale de Nantes, Nantes, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          19 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jean-Remy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Jean-Remy-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Jean-Remy2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jean-Remy2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jean-Remy2016" class="panel-collapse collapse" id="collapse-Jean-Remy2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, the Non-negative Matrix Factorization is applied for isolating the contribution of road traffic from acoustic measurements in urban sound mixtures. This method is tested on simulated scenes to enable a better control of the presence of different sound sources. The presented first results show the potential of the method.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Non-negative Matrix Factorization, road traffic noise mapping, urban measurements
      </p>
<p>
<strong>
        Cites:
       </strong>
       19 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jean-Remy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Jean-Remy-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jean-Remy2016label" class="modal fade" id="bibtex-Jean-Remy2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJean-Remy2016label">
        Estimating Traffic Noise Levels Using Acoustic Monitoring a Preliminary Study
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jean-Remy2016,
    Author = "Jean-Rémy, Gloaguen and Arnaud, Can and Mathieu, Lagrange and Jean-François, Petiot",
    title = "Estimating Traffic Noise Levels Using Acoustic Monitoring a Preliminary Study",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "40--44",
    abstract = "In this paper, the Non-negative Matrix Factorization is applied for isolating the contribution of road traffic from acoustic measurements in urban sound mixtures. This method is tested on simulated scenes to enable a better control of the presence of different sound sources. The presented first results show the potential of the method.",
    keywords = "Non-negative Matrix Factorization, road traffic noise mapping, urban measurements"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Komatsu2016" style="box-shadow: none">
<div class="panel-heading" id="headingKomatsu2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Event Detection Method Using Semi-Supervised Non-Negative Matrix Factorization with Mixtures of Local Dictionaries
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Tatsuya Komatsu, Takahiro Toizumi, Reishi Kondo, and Yuzo Senda
        </em>
</p>
<p class="text-muted">
<small>
<em>
          NEC Corporation, Data Science Research Laboratories, Kawasaki, Japan
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          85 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Komatsu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Komatsu-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Komatsu2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Komatsu2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Komatsu2016" class="panel-collapse collapse" id="collapse-Komatsu2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes an acoustic event detection (AED) method using semi-supervised non-negative matrix factorization (NMF) with a mixture of local dictionaries (MLD). The proposed method based on semi-supervised NMF newly introduces a noise dictionary and a noise activation matrix both dedicated to unknown acoustic atoms which are not included in MLD. Because unknown acoustic atoms are better modeled by the new noise dictionary learned upon classification and the new activation matrix, the proposed method provides a higher classification performance for event classes modeled by MLD when a signal to be classified is contaminated by unknown acoustic atoms. Evaluation results using DCASE2016 task 2 Dataset show that F-measure by the proposed method with semisupervised NMF is improved by as much as 11.1% compared to that by the conventional method with supervised NMF.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic event detection, Non-negative matrix factorization, Semi-supervised NMF, Mixture of local dictionaries
      </p>
<p>
<strong>
        Cites:
       </strong>
       85 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Komatsu2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Komatsu-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Komatsu2016label" class="modal fade" id="bibtex-Komatsu2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKomatsu2016label">
        Acoustic Event Detection Method Using Semi-Supervised Non-Negative Matrix Factorization with Mixtures of Local Dictionaries
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Komatsu2016,
    Author = "Komatsu, Tatsuya and Toizumi, Takahiro and Kondo, Reishi and Senda, Yuzo",
    title = "Acoustic Event Detection Method Using Semi-Supervised Non-Negative Matrix Factorization with Mixtures of Local Dictionaries",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "45--49",
    abstract = "This paper proposes an acoustic event detection (AED) method using semi-supervised non-negative matrix factorization (NMF) with a mixture of local dictionaries (MLD). The proposed method based on semi-supervised NMF newly introduces a noise dictionary and a noise activation matrix both dedicated to unknown acoustic atoms which are not included in MLD. Because unknown acoustic atoms are better modeled by the new noise dictionary learned upon classification and the new activation matrix, the proposed method provides a higher classification performance for event classes modeled by MLD when a signal to be classified is contaminated by unknown acoustic atoms. Evaluation results using DCASE2016 task 2 Dataset show that F-measure by the proposed method with semisupervised NMF is improved by as much as 11.1\% compared to that by the conventional method with supervised NMF.",
    keywords = "Acoustic event detection, Non-negative matrix factorization, Semi-supervised NMF, Mixture of local dictionaries"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2016" style="box-shadow: none">
<div class="panel-heading" id="headingKong2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Deep Neural Network Baseline for DCASE Challenge 2016
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Qiuqiang Kong, Iwona Sobieraj, Wenwu Wang, and Mark D. Plumbley
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision, Speech and Signal Processing, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          82 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Kong-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Kong2016" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2016" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Kong2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2016" class="panel-collapse collapse" id="collapse-Kong2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The DCASE Challenge 2016 contains tasks for Acoustic Acene Classification (ASC), Acoustic Event Detection (AED), and audio tagging. Since 2006, Deep Neural Networks (DNNs) have been widely applied to computer visions, speech recognition and natural language processing tasks. In this paper, we provide DNN baselines for the DCASE Challenge 2016. For feature extraction, 40 Melfilter bank features are used. Two kinds of Mel banks, same area bank and same height bank are discussed. Experimental results show that the same height bank is better than the same area bank. DNNs with the same structure are applied to all four tasks in the DCASE Challenge 2016. In Task 1 we obtained accuracy of 76.4% using Mel + DNN against 72.5% by using Mel Frequency Ceptral Coefficient (MFCC) + Gaussian Mixture Model (GMM). In Task 2 we obtained F value of 17.4% using Mel + DNN against 41.6% by using Constant Q Transform (CQT) + Nonnegative Matrix Factorization (NMF). In Task 3 we obtained F value of 38.1% using Mel + DNN against 26.6% by using MFCC + GMM. In task 4 we obtained Equal Error Rate (ERR) of 20.9% using Mel + DNN against 21.0% by using MFCC + GMM. Therefore the DNN improves the baseline in Task 1 and Task 3, and is similar to the baseline in Task 4, although is worse than the baseline in Task 2. This indicates that DNNs can be successful in many of these tasks, but may not always work.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Mel-filter bank, Deep Neural Network (DNN), Acoustic Scene Classification (ASC), Acoustic Event Detection (AED), Audio Tagging
      </p>
<p>
<strong>
        Cites:
       </strong>
       82 (
       <a href="https://scholar.google.com/scholar?cites=9126832055753471262" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Kong-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/DCASE2016_Task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="DCASE2016 Task1">
<i class="fa fa-git">
</i>
        DCASE2016 Task1
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/DCASE2016_Task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="DCASE2016 Task2">
<i class="fa fa-git">
</i>
        DCASE2016 Task2
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/DCASE2016_Task3" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="DCASE2016 Task3">
<i class="fa fa-git">
</i>
        DCASE2016 Task3
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/DCASE2016_Task4" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="DCASE2016 Task4">
<i class="fa fa-git">
</i>
        DCASE2016 Task4
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2016label" class="modal fade" id="bibtex-Kong2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2016label">
        Deep Neural Network Baseline for DCASE Challenge 2016
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kong2016,
    Author = "Kong, Qiuqiang and Sobieraj, Iwona and Wang, Wenwu and Plumbley, Mark",
    title = "Deep Neural Network Baseline for {DCASE} Challenge 2016",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "50--54",
    abstract = "The DCASE Challenge 2016 contains tasks for Acoustic Acene Classification (ASC), Acoustic Event Detection (AED), and audio tagging. Since 2006, Deep Neural Networks (DNNs) have been widely applied to computer visions, speech recognition and natural language processing tasks. In this paper, we provide DNN baselines for the DCASE Challenge 2016. For feature extraction, 40 Melfilter bank features are used. Two kinds of Mel banks, same area bank and same height bank are discussed. Experimental results show that the same height bank is better than the same area bank. DNNs with the same structure are applied to all four tasks in the DCASE Challenge 2016. In Task 1 we obtained accuracy of 76.4\% using Mel + DNN against 72.5\% by using Mel Frequency Ceptral Coefficient (MFCC) + Gaussian Mixture Model (GMM). In Task 2 we obtained F value of 17.4\% using Mel + DNN against 41.6\% by using Constant Q Transform (CQT) + Nonnegative Matrix Factorization (NMF). In Task 3 we obtained F value of 38.1\% using Mel + DNN against 26.6\% by using MFCC + GMM. In task 4 we obtained Equal Error Rate (ERR) of 20.9\% using Mel + DNN against 21.0\% by using MFCC + GMM. Therefore the DNN improves the baseline in Task 1 and Task 3, and is similar to the baseline in Task 4, although is worse than the baseline in Task 2. This indicates that DNNs can be successful in many of these tasks, but may not always work.",
    keywords = "Mel-filter bank, Deep Neural Network (DNN), Acoustic Scene Classification (ASC), Acoustic Event Detection (AED), Audio Tagging"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kuerby2016" style="box-shadow: none">
<div class="panel-heading" id="headingKuerby2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Bag-of-Features Acoustic Event Detection for Sensor Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Julian Kürby, Rene Grzeszick, Axel Plinge, and Gernot A. Fink
        </em>
</p>
<p class="text-muted">
<small>
<em>
          UTU Dortmund University, Dortmund, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          30 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kuerby2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Kuerby-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2016/presentations/Plinge-DCASE2016workshop-slides.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kuerby2016').collapse('show');window.location.hash='#Kuerby2016';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-database">
</i>
</a>
<button aria-controls="collapse-Kuerby2016" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kuerby2016" type="button">
<i class="fa fa-git">
</i>
</button>
<button aria-controls="collapse-Kuerby2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kuerby2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kuerby2016" class="panel-collapse collapse" id="collapse-Kuerby2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper a novel approach for acoustic event detection in sensor networks is presented. Improved and more robust recognition is achieved by making use of the signals from multiple sensors. To this end, various known fusion strategies are evaluated along with a novel method using classifier stacking. Detailed comparative evaluation is performed on two different datasets using 32 distributed microphones: the ITC-Irst database, and a set of smart room recordings. The stacking yields a significant improvement. The effect of using events at previously observed as well as unobserved locations is investigated. The performance of recognizing events at previously unobserved locations can be improved by sorting the channels according to their posterior probabilities.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Bag-of-Features, Acoustic Event Detection, Sensor Arrays, Robustness, Acoustic Sensor Networks
      </p>
<p>
<strong>
        Cites:
       </strong>
       30 (
       <a href="https://scholar.google.com/scholar?cites=974472499346816938" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kuerby2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Kuerby-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2016/presentations/Plinge-DCASE2016workshop-slides.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info" data-placement="bottom" href="http://patrec.cs.tu-dortmund.de/files/datasets/dcase2016_multichannel-aed_dortmund.7z" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Dataset">
<i class="fa fa-database">
</i>
        Multichannel AED Dortmund
       </a>
<a class="btn btn-sm btn-success" href="https://github.com/rgrzeszi/bof-aed" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="BOF-AED">
<i class="fa fa-git">
</i>
        BOF-AED
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kuerby2016label" class="modal fade" id="bibtex-Kuerby2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKuerby2016label">
        Bag-of-Features Acoustic Event Detection for Sensor Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kuerby2016,
    Author = "Kürby, Julian and Grzeszick, Rene and Plinge, Axel and Fink, Gernot A.",
    title = "Bag-of-Features Acoustic Event Detection for Sensor Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "55--59",
    abstract = "In this paper a novel approach for acoustic event detection in sensor networks is presented. Improved and more robust recognition is achieved by making use of the signals from multiple sensors. To this end, various known fusion strategies are evaluated along with a novel method using classifier stacking. Detailed comparative evaluation is performed on two different datasets using 32 distributed microphones: the ITC-Irst database, and a set of smart room recordings. The stacking yields a significant improvement. The effect of using events at previously observed as well as unobserved locations is investigated. The performance of recognizing events at previously unobserved locations can be improved by sorting the channels according to their posterior probabilities.",
    keywords = "Bag-of-Features, Acoustic Event Detection, Sensor Arrays, Robustness, Acoustic Sensor Networks"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lidy2016" style="box-shadow: none">
<div class="panel-heading" id="headingLidy2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         CQT-based Convolutional Neural Networks for Audio Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Thomas Lidy<sup>1</sup> and Alexander Schindler<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Vienna University of Technology, Institute of Software Technology, Vienna, Austria, <sup>2</sup>Austrian Institute of Technology, Digital Safety and Security, Vienna, Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          124 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lidy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Lidy-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Lidy2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lidy2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lidy2016" class="panel-collapse collapse" id="collapse-Lidy2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we propose a parallel Convolutional Neural Network architecture for the task of classifying acoustic scenes and urban sound scapes. A popular choice for input to a Convolutional Neural Network in audio classification problems are Mel-transformed spectrograms. We, however, show in this paper that a Constant-Q-transformed input improves results. Furthermore, we evaluated critical parameters such as the number of necessary bands and filter sizes in a Convolutional Neural Network. These are non-trivial in audio tasks due to the different semantics of the two axes of the input data: time vs. frequency. Finally, we propose a parallel (graph-based) neural network architecture which captures relevant audio characteristics both in time and in frequency. Our approach shows a 10.7% relative improvement of the baseline system of the DCASE 2016 Acoustic Scenes Classification task.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Deep Learning, Constant-Q-Transform, Convolutional Neural Networks, Audio Event Classification
      </p>
<p>
<strong>
        Cites:
       </strong>
       124 (
       <a href="https://scholar.google.com/scholar?cites=8309729151901200782" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lidy2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Lidy-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lidy2016label" class="modal fade" id="bibtex-Lidy2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLidy2016label">
        CQT-based Convolutional Neural Networks for Audio Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lidy2016,
    Author = "Lidy, Thomas and Schindler, Alexander",
    title = "{CQT}-based Convolutional Neural Networks for Audio Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "60--64",
    abstract = "In this paper, we propose a parallel Convolutional Neural Network architecture for the task of classifying acoustic scenes and urban sound scapes. A popular choice for input to a Convolutional Neural Network in audio classification problems are Mel-transformed spectrograms. We, however, show in this paper that a Constant-Q-transformed input improves results. Furthermore, we evaluated critical parameters such as the number of necessary bands and filter sizes in a Convolutional Neural Network. These are non-trivial in audio tasks due to the different semantics of the two axes of the input data: time vs. frequency. Finally, we propose a parallel (graph-based) neural network architecture which captures relevant audio characteristics both in time and in frequency. Our approach shows a 10.7\% relative improvement of the baseline system of the DCASE 2016 Acoustic Scenes Classification task.",
    keywords = "Deep Learning, Constant-Q-Transform, Convolutional Neural Networks, Audio Event Classification"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Marchi2016" style="box-shadow: none">
<div class="panel-heading" id="headingMarchi2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Erik Marchi<sup>1,3</sup>, Dario Tonelli<sup>2</sup>, Xinzhou Xu<sup>1</sup>, Fabien Ringeval<sup>1,3</sup>, Jun Deng<sup>1</sup>, Stefano Squartini<sup>2</sup>, and Björn Schuller<sup>1,3,4</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>University of Passau, Chair of Complex and Intelligent Systems, Germany, <sup>2</sup>A3LAB, Department of Information Engineering, Universitá Politecnica delle Marche, Italy, <sup>3</sup>audEERING GmbH, Gilching, Germany,<sup>4</sup>Imperial College London, Department of Computing, London, United Kingdom
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          37 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Marchi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Marchi-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Marchi2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Marchi2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Marchi2016" class="panel-collapse collapse" id="collapse-Marchi2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We propose a system for acoustic scene classification using pairwise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the `one-against-all' paradigm to perform a pairwise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.4% is achieved.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Computational Acoustic Scene Analysis, Acoustic Scene Classification, Multilayer Perceptron, Deep Neural Networks, Multiscale Kernel Analysis
      </p>
<p>
<strong>
        Cites:
       </strong>
       37 (
       <a href="https://scholar.google.com/scholar?cites=7486069541104733361" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Marchi2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Marchi-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Marchi2016label" class="modal fade" id="bibtex-Marchi2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMarchi2016label">
        Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Marchi2016,
    Author = "Marchi, Erik and Tonelli, Dario and Xu, Xinzhou and Ringeval, Fabien and Deng, Jun and Squartini, Stefano and Schuller, Bjoern",
    title = "Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "65--69",
    abstract = "We propose a system for acoustic scene classification using pairwise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the `one-against-all' paradigm to perform a pairwise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.4\% is achieved.",
    keywords = "Computational Acoustic Scene Analysis, Acoustic Scene Classification, Multilayer Perceptron, Deep Neural Networks, Multiscale Kernel Analysis"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Moritz2016" style="box-shadow: none">
<div class="panel-heading" id="headingMoritz2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Niko Moritz<sup>1</sup>, Jens Schröder<sup>1</sup>, Stefan Goetze<sup>1</sup>, Jörn Anemüller<sup>2</sup>, and Birger Kollmeier<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Fraunhofer IDMT, Project Group for Hearing, Speech, and Audio Technology, Oldenburg, Germany, <sup>2</sup>University of Oldenburg, Medizinische Physik &amp; Hearing4all, Oldenburg, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          12 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Moritz2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Moritz-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Moritz2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Moritz2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Moritz2016" class="panel-collapse collapse" id="collapse-Moritz2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE’16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in com-parison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual information similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recogni-tion accuracy of 75%, which is 2.5% higher compared to the DCASE’16 baseline system.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Time-delay neural networks, acoustic scene classification, DCASE, amplitude modulation filter bank features
      </p>
<p>
<strong>
        Cites:
       </strong>
       12 (
       <a href="https://scholar.google.com/scholar?cites=3929040454847469719" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Moritz2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Moritz-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Moritz2016label" class="modal fade" id="bibtex-Moritz2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMoritz2016label">
        Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Moritz2016,
    Author = "Moritz, Niko and Schröder, Jens and Goetze, Stefan and Anemüller, Jörn and Kollmeier, Birger",
    title = "Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "70--74",
    abstract = "This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE’16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in com-parison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual information similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recogni-tion accuracy of 75\%, which is 2.5\% higher compared to the DCASE’16 baseline system.",
    keywords = "Time-delay neural networks, acoustic scene classification, DCASE, amplitude modulation filter bank features"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pillos2016" style="box-shadow: none">
<div class="panel-heading" id="headingPillos2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         A Real-Time Environmental Sound Recognition System for the Android OS
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Angelos Pillos<sup>1</sup>, Khalid Alghamidi<sup>1</sup>, Noura Alzamel<sup>1</sup>, Veselin Pavlov<sup>1</sup>, Swetha Machanavajhala<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Computer Science Department, University College London, UK, <sup>2</sup>Microsoft, Redmond, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          36 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pillos2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Pillos-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Pillos2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pillos2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pillos2016" class="panel-collapse collapse" id="collapse-Pillos2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Sounds around us convey the context of daily life activities. There are 360 million ndividuals worldwide who experience some form of deafness. For them, missing these contexts such as fire alarm can not only be inconvenient but also life threatening. In this paper, we explore a combination of different audio feature extraction algorithms that would aid in increasing the accuracy of identifying environmental sounds and also reduce power consumption. We also design a simple approach that alleviates some of the privacy concerns, and evaluate the implemented real-time environmental sound recognition system on Android mobile devices. Our solution works in embedded mode where sound processing and recognition are performed directly on a mobile device in a way that conserves battery power. Sound signals were detected using standard deviation of normalized power sequences. Multiple feature extraction techniques like zero crossing rate, Mel-frequency cepstral coefficient (MFCC), spectral flatness, and spectral centroid were applied on the raw sound signal. Multi-layer perceptron classifier was used to identify the sound. Experimental results show improved over state-of-the-art.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Environmental sound recognition, signal processing, machine learning, Android OS
      </p>
<p>
<strong>
        Cites:
       </strong>
       36 (
       <a href="https://scholar.google.com/scholar?cites=10594589293308526362" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pillos2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Pillos-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pillos2016label" class="modal fade" id="bibtex-Pillos2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPillos2016label">
        A Real-Time Environmental Sound Recognition System for the Android OS
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Pillos2016,
    Author = "Pillos, Angelos and Alghamidi, Khalid and Alzamel, Nora and Pavlov, Veselin and Machanavajhala, Swetha",
    title = "A Real-Time Environmental Sound Recognition System for the Android {OS}",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "75--79",
    abstract = "Sounds around us convey the context of daily life activities. There are 360 million ndividuals worldwide who experience some form of deafness. For them, missing these contexts such as fire alarm can not only be inconvenient but also life threatening. In this paper, we explore a combination of different audio feature extraction algorithms that would aid in increasing the accuracy of identifying environmental sounds and also reduce power consumption. We also design a simple approach that alleviates some of the privacy concerns, and evaluate the implemented real-time environmental sound recognition system on Android mobile devices. Our solution works in embedded mode where sound processing and recognition are performed directly on a mobile device in a way that conserves battery power. Sound signals were detected using standard deviation of normalized power sequences. Multiple feature extraction techniques like zero crossing rate, Mel-frequency cepstral coefficient (MFCC), spectral flatness, and spectral centroid were applied on the raw sound signal. Multi-layer perceptron classifier was used to identify the sound. Experimental results show improved over state-of-the-art.",
    keywords = "Environmental sound recognition, signal processing, machine learning, Android OS"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schroeder2016" style="box-shadow: none">
<div class="panel-heading" id="headingSchroeder2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Performance comparison of GMM, HMM and DNN based approaches for acoustic event detection within Task 3 of the DCASE 2016 challenge
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Jens Schröder<sup>1,3</sup>, Jörn Anemüller<sup>2,3</sup>, and Stefan Goetze<sup>1,3</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Fraunhofer Institute for Digital Media Technology IDMT, Oldenburg, Germany, <sup>2</sup>University of Oldenburg, Department of Medical Physics and Acoustics, Oldenburg, Germany,<sup>3</sup>Cluster of Excellence, Hearing4all, Germany
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          24 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schroeder2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Schroeder-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Schroeder2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schroeder2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schroeder2016" class="panel-collapse collapse" id="collapse-Schroeder2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This contribution reports on the performance of systems for polyphonic acoustic event detection (AED) compared within the framework of the “detection and classification of acoustic scenes and events 2016” (DCASE’16) challenge. State-of-the-art Gaussian mixture model (GMM) and GMM-hidden Markov model (HMM) approaches are applied using Mel-frequency cepstral coefficients (MFCCs) and Gabor filterbank (GFB) features and a non-negative matrix factorization (NMF) based system. Furthermore, tandem and hybrid deep neural network (DNN)-HMMsystems are adopted. All HMM systems that usually are of multiclass type, i.e., systems that just output one label per time segment from a set of possible classes, are extended to binary classification systems that are compound of single binary classifiers classifying between target and non-target classes and, thus, are capable of multi labeling. These systems are evaluated for the data of residential areas of Task 3 from the DCASE’16 challenge. It is shown, that the DNN based system performs worse than the traditional systems for this task. Best results are achieved using GFB features in combination with a multiclass GMM-HMM approach.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       acoustic event detection, DCASE2016, Gabor filterbank, deep neural network
      </p>
<p>
<strong>
        Cites:
       </strong>
       24 (
       <a href="https://scholar.google.com/scholar?cites=758416649109089726" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schroeder2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Schroeder-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schroeder2016label" class="modal fade" id="bibtex-Schroeder2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchroeder2016label">
        Performance comparison of GMM, HMM and DNN based approaches for acoustic event detection within Task 3 of the DCASE 2016 challenge
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schroeder2016,
    Author = "Schröder, Jens and Anemüller, Jörn and Goetze, Stefan",
    title = "Performance comparison of {GMM}, {HMM} and {DNN} based approaches for acoustic event detection within Task 3 of the {DCASE} 2016 challenge",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "80--84",
    abstract = "This contribution reports on the performance of systems for polyphonic acoustic event detection (AED) compared within the framework of the “detection and classification of acoustic scenes and events 2016” (DCASE’16) challenge. State-of-the-art Gaussian mixture model (GMM) and GMM-hidden Markov model (HMM) approaches are applied using Mel-frequency cepstral coefficients (MFCCs) and Gabor filterbank (GFB) features and a non-negative matrix factorization (NMF) based system. Furthermore, tandem and hybrid deep neural network (DNN)-HMMsystems are adopted. All HMM systems that usually are of multiclass type, i.e., systems that just output one label per time segment from a set of possible classes, are extended to binary classification systems that are compound of single binary classifiers classifying between target and non-target classes and, thus, are capable of multi labeling. These systems are evaluated for the data of residential areas of Task 3 from the DCASE’16 challenge. It is shown, that the DNN based system performs worse than the traditional systems for this task. Best results are achieved using GFB features in combination with a multiclass GMM-HMM approach.",
    keywords = "acoustic event detection, DCASE2016, Gabor filterbank, deep neural network"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="SenaMafra2016" style="box-shadow: none">
<div class="panel-heading" id="headingSenaMafra2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Acoustic Scene Classification: An evaluation of an extremely compact feature representation
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Gustavo Sena Mafra<sup>1</sup>, Ngoc Q. K. Duong<sup>2</sup>, Alexey Ozerov<sup>2</sup>, and Patrick Perez<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universidade Federal de Santa Catarina, Santa Catarina, Brazil, <sup>2</sup>Technicolor, Cesson-Sévigné, France
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          17 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-SenaMafra2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/SenaMafra-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-SenaMafra2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-SenaMafra2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-SenaMafra2016" class="panel-collapse collapse" id="collapse-SenaMafra2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper investigates several approaches to address the acoustic scene classification (ASC) task. We start from low-level feature representation for segmented audio frames and investigate different time granularity for feature aggregation. We study the use of support vector machine (SVM), as a well-known classifier, together with two popular neural network (NN) architectures, namely multilayer perceptron (MLP) and convolutional neural network (CNN), for higher level feature learning and classification. We evaluate the performance of these approaches on benchmark datasets provided from the 2013 and 2016 Detection and Classification of Acoustic Scenes and Events (DCASE) challenges. We observe that a simple approach exploiting averaged Mel-log-spectrogram, as an extremely compact feature, and SVM can obtain even better result than NN-based approaches and comparable performance with the best systems in the DCASE 2013 challenge.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, Audio features, Multilayer Perceptron, Convolutional Neural Network, Support Vector Machine
      </p>
<p>
<strong>
        Cites:
       </strong>
       17 (
       <a href="https://scholar.google.com/scholar?cites=6842676904487590166" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-SenaMafra2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/SenaMafra-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-SenaMafra2016label" class="modal fade" id="bibtex-SenaMafra2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSenaMafra2016label">
        Acoustic Scene Classification: An evaluation of an extremely compact feature representation
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{SenaMafra2016,
    Author = "Sena Mafra, Gustavo and Duong, Ngoc Q. K. and Ozerov, Alexey and Perez, Patrick",
    title = "Acoustic Scene Classification: An evaluation of an extremely compact feature representation",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "85--89",
    abstract = "This paper investigates several approaches to address the acoustic scene classification (ASC) task. We start from low-level feature representation for segmented audio frames and investigate different time granularity for feature aggregation. We study the use of support vector machine (SVM), as a well-known classifier, together with two popular neural network (NN) architectures, namely multilayer perceptron (MLP) and convolutional neural network (CNN), for higher level feature learning and classification. We evaluate the performance of these approaches on benchmark datasets provided from the 2013 and 2016 Detection and Classification of Acoustic Scenes and Events (DCASE) challenges. We observe that a simple approach exploiting averaged Mel-log-spectrogram, as an extremely compact feature, and SVM can obtain even better result than NN-based approaches and comparable performance with the best systems in the DCASE 2013 challenge.",
    keywords = "Acoustic scene classification, Audio features, Multilayer Perceptron, Convolutional Neural Network, Support Vector Machine"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sobieraj2016" style="box-shadow: none">
<div class="panel-heading" id="headingSobieraj2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Coupled Sparse NMF vs. Random Forest Classification for Real Life Acoustic Event Detection
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Iwona Sobieraj and Mark D. Plumbley
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision Speech and Signal Processing, Surrey, United Kingdom
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          6 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sobieraj2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Sobieraj-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Sobieraj2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sobieraj2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sobieraj2016" class="panel-collapse collapse" id="collapse-Sobieraj2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Coupled non-negative matrix factorization (NMF) of spectral representations and class activity annotations has shown promising results for acoustic event detection (AED) in real life environments. Recently, a new dataset has been proposed for development of algorithms for real life AED. In this paper we propose two methods for real life polyphonic AED: Coupled Sparse Non-negative Matrix Factorization (CSNMF) of time-frequency patches with class activity annotations and Multi-class Random Forest classification (MRF) of time-frequency patches, and compare their performance on this new dataset. Both our methods outperform the DCASE2016 baseline in terms of F-score. Moreover, we show that as the dataset is unbalanced, a classifier that recognizes a few most frequent classes may outperform the sparse NMF-approach and a baseline based on Gaussian Mixture Models.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic event detection, random forest classifier, non-negative matrix factorization, sparse representation
      </p>
<p>
<strong>
        Cites:
       </strong>
       6 (
       <a href="https://scholar.google.com/scholar?cites=12195392041203844974" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sobieraj2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Sobieraj-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sobieraj2016label" class="modal fade" id="bibtex-Sobieraj2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSobieraj2016label">
        Coupled Sparse NMF vs. Random Forest Classification for Real Life Acoustic Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Sobieraj2016,
    Author = "Sobieraj, Iwona and Plumbley, Mark D.",
    title = "Coupled Sparse {NMF} vs. Random Forest Classification for Real Life Acoustic Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "90--94",
    abstract = "Coupled non-negative matrix factorization (NMF) of spectral representations and class activity annotations has shown promising results for acoustic event detection (AED) in real life environments. Recently, a new dataset has been proposed for development of algorithms for real life AED. In this paper we propose two methods for real life polyphonic AED: Coupled Sparse Non-negative Matrix Factorization (CSNMF) of time-frequency patches with class activity annotations and Multi-class Random Forest classification (MRF) of time-frequency patches, and compare their performance on this new dataset. Both our methods outperform the DCASE2016 baseline in terms of F-score. Moreover, we show that as the dataset is unbalanced, a classifier that recognizes a few most frequent classes may outperform the sparse NMF-approach and a baseline based on Gaussian Mixture Models.",
    keywords = "Acoustic event detection, random forest classifier, non-negative matrix factorization, sparse representation"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Valenti2016" style="box-shadow: none">
<div class="panel-heading" id="headingValenti2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Michele Valenti<sup>1</sup>, Aleksandr Diment<sup>2</sup>, Giambattista Parascandolo<sup>2</sup>, Stefano Squartini<sup>1</sup>, Tuomas Virtanen<sup>2</sup>
</em>
</p>
<p class="text-muted">
<small>
<em>
<sup>1</sup>Universita Politecnica delle Marche, Department of Information Engineering, Ancona, Italy, <sup>2</sup>Tampere University of Technology, Department of Signal Processing, Tampere, Finland
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          191 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Valenti2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Valenti-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-info" data-placement="bottom" href="../documents/workshop2016/presentations/Valenti-DCASE2016workshop-slides.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download slides">
<i class="fa fa-file-text-o fa-1x">
</i>
         Slides
        </a>
<button aria-controls="collapse-Valenti2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Valenti2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Valenti2016" class="panel-collapse collapse" id="collapse-Valenti2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This workshop paper presents our contribution for the task of acoustic scene classification proposed for the “detection and classification of acoustic scenes and events” (D-CASE) 2016 challenge. We propose the use of a convolutional neural network trained to classify short sequences of audio, represented by their log-mel spectrogram. In addition we use a training method that can be used when the validation performance of the system saturates as the training proceeds. The performance is evaluated on the public acoustic scene classification development dataset provided for the D-CASE challenge. The best accuracy score obtained by our configuration on a four-folded cross-validation setup is 79.0%. It constitutes a 8.8% relative improvement with respect to the baseline system, based on a Gaussian mixture model classifier.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, convolutional neural networks, DCASE, computational audio processing
      </p>
<p>
<strong>
        Cites:
       </strong>
       191 (
       <a href="https://scholar.google.com/scholar?cites=10420819719932043802" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Valenti2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Valenti-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
<a class="btn btn-sm btn-info" data-placement="bottom" href="../documents/workshop2016/presentations/Valenti-DCASE2016workshop-slides.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download slides">
<i class="fa fa-file-powerpoint-o">
</i>
        Slides
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Valenti2016label" class="modal fade" id="bibtex-Valenti2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexValenti2016label">
        DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Valenti2016,
    Author = "Valenti, Michele and Diment, Aleksandr and Parascandolo, Giambattista and Squartini, Stefano and Virtanen, Tuomas",
    title = "{DCASE} 2016 Acoustic Scene Classification Using Convolutional Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "95--99",
    abstract = "This workshop paper presents our contribution for the task of acoustic scene classification proposed for the “detection and classification of acoustic scenes and events” (D-CASE) 2016 challenge. We propose the use of a convolutional neural network trained to classify short sequences of audio, represented by their log-mel spectrogram. In addition we use a training method that can be used when the validation performance of the system saturates as the training proceeds. The performance is evaluated on the public acoustic scene classification development dataset provided for the D-CASE challenge. The best accuracy score obtained by our configuration on a four-folded cross-validation setup is 79.0\%. It constitutes a 8.8\% relative improvement with respect to the baseline system, based on a Gaussian mixture model classifier.",
    keywords = "Acoustic scene classification, convolutional neural networks, DCASE, computational audio processing"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Valle2016" style="box-shadow: none">
<div class="panel-heading" id="headingValle2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         ABROA: Audio-Based Room-Occupancy Analysis Using Gaussian Mixtures and Hidden Markov Models
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Rafael Valle
        </em>
</p>
<p class="text-muted">
<small>
<em>
          UC Berkeley, Center for New Music and Audio Technologies (CNMAT), Berkeley, USA
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          13 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Valle2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Valle-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Valle2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Valle2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Valle2016" class="panel-collapse collapse" id="collapse-Valle2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper outlines preliminary steps towards the development of an audio-based room-occupancy analysis model. Our approach borrows from speech recognition tradition and is based on Gaussian Mixtures and Hidden Markov Models. We analyze possible challenges encountered in the development of such a model, and offer several solutions including feature design and prediction strategies. We provide results obtained from experiments with audio data from a retail store in Palo Alto, California. Model assessment is done via leave- two-out Bootstrap and model convergence achieves good ac- curacy, thus representing a contribution to multimodal people counting algorithms.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic Traffic Monitoring, Audio Forensics, Retail Analytics
      </p>
<p>
<strong>
        Cites:
       </strong>
       13 (
       <a href="https://scholar.google.com/scholar?cites=4296943606740221501" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Valle2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Valle-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Valle2016label" class="modal fade" id="bibtex-Valle2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexValle2016label">
        ABROA: Audio-Based Room-Occupancy Analysis Using Gaussian Mixtures and Hidden Markov Models
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Valle2016,
    Author = "Valle, Rafael",
    title = "{ABROA}: Audio-Based Room-Occupancy Analysis Using Gaussian Mixtures and Hidden Markov Models",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "100--104",
    abstract = "This paper outlines preliminary steps towards the development of an audio-based room-occupancy analysis model. Our approach borrows from speech recognition tradition and is based on Gaussian Mixtures and Hidden Markov Models. We analyze possible challenges encountered in the development of such a model, and offer several solutions including feature design and prediction strategies. We provide results obtained from experiments with audio data from a retail store in Palo Alto, California. Model assessment is done via leave- two-out Bootstrap and model convergence achieves good ac- curacy, thus representing a contribution to multimodal people counting algorithms.",
    keywords = "Acoustic Traffic Monitoring, Audio Forensics, Retail Analytics"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2016a" style="box-shadow: none">
<div class="panel-heading" id="headingXu2016a" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Hierarchical Learning for DNN-Based Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yong Xu, Qiang Huang, Wenwu Wang, and Mark D. Plumbley
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision, Speech and Signal Processing, Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          37 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2016a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Xu-a-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Xu2016a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2016a" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2016a" class="panel-collapse collapse" id="collapse-Xu2016a" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present a deep neural network (DNN)-based acoustic scene classification framework. Two hierarchical learning methods are proposed to improve the DNN baseline performance by incorporating the hierarchical taxonomy information of environmental sounds. Firstly, the parameters of the DNN are initialized by the proposed hierarchical pre-training. Multi-level objective function is then adopted to add more constraint on the cross-entropy based loss function. A series of experiments were conducted on the Task1 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The final DNN-based system achieved a 22.9% relative improvement on average scene classification error as compared with the Gaussian Mixture Model (GMM)-based benchmark system across four standard folds.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic scene classification, deep neural network, hierarchical pre-training, multi-level objective function
      </p>
<p>
<strong>
        Cites:
       </strong>
       37 (
       <a href="https://scholar.google.com/scholar?cites=16044408344346812426" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2016a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Xu-a-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2016alabel" class="modal fade" id="bibtex-Xu2016a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2016alabel">
        Hierarchical Learning for DNN-Based Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xu2016a,
    Author = "Xu, Yong and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.",
    title = "Hierarchical Learning for {DNN}-Based Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "110--114",
    abstract = "In this paper, we present a deep neural network (DNN)-based acoustic scene classification framework. Two hierarchical learning methods are proposed to improve the DNN baseline performance by incorporating the hierarchical taxonomy information of environmental sounds. Firstly, the parameters of the DNN are initialized by the proposed hierarchical pre-training. Multi-level objective function is then adopted to add more constraint on the cross-entropy based loss function. A series of experiments were conducted on the Task1 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The final DNN-based system achieved a 22.9\% relative improvement on average scene classification error as compared with the Gaussian Mixture Model (GMM)-based benchmark system across four standard folds.",
    keywords = "Acoustic scene classification, deep neural network, hierarchical pre-training, multi-level objective function"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2016b" style="box-shadow: none">
<div class="panel-heading" id="headingXu2016b" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Fully DNN-Based Multi-Label Regression for Audio Tagging
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Yong Xu, Qiang Huang, Wenwu Wang, Philip J. B. Jackson, and Mark D. Plumbley
        </em>
</p>
<p class="text-muted">
<small>
<em>
          University of Surrey, Centre for Vision, Speech and Signal Processing, Surrey, UK
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          23 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2016b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Xu-b-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Xu2016b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2016b" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2016b" class="panel-collapse collapse" id="collapse-Xu2016b" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Acoustic event detection for content analysis in most cases relies on lots of labeled data. However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far. Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data. This is highly desirable to some practical applications using audio analysis. In this paper we propose to use a fully deep neural network (DNN) framework to handle the multi-label classification task in a regression way. Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags. The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector. A deep pyramid structure was also designed to extract more robust high-level features related to the target tags. Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments. Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input. The results show that our approach obtained a 15% relative improvement compared with the official GMM-based method of DCASE 2016 challenge.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Audio tagging, deep neural networks, multilabel regression, dropout, DCASE 2016
      </p>
<p>
<strong>
        Cites:
       </strong>
       23 (
       <a href="https://scholar.google.com/scholar?cites=5357511615038785111" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2016b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Xu-b-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2016blabel" class="modal fade" id="bibtex-Xu2016b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2016blabel">
        Fully DNN-Based Multi-Label Regression for Audio Tagging
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Xu2016b,
    Author = "Xu, Yong and Huang, Qiang and Wang, Wenwu and Jackson, Philip and Plumbley, Mark D.",
    title = "Fully {DNN}-Based Multi-Label Regression for Audio Tagging",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "105-109",
    abstract = "Acoustic event detection for content analysis in most cases relies on lots of labeled data. However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far. Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data. This is highly desirable to some practical applications using audio analysis. In this paper we propose to use a fully deep neural network (DNN) framework to handle the multi-label classification task in a regression way. Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags. The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector. A deep pyramid structure was also designed to extract more robust high-level features related to the target tags. Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments. Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input. The results show that our approach obtained a 15\% relative improvement compared with the official GMM-based method of DCASE 2016 challenge.",
    keywords = "Audio tagging, deep neural networks, multilabel regression, dropout, DCASE 2016"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zoehrer2016" style="box-shadow: none">
<div class="panel-heading" id="headingZoehrer2016" role="tab">
<div class="row">
<div class="col-xs-8">
<h4 style="color:#444;">
<strong>
         Gated Recurrent Networks applied to Acoustic Scene Classification
        </strong>
</h4>
<p class="text-left" style="font-size:110%">
<em>
         Matthias Zöhrer and Franz Pernkopf
        </em>
</p>
<p class="text-muted">
<small>
<em>
          Graz University of Technology, Signal Processing and Speech Communication Laboratory, Austria
         </em>
</small>
</p>
<p class="text-left">
<span style="padding-left:5px">
<span class="badge" title="Number of citations">
          49 cites
         </span>
</span>
</p>
</div>
<div class="col-xs-4">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zoehrer2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Zohrer-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<button aria-controls="collapse-Zoehrer2016" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zoehrer2016" type="button">
<i class="fa fa-align-left">
</i>
         Abstract
        </button>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zoehrer2016" class="panel-collapse collapse" id="collapse-Zoehrer2016" role="tabpanel">
<div class="panel-body well well-sm">
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We present a resource efficient framework for acoustic scene classification. In particular, we combine gated recurrent neural networks (GRNNs) and a linear discriminant analysis (LDA) objective for efficiently classifying environmental sound scenes of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). Our system reaches an overall accuracy of 79.1% on development data, resulting in a relative improvement of 8.34% compared to the baseline GMM system. We further investigate semi-supervised learning applied to acoustic scene analysis. In particular, we evaluate the effects of virtual adversarial training (VAT), the use of a hybrid, i.e. generative-discriminative, objective function.
      </p>
<h5>
       Keywords
      </h5>
<p class="text-justify">
       Acoustic Scene Labeling, Gated Recurrent Networks, Deep Linear Discriminant Analysis, Semi-Supervised Learning
      </p>
<p>
<strong>
        Cites:
       </strong>
       49 (
       <a href="None" target="_blank">
        see at Google Scholar
       </a>
       )
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zoehrer2016" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/workshop2016/proceedings/Zohrer-DCASE2016workshop.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zoehrer2016label" class="modal fade" id="bibtex-Zoehrer2016" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZoehrer2016label">
        Gated Recurrent Networks applied to Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zoehrer2016,
    Author = "Zöhrer, Matthias and Pernkopf, Franz",
    title = "Gated Recurrent Networks applied to Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    year = "2016",
    month = "September",
    pages = "115--119",
    abstract = "We present a resource efficient framework for acoustic scene classification. In particular, we combine gated recurrent neural networks (GRNNs) and a linear discriminant analysis (LDA) objective for efficiently classifying environmental sound scenes of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). Our system reaches an overall accuracy of 79.1\% on development data, resulting in a relative improvement of 8.34\% compared to the baseline GMM system. We further investigate semi-supervised learning applied to acoustic scene analysis. In particular, we evaluate the effects of virtual adversarial training (VAT), the use of a hybrid, i.e. generative-discriminative, objective function.",
    keywords = "Acoustic Scene Labeling, Gated Recurrent Networks, Deep Linear Discriminant Analysis, Semi-Supervised Learning"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>