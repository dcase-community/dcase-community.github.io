<!DOCTYPE html><html lang="en">
<head>
    <title>General-purpose audio tagging of Freesound content with AudioSet labels - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2018/task-general-purpose-audio-tagging-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description This task evaluates systems for general-purpose audio tagging with an increased number of categories and using data with annotations of varying reliability. This task will provide insight towards the development of broadly-applicable sound event classifiers that consider an increased and diverse amount of categories. More detailed task description …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2018/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2018/task-general-purpose-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2018/task-general-purpose-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-bird-audio-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-monitoring-domestic-activities" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-home text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2018/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2018/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/marina-bay-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-success"></i><i class="fa dc-tags fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Tags</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 2</span></span><img src="../images/logos/dcase/dcase2018_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">General-purpose audio tagging of Freesound content with AudioSet labels</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#input-characteristics">Input characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>This task evaluates systems for general-purpose audio tagging with an increased number of categories and using data with annotations of varying reliability. This task will provide insight towards the development of broadly-applicable sound event classifiers that consider an increased and diverse amount of categories.</p>
<p>More detailed task description can be found in the <a href="/challenge2018/task-general-purpose-audio-tagging">task description page</a> or in the <a href="https://www.kaggle.com/c/freesound-audio-tagging">competition page in Kaggle</a>.</p>
<p><strong>IMPORTANT NOTE 1</strong>: the task results shown in this page <strong>only include</strong> the submissions that were made using the DCASE submission system. Therefore, there might be some entries appearing in the <a href="https://www.kaggle.com/c/freesound-audio-tagging/leaderboard">official Kaggle leaderboard</a> that do not appear here. Be aware that because of the missing entries, the ranking of some teams might be different in this page when compared to the Kaggle leaderboard.</p>
<p><strong>IMPORTANT NOTE 2</strong>: some of the DCASE submissions failed to run evaluation in the private leaderboard because of breaking the runtime or memory constraints set by <a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/overview/kernels-requirements">Kaggle's Kernels-only competition rules</a>. These submissions are listed here with a score of 0.0 in the private leaderboard. We're working on computing private leaderboard scores for these submissions as well even though they will be disqualified from the official ranking.</p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_map3" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3 (Private leaderboard)" data-chartable="true" data-field="eval_map3" data-sortable="true" data-value-type="float4">
                mAP@3 <br/>(Private leaderboard)*
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3 (Public leaderboard)" data-chartable="true" data-field="eval_map3_pub" data-sortable="true" data-value-type="float4">
                mAP@3 <br/>(Public leaderboard)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Fonseca2018</td>
<td>0.6943</td>
<td>0.7049</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_1</td>
<td>Cochlear.ai_1</td>
<td>Jeong2018</td>
<td>0.9538</td>
<td>0.9751</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_2</td>
<td>Cochlear.ai_2</td>
<td>Jeong2018</td>
<td>0.9506</td>
<td>0.9751</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_3</td>
<td>Cochlear.ai_3</td>
<td>Jeong2018</td>
<td>0.9405</td>
<td>0.9729</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_1</td>
<td>NTU_ensemble8</td>
<td>Nguyen2018</td>
<td>0.9496</td>
<td>0.9635</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_2</td>
<td>NTU_labelsmoothing</td>
<td>Nguyen2018</td>
<td>0.9251</td>
<td>0.9413</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_3</td>
<td>NTU_bgnormalization</td>
<td>Nguyen2018</td>
<td>0.9213</td>
<td>0.9297</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_4</td>
<td>NTU_en8_augment_test</td>
<td>Nguyen2018</td>
<td>0.9478</td>
<td>0.9601</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_1</td>
<td>CNN on Raw-Audio and Spectrogram</td>
<td>Wilhelm2018</td>
<td>0.9435</td>
<td>0.9662</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_2</td>
<td>CNN on Raw-Audio and Spectrogram</td>
<td>Wilhelm2018</td>
<td>0.9416</td>
<td>0.9568</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_1</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9151</td>
<td>0.9585</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_2</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9133</td>
<td>0.9585</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_3</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9139</td>
<td>0.9579</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_4</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9174</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_1</td>
<td>Multi-level attention model on fine-tuned AudioSet features</td>
<td>Xu2018</td>
<td>0.9065</td>
<td>0.9363</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_2</td>
<td>Multi-level attention model on fine-tuned AudioSet features</td>
<td>Xu2018</td>
<td>0.9081</td>
<td>0.9319</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_1</td>
<td>3 CNN L1 Stacked Fused Spectral XGBoost L2</td>
<td>Chakraborty2018</td>
<td>0.9328</td>
<td>0.9480</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_2</td>
<td>2 CNN results geometrically averaged</td>
<td>Chakraborty2018</td>
<td>0.9320</td>
<td>0.9452</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_judges_award</td>
<td>VGG Style CNN with 3 channel input</td>
<td>Chakraborty2018</td>
<td>0.9079</td>
<td>0.9258</td>
</tr>
<tr>
<td></td>
<td>Han_NPU_task2_1</td>
<td>2ModEnsem</td>
<td>Han2018</td>
<td>0.8723</td>
<td>0.9181</td>
</tr>
<tr>
<td></td>
<td>Zhesong_PKU_task2_1</td>
<td>BCNN_WaveNet</td>
<td>Yu2018</td>
<td>0.8807</td>
<td>0.9197</td>
</tr>
<tr>
<td></td>
<td>Hanyu_BUPT_task2</td>
<td>CRNN</td>
<td>Hanyu2018</td>
<td>0.7877</td>
<td>0.8029</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_1</td>
<td>Kuaiyu tagging system</td>
<td>WEI2018</td>
<td>0.9409</td>
<td>0.9690</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_2</td>
<td>Kuaiyu tagging system</td>
<td>WEI2018</td>
<td>0.9423</td>
<td>0.9673</td>
</tr>
<tr>
<td></td>
<td>Colangelo_RM3_task2_1</td>
<td>DCASE2018 Task2 CRNN RM3</td>
<td>Colangelo2018</td>
<td>0.6978</td>
<td>0.7309</td>
</tr>
<tr>
<td></td>
<td>Shan_DBSonics_task2_1</td>
<td>Shan DBSonics approach</td>
<td>Ren2018</td>
<td>0.9405</td>
<td>0.9734</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_1</td>
<td>DCASE2018 Meta-learning system</td>
<td>Kele2018</td>
<td>0.9498</td>
<td>0.9779</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_2</td>
<td>DCASE2018 Meta-learning system</td>
<td>Kele2018</td>
<td>0.9441</td>
<td>0.9662</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_1</td>
<td>Fusion of 4 CNN</td>
<td>Agafonov2018</td>
<td>0.9174</td>
<td>0.9502</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_2</td>
<td>Fusion of 4 CNN</td>
<td>Agafonov2018</td>
<td>0.9275</td>
<td>0.9491</td>
</tr>
<tr>
<td></td>
<td>Wilkinghoff_FKIE_task2_1</td>
<td>CNN Ensemble based on Multiple Features</td>
<td>Wilkinghoff2018</td>
<td>0.9414</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Pantic_ETF_task2_1</td>
<td>Ensemble of convolutional neural networks for general purpose audio tagging</td>
<td>Pantic2018</td>
<td>0.9419</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_1</td>
<td>2 average pooling</td>
<td>Khadkevich2018</td>
<td>0.9188</td>
<td>0.9131</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_2</td>
<td>2 max pooling</td>
<td>Khadkevich2018</td>
<td>0.9178</td>
<td>0.9103</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_1</td>
<td>Stacked CNN-CRNN (4 Models)</td>
<td>Iqbal2018</td>
<td>0.9484</td>
<td>0.9568</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_2</td>
<td>Stacked CNN-CRNN (8 Models)</td>
<td>Iqbal2018</td>
<td>0.9512</td>
<td>0.9612</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_1</td>
<td>Surrey baseline CNN 8 layers</td>
<td>Kong2018</td>
<td>0.9034</td>
<td>0.9203</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_2</td>
<td>Surrey baseline CNN 4 layers</td>
<td>Kong2018</td>
<td>0.8622</td>
<td>0.8854</td>
</tr>
<tr>
<td></td>
<td>Dorfer_CPJKU_task2_1</td>
<td>CNN - Iterative Self-Verification</td>
<td>Dorfer2018</td>
<td>0.9518</td>
<td>0.9563</td>
</tr>
</tbody>
</table>
<p><b>*</b> Unless stated otherwise, all reported mAP@3 scores are computed using the ground truth for the private leaderboard.</p>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="eval_map3" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3 (Private leaderboard)" data-chartable="true" data-field="eval_map3" data-sortable="true" data-value-type="float4">
                mAP@3 <br/>(Private leaderboard)
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3 (Public leaderboard)" data-chartable="true" data-field="eval_map3_pub" data-sortable="true" data-value-type="float4">
                mAP@3 <br/>(Public leaderboard)
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Fonseca2018</td>
<td>0.6943</td>
<td>0.7049</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_1</td>
<td>3 CNN L1 Stacked Fused Spectral XGBoost L2</td>
<td>Chakraborty2018</td>
<td>0.9328</td>
<td>0.9480</td>
</tr>
<tr>
<td></td>
<td>Han_NPU_task2_1</td>
<td>2ModEnsem</td>
<td>Han2018</td>
<td>0.8723</td>
<td>0.9181</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_1</td>
<td>Surrey baseline CNN 8 layers</td>
<td>Kong2018</td>
<td>0.9034</td>
<td>0.9203</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_1</td>
<td>Cochlear.ai_1</td>
<td>Jeong2018</td>
<td>0.9538</td>
<td>0.9751</td>
</tr>
<tr>
<td></td>
<td>Dorfer_CPJKU_task2_1</td>
<td>CNN - Iterative Self-Verification</td>
<td>Dorfer2018</td>
<td>0.9518</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Shan_DBSonics_task2_1</td>
<td>Shan DBSonics approach</td>
<td>Ren2018</td>
<td>0.9405</td>
<td>0.9734</td>
</tr>
<tr>
<td></td>
<td>Wilkinghoff_FKIE_task2_1</td>
<td>CNN Ensemble based on Multiple Features</td>
<td>Wilkinghoff2018</td>
<td>0.9414</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_1</td>
<td>NTU_ensemble8</td>
<td>Nguyen2018</td>
<td>0.9496</td>
<td>0.9635</td>
</tr>
<tr>
<td></td>
<td>Zhesong_PKU_task2_1</td>
<td>BCNN_WaveNet</td>
<td>Yu2018</td>
<td>0.8807</td>
<td>0.9197</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_1</td>
<td>CNN on Raw-Audio and Spectrogram</td>
<td>Wilhelm2018</td>
<td>0.9435</td>
<td>0.9662</td>
</tr>
<tr>
<td></td>
<td>Hanyu_BUPT_task2</td>
<td>CRNN</td>
<td>Hanyu2018</td>
<td>0.7877</td>
<td>0.8029</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_1</td>
<td>DCASE2018 Meta-learning system</td>
<td>Kele2018</td>
<td>0.9498</td>
<td>0.9779</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_2</td>
<td>Fusion of 4 CNN</td>
<td>Agafonov2018</td>
<td>0.9275</td>
<td>0.9491</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_4</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9174</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Pantic_ETF_task2_1</td>
<td>Ensemble of convolutional neural networks for general purpose audio tagging</td>
<td>Pantic2018</td>
<td>0.9419</td>
<td>0.9563</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_1</td>
<td>2 average pooling</td>
<td>Khadkevich2018</td>
<td>0.9188</td>
<td>0.9131</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_2</td>
<td>Stacked CNN-CRNN (8 Models)</td>
<td>Iqbal2018</td>
<td>0.9512</td>
<td>0.9612</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_2</td>
<td>Multi-level attention model on fine-tuned AudioSet features</td>
<td>Xu2018</td>
<td>0.9081</td>
<td>0.9319</td>
</tr>
<tr>
<td></td>
<td>Colangelo_RM3_task2_1</td>
<td>DCASE2018 Task2 CRNN RM3</td>
<td>Colangelo2018</td>
<td>0.6978</td>
<td>0.7309</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_2</td>
<td>Kuaiyu tagging system</td>
<td>WEI2018</td>
<td>0.9423</td>
<td>0.9673</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<p>The table below shows the mAP@3 scores computed per class.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2018 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Jeong_COCAI_task2_1" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "mAP@3", 
        "fields": ["eval_map3_class_acoustic_guitar", "eval_map3_class_applause", "eval_map3_class_bark", "eval_map3_class_bass_drum", "eval_map3_class_burping_or_eructation", "eval_map3_class_bus", "eval_map3_class_cello", "eval_map3_class_chime", "eval_map3_class_clarinet", "eval_map3_class_computer_keyboard", "eval_map3_class_cough", "eval_map3_class_cowbell", "eval_map3_class_double_bass", "eval_map3_class_drawer_open_or_close", "eval_map3_class_electric_piano", "eval_map3_class_fart", "eval_map3_class_finger_snapping", "eval_map3_class_fireworks", "eval_map3_class_flute", "eval_map3_class_glockenspiel", "eval_map3_class_gong", "eval_map3_class_gunshot_or_gunfire", "eval_map3_class_harmonica", "eval_map3_class_hi-hat", "eval_map3_class_keys_jangling", "eval_map3_class_knock", "eval_map3_class_meow", "eval_map3_class_microwave_oven", "eval_map3_class_oboe", "eval_map3_class_saxophone", "eval_map3_class_scissors", "eval_map3_class_shatter", "eval_map3_class_snare_drum", "eval_map3_class_squeak", "eval_map3_class_tambourine", "eval_map3_class_tearing", "eval_map3_class_telephone", "eval_map3_class_trumpet", "eval_map3_class_violin_or_fiddle", "eval_map3_class_writing"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="eval_map3" data-scatter-y="eval_map3" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="eval_map3" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
            Submission code
        </th>
<th class="sm-cell" data-field="name" data-sortable="true">
            Name
        </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
            Tech. report
        </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3" data-chartable="true" data-field="eval_map3" data-sortable="true" data-value-type="float4">
            mAP@3
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_acoustic_guitar" data-sortable="true" data-value-type="float4">
            acoustic guitar 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_applause" data-sortable="true" data-value-type="float4">
            applause 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_bark" data-sortable="true" data-value-type="float4">
            bark 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_bass_drum" data-sortable="true" data-value-type="float4">
            bass drum 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_burping_or_eructation" data-sortable="true" data-value-type="float4">
            burping or eructation 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_bus" data-sortable="true" data-value-type="float4">
            bus 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_cello" data-sortable="true" data-value-type="float4">
            cello 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_chime" data-sortable="true" data-value-type="float4">
            chime 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_clarinet" data-sortable="true" data-value-type="float4">
            clarinet 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_computer_keyboard" data-sortable="true" data-value-type="float4">
            computer keyboard 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_cough" data-sortable="true" data-value-type="float4">
            cough 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_cowbell" data-sortable="true" data-value-type="float4">
            cowbell 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_double_bass" data-sortable="true" data-value-type="float4">
            double bass 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_drawer_open_or_close" data-sortable="true" data-value-type="float4">
            drawer open or close 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_electric_piano" data-sortable="true" data-value-type="float4">
            electric piano 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_fart" data-sortable="true" data-value-type="float4">
            fart 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_finger_snapping" data-sortable="true" data-value-type="float4">
            finger snapping 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_fireworks" data-sortable="true" data-value-type="float4">
            fireworks 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_flute" data-sortable="true" data-value-type="float4">
            flute 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_glockenspiel" data-sortable="true" data-value-type="float4">
            glockenspiel 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_gong" data-sortable="true" data-value-type="float4">
            gong 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_gunshot_or_gunfire" data-sortable="true" data-value-type="float4">
            gunshot or gunfire 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_harmonica" data-sortable="true" data-value-type="float4">
            harmonica 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_hi-hat" data-sortable="true" data-value-type="float4">
            hi-hat 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_keys_jangling" data-sortable="true" data-value-type="float4">
            keys jangling 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_knock" data-sortable="true" data-value-type="float4">
            knock 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_meow" data-sortable="true" data-value-type="float4">
            meow 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_microwave_oven" data-sortable="true" data-value-type="float4">
            microwave oven 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_oboe" data-sortable="true" data-value-type="float4">
            oboe 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_saxophone" data-sortable="true" data-value-type="float4">
            saxophone 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_scissors" data-sortable="true" data-value-type="float4">
            scissors 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_shatter" data-sortable="true" data-value-type="float4">
            shatter 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_snare_drum" data-sortable="true" data-value-type="float4">
            snare drum 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_squeak" data-sortable="true" data-value-type="float4">
            squeak 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_tambourine" data-sortable="true" data-value-type="float4">
            tambourine 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_tearing" data-sortable="true" data-value-type="float4">
            tearing 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_telephone" data-sortable="true" data-value-type="float4">
            telephone 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_trumpet" data-sortable="true" data-value-type="float4">
            trumpet 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_violin_or_fiddle" data-sortable="true" data-value-type="float4">
            violin or fiddle 
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="eval_map3_class_writing" data-sortable="true" data-value-type="float4">
            writing 
        </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Fonseca2018</td>
<td>0.6943</td>
<td>0.6713</td>
<td>0.9744</td>
<td>0.8623</td>
<td>0.5797</td>
<td>0.7051</td>
<td>0.4833</td>
<td>0.8447</td>
<td>0.7708</td>
<td>0.9778</td>
<td>0.5397</td>
<td>0.7222</td>
<td>0.5637</td>
<td>0.6458</td>
<td>0.0625</td>
<td>0.7372</td>
<td>0.6181</td>
<td>0.7222</td>
<td>0.5449</td>
<td>0.8939</td>
<td>0.5694</td>
<td>0.8500</td>
<td>0.1373</td>
<td>0.8827</td>
<td>0.5208</td>
<td>0.7536</td>
<td>0.8854</td>
<td>0.8681</td>
<td>0.5764</td>
<td>0.8922</td>
<td>0.8447</td>
<td>0.2667</td>
<td>0.6528</td>
<td>0.3155</td>
<td>0.1806</td>
<td>0.8229</td>
<td>0.9773</td>
<td>0.6496</td>
<td>0.8611</td>
<td>0.7356</td>
<td>0.6319</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_1</td>
<td>Cochlear.ai_1</td>
<td>Jeong2018</td>
<td>0.9538</td>
<td>0.9398</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8917</td>
<td>0.9659</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.9683</td>
<td>1.0000</td>
<td>0.9853</td>
<td>1.0000</td>
<td>0.8681</td>
<td>0.9744</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8077</td>
<td>1.0000</td>
<td>0.8681</td>
<td>0.9833</td>
<td>0.9412</td>
<td>0.9815</td>
<td>1.0000</td>
<td>0.9348</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.9722</td>
<td>1.0000</td>
<td>0.9451</td>
<td>0.8250</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6458</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7479</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.9375</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_2</td>
<td>Cochlear.ai_2</td>
<td>Jeong2018</td>
<td>0.9506</td>
<td>0.9444</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8917</td>
<td>0.9659</td>
<td>0.8681</td>
<td>1.0000</td>
<td>0.9683</td>
<td>1.0000</td>
<td>0.9804</td>
<td>1.0000</td>
<td>0.8681</td>
<td>0.9744</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7692</td>
<td>1.0000</td>
<td>0.8681</td>
<td>0.9833</td>
<td>0.9510</td>
<td>0.9815</td>
<td>1.0000</td>
<td>0.9058</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.9514</td>
<td>1.0000</td>
<td>0.9318</td>
<td>0.8000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6319</td>
<td>0.9844</td>
<td>1.0000</td>
<td>0.7393</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_3</td>
<td>Cochlear.ai_3</td>
<td>Jeong2018</td>
<td>0.9405</td>
<td>0.9398</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7500</td>
<td>0.9508</td>
<td>0.8819</td>
<td>1.0000</td>
<td>0.9444</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8403</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7564</td>
<td>1.0000</td>
<td>0.8264</td>
<td>0.9833</td>
<td>0.8922</td>
<td>0.9444</td>
<td>1.0000</td>
<td>0.8406</td>
<td>0.9167</td>
<td>0.9792</td>
<td>0.9375</td>
<td>1.0000</td>
<td>0.9527</td>
<td>0.5750</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6181</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7692</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9167</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_1</td>
<td>NTU_ensemble8</td>
<td>Nguyen2018</td>
<td>0.9496</td>
<td>0.8981</td>
<td>1.0000</td>
<td>0.9710</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9250</td>
<td>0.9621</td>
<td>0.8264</td>
<td>1.0000</td>
<td>0.9286</td>
<td>1.0000</td>
<td>0.9804</td>
<td>1.0000</td>
<td>0.8889</td>
<td>0.9808</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7949</td>
<td>1.0000</td>
<td>0.8264</td>
<td>0.9667</td>
<td>0.9085</td>
<td>0.9444</td>
<td>1.0000</td>
<td>0.8913</td>
<td>0.9583</td>
<td>0.9792</td>
<td>0.9792</td>
<td>0.9804</td>
<td>0.9886</td>
<td>0.6000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6736</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7949</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_2</td>
<td>NTU_labelsmoothing</td>
<td>Nguyen2018</td>
<td>0.9251</td>
<td>0.8380</td>
<td>1.0000</td>
<td>0.9783</td>
<td>0.9565</td>
<td>1.0000</td>
<td>0.9000</td>
<td>0.9356</td>
<td>0.7708</td>
<td>1.0000</td>
<td>0.9762</td>
<td>0.9792</td>
<td>0.9412</td>
<td>0.9844</td>
<td>0.8958</td>
<td>0.8846</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.7564</td>
<td>0.9432</td>
<td>0.7917</td>
<td>0.9667</td>
<td>0.9183</td>
<td>0.9012</td>
<td>0.9792</td>
<td>0.8261</td>
<td>0.9583</td>
<td>0.9792</td>
<td>0.8681</td>
<td>0.9804</td>
<td>0.9943</td>
<td>0.6417</td>
<td>0.9375</td>
<td>0.9643</td>
<td>0.5625</td>
<td>0.9792</td>
<td>0.9091</td>
<td>0.7991</td>
<td>0.9611</td>
<td>0.9808</td>
<td>0.9097</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_3</td>
<td>NTU_bgnormalization</td>
<td>Nguyen2018</td>
<td>0.9213</td>
<td>0.8796</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8250</td>
<td>0.9583</td>
<td>0.8958</td>
<td>0.9593</td>
<td>0.8492</td>
<td>1.0000</td>
<td>0.9559</td>
<td>0.9844</td>
<td>0.7778</td>
<td>0.9423</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6538</td>
<td>1.0000</td>
<td>0.7431</td>
<td>0.9167</td>
<td>0.8268</td>
<td>0.9198</td>
<td>1.0000</td>
<td>0.8623</td>
<td>0.9583</td>
<td>0.9792</td>
<td>0.9583</td>
<td>0.9804</td>
<td>0.9394</td>
<td>0.5000</td>
<td>0.9792</td>
<td>0.9643</td>
<td>0.6458</td>
<td>1.0000</td>
<td>0.9773</td>
<td>0.7991</td>
<td>0.9833</td>
<td>0.9693</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_4</td>
<td>NTU_en8_augment_test</td>
<td>Nguyen2018</td>
<td>0.9478</td>
<td>0.8981</td>
<td>1.0000</td>
<td>0.9493</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9250</td>
<td>0.9621</td>
<td>0.8264</td>
<td>1.0000</td>
<td>0.9286</td>
<td>1.0000</td>
<td>0.9804</td>
<td>1.0000</td>
<td>0.8889</td>
<td>0.9808</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8077</td>
<td>1.0000</td>
<td>0.8472</td>
<td>0.9500</td>
<td>0.8987</td>
<td>0.9444</td>
<td>1.0000</td>
<td>0.8841</td>
<td>0.9583</td>
<td>0.9792</td>
<td>0.9792</td>
<td>0.9804</td>
<td>0.9886</td>
<td>0.5750</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6597</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7778</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_1</td>
<td>CNN on Raw-Audio and Spectrogram</td>
<td>Wilhelm2018</td>
<td>0.9435</td>
<td>0.8657</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9417</td>
<td>0.9394</td>
<td>0.8681</td>
<td>0.9889</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9323</td>
<td>0.9097</td>
<td>1.0000</td>
<td>0.9306</td>
<td>1.0000</td>
<td>0.7949</td>
<td>1.0000</td>
<td>0.8472</td>
<td>0.9611</td>
<td>0.9510</td>
<td>0.9630</td>
<td>0.9792</td>
<td>0.9130</td>
<td>0.9479</td>
<td>1.0000</td>
<td>0.9792</td>
<td>0.9706</td>
<td>0.9356</td>
<td>0.9083</td>
<td>0.9514</td>
<td>0.9107</td>
<td>0.5903</td>
<td>1.0000</td>
<td>0.9773</td>
<td>0.7692</td>
<td>0.9833</td>
<td>0.9828</td>
<td>0.9375</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_2</td>
<td>CNN on Raw-Audio and Spectrogram</td>
<td>Wilhelm2018</td>
<td>0.9416</td>
<td>0.8519</td>
<td>1.0000</td>
<td>0.9710</td>
<td>0.9565</td>
<td>1.0000</td>
<td>0.9167</td>
<td>0.9773</td>
<td>0.8472</td>
<td>0.9889</td>
<td>0.9762</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9219</td>
<td>0.9514</td>
<td>0.9615</td>
<td>0.9514</td>
<td>1.0000</td>
<td>0.8013</td>
<td>0.9886</td>
<td>0.9167</td>
<td>0.9444</td>
<td>0.9412</td>
<td>0.9444</td>
<td>0.9688</td>
<td>0.8768</td>
<td>0.9375</td>
<td>1.0000</td>
<td>0.9583</td>
<td>0.9657</td>
<td>0.9242</td>
<td>0.9250</td>
<td>0.9514</td>
<td>0.8929</td>
<td>0.6319</td>
<td>0.9844</td>
<td>0.9318</td>
<td>0.8077</td>
<td>0.9444</td>
<td>0.9943</td>
<td>0.9792</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_1</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9151</td>
<td>0.8843</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6750</td>
<td>0.9545</td>
<td>0.8681</td>
<td>1.0000</td>
<td>0.8254</td>
<td>0.9792</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8125</td>
<td>0.9423</td>
<td>0.9167</td>
<td>0.9383</td>
<td>0.7500</td>
<td>0.9167</td>
<td>0.9306</td>
<td>0.9333</td>
<td>0.8562</td>
<td>0.9198</td>
<td>0.9792</td>
<td>0.8623</td>
<td>0.9688</td>
<td>0.9583</td>
<td>0.9722</td>
<td>0.9559</td>
<td>0.9602</td>
<td>0.5167</td>
<td>0.8611</td>
<td>1.0000</td>
<td>0.5486</td>
<td>1.0000</td>
<td>0.9470</td>
<td>0.6795</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.7639</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_2</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9133</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6750</td>
<td>0.9545</td>
<td>0.8611</td>
<td>1.0000</td>
<td>0.8016</td>
<td>0.9722</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8333</td>
<td>0.9423</td>
<td>0.8958</td>
<td>0.9444</td>
<td>0.7372</td>
<td>0.9167</td>
<td>0.9236</td>
<td>0.9444</td>
<td>0.8497</td>
<td>0.8951</td>
<td>0.9792</td>
<td>0.8623</td>
<td>0.9688</td>
<td>0.9583</td>
<td>0.9514</td>
<td>0.9510</td>
<td>0.9564</td>
<td>0.5250</td>
<td>0.8681</td>
<td>1.0000</td>
<td>0.5208</td>
<td>1.0000</td>
<td>0.9773</td>
<td>0.6923</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.7639</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_3</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9139</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6750</td>
<td>0.9545</td>
<td>0.8611</td>
<td>1.0000</td>
<td>0.8492</td>
<td>0.9792</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8542</td>
<td>0.9423</td>
<td>0.9167</td>
<td>0.9444</td>
<td>0.7308</td>
<td>0.9167</td>
<td>0.9097</td>
<td>0.9444</td>
<td>0.8693</td>
<td>0.8951</td>
<td>0.9792</td>
<td>0.8188</td>
<td>0.9688</td>
<td>0.9583</td>
<td>0.9514</td>
<td>0.9510</td>
<td>0.9564</td>
<td>0.5167</td>
<td>0.8611</td>
<td>1.0000</td>
<td>0.5208</td>
<td>1.0000</td>
<td>0.9773</td>
<td>0.6923</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.7431</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_4</td>
<td>ConResNet</td>
<td>Kim2018</td>
<td>0.9174</td>
<td>0.8981</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.6750</td>
<td>0.9545</td>
<td>0.8889</td>
<td>1.0000</td>
<td>0.8730</td>
<td>1.0000</td>
<td>0.9853</td>
<td>1.0000</td>
<td>0.8056</td>
<td>0.9231</td>
<td>0.9167</td>
<td>0.9753</td>
<td>0.7628</td>
<td>0.9280</td>
<td>0.9514</td>
<td>0.9444</td>
<td>0.8693</td>
<td>0.9259</td>
<td>0.9688</td>
<td>0.8406</td>
<td>0.9688</td>
<td>0.9583</td>
<td>0.9722</td>
<td>0.9559</td>
<td>0.9545</td>
<td>0.5417</td>
<td>0.8681</td>
<td>1.0000</td>
<td>0.5347</td>
<td>1.0000</td>
<td>0.9242</td>
<td>0.6795</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.7431</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_1</td>
<td>Multi-level attention model on fine-tuned AudioSet features</td>
<td>Xu2018</td>
<td>0.9065</td>
<td>0.8611</td>
<td>1.0000</td>
<td>0.9348</td>
<td>0.9710</td>
<td>1.0000</td>
<td>0.9667</td>
<td>0.9205</td>
<td>0.8264</td>
<td>0.9778</td>
<td>0.8810</td>
<td>0.9514</td>
<td>1.0000</td>
<td>0.9688</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.7986</td>
<td>0.9815</td>
<td>0.7821</td>
<td>0.9432</td>
<td>0.8125</td>
<td>0.9222</td>
<td>0.9183</td>
<td>0.9383</td>
<td>0.8385</td>
<td>0.8478</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.9306</td>
<td>0.9804</td>
<td>0.8939</td>
<td>0.6500</td>
<td>0.8889</td>
<td>0.8571</td>
<td>0.4097</td>
<td>0.9375</td>
<td>1.0000</td>
<td>0.7821</td>
<td>0.9056</td>
<td>0.9770</td>
<td>0.7917</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_2</td>
<td>Multi-level attention model on fine-tuned AudioSet features</td>
<td>Xu2018</td>
<td>0.9081</td>
<td>0.8565</td>
<td>1.0000</td>
<td>0.9348</td>
<td>0.9783</td>
<td>1.0000</td>
<td>0.9333</td>
<td>0.9545</td>
<td>0.8611</td>
<td>0.9852</td>
<td>0.9048</td>
<td>0.9514</td>
<td>1.0000</td>
<td>0.9531</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.8264</td>
<td>0.9815</td>
<td>0.7949</td>
<td>0.9432</td>
<td>0.7917</td>
<td>0.9222</td>
<td>0.9118</td>
<td>0.9630</td>
<td>0.8229</td>
<td>0.8478</td>
<td>0.9479</td>
<td>0.9792</td>
<td>0.9097</td>
<td>0.9853</td>
<td>0.8939</td>
<td>0.5667</td>
<td>0.9167</td>
<td>0.8690</td>
<td>0.4028</td>
<td>0.9375</td>
<td>1.0000</td>
<td>0.7949</td>
<td>0.9111</td>
<td>0.9770</td>
<td>0.8264</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_1</td>
<td>3 CNN L1 Stacked Fused Spectral XGBoost L2</td>
<td>Chakraborty2018</td>
<td>0.9328</td>
<td>0.9120</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7917</td>
<td>0.9621</td>
<td>0.8542</td>
<td>1.0000</td>
<td>0.8968</td>
<td>1.0000</td>
<td>0.9853</td>
<td>0.9844</td>
<td>0.8750</td>
<td>0.9167</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7115</td>
<td>0.9621</td>
<td>0.8125</td>
<td>0.9500</td>
<td>0.9118</td>
<td>0.9074</td>
<td>0.9323</td>
<td>0.7971</td>
<td>0.9635</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.9804</td>
<td>0.9716</td>
<td>0.6167</td>
<td>0.9583</td>
<td>0.9643</td>
<td>0.6458</td>
<td>1.0000</td>
<td>0.9773</td>
<td>0.7650</td>
<td>0.9778</td>
<td>0.9943</td>
<td>0.9097</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_2</td>
<td>2 CNN results geometrically averaged</td>
<td>Chakraborty2018</td>
<td>0.9320</td>
<td>0.9074</td>
<td>1.0000</td>
<td>0.9348</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8750</td>
<td>0.9545</td>
<td>0.8125</td>
<td>0.9889</td>
<td>0.9762</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9844</td>
<td>0.9167</td>
<td>0.9744</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7308</td>
<td>0.9773</td>
<td>0.8542</td>
<td>0.9500</td>
<td>0.9150</td>
<td>0.9136</td>
<td>0.9479</td>
<td>0.8623</td>
<td>0.9844</td>
<td>0.9583</td>
<td>0.9722</td>
<td>0.9853</td>
<td>0.9621</td>
<td>0.5667</td>
<td>1.0000</td>
<td>0.9821</td>
<td>0.4306</td>
<td>1.0000</td>
<td>0.9773</td>
<td>0.7393</td>
<td>0.9667</td>
<td>0.9943</td>
<td>0.8194</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_judges_award</td>
<td>VGG Style CNN with 3 channel input</td>
<td>Chakraborty2018</td>
<td>0.9079</td>
<td>0.8333</td>
<td>1.0000</td>
<td>0.8841</td>
<td>0.9783</td>
<td>1.0000</td>
<td>0.8917</td>
<td>0.9508</td>
<td>0.8542</td>
<td>0.9889</td>
<td>0.9048</td>
<td>0.9792</td>
<td>0.9853</td>
<td>0.9688</td>
<td>0.8958</td>
<td>0.9038</td>
<td>0.9306</td>
<td>1.0000</td>
<td>0.6603</td>
<td>0.9508</td>
<td>0.8542</td>
<td>0.9000</td>
<td>0.8758</td>
<td>0.9198</td>
<td>0.8958</td>
<td>0.8116</td>
<td>0.9844</td>
<td>0.9375</td>
<td>1.0000</td>
<td>0.9412</td>
<td>0.9413</td>
<td>0.6333</td>
<td>0.8750</td>
<td>0.9464</td>
<td>0.4028</td>
<td>0.9479</td>
<td>0.8561</td>
<td>0.7265</td>
<td>0.9500</td>
<td>0.9923</td>
<td>0.8194</td>
</tr>
<tr>
<td></td>
<td>Han_NPU_task2_1</td>
<td>2ModEnsem</td>
<td>Han2018</td>
<td>0.8723</td>
<td>0.8056</td>
<td>1.0000</td>
<td>0.9130</td>
<td>0.9130</td>
<td>1.0000</td>
<td>0.6750</td>
<td>0.9280</td>
<td>0.7986</td>
<td>0.9778</td>
<td>0.7937</td>
<td>1.0000</td>
<td>0.9118</td>
<td>0.9375</td>
<td>0.8264</td>
<td>0.8333</td>
<td>0.9514</td>
<td>0.9383</td>
<td>0.6410</td>
<td>0.8864</td>
<td>0.7708</td>
<td>0.9667</td>
<td>0.7516</td>
<td>0.9012</td>
<td>0.9375</td>
<td>0.8768</td>
<td>0.8958</td>
<td>0.8472</td>
<td>0.7917</td>
<td>0.9412</td>
<td>0.8826</td>
<td>0.6083</td>
<td>0.8472</td>
<td>0.8750</td>
<td>0.5764</td>
<td>0.9792</td>
<td>0.9470</td>
<td>0.6239</td>
<td>0.9444</td>
<td>0.9674</td>
<td>0.8750</td>
</tr>
<tr>
<td></td>
<td>Zhesong_PKU_task2_1</td>
<td>BCNN_WaveNet</td>
<td>Yu2018</td>
<td>0.8807</td>
<td>0.8287</td>
<td>0.9808</td>
<td>0.9348</td>
<td>0.9783</td>
<td>0.9808</td>
<td>0.7750</td>
<td>0.9773</td>
<td>0.7847</td>
<td>0.9741</td>
<td>0.7222</td>
<td>0.9583</td>
<td>0.8971</td>
<td>0.9792</td>
<td>0.8958</td>
<td>0.8141</td>
<td>0.8264</td>
<td>0.9753</td>
<td>0.6090</td>
<td>0.9053</td>
<td>0.7153</td>
<td>0.8944</td>
<td>0.8301</td>
<td>0.9198</td>
<td>0.9635</td>
<td>0.8406</td>
<td>0.9062</td>
<td>0.9792</td>
<td>0.8819</td>
<td>0.9559</td>
<td>0.8939</td>
<td>0.4583</td>
<td>0.8681</td>
<td>0.9048</td>
<td>0.5139</td>
<td>0.9844</td>
<td>0.8864</td>
<td>0.6709</td>
<td>0.9333</td>
<td>0.9828</td>
<td>0.8333</td>
</tr>
<tr>
<td></td>
<td>Hanyu_BUPT_task2</td>
<td>CRNN</td>
<td>Hanyu2018</td>
<td>0.7877</td>
<td>0.7685</td>
<td>1.0000</td>
<td>0.9348</td>
<td>0.7174</td>
<td>0.9423</td>
<td>0.7583</td>
<td>0.9129</td>
<td>0.7500</td>
<td>0.9444</td>
<td>0.7698</td>
<td>0.9167</td>
<td>0.9265</td>
<td>0.8438</td>
<td>0.6806</td>
<td>0.7756</td>
<td>0.8056</td>
<td>0.5988</td>
<td>0.7949</td>
<td>0.9205</td>
<td>0.5278</td>
<td>0.8833</td>
<td>0.5131</td>
<td>0.7901</td>
<td>0.7344</td>
<td>0.7971</td>
<td>0.7135</td>
<td>0.7708</td>
<td>0.6389</td>
<td>0.9216</td>
<td>0.9318</td>
<td>0.2667</td>
<td>0.8819</td>
<td>0.6071</td>
<td>0.3542</td>
<td>0.8906</td>
<td>1.0000</td>
<td>0.5256</td>
<td>0.7889</td>
<td>0.8774</td>
<td>0.6458</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_1</td>
<td>Kuaiyu tagging system</td>
<td>WEI2018</td>
<td>0.9409</td>
<td>0.9583</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8250</td>
<td>0.9773</td>
<td>0.8403</td>
<td>1.0000</td>
<td>0.9683</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8542</td>
<td>0.8846</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8205</td>
<td>0.9659</td>
<td>0.8819</td>
<td>0.9667</td>
<td>0.9346</td>
<td>0.9136</td>
<td>1.0000</td>
<td>0.8913</td>
<td>0.9844</td>
<td>0.9792</td>
<td>0.9167</td>
<td>0.9657</td>
<td>0.9773</td>
<td>0.7333</td>
<td>0.9792</td>
<td>0.9821</td>
<td>0.5208</td>
<td>1.0000</td>
<td>0.9545</td>
<td>0.7436</td>
<td>0.9278</td>
<td>1.0000</td>
<td>0.8750</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_2</td>
<td>Kuaiyu tagging system</td>
<td>WEI2018</td>
<td>0.9423</td>
<td>0.9583</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8250</td>
<td>0.9773</td>
<td>0.8681</td>
<td>1.0000</td>
<td>0.9683</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8542</td>
<td>0.8846</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8205</td>
<td>0.9659</td>
<td>0.8819</td>
<td>0.9667</td>
<td>0.9379</td>
<td>0.9321</td>
<td>1.0000</td>
<td>0.8913</td>
<td>0.9844</td>
<td>0.9792</td>
<td>0.9167</td>
<td>0.9657</td>
<td>0.9754</td>
<td>0.7333</td>
<td>0.9792</td>
<td>0.9821</td>
<td>0.5278</td>
<td>1.0000</td>
<td>0.9545</td>
<td>0.7521</td>
<td>0.9500</td>
<td>1.0000</td>
<td>0.8542</td>
</tr>
<tr>
<td></td>
<td>Colangelo_RM3_task2_1</td>
<td>DCASE2018 Task2 CRNN RM3</td>
<td>Colangelo2018</td>
<td>0.6978</td>
<td>0.7361</td>
<td>0.9808</td>
<td>0.6812</td>
<td>0.8841</td>
<td>0.7885</td>
<td>0.6750</td>
<td>0.7538</td>
<td>0.7778</td>
<td>0.8000</td>
<td>0.5873</td>
<td>0.7083</td>
<td>0.9216</td>
<td>0.7656</td>
<td>0.6875</td>
<td>0.5641</td>
<td>0.7847</td>
<td>0.5741</td>
<td>0.5064</td>
<td>0.7500</td>
<td>0.6528</td>
<td>0.7444</td>
<td>0.6895</td>
<td>0.5247</td>
<td>0.8438</td>
<td>0.7029</td>
<td>0.9323</td>
<td>0.8472</td>
<td>0.6042</td>
<td>0.3480</td>
<td>0.6742</td>
<td>0.5833</td>
<td>0.7431</td>
<td>0.8750</td>
<td>0.3403</td>
<td>0.9010</td>
<td>0.8030</td>
<td>0.4145</td>
<td>0.7111</td>
<td>0.5632</td>
<td>0.6250</td>
</tr>
<tr>
<td></td>
<td>Shan_DBSonics_task2_1</td>
<td>Shan DBSonics approach</td>
<td>Ren2018</td>
<td>0.9405</td>
<td>0.9120</td>
<td>1.0000</td>
<td>0.9710</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7167</td>
<td>0.9508</td>
<td>0.8264</td>
<td>1.0000</td>
<td>0.9206</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8750</td>
<td>0.9038</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.8013</td>
<td>0.9886</td>
<td>0.8264</td>
<td>0.9667</td>
<td>0.9020</td>
<td>0.9568</td>
<td>1.0000</td>
<td>0.8768</td>
<td>0.9635</td>
<td>0.9583</td>
<td>1.0000</td>
<td>0.9559</td>
<td>0.9716</td>
<td>0.6750</td>
<td>0.9514</td>
<td>0.9286</td>
<td>0.6875</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7906</td>
<td>0.9833</td>
<td>0.9943</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_1</td>
<td>DCASE2018 Meta-learning system</td>
<td>Kele2018</td>
<td>0.9498</td>
<td>0.9120</td>
<td>1.0000</td>
<td>0.9783</td>
<td>0.9783</td>
<td>1.0000</td>
<td>0.8083</td>
<td>0.9735</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.9762</td>
<td>1.0000</td>
<td>0.9706</td>
<td>1.0000</td>
<td>0.9306</td>
<td>0.8974</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8846</td>
<td>0.9773</td>
<td>0.7917</td>
<td>0.9500</td>
<td>0.8954</td>
<td>0.9815</td>
<td>1.0000</td>
<td>0.9130</td>
<td>0.9635</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9853</td>
<td>0.9886</td>
<td>0.7250</td>
<td>0.9514</td>
<td>0.9821</td>
<td>0.6389</td>
<td>0.9844</td>
<td>1.0000</td>
<td>0.8162</td>
<td>1.0000</td>
<td>0.9943</td>
<td>0.9514</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_2</td>
<td>DCASE2018 Meta-learning system</td>
<td>Kele2018</td>
<td>0.9441</td>
<td>0.8750</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7250</td>
<td>0.9659</td>
<td>0.9167</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9706</td>
<td>1.0000</td>
<td>0.9167</td>
<td>0.8654</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8654</td>
<td>0.9886</td>
<td>0.7986</td>
<td>0.9667</td>
<td>0.8889</td>
<td>0.9630</td>
<td>1.0000</td>
<td>0.9130</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.9722</td>
<td>0.9804</td>
<td>0.9830</td>
<td>0.7500</td>
<td>0.9583</td>
<td>0.9821</td>
<td>0.6250</td>
<td>0.9844</td>
<td>0.9773</td>
<td>0.7949</td>
<td>1.0000</td>
<td>0.9943</td>
<td>0.9097</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_1</td>
<td>Fusion of 4 CNN</td>
<td>Agafonov2018</td>
<td>0.9174</td>
<td>0.8704</td>
<td>0.9615</td>
<td>0.9710</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8250</td>
<td>0.9091</td>
<td>0.9097</td>
<td>1.0000</td>
<td>0.8492</td>
<td>1.0000</td>
<td>0.9706</td>
<td>0.9844</td>
<td>0.8542</td>
<td>0.9615</td>
<td>0.9167</td>
<td>0.9815</td>
<td>0.7564</td>
<td>0.9886</td>
<td>0.9097</td>
<td>0.9444</td>
<td>0.8693</td>
<td>0.9198</td>
<td>0.9219</td>
<td>0.8986</td>
<td>0.9427</td>
<td>0.9792</td>
<td>0.9375</td>
<td>0.9608</td>
<td>0.9015</td>
<td>0.6083</td>
<td>0.8681</td>
<td>0.9405</td>
<td>0.5000</td>
<td>0.9844</td>
<td>1.0000</td>
<td>0.7436</td>
<td>0.8722</td>
<td>1.0000</td>
<td>0.9792</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_2</td>
<td>Fusion of 4 CNN</td>
<td>Agafonov2018</td>
<td>0.9275</td>
<td>0.8981</td>
<td>0.9808</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7750</td>
<td>0.9508</td>
<td>0.9097</td>
<td>1.0000</td>
<td>0.8810</td>
<td>1.0000</td>
<td>0.9853</td>
<td>1.0000</td>
<td>0.8472</td>
<td>0.9423</td>
<td>0.9375</td>
<td>0.9815</td>
<td>0.7500</td>
<td>0.9659</td>
<td>0.8750</td>
<td>0.9611</td>
<td>0.9085</td>
<td>0.9321</td>
<td>0.9375</td>
<td>0.8768</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.9792</td>
<td>0.9804</td>
<td>0.9205</td>
<td>0.6250</td>
<td>0.8611</td>
<td>0.9583</td>
<td>0.5556</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.7521</td>
<td>0.9167</td>
<td>1.0000</td>
<td>0.9792</td>
</tr>
<tr>
<td></td>
<td>Wilkinghoff_FKIE_task2_1</td>
<td>CNN Ensemble based on Multiple Features</td>
<td>Wilkinghoff2018</td>
<td>0.9414</td>
<td>0.9167</td>
<td>1.0000</td>
<td>0.9783</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8667</td>
<td>0.9773</td>
<td>0.8125</td>
<td>0.9889</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9706</td>
<td>1.0000</td>
<td>0.9097</td>
<td>0.9744</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7436</td>
<td>0.9886</td>
<td>0.8542</td>
<td>0.9667</td>
<td>0.9706</td>
<td>0.9321</td>
<td>0.9688</td>
<td>0.9130</td>
<td>0.9427</td>
<td>0.9792</td>
<td>0.9583</td>
<td>1.0000</td>
<td>0.9451</td>
<td>0.8167</td>
<td>0.9306</td>
<td>0.9821</td>
<td>0.5069</td>
<td>0.9792</td>
<td>0.9470</td>
<td>0.7692</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.8611</td>
</tr>
<tr>
<td></td>
<td>Pantic_ETF_task2_1</td>
<td>Ensemble of convolutional neural networks for general purpose audio tagging</td>
<td>Pantic2018</td>
<td>0.9419</td>
<td>0.9259</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8417</td>
<td>0.9773</td>
<td>0.8750</td>
<td>0.9852</td>
<td>0.8810</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9844</td>
<td>0.9167</td>
<td>0.9615</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.7692</td>
<td>0.9659</td>
<td>0.8403</td>
<td>1.0000</td>
<td>0.9020</td>
<td>0.9630</td>
<td>0.9688</td>
<td>0.9348</td>
<td>0.9479</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9510</td>
<td>0.9527</td>
<td>0.7250</td>
<td>0.9792</td>
<td>0.9226</td>
<td>0.6736</td>
<td>0.9844</td>
<td>0.9773</td>
<td>0.7778</td>
<td>0.9667</td>
<td>0.9943</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_1</td>
<td>2 average pooling</td>
<td>Khadkevich2018</td>
<td>0.9188</td>
<td>0.8981</td>
<td>1.0000</td>
<td>0.9710</td>
<td>0.9565</td>
<td>1.0000</td>
<td>0.9417</td>
<td>0.9545</td>
<td>0.8472</td>
<td>0.9889</td>
<td>0.9524</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9219</td>
<td>0.8681</td>
<td>0.9359</td>
<td>0.9375</td>
<td>0.8889</td>
<td>0.9167</td>
<td>0.9659</td>
<td>0.8333</td>
<td>0.9667</td>
<td>0.8399</td>
<td>0.9815</td>
<td>0.8906</td>
<td>0.8261</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.8542</td>
<td>0.9804</td>
<td>0.9470</td>
<td>0.5333</td>
<td>0.9167</td>
<td>0.9107</td>
<td>0.4931</td>
<td>1.0000</td>
<td>0.9697</td>
<td>0.7265</td>
<td>0.9444</td>
<td>0.9579</td>
<td>0.9375</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_2</td>
<td>2 max pooling</td>
<td>Khadkevich2018</td>
<td>0.9178</td>
<td>0.9306</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9750</td>
<td>0.9470</td>
<td>0.8403</td>
<td>0.9667</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9706</td>
<td>0.9010</td>
<td>0.8403</td>
<td>0.8205</td>
<td>0.9306</td>
<td>0.9074</td>
<td>0.9167</td>
<td>0.9545</td>
<td>0.7986</td>
<td>0.9444</td>
<td>0.9085</td>
<td>0.9630</td>
<td>0.8906</td>
<td>0.8478</td>
<td>0.9531</td>
<td>0.9792</td>
<td>0.8889</td>
<td>0.9657</td>
<td>0.9223</td>
<td>0.5250</td>
<td>0.9375</td>
<td>0.9286</td>
<td>0.5556</td>
<td>1.0000</td>
<td>0.9545</td>
<td>0.7393</td>
<td>0.9500</td>
<td>0.9540</td>
<td>0.9375</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_1</td>
<td>Stacked CNN-CRNN (4 Models)</td>
<td>Iqbal2018</td>
<td>0.9484</td>
<td>0.8889</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7833</td>
<td>0.9545</td>
<td>0.9167</td>
<td>1.0000</td>
<td>0.9762</td>
<td>0.9583</td>
<td>0.9706</td>
<td>1.0000</td>
<td>0.8750</td>
<td>0.9808</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.7949</td>
<td>0.9848</td>
<td>0.7431</td>
<td>0.9500</td>
<td>0.9281</td>
<td>0.9198</td>
<td>0.9844</td>
<td>0.8333</td>
<td>0.9635</td>
<td>0.9792</td>
<td>1.0000</td>
<td>0.9853</td>
<td>1.0000</td>
<td>0.8083</td>
<td>0.9583</td>
<td>1.0000</td>
<td>0.6597</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8333</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9583</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_2</td>
<td>Stacked CNN-CRNN (8 Models)</td>
<td>Iqbal2018</td>
<td>0.9512</td>
<td>0.9028</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7833</td>
<td>0.9432</td>
<td>0.8958</td>
<td>1.0000</td>
<td>0.9762</td>
<td>0.9583</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8958</td>
<td>0.9808</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8205</td>
<td>0.9886</td>
<td>0.7569</td>
<td>0.9667</td>
<td>0.9216</td>
<td>0.9198</td>
<td>0.9844</td>
<td>0.8406</td>
<td>0.9635</td>
<td>0.9792</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9943</td>
<td>0.8250</td>
<td>0.9583</td>
<td>1.0000</td>
<td>0.6944</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.8205</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9792</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_1</td>
<td>Surrey baseline CNN 8 layers</td>
<td>Kong2018</td>
<td>0.9034</td>
<td>0.8889</td>
<td>1.0000</td>
<td>0.9565</td>
<td>0.9348</td>
<td>1.0000</td>
<td>0.7417</td>
<td>0.9545</td>
<td>0.8750</td>
<td>0.9889</td>
<td>0.8810</td>
<td>0.9167</td>
<td>1.0000</td>
<td>0.9688</td>
<td>0.8194</td>
<td>0.8654</td>
<td>0.8681</td>
<td>0.9815</td>
<td>0.7115</td>
<td>0.9773</td>
<td>0.7014</td>
<td>0.9667</td>
<td>0.8922</td>
<td>0.8395</td>
<td>0.9479</td>
<td>0.8478</td>
<td>0.9844</td>
<td>0.9514</td>
<td>0.9167</td>
<td>0.9510</td>
<td>0.9489</td>
<td>0.6750</td>
<td>0.7917</td>
<td>0.9405</td>
<td>0.4306</td>
<td>0.9792</td>
<td>0.9470</td>
<td>0.6880</td>
<td>0.9333</td>
<td>0.9885</td>
<td>0.8958</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_2</td>
<td>Surrey baseline CNN 4 layers</td>
<td>Kong2018</td>
<td>0.8622</td>
<td>0.8889</td>
<td>1.0000</td>
<td>0.9493</td>
<td>0.9348</td>
<td>0.9551</td>
<td>0.6500</td>
<td>0.9318</td>
<td>0.7500</td>
<td>0.9556</td>
<td>0.8095</td>
<td>0.9167</td>
<td>0.9559</td>
<td>0.9844</td>
<td>0.6875</td>
<td>0.9231</td>
<td>0.6944</td>
<td>0.9444</td>
<td>0.6667</td>
<td>0.9394</td>
<td>0.6806</td>
<td>0.9111</td>
<td>0.8725</td>
<td>0.7593</td>
<td>0.9219</td>
<td>0.7174</td>
<td>0.9375</td>
<td>0.7917</td>
<td>0.8472</td>
<td>0.9510</td>
<td>0.9072</td>
<td>0.5250</td>
<td>0.5903</td>
<td>0.9821</td>
<td>0.3819</td>
<td>0.9844</td>
<td>0.8864</td>
<td>0.6752</td>
<td>0.9444</td>
<td>0.9808</td>
<td>0.8125</td>
</tr>
<tr>
<td></td>
<td>Dorfer_CPJKU_task2_1</td>
<td>CNN - Iterative Self-Verification</td>
<td>Dorfer2018</td>
<td>0.9518</td>
<td>0.9213</td>
<td>1.0000</td>
<td>0.9565</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9000</td>
<td>0.9773</td>
<td>0.8958</td>
<td>1.0000</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.9853</td>
<td>1.0000</td>
<td>0.9097</td>
<td>0.9423</td>
<td>0.9444</td>
<td>0.9815</td>
<td>0.8077</td>
<td>0.9886</td>
<td>0.8542</td>
<td>0.9667</td>
<td>0.9510</td>
<td>0.9383</td>
<td>0.9688</td>
<td>0.9275</td>
<td>0.9635</td>
<td>0.9792</td>
<td>0.9792</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7083</td>
<td>0.9306</td>
<td>0.9821</td>
<td>0.6250</td>
<td>1.0000</td>
<td>1.0000</td>
<td>0.7821</td>
<td>0.9833</td>
<td>1.0000</td>
<td>0.9236</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="input-characteristics">Input characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_map3" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3" data-chartable="true" data-field="eval_map3" data-sortable="true" data-value-type="float4">
                mAP@3
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Acoustic features
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data augmentation
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_external_data" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                External data
            </th>
<th class="text-center narrow-col" data-field="system_re_labeling" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Re-labeling
            </th>
<th class="text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling rate
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Fonseca2018</td>
<td>0.6943</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_1</td>
<td>Jeong2018</td>
<td>0.9538</td>
<td>log-mel energies, waveform</td>
<td>mixup</td>
<td></td>
<td>automatic</td>
<td>16k,32k,44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_2</td>
<td>Jeong2018</td>
<td>0.9506</td>
<td>log-mel energies, waveform</td>
<td>mixup</td>
<td></td>
<td>automatic</td>
<td>16k,32kHz</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_3</td>
<td>Jeong2018</td>
<td>0.9405</td>
<td>log-mel energies</td>
<td>mixup</td>
<td></td>
<td>automatic</td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_1</td>
<td>Nguyen2018</td>
<td>0.9496</td>
<td>log-mel energies</td>
<td>block mixing, randomly erase/cutout, time stretching, pitch shifting</td>
<td></td>
<td>automatic</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_2</td>
<td>Nguyen2018</td>
<td>0.9251</td>
<td>log-mel energies</td>
<td>block mixing, randomly cutout, time stretching, pitch shifting</td>
<td></td>
<td>automatic</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_3</td>
<td>Nguyen2018</td>
<td>0.9213</td>
<td>log-mel energies</td>
<td>block mixing, randomly cutout, time stretching, pitch shifting</td>
<td></td>
<td>automatic</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_4</td>
<td>Nguyen2018</td>
<td>0.9478</td>
<td>log-mel energies</td>
<td>block mixing, randomly erase/cutout, time stretching, pitch shifting</td>
<td></td>
<td>automatic</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_1</td>
<td>Wilhelm2018</td>
<td>0.9435</td>
<td>log-mel energies, raw audio</td>
<td>cropping, padding, time shifting, same class blending, different class blending</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_2</td>
<td>Wilhelm2018</td>
<td>0.9416</td>
<td>log-mel energies, raw audio</td>
<td>cropping, padding, time shifting, same class blending, different class blending</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_1</td>
<td>Kim2018</td>
<td>0.9151</td>
<td>MFCC, delta of MFCC, delta-delta of MFCC</td>
<td>time stretching, time shifting, additive wgn</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_2</td>
<td>Kim2018</td>
<td>0.9133</td>
<td>MFCC, delta of MFCC, delta-delta of MFCC</td>
<td>time stretching, time shifting, additive wgn</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_3</td>
<td>Kim2018</td>
<td>0.9139</td>
<td>MFCC, delta of MFCC, delta-delta of MFCC</td>
<td>time stretching, time shifting, additive wgn</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_4</td>
<td>Kim2018</td>
<td>0.9174</td>
<td>MFCC, delta of MFCC, delta-delta of MFCC</td>
<td>time stretching, time shifting, additive wgn</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_1</td>
<td>Xu2018</td>
<td>0.9065</td>
<td>log-mel energies</td>
<td>pitch shifting, mixup</td>
<td>Google AudioSet VGGish model</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_2</td>
<td>Xu2018</td>
<td>0.9081</td>
<td>log-mel energies</td>
<td>pitch shifting, mixup</td>
<td>Google AudioSet VGGish model</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_1</td>
<td>Chakraborty2018</td>
<td>0.9328</td>
<td>spectogram, spectral summaries</td>
<td>chunking, mixup, time-shift</td>
<td></td>
<td></td>
<td>22050Hz</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_2</td>
<td>Chakraborty2018</td>
<td>0.9320</td>
<td>spectogram</td>
<td>chunking, mixup, time-shift</td>
<td></td>
<td></td>
<td>22050Hz</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_judges_award</td>
<td>Chakraborty2018</td>
<td>0.9079</td>
<td>spectogram with delta and delta-delta augmentation</td>
<td>chunking, mixup</td>
<td></td>
<td></td>
<td>22050Hz</td>
</tr>
<tr>
<td></td>
<td>Han_NPU_task2_1</td>
<td>Han2018</td>
<td>0.8723</td>
<td>MFCC</td>
<td></td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Zhesong_PKU_task2_1</td>
<td>Yu2018</td>
<td>0.8807</td>
<td>MFCC &amp; raw audio</td>
<td></td>
<td>trim silence</td>
<td>automatic</td>
<td>44.1kHz &amp; 16kHZ</td>
</tr>
<tr>
<td></td>
<td>Hanyu_BUPT_task2</td>
<td>Hanyu2018</td>
<td>0.7877</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td>automatic</td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_1</td>
<td>WEI2018</td>
<td>0.9409</td>
<td>log-mel energies</td>
<td>mixup, random erasing</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_2</td>
<td>WEI2018</td>
<td>0.9423</td>
<td>log-mel energies</td>
<td>mixup, random erasing</td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Colangelo_RM3_task2_1</td>
<td>Colangelo2018</td>
<td>0.6978</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Shan_DBSonics_task2_1</td>
<td>Ren2018</td>
<td>0.9405</td>
<td>log-mel energies, MFCC</td>
<td>time stretching, pitch shift, reverb, dynamic range compression</td>
<td>VEGAS, SoundNet</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_1</td>
<td>Kele2018</td>
<td>0.9498</td>
<td>log-mel energies</td>
<td>mixup</td>
<td>ImageNet-based pre-trained model</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_2</td>
<td>Kele2018</td>
<td>0.9441</td>
<td>log-mel energies</td>
<td>mixup</td>
<td>ImageNet-based pre-trained model</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_1</td>
<td>Agafonov2018</td>
<td>0.9174</td>
<td>log-mel energies</td>
<td>time stretching, pitch shifting</td>
<td></td>
<td></td>
<td>16kHz</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_2</td>
<td>Agafonov2018</td>
<td>0.9275</td>
<td>log-mel energies</td>
<td>time stretching, pitch shifting</td>
<td></td>
<td></td>
<td>16kHz</td>
</tr>
<tr>
<td></td>
<td>Wilkinghoff_FKIE_task2_1</td>
<td>Wilkinghoff2018</td>
<td>0.9414</td>
<td>PLP, MFCC, mel-spectrogram, raw data</td>
<td>mix-up, cutout, dropout, vertical shifts</td>
<td></td>
<td></td>
<td>24kHz</td>
</tr>
<tr>
<td></td>
<td>Pantic_ETF_task2_1</td>
<td>Pantic2018</td>
<td>0.9419</td>
<td>CQT, mel-spectrogram</td>
<td>mixup, random erasing, width shift, zoom</td>
<td>pre-trained model</td>
<td></td>
<td>44.1kHz</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_1</td>
<td>Khadkevich2018</td>
<td>0.9188</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td></td>
<td>16kHz</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_2</td>
<td>Khadkevich2018</td>
<td>0.9178</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td></td>
<td>16kHz</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_1</td>
<td>Iqbal2018</td>
<td>0.9484</td>
<td>log-mel energies</td>
<td>mixup</td>
<td></td>
<td>automatic</td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_2</td>
<td>Iqbal2018</td>
<td>0.9512</td>
<td>log-mel energies</td>
<td>mixup</td>
<td></td>
<td>automatic</td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_1</td>
<td>Kong2018</td>
<td>0.9034</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_2</td>
<td>Kong2018</td>
<td>0.8622</td>
<td>log-mel energies</td>
<td></td>
<td></td>
<td></td>
<td>32kHz</td>
</tr>
<tr>
<td></td>
<td>Dorfer_CPJKU_task2_1</td>
<td>Dorfer2018</td>
<td>0.9518</td>
<td>Perceptual weighted power spectrogram, Logarithmic-filtered log-spectrogram</td>
<td>mixup</td>
<td></td>
<td>automatic</td>
<td>32.0kHz</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f1_eval" data-scatter-y="f1_evalof" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="eval_map3" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Tech. report
            </th>
<th class="sep-left-cell text-center" data-axis-label="mAP@3" data-chartable="true" data-field="eval_map3" data-sortable="true" data-value-type="float4">
                mAP@3
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision making
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="int">
                System complexity
            </th>
</tr>
</thead>
<tbody>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Fonseca2018</td>
<td>0.6943</td>
<td>CNN</td>
<td></td>
<td></td>
<td>658100</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_1</td>
<td>Jeong2018</td>
<td>0.9538</td>
<td>CNN</td>
<td>30</td>
<td>geometric mean</td>
<td>414200805</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_2</td>
<td>Jeong2018</td>
<td>0.9506</td>
<td>CNN</td>
<td>20</td>
<td>geometric mean</td>
<td>276133870</td>
</tr>
<tr>
<td></td>
<td>Jeong_COCAI_task2_3</td>
<td>Jeong2018</td>
<td>0.9405</td>
<td>CNN</td>
<td>5</td>
<td>geometric mean</td>
<td>55461000</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_1</td>
<td>Nguyen2018</td>
<td>0.9496</td>
<td>CNN</td>
<td>8</td>
<td>geometric mean</td>
<td>5679784</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_2</td>
<td>Nguyen2018</td>
<td>0.9251</td>
<td>CNN</td>
<td></td>
<td></td>
<td>652705</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_3</td>
<td>Nguyen2018</td>
<td>0.9213</td>
<td>CNN</td>
<td></td>
<td></td>
<td>769297</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task2_4</td>
<td>Nguyen2018</td>
<td>0.9478</td>
<td>CNN</td>
<td>8</td>
<td>geometric mean</td>
<td>5679784</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_1</td>
<td>Wilhelm2018</td>
<td>0.9435</td>
<td>CNN</td>
<td>5</td>
<td>geometric mean</td>
<td>15683245</td>
</tr>
<tr>
<td></td>
<td>Wilhelm_UKON_task2_2</td>
<td>Wilhelm2018</td>
<td>0.9416</td>
<td>CNN</td>
<td></td>
<td>geometric mean</td>
<td>3136649</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_1</td>
<td>Kim2018</td>
<td>0.9151</td>
<td>CNN,ensemble</td>
<td>10</td>
<td>mean probability</td>
<td>7197609</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_2</td>
<td>Kim2018</td>
<td>0.9133</td>
<td>CNN,ensemble</td>
<td>10</td>
<td>mean probability</td>
<td>7197609</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_3</td>
<td>Kim2018</td>
<td>0.9139</td>
<td>CNN,ensemble</td>
<td>10</td>
<td>mean probability</td>
<td>7197609</td>
</tr>
<tr>
<td></td>
<td>Kim_GIST_WisenetAI_task2_4</td>
<td>Kim2018</td>
<td>0.9174</td>
<td>CNN,ensemble</td>
<td>10</td>
<td>mean probability</td>
<td>7197609</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_1</td>
<td>Xu2018</td>
<td>0.9065</td>
<td>CNN, DNN, multi-level attention,</td>
<td>20</td>
<td>geometric mean</td>
<td>19119200</td>
</tr>
<tr>
<td></td>
<td>Xu_Aalto_task2_2</td>
<td>Xu2018</td>
<td>0.9081</td>
<td>CNN, DNN, multi-level attention,</td>
<td>120</td>
<td>geometric mean</td>
<td>114715200</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_1</td>
<td>Chakraborty2018</td>
<td>0.9328</td>
<td>CNN, xgboost, ensemble</td>
<td></td>
<td>geometric averaging, xgboost classifier</td>
<td>29270658</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_2</td>
<td>Chakraborty2018</td>
<td>0.9320</td>
<td>CNN</td>
<td></td>
<td>geometric averaging</td>
<td>27603245</td>
</tr>
<tr>
<td></td>
<td>Chakraborty_IBM_Task2_judges_award</td>
<td>Chakraborty2018</td>
<td>0.9079</td>
<td>CNN</td>
<td></td>
<td>geometric averaging</td>
<td>1464813</td>
</tr>
<tr>
<td></td>
<td>Han_NPU_task2_1</td>
<td>Han2018</td>
<td>0.8723</td>
<td>CNN</td>
<td>2</td>
<td></td>
<td>24810</td>
</tr>
<tr>
<td></td>
<td>Zhesong_PKU_task2_1</td>
<td>Yu2018</td>
<td>0.8807</td>
<td>Bilinear-CNN &amp; WaveNet</td>
<td>2</td>
<td></td>
<td>658100</td>
</tr>
<tr>
<td></td>
<td>Hanyu_BUPT_task2</td>
<td>Hanyu2018</td>
<td>0.7877</td>
<td>CRNN</td>
<td></td>
<td>mean probability</td>
<td>980000</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_1</td>
<td>WEI2018</td>
<td>0.9409</td>
<td>CNN</td>
<td>2</td>
<td>rank averaging</td>
<td>9764770</td>
</tr>
<tr>
<td></td>
<td>Wei_Kuaiyu_task2_2</td>
<td>WEI2018</td>
<td>0.9423</td>
<td>CNN</td>
<td>2</td>
<td>rank averaging</td>
<td>9764770</td>
</tr>
<tr>
<td></td>
<td>Colangelo_RM3_task2_1</td>
<td>Colangelo2018</td>
<td>0.6978</td>
<td>CRNN</td>
<td></td>
<td></td>
<td>2446185</td>
</tr>
<tr>
<td></td>
<td>Shan_DBSonics_task2_1</td>
<td>Ren2018</td>
<td>0.9405</td>
<td>CNN</td>
<td>4</td>
<td>geometric mean</td>
<td>6000000</td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_1</td>
<td>Kele2018</td>
<td>0.9498</td>
<td>ensemble</td>
<td>5</td>
<td>probability vote</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Kele_NUDT_task2_2</td>
<td>Kele2018</td>
<td>0.9441</td>
<td>ensemble</td>
<td>5</td>
<td>probability vote</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_1</td>
<td>Agafonov2018</td>
<td>0.9174</td>
<td>ensemble</td>
<td>4</td>
<td>average fusion</td>
<td>1880684</td>
</tr>
<tr>
<td></td>
<td>Agafonov_ITMO_task2_2</td>
<td>Agafonov2018</td>
<td>0.9275</td>
<td>ensemble</td>
<td>4</td>
<td>geometric mean fusion</td>
<td>12312447</td>
</tr>
<tr>
<td></td>
<td>Wilkinghoff_FKIE_task2_1</td>
<td>Wilkinghoff2018</td>
<td>0.9414</td>
<td>CNN, ensemble</td>
<td>25</td>
<td>logistic regression, neural network</td>
<td>349396065</td>
</tr>
<tr>
<td></td>
<td>Pantic_ETF_task2_1</td>
<td>Pantic2018</td>
<td>0.9419</td>
<td>CNN</td>
<td>11</td>
<td>Logistic regression</td>
<td>94075101</td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_1</td>
<td>Khadkevich2018</td>
<td>0.9188</td>
<td>CNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Khadkevich_FB_task2_2</td>
<td>Khadkevich2018</td>
<td>0.9178</td>
<td>CNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_1</td>
<td>Iqbal2018</td>
<td>0.9484</td>
<td>GCNN, GCRNN</td>
<td>4</td>
<td>geometric mean</td>
<td>81701540</td>
</tr>
<tr>
<td></td>
<td>Iqbal_Surrey_task2_2</td>
<td>Iqbal2018</td>
<td>0.9512</td>
<td>CNN, GCNN, CRNN, GCRNN</td>
<td>8</td>
<td>geometric mean</td>
<td>125787722</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_1</td>
<td>Kong2018</td>
<td>0.9034</td>
<td>VGGish 8 layer CNN with global max pooling</td>
<td></td>
<td></td>
<td>4691274</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task2_2</td>
<td>Kong2018</td>
<td>0.8622</td>
<td>AlexNetish 4 layer CNN with global max pooling</td>
<td></td>
<td></td>
<td>4309450</td>
</tr>
<tr>
<td></td>
<td>Dorfer_CPJKU_task2_1</td>
<td>Dorfer2018</td>
<td>0.9518</td>
<td>CNN, ensemble</td>
<td>3</td>
<td>average</td>
<td>8369492</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2018/technical_reports_task2.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Agafonov2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Agafonov2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUDIO TAGGING USING LABELED DATASET WITH NEURAL NETWORKS
       </h4>
<p style="text-align:left">
        Iurii Agafonov and Evgeniy Shuranov
       </p>
<p style="text-align:left">
<em>
         Speech Information Systems (ITMO), ITMO University, Saint-Petersburg, Saint-Petersburg, Russia. Database Collection and Processing Group (STC), Speech Technology Center, Saint-Petersburg, Saint-Petersburg, Russia.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Agafonov_ITMO_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Agafonov2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Agafonov2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Agafonov2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Agafonov_74.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Agafonov2018" class="panel-collapse collapse" id="collapse-Agafonov2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUDIO TAGGING USING LABELED DATASET WITH NEURAL NETWORKS
      </h4>
<p style="text-align:left">
<small>
        Iurii Agafonov and Evgeniy Shuranov
       </small>
<br/>
<small>
<em>
         Speech Information Systems (ITMO), ITMO University, Saint-Petersburg, Saint-Petersburg, Russia. Database Collection and Processing Group (STC), Speech Technology Center, Saint-Petersburg, Saint-Petersburg, Russia.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, an audio tagging system is proposed. This system uses fusion of 5 Convolutional Neural Network (CNN) and 1 Convolutional Recurrent Neural Network (CRNN) classifiers in attempt to achieve better results. The proposed system reaches 0.95 score in public leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean fusion
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         12312447 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Agafonov2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Agafonov_74.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Agafonov2018label" class="modal fade" id="bibtex-Agafonov2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAgafonov2018label">
        AUDIO TAGGING USING LABELED DATASET WITH NEURAL NETWORKS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Agafonov2018,
    Author = "Agafonov, Iurii and Shuranov, Evgeniy",
    title = "AUDIO TAGGING USING LABELED DATASET WITH NEURAL NETWORKS",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, an audio tagging system is proposed. This system uses fusion of 5 Convolutional Neural Network (CNN) and 1 Convolutional Recurrent Neural Network (CRNN) classifiers in attempt to achieve better results. The proposed system reaches 0.95 score in public leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chakraborty2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Chakraborty2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DIVERSIFIED SYSTEM OF DEEP CONVOLUTIONAL NEURAL NETWORKS WITH STACKED SPECTRAL FUSION FOR AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Ria Chakraborty
       </p>
<p style="text-align:left">
<em>
         Cognitive Business Decision Services (IBM), International Business Machines, India, Kolkata, India.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chakraborty_IBM_Task2_judges_award</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chakraborty2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chakraborty2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chakraborty2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Chakraborty_33.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chakraborty2018" class="panel-collapse collapse" id="collapse-Chakraborty2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DIVERSIFIED SYSTEM OF DEEP CONVOLUTIONAL NEURAL NETWORKS WITH STACKED SPECTRAL FUSION FOR AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Ria Chakraborty
       </small>
<br/>
<small>
<em>
         Cognitive Business Decision Services (IBM), International Business Machines, India, Kolkata, India.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper outlines a diversified system of deep convolutional neural networks with stacked fusion of spectral features for the DCASE 2018 Task 2 [1], freesound general-purpose audio tagging. The primary objective of this research has been to develop a solution which can churn out decent performance and be deployed within reasonable resource constraints. The two best performing submissions are the results of only two and three different CNNs, with their results being combined based on a boosted tree algorithm with fused spectral features. This paper describes the submissions which are made under the team name Gyat, leveraging different feature representations and data augmentations along with the marginal benefits they bring on the table. Experimental results show that the proposed systems and preprocessing methods effectively learn acoustic characteristics from the audio recordings, and their ensemble model significantly reduces the error rate further, exhibiting a MAP@3 score of 0.945 and 0.947 respectively on the public leaderboard. The baseline score for this task has been 0.704 (public LB score).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22050Hz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         chunking, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectogram with delta and delta-delta augmentation
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric averaging
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         1464813 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chakraborty2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Chakraborty_33.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chakraborty2018label" class="modal fade" id="bibtex-Chakraborty2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChakraborty2018label">
        DIVERSIFIED SYSTEM OF DEEP CONVOLUTIONAL NEURAL NETWORKS WITH STACKED SPECTRAL FUSION FOR AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chakraborty2018,
    Author = "Chakraborty, Ria",
    title = "DIVERSIFIED SYSTEM OF DEEP CONVOLUTIONAL NEURAL NETWORKS WITH STACKED SPECTRAL FUSION FOR AUDIO TAGGING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper outlines a diversified system of deep convolutional neural networks with stacked fusion of spectral features for the DCASE 2018 Task 2 [1], freesound general-purpose audio tagging. The primary objective of this research has been to develop a solution which can churn out decent performance and be deployed within reasonable resource constraints. The two best performing submissions are the results of only two and three different CNNs, with their results being combined based on a boosted tree algorithm with fused spectral features. This paper describes the submissions which are made under the team name Gyat, leveraging different feature representations and data augmentations along with the marginal benefits they bring on the table. Experimental results show that the proposed systems and preprocessing methods effectively learn acoustic characteristics from the audio recordings, and their ensemble model significantly reduces the error rate further, exhibiting a MAP@3 score of 0.945 and 0.947 respectively on the public leaderboard. The baseline score for this task has been 0.704 (public LB score)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Colangelo2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Colangelo2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CONVOLUTIONAL RECURRENT NEURAL NETWORK FOR AUDIO EVENTS CLASSIFICATION
       </h4>
<p style="text-align:left">
        Federico Colangelo, Federica Battisti, Alessandro Neri and Marco Carli
       </p>
<p style="text-align:left">
<em>
         Department of Engineering (RM3), Universita degli studi Roma Tre, Rome, Italy.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Colangelo_RM3_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Colangelo2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Colangelo2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Colangelo2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Colangelo_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Colangelo2018" class="panel-collapse collapse" id="collapse-Colangelo2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CONVOLUTIONAL RECURRENT NEURAL NETWORK FOR AUDIO EVENTS CLASSIFICATION
      </h4>
<p style="text-align:left">
<small>
        Federico Colangelo, Federica Battisti, Alessandro Neri and Marco Carli
       </small>
<br/>
<small>
<em>
         Department of Engineering (RM3), Universita degli studi Roma Tre, Rome, Italy.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Audio event recognition is becoming a hot topic both in the research and in the industrial field. Nowadays, thanks to the availability of cheap sensors, the acquisition of high-quality audio is much easier. However, new challenges arise: the large number of inputs requires adequate means for coding, transmitting, and storing the recorded data. Moreover, to build systems that can act based on their surroundings (e.g. autonomous cars), automatic tools for detecting specific audio events are needed. In this paper, the effectiveness of an architecture based on a combination of convolutional and re- current neural networks for general purpose audio event detection is evaluated. Specifically, the architecture is evaluated in the context of the DCASE challenge on general purpose audio tagging, in order to provide a clear comparison with architectures based on different principles.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         2446185 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Colangelo2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Colangelo_61.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Colangelo2018label" class="modal fade" id="bibtex-Colangelo2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexColangelo2018label">
        CONVOLUTIONAL RECURRENT NEURAL NETWORK FOR AUDIO EVENTS CLASSIFICATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Colangelo2018,
    Author = "Colangelo, Federico and Battisti, Federica and Neri, Alessandro and Carli, Marco",
    title = "CONVOLUTIONAL RECURRENT NEURAL NETWORK FOR AUDIO EVENTS CLASSIFICATION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Audio event recognition is becoming a hot topic both in the research and in the industrial field. Nowadays, thanks to the availability of cheap sensors, the acquisition of high-quality audio is much easier. However, new challenges arise: the large number of inputs requires adequate means for coding, transmitting, and storing the recorded data. Moreover, to build systems that can act based on their surroundings (e.g. autonomous cars), automatic tools for detecting specific audio events are needed. In this paper, the effectiveness of an architecture based on a combination of convolutional and re- current neural networks for general purpose audio event detection is evaluated. Specifically, the architecture is evaluated in the context of the DCASE challenge on general purpose audio tagging, in order to provide a clear comparison with architectures based on different principles."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dorfer2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Dorfer2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GTRAINING GENERAL-PURPOSE AUDIO TAGGING NETWORKS WITH NOISY LABELS AND ITERATIVE SELF-VERIFICATION
       </h4>
<p style="text-align:left">
        Matthias Dorfer and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception (JKU), Johannes Kepler University Linz, Linz, Austria.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dorfer_CPJKU_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dorfer2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dorfer2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dorfer2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dorfer_999.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Dorfer2018').collapse('show');window.location.hash='#Dorfer2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dorfer2018" class="panel-collapse collapse" id="collapse-Dorfer2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GTRAINING GENERAL-PURPOSE AUDIO TAGGING NETWORKS WITH NOISY LABELS AND ITERATIVE SELF-VERIFICATION
      </h4>
<p style="text-align:left">
<small>
        Matthias Dorfer and Gerhard Widmer
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception (JKU), Johannes Kepler University Linz, Linz, Austria.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This work describes our submission to the first Freesound general- purpose audio tagging challenge carried out within the DCASE 2018 challenge. Our solution is based on a fully convolutional neural network that predicts one out of 41 possible audio class labels when given an audio spectrogram excerpt as an input. What makes this classification dataset and the task in general special, is the fact that only 3,700 of the 9,500 provided training examples are delivered with manually verified ground truth labels. The remaining non- verified observations are expected to contain a substantial amount of label noise (30-35%). We propose to address this issue by a simple, iterative self-verification process, which gradually shifts unverified labels into the verified, trusted training set. The decision criterion for self-verifying a training example is the prediction consensus of a previous snapshot of the network on multiple short sliding window excerpts of the training example at hand. This procedure requires a carefully chosen cross-validation setup, as large enough neural net- works are able to learn an entire dataset by heart, even in the face of noisy label data. On the unseen test data, an ensemble of three net- works trained with this self-verification approach achieves a mean average precision (MAP@3) of 0.951. This is the second best out of 558 submissions to the corresponding Kaggle challenge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         Perceptual weighted power spectrogram, Logarithmic-filtered log-spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         3
        </td>
</tr>
<tr>
<td class="col-md-3">
         Re-labeling
        </td>
<td>
         automatic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         8369492 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dorfer2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dorfer_999.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/CPJKU/dcase_task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dorfer2018label" class="modal fade" id="bibtex-Dorfer2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDorfer2018label">
        GTRAINING GENERAL-PURPOSE AUDIO TAGGING NETWORKS WITH NOISY LABELS AND ITERATIVE SELF-VERIFICATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dorfer2018,
    Author = "Dorfer, Matthias and Widmer, Gerhard",
    title = "GTRAINING GENERAL-PURPOSE AUDIO TAGGING NETWORKS WITH NOISY LABELS AND ITERATIVE SELF-VERIFICATION",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This work describes our submission to the first Freesound general- purpose audio tagging challenge carried out within the DCASE 2018 challenge. Our solution is based on a fully convolutional neural network that predicts one out of 41 possible audio class labels when given an audio spectrogram excerpt as an input. What makes this classification dataset and the task in general special, is the fact that only 3,700 of the 9,500 provided training examples are delivered with manually verified ground truth labels. The remaining non- verified observations are expected to contain a substantial amount of label noise (30-35\%). We propose to address this issue by a simple, iterative self-verification process, which gradually shifts unverified labels into the verified, trusted training set. The decision criterion for self-verifying a training example is the prediction consensus of a previous snapshot of the network on multiple short sliding window excerpts of the training example at hand. This procedure requires a carefully chosen cross-validation setup, as large enough neural net- works are able to learn an entire dataset by heart, even in the face of noisy label data. On the unseen test data, an ensemble of three net- works trained with this self-verification approach achieves a mean average precision (MAP@3) of 0.951. This is the second best out of 558 submissions to the corresponding Kaggle challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fonseca2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Fonseca2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GENERAL-PURPOSE TAGGING OF FREESOUND AUDIO WITH AUDIOSET LABELS: TASK DESCRIPTION, DATASET, AND BASELINE
       </h4>
<p style="text-align:left">
        Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis and Xavier Serra
       </p>
<p style="text-align:left">
<em>
         Music Technology Group (UPF), Universitat Pompeu Fabra, Barcelona, Spain. Machine Perception Team (GOOGLE), Google Research, New York, USA.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">DCASE2018 baseline</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fonseca2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fonseca2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fonseca2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://arxiv.org/pdf/1807.09902.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Fonseca2018').collapse('show');window.location.hash='#Fonseca2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fonseca2018" class="panel-collapse collapse" id="collapse-Fonseca2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GENERAL-PURPOSE TAGGING OF FREESOUND AUDIO WITH AUDIOSET LABELS: TASK DESCRIPTION, DATASET, AND BASELINE
      </h4>
<p style="text-align:left">
<small>
        Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis and Xavier Serra
       </small>
<br/>
<small>
<em>
         Music Technology Group (UPF), Universitat Pompeu Fabra, Barcelona, Spain. Machine Perception Team (GOOGLE), Google Research, New York, USA.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes Task 2 of the DCASE 2018 Challenge, titled “General-purpose audio tagging of Freesound content with Au- dioSet labels”. This task was hosted on the Kaggle platform as “Freesound General-Purpose Audio Tagging Challenge”. The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 diverse categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         658100 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fonseca2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://arxiv.org/pdf/1807.09902.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task2/" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fonseca2018label" class="modal fade" id="bibtex-Fonseca2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFonseca2018label">
        GENERAL-PURPOSE TAGGING OF FREESOUND AUDIO WITH AUDIOSET LABELS: TASK DESCRIPTION, DATASET, AND BASELINE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fonseca2018,
    Author = "Fonseca, Eduardo and Plakal, Manoj and Font, Frederic and Ellis, Daniel P. W. and Serra, Xavier",
    title = "GENERAL-PURPOSE TAGGING OF FREESOUND AUDIO WITH AUDIOSET LABELS: TASK DESCRIPTION, DATASET, AND BASELINE",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper describes Task 2 of the DCASE 2018 Challenge, titled “General-purpose audio tagging of Freesound content with Au- dioSet labels”. This task was hosted on the Kaggle platform as “Freesound General-Purpose Audio Tagging Challenge”. The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 diverse categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Han2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Han2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CIAIC-GATFC SYSTEM FOR DCASE2018 CHALLENGE TASK2
       </h4>
<p style="text-align:left">
        Xueyu Han, Di Li and Qing Liu
       </p>
<p style="text-align:left">
<em>
         Center of Intelligence Acoustics and Immersive Communication (NPU), Northwestern Polytechnical University, Xi'an, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Han_NPU_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Han2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Han2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Han2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Han_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Han2018" class="panel-collapse collapse" id="collapse-Han2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CIAIC-GATFC SYSTEM FOR DCASE2018 CHALLENGE TASK2
      </h4>
<p style="text-align:left">
<small>
        Xueyu Han, Di Li and Qing Liu
       </small>
<br/>
<small>
<em>
         Center of Intelligence Acoustics and Immersive Communication (NPU), Northwestern Polytechnical University, Xi'an, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our method to tackle the problem of general-purpose automatic audio tagging described in DCASE 2018 challenge task 2. Two convolutional neural networks (CNN) models with different inputs and different architectures were trained respectively. Outputs from the two CNN models were then fused together to give a final decision. In particular, the distribution of training samples among 41 categories were unequally. Therefore, we presented a data augmentation method, which guaranteed the number of training samples per category was equal. A relative 21.4% improvement over DCASE baseline system [1] is achieved on the public Kaggle leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         24810 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Han2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Han_35.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Han2018label" class="modal fade" id="bibtex-Han2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHan2018label">
        CIAIC-GATFC SYSTEM FOR DCASE2018 CHALLENGE TASK2
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Han2018,
    Author = "Han, Xueyu and Li, Di and Liu, Qing",
    title = "CIAIC-GATFC SYSTEM FOR DCASE2018 CHALLENGE TASK2",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this report, we present our method to tackle the problem of general-purpose automatic audio tagging described in DCASE 2018 challenge task 2. Two convolutional neural networks (CNN) models with different inputs and different architectures were trained respectively. Outputs from the two CNN models were then fused together to give a final decision. In particular, the distribution of training samples among 41 categories were unequally. Therefore, we presented a data augmentation method, which guaranteed the number of training samples per category was equal. A relative 21.4\% improvement over DCASE baseline system [1] is achieved on the public Kaggle leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hanyu2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Hanyu2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A SYSTEM FOR DCASE CHALLENGE USING 2018 CRNN WITH MEL FEATURES
       </h4>
<p style="text-align:left">
        Zhang Hanyu and Li Shengchen
       </p>
<p style="text-align:left">
<em>
         Embedded Artificial Intelligence Group (BUPT), University of Posts and Telecommunications, Beijing, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hanyu_BUPT_task2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hanyu2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hanyu2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hanyu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Hanyu_48.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hanyu2018" class="panel-collapse collapse" id="collapse-Hanyu2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A SYSTEM FOR DCASE CHALLENGE USING 2018 CRNN WITH MEL FEATURES
      </h4>
<p style="text-align:left">
<small>
        Zhang Hanyu and Li Shengchen
       </small>
<br/>
<small>
<em>
         Embedded Artificial Intelligence Group (BUPT), University of Posts and Telecommunications, Beijing, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       For the Acoustic Scene Classification task (General-purpose audio tagging of Freesound content with AudioSet labels) of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2018), we propose a method to classify 41 different acoustic events using a Convolutional Recurrent Neural Network (CRNN) with log Mel spectrogram. First, the waveform of the audio recordings is transformed to log Mel spectrogram and MFCC. The convolutional layers are then applied on the log Mel spectrogram and Mel-frequency cepstral coefficients to extract high level features. The features are fed into the Recurrent Neural Network (RNN) for classification. On the official development set of the challenge, the best MAP is 0.8613, which increases 6.83% compared with the base- line.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability
        </td>
</tr>
<tr>
<td class="col-md-3">
         Re-labeling
        </td>
<td>
         automatic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         980000 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hanyu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Hanyu_48.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hanyu2018label" class="modal fade" id="bibtex-Hanyu2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHanyu2018label">
        A SYSTEM FOR DCASE CHALLENGE USING 2018 CRNN WITH MEL FEATURES
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hanyu2018,
    Author = "Hanyu, Zhang and Shengchen, Li",
    title = "A SYSTEM FOR DCASE CHALLENGE USING 2018 CRNN WITH MEL FEATURES",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "For the Acoustic Scene Classification task (General-purpose audio tagging of Freesound content with AudioSet labels) of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2018), we propose a method to classify 41 different acoustic events using a Convolutional Recurrent Neural Network (CRNN) with log Mel spectrogram. First, the waveform of the audio recordings is transformed to log Mel spectrogram and MFCC. The convolutional layers are then applied on the log Mel spectrogram and Mel-frequency cepstral coefficients to extract high level features. The features are fed into the Recurrent Neural Network (RNN) for classification. On the official development set of the challenge, the best MAP is 0.8613, which increases 6.83\% compared with the base- line."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Iqbal2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Iqbal2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        STACKED CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL-PURPOSE AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Turab Iqbal, Qiuqiang Kong, Mark Plumbley and Wenwu Wang
       </p>
<p style="text-align:left">
<em>
         Centre for Vision, Speech and Signal Processing (Surrey), University of Surrey, UK, Surrey, UK.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Iqbal_Surrey_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Iqbal2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Iqbal2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Iqbal2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Iqbal_89.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Iqbal2018').collapse('show');window.location.hash='#Iqbal2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Iqbal2018" class="panel-collapse collapse" id="collapse-Iqbal2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       STACKED CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL-PURPOSE AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Turab Iqbal, Qiuqiang Kong, Mark Plumbley and Wenwu Wang
       </small>
<br/>
<small>
<em>
         Centre for Vision, Speech and Signal Processing (Surrey), University of Surrey, UK, Surrey, UK.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the methods used for classifying sound events as part of Task 2 of the DCASE 2018 challenge. The data used in this task requires a number of considerations, including how to handle variable-length audio samples and the presence of noisy labels. We propose a number of neural network architectures that learn from mel-spectrogram inputs. These baseline models involve the use of preprocessing techniques, data augmentation, and pseudo-labeling in order to improve their performance. They are then ensembled using a popular technique known as stacking. On the test set used for evaluation, compared to the baseline mean average precision score of 0.704, our system achieved a score of 0.961.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, GCNN, CRNN, GCRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         8
        </td>
</tr>
<tr>
<td class="col-md-3">
         Re-labeling
        </td>
<td>
         automatic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         125787722 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Iqbal2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Iqbal_89.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/turab95/dcase2018_task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Iqbal2018label" class="modal fade" id="bibtex-Iqbal2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexIqbal2018label">
        STACKED CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL-PURPOSE AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Iqbal2018,
    Author = "Iqbal, Turab and Kong, Qiuqiang and Plumbley, Mark and Wang, Wenwu",
    title = "STACKED CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL-PURPOSE AUDIO TAGGING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This technical report describes the methods used for classifying sound events as part of Task 2 of the DCASE 2018 challenge. The data used in this task requires a number of considerations, including how to handle variable-length audio samples and the presence of noisy labels. We propose a number of neural network architectures that learn from mel-spectrogram inputs. These baseline models involve the use of preprocessing techniques, data augmentation, and pseudo-labeling in order to improve their performance. They are then ensembled using a popular technique known as stacking. On the test set used for evaluation, compared to the baseline mean average precision score of 0.704, our system achieved a score of 0.961."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jeong2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Jeong2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUDIO TAGGING SYSTEM FOR DCASE 2018: FOCUSING ON LABEL NOISE, DATA AUGMENTATION AND ITS EFFICIENT LEARNING
       </h4>
<p style="text-align:left">
        Il-Young Jeong and Hyungui Lim
       </p>
<p style="text-align:left">
<em>
         COCAI, Cochlear.ai, Seoul, South Korea.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jeong_COCAI_task2_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jeong2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jeong2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jeong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Jeong_102.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Jeong2018').collapse('show');window.location.hash='#Jeong2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jeong2018" class="panel-collapse collapse" id="collapse-Jeong2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUDIO TAGGING SYSTEM FOR DCASE 2018: FOCUSING ON LABEL NOISE, DATA AUGMENTATION AND ITS EFFICIENT LEARNING
      </h4>
<p style="text-align:left">
<small>
        Il-Young Jeong and Hyungui Lim
       </small>
<br/>
<small>
<em>
         COCAI, Cochlear.ai, Seoul, South Korea.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we expound on the techniques and models applied to our submission for DCASE 2018: General-purpose audio tagging of Freesound content with AudioSet labels. We aim to focus primarily on how to train a deep-learning model efficiently against strong augmentation and label noise. First, we conducted a single-block DenseNet architecture and multi-head softmax classifier for efficient learning with mixup augmentation. For the label noise, we tried the batch-wise loss masking, which eliminates the loss of outliers in a mini-batch. We also tried an ensemble of various models, trained by using different sampling rate or audio representation.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         5
        </td>
</tr>
<tr>
<td class="col-md-3">
         Re-labeling
        </td>
<td>
         automatic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         55461000 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jeong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Jeong_102.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/finejuly/dcase2018_task2_cochlearai" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jeong2018label" class="modal fade" id="bibtex-Jeong2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJeong2018label">
        AUDIO TAGGING SYSTEM FOR DCASE 2018: FOCUSING ON LABEL NOISE, DATA AUGMENTATION AND ITS EFFICIENT LEARNING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jeong2018,
    Author = "Jeong, Il-Young and Lim, Hyungui",
    title = "AUDIO TAGGING SYSTEM FOR DCASE 2018: FOCUSING ON LABEL NOISE, DATA AUGMENTATION AND ITS EFFICIENT LEARNING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technical report, we expound on the techniques and models applied to our submission for DCASE 2018: General-purpose audio tagging of Freesound content with AudioSet labels. We aim to focus primarily on how to train a deep-learning model efficiently against strong augmentation and label noise. First, we conducted a single-block DenseNet architecture and multi-head softmax classifier for efficient learning with mixup augmentation. For the label noise, we tried the batch-wise loss masking, which eliminates the loss of outliers in a mini-batch. We also tried an ensemble of various models, trained by using different sampling rate or audio representation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kele2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Kele2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        NUDT SOLUTION FOR AUDIO TAGGING TASK OF DCASE 2018 CHALLENGE
       </h4>
<p style="text-align:left">
        Xu Kele, Zhu Boqing, Wang Dezhi, Peng Yuxing, Wang Huaimin, Zhang Lilun and Li Bo
       </p>
<p style="text-align:left">
<em>
         College of Meteorology and Oceanography (NUDT), National University of Defense Technology, Changsha, China. Department of Computer Science (NUDT), National University of Defense Technology, Changsha, China. Department of Automation (BUPT), Beijing University of Posts and Telecommunications, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kele_NUDT_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kele2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kele2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kele2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Kele_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kele2018" class="panel-collapse collapse" id="collapse-Kele2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       NUDT SOLUTION FOR AUDIO TAGGING TASK OF DCASE 2018 CHALLENGE
      </h4>
<p style="text-align:left">
<small>
        Xu Kele, Zhu Boqing, Wang Dezhi, Peng Yuxing, Wang Huaimin, Zhang Lilun and Li Bo
       </small>
<br/>
<small>
<em>
         College of Meteorology and Oceanography (NUDT), National University of Defense Technology, Changsha, China. Department of Computer Science (NUDT), National University of Defense Technology, Changsha, China. Department of Automation (BUPT), Beijing University of Posts and Telecommunications, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe our solution for the general- purpose audio tagging task, which belongs to one of the subtasks in the DCASE 2018 challenge. For the solution, we employed both deep learning methods and statistic features-based shallow architecture learners. For single model, only deep learning approaches are investigated, and different deep neural network architectures are tested with different kinds of input, which ranges from the raw- signal, log-scaled Mel-spectrograms (log Mel) to Mel Frequency Cepstral Coefficents (MFCC). For log Mel and MFCC, the delta and delta-delta information are also used to formulate three-channels features. Inception, ResNet, ResNeXt, Dual Path Networks (DPN) are selected as the neural network architectures, while Mixup is used for the data augmentation. Using ResNeXt, our best single convolutional neural network architecture provides an mAP@3 of 0.967 on the public Kaggle leaderboard. Moreover, to improve the accuracy further, we also propose a meta learning-based ensemble method. By employing the diversities between different architectures, the meta learning- based model can provide higher prediction accuracy and robustness with comparison to the single model. Using the proposed meta- learning method, our solution achieves an mAP@3 of 0.977 (rank 1 of 555) on the public Kaggle leaderboard, while the baseline gives an mAP@3 of 0.70.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         probability vote
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         5
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kele2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Kele_72.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kele2018label" class="modal fade" id="bibtex-Kele2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKele2018label">
        NUDT SOLUTION FOR AUDIO TAGGING TASK OF DCASE 2018 CHALLENGE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kele2018,
    Author = "Kele, Xu and Boqing, Zhu and Dezhi, Wang and Yuxing, Peng and Huaimin, Wang and Lilun, Zhang and Bo, Li",
    title = "NUDT SOLUTION FOR AUDIO TAGGING TASK OF DCASE 2018 CHALLENGE",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technical report, we describe our solution for the general- purpose audio tagging task, which belongs to one of the subtasks in the DCASE 2018 challenge. For the solution, we employed both deep learning methods and statistic features-based shallow architecture learners. For single model, only deep learning approaches are investigated, and different deep neural network architectures are tested with different kinds of input, which ranges from the raw- signal, log-scaled Mel-spectrograms (log Mel) to Mel Frequency Cepstral Coefficents (MFCC). For log Mel and MFCC, the delta and delta-delta information are also used to formulate three-channels features. Inception, ResNet, ResNeXt, Dual Path Networks (DPN) are selected as the neural network architectures, while Mixup is used for the data augmentation. Using ResNeXt, our best single convolutional neural network architecture provides an mAP@3 of 0.967 on the public Kaggle leaderboard. Moreover, to improve the accuracy further, we also propose a meta learning-based ensemble method. By employing the diversities between different architectures, the meta learning- based model can provide higher prediction accuracy and robustness with comparison to the single model. Using the proposed meta- learning method, our solution achieves an mAP@3 of 0.977 (rank 1 of 555) on the public Kaggle leaderboard, while the baseline gives an mAP@3 of 0.70."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Khadkevich2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Khadkevich2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        ACOUSTIC SCENE AND EVENT DETECTION SYSTEMS SUBMITTED TO DCASE 2018 CHALLENGE
       </h4>
<p style="text-align:left">
        Maksim Khadkevich
       </p>
<p style="text-align:left">
<em>
         AML (FB), Facebook, Menlo Park, CA, USA.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Khadkevich_FB_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Khadkevich2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Khadkevich2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Khadkevich2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Khadkevich_88.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Khadkevich2018" class="panel-collapse collapse" id="collapse-Khadkevich2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       ACOUSTIC SCENE AND EVENT DETECTION SYSTEMS SUBMITTED TO DCASE 2018 CHALLENGE
      </h4>
<p style="text-align:left">
<small>
        Maksim Khadkevich
       </small>
<br/>
<small>
<em>
         AML (FB), Facebook, Menlo Park, CA, USA.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report we describe systems that have been submitted to DCASE 2018 challenge. Feature extraction and convolutional neural network (CNN) architecture are outlined. For tasks 1c and 2 we describe transfer learning approach that has been applied. Model training and inference are finally presented.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Khadkevich2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Khadkevich_88.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Khadkevich2018label" class="modal fade" id="bibtex-Khadkevich2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKhadkevich2018label">
        ACOUSTIC SCENE AND EVENT DETECTION SYSTEMS SUBMITTED TO DCASE 2018 CHALLENGE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Khadkevich2018,
    Author = "Khadkevich, Maksim",
    title = "ACOUSTIC SCENE AND EVENT DETECTION SYSTEMS SUBMITTED TO DCASE 2018 CHALLENGE",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technical report we describe systems that have been submitted to DCASE 2018 challenge. Feature extraction and convolutional neural network (CNN) architecture are outlined. For tasks 1c and 2 we describe transfer learning approach that has been applied. Model training and inference are finally presented."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GIST_WISENETAI AUDIO TAGGER BASED ON CONCATENATED RESIDUAL NETWORK FOR DCASE 2018 CHALLENGE TASK 2
       </h4>
<p style="text-align:left">
        Nam Kyun Kim, Jeong Hyeon Yang, Hong Kook Kim, Jeong Eun Lim, Jin Soo Park and Ji Hyun Park
       </p>
<p style="text-align:left">
<em>
         School of Electrical Engineering and Computer Science (GIST), Gwangju Institute of Science and Technology, Gwangju, Korea. Algorithm R&amp;D Team (Hanwha Techwin), Hanwha Techwin, Sungnam, Korea.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kim_GIST_WisenetAI_task2_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Kim_21.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2018" class="panel-collapse collapse" id="collapse-Kim2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GIST_WISENETAI AUDIO TAGGER BASED ON CONCATENATED RESIDUAL NETWORK FOR DCASE 2018 CHALLENGE TASK 2
      </h4>
<p style="text-align:left">
<small>
        Nam Kyun Kim, Jeong Hyeon Yang, Hong Kook Kim, Jeong Eun Lim, Jin Soo Park and Ji Hyun Park
       </small>
<br/>
<small>
<em>
         School of Electrical Engineering and Computer Science (GIST), Gwangju Institute of Science and Technology, Gwangju, Korea. Algorithm R&amp;D Team (Hanwha Techwin), Hanwha Techwin, Sungnam, Korea.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we describe the method and performance of an acoustic event tagger applied to the Task 2 of the Detection and Classification of Acoustic Scene and Events 2018 (DCASE 2018) challenge, where the task evaluates systems for general-purpose audio tagging with an increased number of categories and using data with annotations of varying reliability. The proposed audio tagger, which is call GIST_WisenetAI and developed by the collaboration of GIST and Hanwha Techwin, is based on a concatenated residual network (ConResNet). In particular, the proposed ConResNet is composed of two types of convolutional neural net- work (CNN) residual networks (CNN-ResNet) such as a 2D CNN-ResNet and an 1D CNN-ResNet using a sequence of mel- frequency cepstrum coefficients (MFCCs) and their statistics, respectively, as input features. In order to improve the performance of audio tagging, k different ConResNets are trained using k-fold cross-validation, and then they are linearly combined to generate an ensemble classifier. In this task, 9,473 audio samples for training/validation are divided into 10 folds, and 9,400 audio sample are given for testing. Consequently, the proposed method provides the mean average precision up to top 3 (MAP@3) of 0.958, which is measured through the Kaggle platform.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, time shifting, additive wgn
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, delta of MFCC, delta-delta of MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN,ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         10
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         7197609 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Kim_21.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2018label" class="modal fade" id="bibtex-Kim2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2018label">
        GIST_WISENETAI AUDIO TAGGER BASED ON CONCATENATED RESIDUAL NETWORK FOR DCASE 2018 CHALLENGE TASK 2
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2018,
    Author = "Kim, Nam Kyun and Yang, Jeong Hyeon and Kim, Hong Kook and Lim, Jeong Eun and Park, Jin Soo and Park, Ji Hyun",
    title = "GIST\_WISENETAI AUDIO TAGGER BASED ON CONCATENATED RESIDUAL NETWORK FOR DCASE 2018 CHALLENGE TASK 2",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this report, we describe the method and performance of an acoustic event tagger applied to the Task 2 of the Detection and Classification of Acoustic Scene and Events 2018 (DCASE 2018) challenge, where the task evaluates systems for general-purpose audio tagging with an increased number of categories and using data with annotations of varying reliability. The proposed audio tagger, which is call GIST\_WisenetAI and developed by the collaboration of GIST and Hanwha Techwin, is based on a concatenated residual network (ConResNet). In particular, the proposed ConResNet is composed of two types of convolutional neural net- work (CNN) residual networks (CNN-ResNet) such as a 2D CNN-ResNet and an 1D CNN-ResNet using a sequence of mel- frequency cepstrum coefficients (MFCCs) and their statistics, respectively, as input features. In order to improve the performance of audio tagging, k different ConResNets are trained using k-fold cross-validation, and then they are linearly combined to generate an ensemble classifier. In this task, 9,473 audio samples for training/validation are divided into 10 folds, and 9,400 audio sample are given for testing. Consequently, the proposed method provides the mean average precision up to top 3 (MAP@3) of 0.958, which is measured through the Kaggle platform."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2018 CHALLENGE SURREY CROSS-TASK CONVOLUTIONAL NEURAL NETWORK BASELINE
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Iqbal Turab, Xu Yong, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vission, Speech and Signal Processing (CVSSP) (Surrey), University of Surrey, Guildford, UK.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Baseline_Surrey_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Baseline_87.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2018').collapse('show');window.location.hash='#Kong2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2018" class="panel-collapse collapse" id="collapse-Kong2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2018 CHALLENGE SURREY CROSS-TASK CONVOLUTIONAL NEURAL NETWORK BASELINE
      </h4>
<p style="text-align:left">
<small>
        Qiuqiang Kong, Iqbal Turab, Xu Yong, Wenwu Wang and Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         Centre for Vission, Speech and Signal Processing (CVSSP) (Surrey), University of Surrey, Guildford, UK.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Detection and classification of acoustic scenes and events (DCASE) 2018 challenge is a well known IEEE AASP challenge consists of several audio classification and sound event detection tasks. DCASE 2018 challenge includes five tasks: 1) Acoustic scene classification, 2) Audio tagging of Freesound, 3) Bird audio detection, 4) Weakly labeled semi-supervised sound event detection and 5) Multi-channel audio tagging. In this paper we open source the python code of all of Task 1 - 5 of DCASE 2018 challenge. The baseline source code contains the implementation of the convolutioanl neural networks (CNNs) including the AlexNetish and the VGGish from the image processing area. We researched how the performance varies from task to task when the configuration of the neural networks are the same. The experiment shows deeper VGGish network performs better than AlexNetish on Task 2 - 5 except Task 1 where VGGish and AlexNetish network perform similar. With the VGGish network, we achieve an accuracy of 0.680 on Task 1, a mean average precision (mAP) of 0.928 on Task 2, an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8% on Task 4 and a F1 score of 87.75% on Task 5.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         AlexNetish 4 layer CNN with global max pooling
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         4309450 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Baseline_87.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task2" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2018label" class="modal fade" id="bibtex-Kong2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2018label">
        DCASE 2018 CHALLENGE SURREY CROSS-TASK CONVOLUTIONAL NEURAL NETWORK BASELINE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2018,
    Author = "Kong, Qiuqiang and Turab, Iqbal and Yong, Xu and Wang, Wenwu and Plumbley, Mark D.",
    title = "DCASE 2018 CHALLENGE SURREY CROSS-TASK CONVOLUTIONAL NEURAL NETWORK BASELINE",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Detection and classification of acoustic scenes and events (DCASE) 2018 challenge is a well known IEEE AASP challenge consists of several audio classification and sound event detection tasks. DCASE 2018 challenge includes five tasks: 1) Acoustic scene classification, 2) Audio tagging of Freesound, 3) Bird audio detection, 4) Weakly labeled semi-supervised sound event detection and 5) Multi-channel audio tagging. In this paper we open source the python code of all of Task 1 - 5 of DCASE 2018 challenge. The baseline source code contains the implementation of the convolutioanl neural networks (CNNs) including the AlexNetish and the VGGish from the image processing area. We researched how the performance varies from task to task when the configuration of the neural networks are the same. The experiment shows deeper VGGish network performs better than AlexNetish on Task 2 - 5 except Task 1 where VGGish and AlexNetish network perform similar. With the VGGish network, we achieve an accuracy of 0.680 on Task 1, a mean average precision (mAP) of 0.928 on Task 2, an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8\% on Task 4 and a F1 score of 87.75\% on Task 5."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Nguyen2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2018 TASK 2: ITERATIVE TRAINING, LABEL SMOOTHING, AND BACKGROUND NOISE NORMALIZATION FOR AUDIO EVENT TAGGING
       </h4>
<p style="text-align:left">
        Thi Ngoc Tho Nguyen, Ngoc Khanh Nguyen, Douglas L. Jones and Woon Seng Gan
       </p>
<p style="text-align:left">
<em>
         Electrical and Electronic Engineering (NTU), Nanyang Technological University, Singapore. Electrical and Computer Engineering (UIUC), University of Illinois Urbana-Champaign, Illinois, USA. SWAT, SWAT, Singapore. Electrical and Electronic Engineering (NTU), Nanyang Technological University, New York, USA.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nguyen_NTU_task2_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nguyen2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Nguyen_106.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2018" class="panel-collapse collapse" id="collapse-Nguyen2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2018 TASK 2: ITERATIVE TRAINING, LABEL SMOOTHING, AND BACKGROUND NOISE NORMALIZATION FOR AUDIO EVENT TAGGING
      </h4>
<p style="text-align:left">
<small>
        Thi Ngoc Tho Nguyen, Ngoc Khanh Nguyen, Douglas L. Jones and Woon Seng Gan
       </small>
<br/>
<small>
<em>
         Electrical and Electronic Engineering (NTU), Nanyang Technological University, Singapore. Electrical and Computer Engineering (UIUC), University of Illinois Urbana-Champaign, Illinois, USA. SWAT, SWAT, Singapore. Electrical and Electronic Engineering (NTU), Nanyang Technological University, New York, USA.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes an approach from our submissions for DCASE 2018 Task 2: general-purpose audio tagging of Freesound content with AudioSet labels. To tackle the problem of diverse recording environments, we propose to use background noise normalization. To tackle the problem of noisy labels, we propose to use pseudo- label for automatic label verification and label smoothing to reduce the over-fitting. We train several convolutional neural networks with data augmentation and different input sizes for the automatic label verification process. The procedure is promising to improve the quality of datasets for audio classification. On the public leader board for the competition, our single model and an ensemble of 8 models score 0.941 and 0.963 respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         block mixing, randomly erase/cutout, time stretching, pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         8
        </td>
</tr>
<tr>
<td class="col-md-3">
         Re-labeling
        </td>
<td>
         automatic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         5679784 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Nguyen_106.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2018label" class="modal fade" id="bibtex-Nguyen2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2018label">
        DCASE 2018 TASK 2: ITERATIVE TRAINING, LABEL SMOOTHING, AND BACKGROUND NOISE NORMALIZATION FOR AUDIO EVENT TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nguyen2018,
    Author = "Nguyen, Thi Ngoc Tho and Nguyen, Ngoc Khanh and Jones, Douglas L. and Gan, Woon Seng",
    title = "DCASE 2018 TASK 2: ITERATIVE TRAINING, LABEL SMOOTHING, AND BACKGROUND NOISE NORMALIZATION FOR AUDIO EVENT TAGGING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper describes an approach from our submissions for DCASE 2018 Task 2: general-purpose audio tagging of Freesound content with AudioSet labels. To tackle the problem of diverse recording environments, we propose to use background noise normalization. To tackle the problem of noisy labels, we propose to use pseudo- label for automatic label verification and label smoothing to reduce the over-fitting. We train several convolutional neural networks with data augmentation and different input sizes for the automatic label verification process. The procedure is promising to improve the quality of datasets for audio classification. On the public leader board for the competition, our single model and an ensemble of 8 models score 0.941 and 0.963 respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pantic2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Pantic2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        ENSEMBLE OF CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL PURPOSE AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Bogdan Pantic
       </p>
<p style="text-align:left">
<em>
         Signals and Systems Department (ETF), School of Electrical Engineering, Belgrade, Serbia.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Pantic_ETF_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Pantic2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pantic2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pantic2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Pantic_86.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pantic2018" class="panel-collapse collapse" id="collapse-Pantic2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       ENSEMBLE OF CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL PURPOSE AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Bogdan Pantic
       </small>
<br/>
<small>
<em>
         Signals and Systems Department (ETF), School of Electrical Engineering, Belgrade, Serbia.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This work describes our solution for the general purpose audio tagging task of the DCASE 2018 challenge. We propose ensemble of several Convolutional Neural Networks (CNNs) with different properties. Logistic regression is used as meta-classifier to produce final predictions. Experiments demonstrate that ensemble outperforms each CNN individually. Finally, proposed system achieves Mean Average Precision (MAP) score of 0.956 on public leaderboard, which is significant improvement compared to base- line.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, random erasing, width shift, zoom
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT, mel-spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         Logistic regression
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         11
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         94075101 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pantic2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Pantic_86.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pantic2018label" class="modal fade" id="bibtex-Pantic2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPantic2018label">
        ENSEMBLE OF CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL PURPOSE AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Pantic2018,
    Author = "Pantic, Bogdan",
    title = "ENSEMBLE OF CONVOLUTIONAL NEURAL NETWORKS FOR GENERAL PURPOSE AUDIO TAGGING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This work describes our solution for the general purpose audio tagging task of the DCASE 2018 challenge. We propose ensemble of several Convolutional Neural Networks (CNNs) with different properties. Logistic regression is used as meta-classifier to produce final predictions. Experiments demonstrate that ensemble outperforms each CNN individually. Finally, proposed system achieves Mean Average Precision (MAP) score of 0.956 on public leaderboard, which is significant improvement compared to base- line."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ren2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Ren2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        AUTOMATIC AUDIO TAGGING WITH 1D AND 2D CONVOLUTIONAL NEURAL NETWORKS
       </h4>
<p style="text-align:left">
        Siyuan Shan and Yi Ren
       </p>
<p style="text-align:left">
<em>
         DB Sonics, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Shan_DBSonics_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ren2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ren2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ren2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Ren_64.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ren2018" class="panel-collapse collapse" id="collapse-Ren2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       AUTOMATIC AUDIO TAGGING WITH 1D AND 2D CONVOLUTIONAL NEURAL NETWORKS
      </h4>
<p style="text-align:left">
<small>
        Siyuan Shan and Yi Ren
       </small>
<br/>
<small>
<em>
         DB Sonics, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this work, we ensemble four different models for the audio tagging tasks. The first two models are 2D convolutional neural net- works (CNNs) that respectively take Mel spectrogram and MFCC spectrogram as input features, while other last two models are two 1D CNNs architectures that take raw waveform as inputs. Data augmentation techniques, including time stretch, pitch shift, reverb and dynamic range compression, are also employed for better generalization. Transfer learning from two external datasets is also adopted. These components together contribute to our final recognition performance reported on Kaggle.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, pitch shift, reverb, dynamic range compression
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         4
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         6000000 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ren2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Ren_64.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ren2018label" class="modal fade" id="bibtex-Ren2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRen2018label">
        AUTOMATIC AUDIO TAGGING WITH 1D AND 2D CONVOLUTIONAL NEURAL NETWORKS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ren2018,
    Author = "Shan, Siyuan and Ren, Yi",
    title = "AUTOMATIC AUDIO TAGGING WITH 1D AND 2D CONVOLUTIONAL NEURAL NETWORKS",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this work, we ensemble four different models for the audio tagging tasks. The first two models are 2D convolutional neural net- works (CNNs) that respectively take Mel spectrogram and MFCC spectrogram as input features, while other last two models are two 1D CNNs architectures that take raw waveform as inputs. Data augmentation techniques, including time stretch, pitch shift, reverb and dynamic range compression, are also employed for better generalization. Transfer learning from two external datasets is also adopted. These components together contribute to our final recognition performance reported on Kaggle."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="WEI2018" style="box-shadow: none">
<div class="panel-heading" id="heading-WEI2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A REPORT ON AUDIO TAGGING WITH DEEPER CNN, 1D-CONVNET AND 2D-CONVNET
       </h4>
<p style="text-align:left">
        Qingkai WEI, Yanfang LIU and Xiaohui RUAN
       </p>
<p style="text-align:left">
<em>
         Kuaiyu, Beijing Kuaiyu Electronics Co., Ltd, Beijing, PRC.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wei_Kuaiyu_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-WEI2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-WEI2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-WEI2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_WEI_53.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-WEI2018" class="panel-collapse collapse" id="collapse-WEI2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A REPORT ON AUDIO TAGGING WITH DEEPER CNN, 1D-CONVNET AND 2D-CONVNET
      </h4>
<p style="text-align:left">
<small>
        Qingkai WEI, Yanfang LIU and Xiaohui RUAN
       </small>
<br/>
<small>
<em>
         Kuaiyu, Beijing Kuaiyu Electronics Co., Ltd, Beijing, PRC.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       General-purpose audio tagging is a newly proposed task in DCASE 2018, which can provide insight towards broadly-applicable sound event classifiers. In this paper, two systems (named as 1D-ConvNet and 2D-ConvNet in this paper) with small kernel sizes, multiple functional modules, deeper CNN (convolutional neural network- s) are developed to improve performance in this task. Different audio features are used: raw waveforms are for 1D-ConvNet; frequency domain features such as mfcc, log-mel spectrogram, multi-resolution log-mel spectrogram and spectrogram, are compared as the 2D-ConvNet input. Using DCASE 2018 Challenge task 2 dataset to train and evaluate, the best single model with 1D- ConvNet and 2D-ConvNet are chosen, whose kaggle public leader- board score are 0.877 and 0.961 respectively. In addition, a better ensemble rank averaging prediction get a score 0.968 on the public leaderboard, ranking 5/556.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, random erasing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         rank averaging
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         9764770 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-WEI2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_WEI_53.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-WEI2018label" class="modal fade" id="bibtex-WEI2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWEI2018label">
        A REPORT ON AUDIO TAGGING WITH DEEPER CNN, 1D-CONVNET AND 2D-CONVNET
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{WEI2018,
    Author = "WEI, Qingkai and LIU, Yanfang and RUAN, Xiaohui",
    title = "A REPORT ON AUDIO TAGGING WITH DEEPER CNN, 1D-CONVNET AND 2D-CONVNET",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "General-purpose audio tagging is a newly proposed task in DCASE 2018, which can provide insight towards broadly-applicable sound event classifiers. In this paper, two systems (named as 1D-ConvNet and 2D-ConvNet in this paper) with small kernel sizes, multiple functional modules, deeper CNN (convolutional neural network- s) are developed to improve performance in this task. Different audio features are used: raw waveforms are for 1D-ConvNet; frequency domain features such as mfcc, log-mel spectrogram, multi-resolution log-mel spectrogram and spectrogram, are compared as the 2D-ConvNet input. Using DCASE 2018 Challenge task 2 dataset to train and evaluate, the best single model with 1D- ConvNet and 2D-ConvNet are chosen, whose kaggle public leader- board score are 0.877 and 0.961 respectively. In addition, a better ensemble rank averaging prediction get a score 0.968 on the public leaderboard, ranking 5/556."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wilhelm2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Wilhelm2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        COMBINING HIGH-LEVEL FEATURES OF RAW AUDIO AND SPECTROGRAMS FOR AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Benjamin Wilhelm and Marcel Lederle
       </p>
<p style="text-align:left">
<em>
         Computer and Information Science (UKON), University of Konstanz, Constance, Germany.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wilhelm_UKON_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wilhelm2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wilhelm2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wilhelm2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wilhelm_109.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wilhelm2018" class="panel-collapse collapse" id="collapse-Wilhelm2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       COMBINING HIGH-LEVEL FEATURES OF RAW AUDIO AND SPECTROGRAMS FOR AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Benjamin Wilhelm and Marcel Lederle
       </small>
<br/>
<small>
<em>
         Computer and Information Science (UKON), University of Konstanz, Constance, Germany.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We introduce a method for general-purpose audio tagging that combines high-level features computed from the spectrogram and raw audio data. We use convolutional neural networks with one- dimensional and two-dimensional convolutions to extract these useful high-level features and combine them with a densely connected neural network to make predictions. Our method performs in the top two percent of on the Freesound General-Purpose Audio Tagging Challenge.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         cropping, padding, time shifting, same class blending, different class blending
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, raw audio
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         3136649 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wilhelm2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wilhelm_109.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wilhelm2018label" class="modal fade" id="bibtex-Wilhelm2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWilhelm2018label">
        COMBINING HIGH-LEVEL FEATURES OF RAW AUDIO AND SPECTROGRAMS FOR AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wilhelm2018,
    Author = "Wilhelm, Benjamin and Lederle, Marcel",
    title = "COMBINING HIGH-LEVEL FEATURES OF RAW AUDIO AND SPECTROGRAMS FOR AUDIO TAGGING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "We introduce a method for general-purpose audio tagging that combines high-level features computed from the spectrogram and raw audio data. We use convolutional neural networks with one- dimensional and two-dimensional convolutions to extract these useful high-level features and combine them with a densely connected neural network to make predictions. Our method performs in the top two percent of on the Freesound General-Purpose Audio Tagging Challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Wilkinghoff2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Wilkinghoff2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GENERAL-PURPOSE AUDIO TAGGING BY ENSEMBLING CONVOLUTIONAL NEURAL NETWORKS BASED ON MULTIPLE FEATURES
       </h4>
<p style="text-align:left">
        Kevin Wilkinghoff
       </p>
<p style="text-align:left">
<em>
         Communication Systems (FKIE), Fraunhofer Institute for Communication, Information Processing and Ergonomics, Wachtberg, Germany.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Wilkinghoff_FKIE_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Wilkinghoff2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Wilkinghoff2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Wilkinghoff2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wilkinghoff_80.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Wilkinghoff2018" class="panel-collapse collapse" id="collapse-Wilkinghoff2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GENERAL-PURPOSE AUDIO TAGGING BY ENSEMBLING CONVOLUTIONAL NEURAL NETWORKS BASED ON MULTIPLE FEATURES
      </h4>
<p style="text-align:left">
<small>
        Kevin Wilkinghoff
       </small>
<br/>
<small>
<em>
         Communication Systems (FKIE), Fraunhofer Institute for Communication, Information Processing and Ergonomics, Wachtberg, Germany.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper describes an audio tagging system which participated in Task 2 “General-purpose audio tagging of Freesound content with AudioSet labels” of the “Detection and Classification of Acoustic Scenes and Events (DCASE)” Challenge 2018. The system is an ensemble consisting of five convolutional neural networks based on Mel-frequency Cepstral Coefficients, Perceptual Linear Prediction features, Mel-spectrograms and the raw audio data. For ensembling all models, score-based fusion via Logistic Regression is per- formed with another neural network. In experimental evaluations, it is shown that ensembling the models significantly improves upon the performances obtained with the individual models.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         24kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mix-up, cutout, dropout, vertical shifts
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         PLP, MFCC, mel-spectrogram, raw data
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         logistic regression, neural network
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         25
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         349396065 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Wilkinghoff2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wilkinghoff_80.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Wilkinghoff2018label" class="modal fade" id="bibtex-Wilkinghoff2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWilkinghoff2018label">
        GENERAL-PURPOSE AUDIO TAGGING BY ENSEMBLING CONVOLUTIONAL NEURAL NETWORKS BASED ON MULTIPLE FEATURES
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Wilkinghoff2018,
    Author = "Wilkinghoff, Kevin",
    title = "GENERAL-PURPOSE AUDIO TAGGING BY ENSEMBLING CONVOLUTIONAL NEURAL NETWORKS BASED ON MULTIPLE FEATURES",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper describes an audio tagging system which participated in Task 2 “General-purpose audio tagging of Freesound content with AudioSet labels” of the “Detection and Classification of Acoustic Scenes and Events (DCASE)” Challenge 2018. The system is an ensemble consisting of five convolutional neural networks based on Mel-frequency Cepstral Coefficients, Perceptual Linear Prediction features, Mel-spectrograms and the raw audio data. For ensembling all models, score-based fusion via Logistic Regression is per- formed with another neural network. In experimental evaluations, it is shown that ensembling the models significantly improves upon the performances obtained with the individual models."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Xu2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        THE AALTO SYSTEM BASED ON FINE-TUNED AUDIOSET FEATURES FOR DCASE2018 TASK2 —— GENERAL PURPOSE AUDIO TAGGING
       </h4>
<p style="text-align:left">
        Zhicun Xu, Peter Smit and Mikko Kurimo
       </p>
<p style="text-align:left">
<em>
         Department of Signal Processing and Acoustics (Aalto), Aalto University, Finland, Espoo, Finland.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xu_Aalto_task2_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xu2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Xu_28.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2018" class="panel-collapse collapse" id="collapse-Xu2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       THE AALTO SYSTEM BASED ON FINE-TUNED AUDIOSET FEATURES FOR DCASE2018 TASK2 —— GENERAL PURPOSE AUDIO TAGGING
      </h4>
<p style="text-align:left">
<small>
        Zhicun Xu, Peter Smit and Mikko Kurimo
       </small>
<br/>
<small>
<em>
         Department of Signal Processing and Acoustics (Aalto), Aalto University, Finland, Espoo, Finland.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this paper, we presented a neural network system for DCASE 2018 task 2, general purpose audio tagging. We fine-tuned the Google AudioSet feature generation model with different settings for the given 41 classes on top of a fully connected layer with 100 units. Then we used the fine-tuned models to generate 128 dimensional features for each 0.960s audio. We tried different neural net- work structures including LSTM and multi-level attention models. In our experiments, the multi-level attention model has shown its superiority over others. Truncating the silence parts, repeating and splitting the audio into the fixed length, pitch shifting augmentation, and mixup techniques have all improved the results with a reason- able amount. The proposed system achieved a result with MAP@3 score at 0.936, which outperforms the baseline result of 0.704 and achieves top 7% in the public leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch shifting, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, DNN, multi-level attention,
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         geometric mean
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         120
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         114715200 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Xu_28.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2018label" class="modal fade" id="bibtex-Xu2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2018label">
        THE AALTO SYSTEM BASED ON FINE-TUNED AUDIOSET FEATURES FOR DCASE2018 TASK2 —— GENERAL PURPOSE AUDIO TAGGING
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xu2018,
    Author = "Xu, Zhicun and Smit, Peter and Kurimo, Mikko",
    title = "THE AALTO SYSTEM BASED ON FINE-TUNED AUDIOSET FEATURES FOR DCASE2018 TASK2 —— GENERAL PURPOSE AUDIO TAGGING",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this paper, we presented a neural network system for DCASE 2018 task 2, general purpose audio tagging. We fine-tuned the Google AudioSet feature generation model with different settings for the given 41 classes on top of a fully connected layer with 100 units. Then we used the fine-tuned models to generate 128 dimensional features for each 0.960s audio. We tried different neural net- work structures including LSTM and multi-level attention models. In our experiments, the multi-level attention model has shown its superiority over others. Truncating the silence parts, repeating and splitting the audio into the fixed length, pitch shifting augmentation, and mixup techniques have all improved the results with a reason- able amount. The proposed system achieved a result with MAP@3 score at 0.936, which outperforms the baseline result of 0.704 and achieves top 7\% in the public leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yu2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Yu2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MODEL OF B-CNN AND WAVENET IN TASK 2
       </h4>
<p style="text-align:left">
        Zhesong Yu
       </p>
<p style="text-align:left">
<em>
         Institute of computer science &amp; technology (PKU), Peking University, Beijing, China.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhesong_PKU_task2_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yu2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yu2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Yu_43.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yu2018" class="panel-collapse collapse" id="collapse-Yu2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MODEL OF B-CNN AND WAVENET IN TASK 2
      </h4>
<p style="text-align:left">
<small>
        Zhesong Yu
       </small>
<br/>
<small>
<em>
         Institute of computer science &amp; technology (PKU), Peking University, Beijing, China.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The system has two parts, one is BCNN in MFCC, another is WaveNet in raw audio. B-CNN is a model proposed to solve Im- age fine classification task, and WaveNet is a model used to music generation. And the propose of this paper is just to see the performance of the two model in music classification task.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz &amp; 16kHZ
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC &amp; raw audio
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Bilinear-CNN &amp; WaveNet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Ensemble subsystems
        </td>
<td>
         2
        </td>
</tr>
<tr>
<td class="col-md-3">
         Re-labeling
        </td>
<td>
         automatic
        </td>
</tr>
<tr>
<td class="col-md-3">
         Complexity
        </td>
<td>
         658100 parameters
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yu2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Yu_43.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yu2018label" class="modal fade" id="bibtex-Yu2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYu2018label">
        MODEL OF B-CNN AND WAVENET IN TASK 2
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yu2018,
    Author = "Yu, Zhesong",
    title = "MODEL OF B-CNN AND WAVENET IN TASK 2",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "The system has two parts, one is BCNN in MFCC, another is WaveNet in raw audio. B-CNN is a model proposed to solve Im- age fine classification task, and WaveNet is a model used to music generation. And the propose of this paper is just to see the performance of the two model in music classification task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>