<!DOCTYPE html><html lang="en">
<head>
    <title>Audio and Audiovisual Sound Event Localization and Detection with Source Distance Estimation - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of the sound event localization and detection task is to detect occurrences of sound events belonging to specific target classes, track their temporal activity, and estimate their directions-of-arrival or positions during it. Challenge has ended. Full results for this task can be found in the Results page. Description …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right navbar-tighter" id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2024</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2024/"><i class="fa fa-home"></i>&nbsp;Intro</a>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-t1"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-t2"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-t3"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-t4"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-event-detection-with-heterogeneous-training-dataset-and-potentially-missing-labels-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-t5"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-automated-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-t6"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-automated-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-sound-scene-synthesis" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthesis text-t7"></i>&nbsp;Task7&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-sound-scene-synthesis-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-retrieval text-t8"></i>&nbsp;Task8&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-language-queried-audio-source-separation" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-separation text-t9"></i>&nbsp;Task9&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-language-queried-audio-source-separation-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-traffic text-t10"></i>&nbsp;Task10&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2024/task-acoustic-based-traffic-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2024/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2024/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/wall-08.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-t3"></i><i class="fa dc-localization fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Localization</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 3</span></span><img src="../images/logos/dcase/dcase2024_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Audio and Audiovisual Sound Event Localization and Detection with Source Distance Estimation</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Archontis Politis" class="img img-circle" src="/images/person/archontis_politis.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Archontis Politis</strong>
<a class="icon" href="mailto:archontis.politis@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Kazuki Shimada" class="img img-circle" src="/images/person/kazuki_shimada.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Kazuki Shimada</strong>
<a class="icon" href="mailto:kazuki.shimada@sony.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Yuki Mitsufuji" class="img img-circle" src="/images/person/yuki_mitsufuji.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Yuki Mitsufuji</strong>
<a class="icon" href="mailto:yuhki.mitsufuji@sony.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Tuomas Virtanen" class="img img-circle" src="/images/person/tuomas_virtanen.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Tuomas Virtanen</strong>
<a class="icon" href="mailto:tuomas.virtanen@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Parthasaarathy Sudarsanam" class="img img-circle" src="/images/person/parthasaarathy_sudarsanam.png" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Parthasaarathy Sudarsanam</strong>
<a class="icon" href="mailto:parthasaarathy.ariyakulamsudarsanam@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Daniel Krause" class="img img-circle" src="/images/person/daniel_krause.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Daniel Krause</strong>
<a class="icon" href="mailto:daniel.krause@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Kengo Uchida" class="img img-circle" src="/images/person/kengo_uchida.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Kengo Uchida</strong>
<a class="icon" href="mailto:kengo.uchida@sony.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="David Diaz-Guerra" class="img img-circle" src="/images/person/david_diaz-guerra.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>David Diaz-Guerra</strong>
<a class="icon" href="mailto:david.diaz-guerra@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://webpages.tuni.fi/arg/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Yuichiro Koyama" class="img img-circle" src="/images/person/yuichiro_koyama.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Yuichiro Koyama</strong>
<a class="icon" href="mailto:yuichiro.koyama@sony.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Naoya Takahashi" class="img img-circle" src="/images/person/naoya_takahashi.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Naoya Takahashi</strong>
<a class="icon" href="mailto:naoya.takahashi@sony.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Takashi Shibuya" class="img img-circle" src="/images/person/takashi_shibuya.png" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Takashi Shibuya</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Shusuke Takahashi" class="img img-circle" src="/images/person/shusuke_takahashi.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Shusuke Takahashi</strong>
<a class="icon" href="mailto:shusuke.takahashi@sony.com"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="https://www.ai.sony/people/">
                                SONY
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#dataset">Dataset</a>
<ul>
<li><a href="#dataset-specifications">Dataset specifications</a></li>
<li><a href="#sound-event-classes">Sound event classes</a></li>
<li><a href="#recording-formats">Recording formats</a></li>
<li><a href="#reference-labels-directions-of-arrival-and-source-distances">Reference labels, directions-of-arrival, and source distances</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#track-a-audio-only-inference">Track A: Audio-only inference</a></li>
<li><a href="#track-b-audiovisual-inference">Track B: Audiovisual inference</a></li>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
</ul>
</li>
<li><a href="#external-data-resources">External data resources</a>
<ul>
<li><a href="#example-external-data-use-with-baseline">Example external data use with baseline</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#submission">Submission</a></li>
<li><a href="#evaluation">Evaluation</a>
<ul>
<li><a href="#metrics">Metrics</a></li>
<li><a href="#ranking">Ranking</a></li>
</ul>
</li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#track-a-audio-only">Track A: Audio-only</a></li>
<li><a href="#track-b-audiovisual">Track B: Audiovisual</a></li>
</ul>
</li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#track-a-audio-only-baseline">Track A: Audio-only baseline</a></li>
<li><a href="#track-b-audiovisual-baseline">Track B: Audiovisual baseline</a></li>
<li><a href="#results-for-the-development-dataset">Results for the development dataset</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of the sound event localization and detection task is to detect occurrences of sound events belonging to specific target classes, track their temporal activity, and estimate their directions-of-arrival or positions during it.</p>
<p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found in the <a class="btn btn-default btn-xs" href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results">Results <i class="fa fa-caret-right"></i></a> page.
</p>
<h1 id="description">Description</h1>
<p>Given multichannel audio input, a <strong>sound event localization and detection (SELD)</strong> system outputs localization estimates of one or more events for each of the target sound classes, whenever such events are detected. This results in a spatio-temporal characterization of the acoustic scene that can be used in a wide range of machine cognition tasks, such as inference on the type of environment, self-localization, navigation with visually occluded targets, tracking of specific types of sound sources, smart-home applications, scene visualization systems, and acoustic monitoring, among others.</p>
<p><strong>Overally, this year the challenge task resembles the previous iteration, evaluating SELD models with audio-only input (Track A) or audiovisual input (Track B) on manually annotated recordings of real interior sound scenes. However, this year the task introduces distance estimation of the detected events, which makes the task significantly more challenging.</strong> The evaluation metrics are also modified to take that extra dimension into account. Regarding the audiovisual track, we believe that there is still plenty of space for developing novel solutions that surpass audio-only models. We encourage participants to submit both audio-only SELD systems and audiovisual SELD systems.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2020/task3_sound_event_localization_and_detection.png"/>
<figcaption>Figure 1: Overview of sound event localization and detection system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<h1 id="dataset">Dataset</h1>
<p>The <strong>Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23)</strong> dataset contains multichannel recordings of sound scenes in various rooms and environments, together with temporal and spatial annotations of prominent events belonging to a set of target classes. The dataset is collected in two different sites, in Tampere, Finland by the Audio Researh Group (ARG) of Tampere University, and in Tokyo, Japan by Sony, using a similar setup and annotation procedure. As in the previous challenges, the dataset is delivered in two spatial recording formats.</p>
<p>The simultaneous 360° video are spatially and temporally aligned with the microphone array recordings. The videos are made available with the participants' consent, after blurring visible faces.</p>
<p>Collection of data from the TAU side has received funding from Google.</p>
<p>Detailed dataset specifications can be found below. More details on the recording and annotation procedure can be found in <a href="https://dcase.community/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes">DCASE2022 Challenge task description</a>, in the technical report of STARSS22:</p>
<div class="btex-item" data-item="Politis2022starss22" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Politis2022starss22"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Archontis Politis, Kazuki Shimada, Parthasaarathy Sudarsanam, Sharath Adavanne, Daniel Krause, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, Yuki Mitsufuji, and Tuomas Virtanen.
<em>STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events.</em>
In Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022), 125–129. Nancy, France, November 2022.
URL: <a href="https://dcase.community/workshop2022/proceedings">https://dcase.community/workshop2022/proceedings</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexPolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2022/proceedings/DCASE2022Workshop_Politis_51.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsePolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsePolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingPolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" class="panel-collapse collapse" id="collapsePolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" role="tabpanel">
<h4>STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events</h4>
<h5>Abstract</h5>
<p class="text-justify">This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARSS22) dataset of spatial recordings of real sound scenes collected in various interiors at two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events belonging to 13 target classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. STARSS22 serves as the development and evaluation dataset for Task 3 (Sound Event Localization and Detection) of the DCASE2022 Challenge and it introduces significant new challenges with regard to the previous iterations, which were based on synthetic data. Additionally, the report introduces the baseline system that accompanies the dataset with emphasis on its differences to the baseline of the previous challenge. Baseline results indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in https://zenodo.org/record/6600531.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexPolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2022/proceedings/DCASE2022Workshop_Politis_51.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" href="https://github.com/sharathadavanne/seld-dcase2022" title=""><i class="fa fa-file-code-o"></i> </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexPolitis2022starss2266b1cb7793c94f5d822e54a2bd42532blabel" class="modal fade" id="bibtexPolitis2022starss2266b1cb7793c94f5d822e54a2bd42532b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexPolitis2022starss2266b1cb7793c94f5d822e54a2bd42532blabel">STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Politis2022starss22,
    author = "Politis, Archontis and Shimada, Kazuki and Sudarsanam, Parthasaarathy and Adavanne, Sharath and Krause, Daniel and Koyama, Yuichiro and Takahashi, Naoya and Takahashi, Shusuke and Mitsufuji, Yuki and Virtanen, Tuomas",
    title = "{STARSS22}: {A} dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    pages = "125--129",
    abstract = "This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARSS22) dataset of spatial recordings of real sound scenes collected in various interiors at two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events belonging to 13 target classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. STARSS22 serves as the development and evaluation dataset for Task 3 (Sound Event Localization and Detection) of the DCASE2022 Challenge and it introduces significant new challenges with regard to the previous iterations, which were based on synthetic data. Additionally, the report introduces the baseline system that accompanies the dataset with emphasis on its differences to the baseline of the previous challenge. Baseline results indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in https://zenodo.org/record/6600531.",
    url = "https://dcase.community/workshop2022/proceedings"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>and in the dataset paper of STARSS23:</p>
<div class="btex-item" data-item="Shimada2023starss23" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Shimada2023starss23"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Kazuki Shimada, Archontis Politis, Parthasaarathy Sudarsanam, Daniel A. Krause, Kengo Uchida, Sharath Adavanne, Aapo Hakala, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, Tuomas Virtanen, and Yuki Mitsufuji.
<em>STARSS23: an audio-visual dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events.</em>
In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, 72931–72957. Curran Associates, Inc., 2023.
URL: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets_and_Benchmarks.html">https://proceedings.neurips.cc/paper_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets_and_Benchmarks.html</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e6c9671ed3b3106b71cafda3ba225c1a-Paper-Datasets_and_Benchmarks.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" class="panel-collapse collapse" id="collapseShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" role="tabpanel">
<h4>STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events</h4>
<h5>Abstract</h5>
<p class="text-justify">While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e6c9671ed3b3106b71cafda3ba225c1a-Paper-Datasets_and_Benchmarks.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" href="https://github.com/sony/audio-visual-seld-dcase2023" title=""><i class="fa fa-file-code-o"></i> </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9flabel" class="modal fade" id="bibtexShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9f" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexShimada2023starss23a750b2b2aa1548a9ac0fe85e8c366f9flabel">STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Shimada2023starss23,
    author = "Shimada, Kazuki and Politis, Archontis and Sudarsanam, Parthasaarathy and Krause, Daniel A. and Uchida, Kengo and Adavanne, Sharath and Hakala, Aapo and Koyama, Yuichiro and Takahashi, Naoya and Takahashi, Shusuke and Virtanen, Tuomas and Mitsufuji, Yuki",
    editor = "Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.",
    booktitle = "Advances in Neural Information Processing Systems",
    pages = "72931--72957",
    publisher = "Curran Associates, Inc.",
    title = "{STARSS23}: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events",
    abstract = "While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.",
    url = "https://proceedings.neurips.cc/paper\_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets\_and\_Benchmarks.html",
    volume = "36",
    year = "2023"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/ZtL-8wBYPow" title="YouTube video player" width="560"></iframe>
<p><em>360° Video excerpts from STARSS23 with spatioatmpoeral labels overlaid on top of the video.</em><br/>
<em>For spatialized binaural listening of the scene, use headphones on Chrome/Firefox.</em></p>
<h2 id="dataset-specifications">Dataset specifications</h2>
<p>The specifications of the dataset can be summarized in the following:</p>
<p>General:</p>
<ul>
<li>Recordings are taken in two different sites.</li>
<li>Each recording clip is part of a recording session happening in a unique room.</li>
<li>Groups of participants, sound making props, and scene scenarios are unique for each session (with a few exceptions).</li>
<li>To achieve good variability and efficiency in the data, in terms of presence, density, movement, and/or spatial distribution of the sounds events, the scenes are loosely scripted.</li>
<li>13 target classes are identified in the recordings and strongly annotated by humans.</li>
<li>Spatial annotations for those active events are captured by an optical tracking system.</li>
<li>Sound events out of the target classes are considered as interference.</li>
<li>Occurrences of up to 3 simultaneous events are fairly common, while higher numbers of overlapping events (up to 6) can occur but are rare.</li>
</ul>
<p>Volume, duration, and data split:</p>
<ul>
<li>A total of 16 unique rooms captured in the recordings, 4 in Tokyo and 12 in Tampere (development set).</li>
<li>70 recording clips of 30 sec ~ 5 min durations, with a total time of ~2hrs, captured in Tokyo (development dataset).</li>
<li>98 recording clips of 40 sec ~ 9 min durations, with a total time of ~5.5hrs, captured in Tampere (development dataset).</li>
<li>79 recordings clips of 40 sec ~ 7 min durations, with a total time of ~3.5hrs, captured in both sites (evaluation dataset).</li>
<li>A training-testing split is provided for reporting results using the development dataset.</li>
<li>40 recordings contributed by Sony for the training split, captured in 2 rooms (dev-train-sony).</li>
<li>30 recordings contributed by Sony for the testing split, captured in 2 rooms (dev-test-sony).</li>
<li>50 recordings contributed by TAU for the training split, captured in 7 rooms (dev-train-tau).</li>
<li>48 recordings contributed by TAU for the testing split, captured in 5 rooms (dev-test-tau).</li>
</ul>
<p>Audio:</p>
<ul>
<li>Sampling rate: 24kHz.</li>
<li>Bit depth:     16 bits.</li>
<li>Two 4-channel 3-dimensional recording formats: first-order Ambisonics (FOA) and tetrahedral microphone array (MIC).</li>
</ul>
<p>Video:</p>
<ul>
<li>Video 360° format: equirectangular</li>
<li>Video resolution: 1920x960.</li>
<li>Video frames per second (fps): 29.97.</li>
<li>All audio recordings are accompanied by synchronised video recordings, apart from 12 audio recordings with missing videos (fold3_room21_mix001.wav - fold3_room21_mix012.wav)</li>
</ul>
<h2 id="sound-event-classes">Sound event classes</h2>
<p>13 target sound event classes were annotated. The classes follow loosely the <a href="https://research.google.com/audioset/ontology/index.html">Audioset ontology</a>.</p>
<ol start="0">
<li><b>Female speech, woman speaking </b></li>
<li><b>Male speech, man speaking</b></li>
<li><b>Clapping</b></li>
<li><b>Telephone</b></li>
<li><b>Laughter</b></li>
<li><b>Domestic sounds</b></li>
<li><b>Walk, footsteps</b></li>
<li><b>Door, open or close</b></li>
<li><b>Music</b></li>
<li><b>Musical instrument</b></li>
<li><b>Water tap, faucet</b></li>
<li><b>Bell</b></li>
<li><b>Knock</b></li>
</ol>
<p>The content of some of these classes corresponds to events of a limited range of Audioset-related subclasses. These are detailed here to aid the participants:</p>
<ul>
<li><b>Telephone</b>
<ul>
<li>Mostly traditional <i>Telephone Bell Ringing</i> and <i>Ringtone</i> sounds, without musical ringtones.</li>
</ul>
</li>
<li><b>Domestic sounds</b>
<ul>
<li>Sounds of <i>Vacuum cleaner</i></li>
<li>Sounds of water boiler, closer to <i>Boiling</i></li>
<li>Sounds of air circulator, closer to <i>Mechanical fan</i></li>
</ul>
</li>
<li><b>Door, open or close</b>
<ul>
<li>Combination of <i>Door</i> and <i>Cupboard open or close</i></li>
</ul>
</li>
<li><b>Music</b>
<ul>
<li><i>Background music</i> and <i>Pop music</i> played by a loudspeaker in the room.</li>
</ul>
</li>
<li><b>Musical Instrument</b>
<ul>
<li><i>Acoustic guitar</i></li>
<li><i>Marimba, xylophone</i></li>
<li><i>Cowbell</i></li>
<li><i>Piano</i></li>
<li><i>Rattle (instrument)</i></li>
</ul>
</li>
<li><b>Bell</b>
<ul>
<li>Combination of sounds from hotel bell and glass bell, closer to <i>Bicycle bell</i> and single <i>Chime</i>.</li>
</ul>
</li>
</ul>
<p>Some additional notes:</p>
<ul>
<li>The speech classes contain speech in a few different languages.</li>
<li>There are occasionally localized sound events that are not annotated and are considered as interferers, with examples such as <i>computer keyboard</i>, <i>shuffling cards</i>, <i>dishes, pots, and pans</i>.</li>
<li>There is natural background noise (e.g. HVAC noise) in all recordings, at very low levels in some and at quite high levels in others. Such mostly diffuse background noise should be distinct from other noisy target sources (e.g. vacuum cleaner, mechanical fan) since these are clearly spatially localized.</li>
</ul>
<h2 id="recording-formats">Recording formats</h2>
<p>The array response of the two recording formats can be considered known. The following theoretical spatial responses (steering vectors) modeling the two formats describe the directional response of each channel to a source incident from direction-of-arrival (DOA) given by azimuth angle <span class="math">\(\phi\)</span> and elevation angle <span class="math">\(\theta\)</span>.</p>
<p><strong>For the first-order ambisonics (FOA):</strong></p>
<div class="math">\begin{eqnarray}
H_1(\phi, \theta, f) &amp;=&amp; 1 \\
H_2(\phi, \theta, f) &amp;=&amp; \sin(\phi) * \cos(\theta) \\
H_3(\phi, \theta, f) &amp;=&amp; \sin(\theta) \\
H_4(\phi, \theta, f) &amp;=&amp; \cos(\phi) * \cos(\theta)
\end{eqnarray}</div>
<p>
The (FOA) format is obtained by converting the 32-channel microphone array signals by means of encoding filters based on anechoic measurements of the Eigenmike array response. Note that in the formulas above the encoding format is assumed frequency-independent, something that holds true up to around 9kHz with the specific microphone array, while the actual encoded responses start to deviate gradually at higher frequencies from the ideal ones provided above. </p>
<p><strong>For the tetrahedral microphone array (MIC):</strong></p>
<p>The four microphone have the following positions, in spherical coordinates <span class="math">\((\phi, \theta, r)\)</span>:</p>
<div class="math">\begin{eqnarray} 
M1: &amp;\quad(&amp;45^\circ, &amp;&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber\\
M2: &amp;\quad(&amp;-45^\circ, &amp;-&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber\\
M3: &amp;\quad(&amp;135^\circ, &amp;-&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber\\
M4: &amp;\quad(&amp;-135^\circ, &amp;&amp;35^\circ, &amp;4.2\mathrm{cm})\nonumber
\end{eqnarray}</div>
<p>Since the microphones are mounted on an acoustically-hard spherical baffle, an analytical expression for the directional array response is given by the expansion:
</p>
<div class="math">\begin{equation}
H_m(\phi_m, \theta_m, \phi, \theta, \omega) = \frac{1}{(\omega R/c)^2}\sum_{n=0}^{30} \frac{i^{n-1}}{h_n'^{(2)}(\omega R/c)}(2n+1)P_n(\cos(\gamma_m))
\end{equation}</div>
<p>where <span class="math">\(m\)</span> is the channel number, <span class="math">\((\phi_m, \theta_m)\)</span> are the specific microphone's azimuth and elevation position, <span class="math">\(\omega = 2\pi f\)</span> is the angular frequency, <span class="math">\(R = 0.042\)</span>m is the array radius, <span class="math">\(c = 343\)</span>m/s is the speed of sound, <span class="math">\(\cos(\gamma_m)\)</span> is the cosine angle between the microphone and the DOA, and <span class="math">\(P_n\)</span> is the unnormalized Legendre polynomial of degree <span class="math">\(n\)</span>, and <span class="math">\(h_n'^{(2)}\)</span> is the derivative with respect to the argument of a spherical Hankel function of the second kind. The expansion is limited to 30 terms which provides negligible modeling error up to 20kHz. Example routines that can generate directional frequency and impulse array responses based on the above formula can be found <a href="https://github.com/polarch/Array-Response-Simulator">here</a>.</p>
<h2 id="reference-labels-directions-of-arrival-and-source-distances">Reference labels, directions-of-arrival, and source distances</h2>
<p>For each recording in the development dataset, the labels, DoAs, and distances are provided in a plain text CSV file of the same filename as the recording, in the following format:</p>
<div class="highlight"><pre><span></span><code>[frame number (int)], [active class index (int)], [source number index (int)], [azimuth (int)], [elevation (int)], [distance (int)]
</code></pre></div>
<p>Frame, class, and source enumeration begins at 0. Frames correspond to a temporal resolution of 100msec. Azimuth and elevation angles are given in degrees, rounded to the closest integer value, with azimuth and elevation being zero at the front, azimuth <span class="math">\(\phi \in [-180^{\circ}, 180^{\circ}]\)</span>, and elevation <span class="math">\(\theta \in [-90^{\circ}, 90^{\circ}]\)</span>. Note that the azimuth angle is increasing counter-clockwise (<span class="math">\(\phi = 90^{\circ}\)</span> at the left). Distances are provided in centimeters, rounded to the closest integer value.</p>
<p>The source index is a unique integer for each source in the scene, and it is provided only as additional information. Note that each unique actor gets assigned one such identifier, but not individual events produced by the same actor; e.g. a <em>clapping</em> event and a <em>laughter</em> event produced by the same person have the same identifier. Independent sources that are not actors (e.g. a loudspeaker playing music in the room) get a 0 identifier.</p>
<p><strong>Note that the source index is only included in the development metadata as additional information that can be exploited during training. It is not required to be estimated or provided by the participants in their results.</strong></p>
<p>Overlapping sound events are indicated with duplicate frame numbers, and can belong to a different or the same class. An example sequence could be as:</p>
<div class="highlight"><pre><span></span><code><span class="mf">10</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="w">  </span><span class="mf">30</span><span class="p">,</span><span class="w"> </span><span class="mf">181</span>
<span class="mf">11</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="o">-</span><span class="mf">50</span><span class="p">,</span><span class="w">  </span><span class="mf">30</span><span class="p">,</span><span class="w"> </span><span class="mf">181</span>
<span class="mf">11</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">2</span><span class="p">,</span><span class="w">   </span><span class="mf">10</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">20</span><span class="p">,</span><span class="w"> </span><span class="mf">243</span>
<span class="mf">12</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">2</span><span class="p">,</span><span class="w">   </span><span class="mf">10</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">20</span><span class="p">,</span><span class="w"> </span><span class="mf">243</span>
<span class="mf">13</span><span class="p">,</span><span class="w">     </span><span class="mf">1</span><span class="p">,</span><span class="w">  </span><span class="mf">2</span><span class="p">,</span><span class="w">   </span><span class="mf">10</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">20</span><span class="p">,</span><span class="w"> </span><span class="mf">243</span>
<span class="mf">13</span><span class="p">,</span><span class="w">     </span><span class="mf">8</span><span class="p">,</span><span class="w">  </span><span class="mf">0</span><span class="p">,</span><span class="w">  </span><span class="o">-</span><span class="mf">40</span><span class="p">,</span><span class="w">   </span><span class="mf">0</span><span class="p">,</span><span class="w">  </span><span class="mf">80</span>
</code></pre></div>
<p>which describes that in frame 10-11, an event of class <em>male speech</em> (<em>class 1</em>) belonging to one actor (<em>source 1</em>) is active at location (-50°,30°,181cm). However, at frame 11 a second instance of the same class appears simultaneously at a different direction (10°,-20°,243cm) belonging to another actor (<em>source 2</em>), while at frame 13 an additional event of class <em>music</em> (<em>class 8</em>) appears belonging to a non-actor source (<em>source 0</em>). Frames that contain no sound events are not included in the sequence.</p>
<h2 id="download">Download</h2>
<p>The development and evaluation version of the dataset can be downloaded at:</p>
<div class="row">
<div class="col-md-11">
<a href="https://zenodo.org/records/7880637" target="_blank">
<span style="font-size:20px;">STARSS23, Development and Evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(16.3 GB)</span>
<br/>
<a href="10.5281/zenodo.7880637">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7880637.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="task-setup">Task setup</h1>
<p>The development dataset is provided with a training/testing split. During the development stage, the testing split can be used for comparison with the baseline results and consistent reporting of results at the technical reports of the submitted systems, before the evaluation stage.</p>
<ul>
<li>
<p><strong>Note that even though there are two origins of the data, Sony and TAU, the challenge task considers the dataset as a single entity. Hence models should not be trained separately for each of the two origins, and tested individually on recordings of each of them. Instead, the recordings of the individual training splits (<em>dev-train-sony</em>, <em>dev-train-tau</em>) and testing splits (<em>dev-test-sony</em>, <em>dev-test-tau</em>) should be combined (<em>dev-train</em>, <em>dev-test</em>) and the models should be trained and evaluated in the respective combined splits.</strong></p>
</li>
<li>
<p><strong>The participants can choose to use as input to their models one of the two formats, FOA or MIC, or both simultaneously.</strong></p>
</li>
</ul>
<p>The evaluation dataset is released a few weeks before the final submission deadline. That dataset consists of only audio and video recordings without any metadata/labels. At the evaluation stage, participants can decide the training procedure, i.e. the amount of training and validation files to be used in the development dataset, the number of ensemble models etc., and submit the results of the SELD performance on the evaluation dataset.</p>
<p>There are two tracks that the participants can follow: the audio-only track and the audiovisual track. <strong>The participants can choose to submit systems on either of the two tracks, or on both.</strong> We encourage participants to submit both audio-only models and audiovisual models.</p>
<p>Submissions for both tracks will be in the same system output format, and both tracks will be evaluated with the same metrics. <strong>During evaluation, ranking results will be presented in separate tables for each track.</strong></p>
<h2 id="track-a-audio-only-inference">Track A: Audio-only inference</h2>
<p>The audio-only track continues the SELD task setup of the previous year, where inference of the SELD labels is performed with multichannel audio input only. <strong>Note that we do not exclude the use of video data or video information during training of such models.</strong> In this sense, the STARSS23 video recordings of the development set could be treated as external data and exploited in various ways to improve the performance of the model. However, <strong>during inference only the audio recordings of the evaluation set should be used.</strong></p>
<h2 id="track-b-audiovisual-inference">Track B: Audiovisual inference</h2>
<p>In the audiovisual track participants have access to 360° video recordings during training and evaluation. <strong>The models in this track are expected to be using both audio and video data during inference to produce the SELD labels.</strong></p>
<h2 id="development-dataset">Development dataset</h2>
<p>The recordings in the development dataset follow the naming convention:</p>
<div class="highlight"><pre><span></span><code><span class="n">fold</span><span class="p">[</span><span class="n">fold</span><span class="w"> </span><span class="n">number</span><span class="p">]</span><span class="n">_room</span><span class="p">[</span><span class="n">room</span><span class="w"> </span><span class="n">number</span><span class="p">]</span><span class="n">_mix</span><span class="p">[</span><span class="n">recording</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">room</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<p>The fold number at the moment is used only to distinguish between the training and testing split. The room information is provided for the user of the dataset to potentially help understand the performance of their method with respect to different conditions.</p>
<p>For the audiovisual track, video files are provided with the same folder structure and naming convention as the audio files. There are 12 audio recordings with missing videos (fold3_room21_mix001.wav - fold3_room21_mix012.wav), hence the data are not exactly equivalent when working with audio-only or audiovisual input.</p>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>The evaluation dataset consists of recordings without any information on the origin (Sony or TAU) or on the location in the naming convention, as below:</p>
<div class="highlight"><pre><span></span><code><span class="n">mix</span><span class="p">[</span><span class="n">recording</span><span class="w"> </span><span class="n">number</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<h1 id="external-data-resources">External data resources</h1>
<p>Since the development set contains recordings of real scenes, the presence of each class and the density of sound events varies greatly. To enable more effective training of models to detect and localize all target classes, apart from spatial and spectrotemporal augmentation of the development set, we additionally allow use of external datasets as long as they are publicly available. External data examples are sound sample banks, annotated sound event datasets, pre-trained models, room and array impulse response libraries.</p>
<p><strong>The following rules apply on the use of external data:</strong></p>
<ul>
<li>The external datasets or pre-trained models used should be freely and publicly accessible before <strong>15 May 2024</strong>.</li>
<li>Participants should <strong>inform the organizers in advance</strong> about such data sources, so that all competitors know about them and have an equal opportunity to use them. Please <strong>send an email or message in the Slack channel to the task coordinators</strong> if you intend to use a dataset or pre-trained model not on the list; we will update a list of external data in the webpage accordingly.</li>
<li>The participants will have to indicate clearly which external data they have used in their system info and technical report.</li>
<li>Once the evaluation set is published, no further requests will be taken and no further external sources will be added to the list.</li>
</ul>
<table class="datatable table table-hover table-condensed" data-filter-control="false" data-filter-show-clear="false" data-id-field="name" data-pagination="true" data-show-pagination-switch="true" data-sort-name="name" data-sort-order="asc">
<thead>
<tr>
<th data-field="name" data-sortable="true">Dataset name</th>
<th data-field="type" data-filter-control="select" data-sortable="true" data-tag="true">Type</th>
<th data-field="date" data-sortable="true">Added</th>
<th data-field="link" data-value-type="url">Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>TAU-SRIR DB</td>
<td>room impulse responses</td>
<td>04.04.2022</td>
<td>https://zenodo.org/records/6408611</td>
</tr>
<tr>
<td>6DOF_SRIRs</td>
<td>room impulse responses</td>
<td>23.11.2021</td>
<td>https://zenodo.org/records/6382405</td>
</tr>
<tr>
<td>METU SRIRs</td>
<td>room impulse responses</td>
<td>10.04.2019</td>
<td>https://zenodo.org/records/2635758</td>
</tr>
<tr>
<td>MIRACLE</td>
<td>room impulse responses</td>
<td>12.10.2023</td>
<td>https://depositonce.tu-berlin.de/items/fc34d59c-c524-4a4b-86ae-4da9289f20e2</td>
</tr>
<tr>
<td>AudioSet</td>
<td>audio, video</td>
<td>30.03.2017</td>
<td>https://research.google.com/audioset/</td>
</tr>
<tr>
<td>FSD50K</td>
<td>audio</td>
<td>02.10.2020</td>
<td>https://zenodo.org/record/4060432</td>
</tr>
<tr>
<td>ESC-50</td>
<td>audio</td>
<td>13.10.2015</td>
<td>https://github.com/karolpiczak/ESC-50</td>
</tr>
<tr>
<td>Wearable SELD dataset</td>
<td>audio</td>
<td>17.02.2022</td>
<td>https://zenodo.org/record/6030111</td>
</tr>
<tr>
<td>IRMAS</td>
<td>audio</td>
<td>08.09.2014</td>
<td>https://zenodo.org/record/1290750</td>
</tr>
<tr>
<td>Kinetics 400</td>
<td>audio, video</td>
<td>22.05.2017</td>
<td>https://www.deepmind.com/open-source/kinetics</td>
</tr>
<tr>
<td>SSAST</td>
<td>pre-trained model</td>
<td>10.02.2022</td>
<td>https://github.com/YuanGongND/ssast</td>
</tr>
<tr>
<td>TAU-NIGENS Spatial Sound Events 2020</td>
<td>audio</td>
<td>06.04.2020</td>
<td>https://zenodo.org/record/4064792</td>
</tr>
<tr>
<td>TAU-NIGENS Spatial Sound Events 2021</td>
<td>audio</td>
<td>28.02.2021</td>
<td>https://zenodo.org/record/5476980</td>
</tr>
<tr>
<td>PANN</td>
<td>pre-trained model</td>
<td>19.10.2020</td>
<td>https://github.com/qiuqiangkong/audioset_tagging_cnn</td>
</tr>
<tr>
<td>wav2vec2.0</td>
<td>pre-trained model</td>
<td>20.08.2020</td>
<td>https://github.com/facebookresearch/fairseq</td>
</tr>
<tr>
<td>PaSST</td>
<td>pre-trained model</td>
<td>18.09.2022</td>
<td>https://github.com/kkoutini/PaSST</td>
</tr>
<tr>
<td>DTF-AT</td>
<td>pre-trained model</td>
<td>19.12.2023</td>
<td>https://github.com/ta012/DTFAT</td>
</tr>
<tr>
<td>FNAC_AVL</td>
<td>pre-trained model</td>
<td>25.03.2023</td>
<td>https://github.com/OpenNLPLab/FNAC_AVL</td>
</tr>
<tr>
<td>CSS10 Japanese</td>
<td>audio</td>
<td>05.08.2019</td>
<td>https://www.kaggle.com/datasets/bryanpark/japanese-single-speaker-speech-dataset</td>
</tr>
<tr>
<td>JSUT</td>
<td>audio</td>
<td>28.10.2017</td>
<td>https://sites.google.com/site/shinnosuketakamichi/publication/jsut</td>
</tr>
<tr>
<td>VoxCeleb1</td>
<td>audio, video</td>
<td>26.06.2017</td>
<td>https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html</td>
</tr>
<tr>
<td>COCO</td>
<td>video</td>
<td>01.05.2014</td>
<td>https://cocodataset.org/</td>
</tr>
<tr>
<td>360-Indoor</td>
<td>video</td>
<td>03.10.2019</td>
<td>http://aliensunmin.github.io/project/360-dataset/</td>
</tr>
<tr>
<td>TorchVision Models and Pre-trained Weights</td>
<td>pre-trained model</td>
<td>03.04.2017</td>
<td>https://pytorch.org/vision/stable/models.html</td>
</tr>
<tr>
<td>YOLOv7</td>
<td>pre-trained model</td>
<td>07.07.2022</td>
<td>https://github.com/WongKinYiu/yolov7</td>
</tr>
<tr>
<td>YOLOv8</td>
<td>pre-trained model</td>
<td>10.01.2023</td>
<td>https://github.com/ultralytics/ultralytics</td>
</tr>
<tr>
<td>Grounding DINO</td>
<td>pre-trained model</td>
<td>10.03.2023</td>
<td>https://github.com/IDEA-Research/GroundingDINO</td>
</tr>
<tr>
<td>MMDetection</td>
<td>pre-trained model</td>
<td>15.12.2021</td>
<td>https://github.com/open-mmlab/mmdetection</td>
</tr>
<tr>
<td>MMPose</td>
<td>pre-trained model</td>
<td>01.01.2020</td>
<td>https://github.com/open-mmlab/mmpose</td>
</tr>
<tr>
<td>MMFlow</td>
<td>pre-trained model</td>
<td>01.01.2021</td>
<td>https://github.com/open-mmlab/mmflow</td>
</tr>
<tr>
<td>Paddle Detection</td>
<td>pre-trained model</td>
<td>01.01.2019</td>
<td>https://github.com/PaddlePaddle/PaddleDetection</td>
</tr>
<tr>
<td>doors Image Dataset</td>
<td>image</td>
<td>18.02.2022</td>
<td>https://universe.roboflow.com/mohammed-naji/doors-6g8eb/dataset/1</td>
</tr>
<tr>
<td>DoorDetect Dataset</td>
<td>image</td>
<td>27.05.2021</td>
<td>https://github.com/MiguelARD/DoorDetect-Dataset</td>
</tr>
<tr>
<td>CLIP</td>
<td>pre-trained model</td>
<td>05.01.2021</td>
<td>https://github.com/openai/CLIP</td>
</tr>
<tr>
<td>CLAP</td>
<td>pre-trained model</td>
<td>06.03.2022</td>
<td>https://github.com/LAION-AI/CLAP</td>
</tr>
<tr>
<td>Depth Anything</td>
<td>pre-trained model</td>
<td>22.01.2024</td>
<td>https://github.com/LiheYoung/Depth-Anything</td>
</tr>
<tr>
<td>PanoFormer</td>
<td>pre-trained model</td>
<td>04.03.2022</td>
<td>https://github.com/zhijieshen-bjtu/PanoFormer</td>
</tr>
<tr>
<td>SoundQ Youtube 360° video list</td>
<td>video</td>
<td>06.10.2023</td>
<td>https://github.com/aromanusc/SoundQ/blob/main/synth_data_gen/dataset.csv</td>
</tr>
<tr>
<td>FMA</td>
<td>audio</td>
<td>02.12.2016</td>
<td>https://github.com/mdeff/fma</td>
</tr>
</tbody>
</table>
<p><br/></p>
<h2 id="example-external-data-use-with-baseline">Example external data use with baseline</h2>
<p>The baseline is trained with a combination of the recordings in the development set and synthetic recordings, generated through convolution of isolated sound samples with real spatial room impulse responses (SRIRs) captured in various spaces of Tampere University. The scene synthesizer is the same as used in DCASE2022-2023 challenge, modified to export additional distance labels from the TAU-SRIR database of measured RIRs. For more details please refer to the <a href="https://dcase.community/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes#example-external-data-use-with-baseline">respective section of the task description of DCASE2022.</a> For reproducibility, the exact synthetic data that were generated and used for training this year's baseline are made available: </p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.10932241" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.10932241" target="_blank">
<span style="font-size:20px;">DCASE 2024 simulated data for baseline training <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(18.9 GB)</span>
<br/>
<a href="10.5281/zenodo.10932241">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.10932241.svg"/>
</a>
</div>
</div>
<p><br/></p>
<p>This year, we do not share the generator code with the additional distance exporting fiunctionality. Instead we recommend the participants to use <em>SpatialScaper</em>, a recent library that integrates the functionality of the earlier SELD scene generator with support for <a href="https://doi.org/10.5281/zenodo.6408611">TAU-SRIR DB</a>, while at the same time offering support for additional real SRIRs as well as synthesized RIRs. </p>
<div class="btex-item" data-item="Roman2024" data-source="content/data/external_publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Roman2024"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Iran R. Roman, Christopher Ick, Sivan Ding, Adrian S. Roman, Brian McFee, and Juan P. Bello.
<em>Spatial scaper: a library to simulate and augment soundscapes for sound event localization and detection in realistic rooms.</em>
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Seoul, South Korea, April 2024.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexRoman2024c847be2a545b43a0823bf6d3e0594ea1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2401.12238.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseRoman2024c847be2a545b43a0823bf6d3e0594ea1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseRoman2024c847be2a545b43a0823bf6d3e0594ea1" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingRoman2024c847be2a545b43a0823bf6d3e0594ea1" class="panel-collapse collapse" id="collapseRoman2024c847be2a545b43a0823bf6d3e0594ea1" role="tabpanel">
<h4>Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms</h4>
<h5>Abstract</h5>
<p class="text-justify">Sound event localization and detection (SELD) is an important task in machine listening. Major advancements rely on simulated data with sound events in specific rooms and strong spatio-temporal labels. SELD data is simulated by convolving spatialy-localized room impulse responses (RIRs) with sound waveforms to place sound events in a soundscape. However, RIRs require manual collection in specific rooms. We present SpatialScaper, a library for SELD data simulation and augmentation. Compared to existing tools, SpatialScaper emulates virtual rooms via parameters such as size and wall absorption. This allows for parameterized placement (including movement) of foreground and background sound sources. SpatialScaper also includes data augmentation pipelines that can be applied to existing SELD data. As a case study, we use SpatialScaper to add rooms to the DCASE SELD data. Training a model with our data led to progressive performance improves as a direct function of acoustic diversity. These results show that SpatialScaper is valuable to train robust SELD models.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexRoman2024c847be2a545b43a0823bf6d3e0594ea1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2401.12238.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexRoman2024c847be2a545b43a0823bf6d3e0594ea1label" class="modal fade" id="bibtexRoman2024c847be2a545b43a0823bf6d3e0594ea1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexRoman2024c847be2a545b43a0823bf6d3e0594ea1label">Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Roman2024,
    Author = "Roman, Iran R. and Ick, Christopher and Ding, Sivan and Roman, Adrian S. and McFee, Brian and Bello, Juan P.",
    title = "Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms",
    abstract = "Sound event localization and detection (SELD) is an important task in machine listening. Major advancements rely on simulated data with sound events in specific rooms and strong spatio-temporal labels. SELD data is simulated by convolving spatialy-localized room impulse responses (RIRs) with sound waveforms to place sound events in a soundscape. However, RIRs require manual collection in specific rooms. We present SpatialScaper, a library for SELD data simulation and augmentation. Compared to existing tools, SpatialScaper emulates virtual rooms via parameters such as size and wall absorption. This allows for parameterized placement (including movement) of foreground and background sound sources. SpatialScaper also includes data augmentation pipelines that can be applied to existing SELD data. As a case study, we use SpatialScaper to add rooms to the DCASE SELD data. Training a model with our data led to progressive performance improves as a direct function of acoustic diversity. These results show that SpatialScaper is valuable to train robust SELD models.",
    month = "April",
    year = "2024",
    address = "Seoul, South Korea",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>Scripts that generate data using the FSD50K sample subset that was hand picked earlier to conform to the target classes of the challenge, along with exporting labels in the challenge format, are kindly provided by the authors of the library. You can find the library with usage info here:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/iranroman/SpatialScaper" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/iranroman/SpatialScaper" target="_blank">
<span style="font-size:20px;">SpatialScaper repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h1 id="task-rules">Task rules</h1>
<ul>
<li>Use of external data is allowed, as long as they are <strong>publicly available</strong>. Check the section on external data for more instructions.</li>
<li>Manipulation of the provided training-test split in the development dataset is <strong>not allowed</strong> for reporting dev-test results using the development dataset..</li>
<li>Participants are <strong>not allowed</strong> to make subjective judgments of the evaluation data, nor to annotate it. The evaluation dataset cannot be used to train the submitted system.</li>
<li>The development dataset can be augmented e.g. using techniques such as pitch shifting or time stretching, rotations, respatialization or re-reverberation of parts, etc.</li>
</ul>
<h1 id="submission">Submission</h1>
<p>During the evaluation phase of the challenge, the results for each of the recordings in the evaluation dataset should be collected in individual CSV files. Each result file should have the same name as the file name of the respective audio recording, but with the <code>.csv</code> extension, and should contain the same information at each row as the reference labels, excluding the <em>source index</em>:</p>
<div class="highlight"><pre><span></span><code>[frame number (int)],[active class index (int)],[azimuth (int)],[elevation (int)],[distance (int)]
</code></pre></div>
<p>Enumeration of frame and class indices begins at zero. The class indices are as ordered in the class descriptions mentioned above. The evaluation will be performed at a temporal resolution of 100msec. In case the participants use a different frame or hop length for their study, we expect them to use a suitable method to resample the information at the specified resolution before submitting the evaluation results.</p>
<p>In addition to the CSV files, the participants are asked to update the information of their method in the provided file and submit a technical report describing the method. We allow upto 4 system output submissions per participant/team <strong>for each of the two tracks (audio-only, audiovisual)</strong>. For each system, meta-information should be provided in a separate file, containing the task specific information. All files should be packaged into a zip file for submission. The detailed information regarding the challenge information can be found in the submission page.</p>
<p>General information for all DCASE submissions can be found on the <a href="/challenge2024/submission">Submission page</a>.</p>
<h1 id="evaluation">Evaluation</h1>
<p>The evaluation is based on metrics evaluating jointly localization and detection performance. Contrary to the previous challenges, <strong>there are changes to the metrics this year, to take distance estimation into account.</strong> The changes are the following:</p>
<ul>
<li>From the 4 earlier metrics (location-dependent F1 score, location-dependent error rate, direction-of-arrival (DOA) localization error, and localization recall), only the location-dependent F1 score and DOA error (DOAE) are used. This decision was taken to simplify a growing amount of somewhat complementary metrics including the new distance estimation, which was making the ranking of the systems quite complicated to perform in a balanced manner. </li>
<li>A new source distance error is introduced, betwen the reference event distance and any matched predictions of it. Instead of using an absolute distance error, a relative distance error (RDE) is used with respect to the reference distance.</li>
<li>The F1 score is spatially thresholded not only on the angular distance of predictions from the reference events, but also on the distances from the references. Instead of imposing a single 3D Cartesian distance threshold from the reference, we impose two separate thresholds: one angular threshold connected to the DOAE, and a distance threhsold connected to the RDE. In this way the metric allows separate penalization of DOA estimation performance and distance estimation performance. We deem that appropriate since each of those estimates can target different applications.  </li>
<li>Instead of segment-based evaluation of the metrics, frame-based evaluation is used in this challenge. This decision was taken to simplify hand-crafted rules arising from associating multiple localization estimates over the duration of a segment with positives or negatives, and the increased such complexity that distance estimation was bringing to the table.</li>
<li>This year's localization error (DOAE) does not penalize with 180° in undetected classes/events. </li>
</ul>
<h2 id="metrics">Metrics</h2>
<p>The metrics are based on true positives (<span class="math">\(TP\)</span>) and false positives (<span class="math">\(FP\)</span>) determined not only by correct or wrong detections, but also based on if a) the angular distance <span class="math">\(\Omega = \angle(DOA_p, DOA_r)\)</span> between the DOA of the prediction and the DOA of the matched reference event (if any) being smaller than an angular threhold <span class="math">\(T_{DOA}\)</span>. b) The relative distance error <span class="math">\(\Delta = |L_p-L_r|/L_r\)</span> between the distance <span class="math">\(L_p\)</span> of the prediction and the distance <span class="math">\(L_r\)</span> of the matched reference (if any) being closer than a relative distance error threshold of <span class="math">\(T_{RD}\)</span>. For the evaluation of this challenge we take the angular threshold to be <span class="math">\(T_{DOA} = 20^\circ\)</span> and the relative distance threshold <span class="math">\(T_{RD} = 1\)</span>. The reason that a relative error is chosen instead of an absolute one is that we would like to penalize less farther distances, at which distance cues at the received signals become increasingly unreliable, than closer distances at which a better estimation performance is expected.  </p>
<p>More specifically, for each class <span class="math">\(c\in[1,...,C]\)</span> and each frame:</p>
<ul>
<li><span class="math">\(P_c\)</span> predicted events of class <span class="math">\(c\)</span> are associated with <span class="math">\(R_c\)</span> reference events of class <span class="math">\(c\)</span></li>
<li>false negatives are counted for misses: <span class="math">\(FN_c = \max(0, R_c-P_c)\)</span></li>
<li>false positives are counted for extraneous predictions: <span class="math">\(FP_{c}=\max(0,P_c-R_c)\)</span></li>
<li><span class="math">\(K_c\)</span> predictions are spatially associated with references based on Hungarian algorithm: <span class="math">\(K_c=\min(P_c,R_c)\)</span>. Those can also be considered as the unthresholded true positives <span class="math">\(TP_c = K_c\)</span>.</li>
<li>the application of the spatial thresholds moves <span class="math">\(L_c\leq K_c\)</span> predictions further than the threholds to false positives: <span class="math">\(FP_{c,(\Omega&gt;T_{DOA})\cup(\Delta&gt;T_{RD})} = L_c\)</span>, and 
<span class="math">\(FP_{c,+} = FP_{c}+FP_{c,(\Omega &gt; T_{DOA})\cup(\Delta&gt; T_{RD})}\)</span></li>
<li>the remaining matched estimates per class are counted as true positives: <span class="math">\(TP_{c,(\Omega\leq T_{DOA})\cap(\Delta\leq T_{RD})} = K_c-FP_{c,(\Omega&gt;  T_{DOA})\cup(\Delta&gt;T_{RD})}\)</span></li>
<li>finally: predictions <span class="math">\(P_c = TP_{c,(\Omega\leq T_{DOA})\cap(\Delta\leq T_{RD})}+ FP_{c,+}\)</span>, but references <span class="math">\(R_c = TP_{c,(\Omega\leq T_{DOA})\cap(\Delta\leq1)}+FP_{c,(\Omega&gt; T_{DOA})\cup(\Delta&gt; T_{RD})}+FN_c\)</span></li>
</ul>
<p>Based on those, we form the location-dependent F-score <span class="math">\(F_{c,LD}\)</span>. We perform macro-averaging of the location-dependent F-score: <span class="math">\(F_{LD}= \sum_c F_{c,LD}/C\)</span>.</p>
<p>We evaluate DOA localization accuracy through a class-dependent DOA error <span class="math">\(DOAE_c\)</span>, computed as the mean angular error of the matched true positives per class, and then macro-averaged:</p>
<ul>
<li><span class="math">\(DOAE_c = \sum_k \Omega_k/ K_c = \sum_k \Omega_k /TP_c\)</span> for each frame or segment with <span class="math">\(K_c&gt;0\)</span>, and with <span class="math">\(\Omega_k\)</span> being the angular error between the <span class="math">\(k\)</span>th matched prediction and reference, </li>
<li>and after averaging across all frames that have any true positives, <span class="math">\(DOAE = \sum_c DOAE_c/C\)</span>.</li>
</ul>
<p>Finally, distance localization accuracy is evaluated through a class-dependent relative distance error <span class="math">\(RDE_c\)</span>, computed as the mean relative distance error of the matched true positives per class, and then macro-averaged:</p>
<ul>
<li><span class="math">\(RDE_c = \sum_k \Delta_k/ K_c = \sum_k \Delta_k /TP_c\)</span> for each frame or segment with <span class="math">\(K_c&gt;0\)</span>, and with <span class="math">\(\Delta_k\)</span> being the relative distance error between the <span class="math">\(k\)</span>th matched prediction and reference, </li>
<li>and after averaging across all frames that have any true positives, <span class="math">\(RDE = \sum_c RDE_c/C\)</span>.</li>
</ul>
<p>Note that the DOA and distance localization errors are <strong>not thresholded</strong> in order to give more varied complementary information to the location-dependent F1-score, presenting localization accuracy outside of the spatial threshold.</p>
<h2 id="ranking">Ranking</h2>
<p>Overall ranking will be based on the cumulative rank of the metrics mentioned above, sorted in ascending order. By cumulative rank we mean the following: if system A was ranked individually for each metric as <span class="math">\(F1:1, DOAE:3, RDE: 1\)</span>, then its cumulative rank is <span class="math">\(1+3+1=5\)</span>. Then if system B has <span class="math">\(F1:2, DOAE:2, RDE:3\)</span> (7), and system C has <span class="math">\(F1:3, DOAE:1, RDE:2\)</span> (6), then the overall rank of the systems is A,C,B. If two systems end up with the same cumulative rank, then they are assumed to have equal place in the challenge, even though they will be listed alphabetically in the ranking tables.</p>
<h1 id="results">Results</h1>
<p>The SELD task received 47 submissions in total from 13 teams across the world. From those, 29 submissions were on the audio-only track, and 18 submissions on the audiovisual track. </p>
<p>Complete results and technical reports can be found in the <a class="btn btn-primary" href="/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation-results">results page</a></p>
<p>Main results for these submissions are as following (the table below includes only the best performing system per submitting team):</p>
<h2 id="track-a-audio-only">Track A: Audio-only</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="scatter" data-id-field="anchor" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="team_rank" data-scatter-y="f_20_1" data-show-chart="false" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="team_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-chartable="true" data-field="team_rank" data-sortable="true" data-value-type="int" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="3">Submission Information</th>
<th class="sep-left-cell" colspan="4">Evaluation dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
            Submission
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
            Corresponding<br/> author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
            Affiliation
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_20_1" data-sortable="true" data-value-type="float1-percentage-interval-muted">
            F-score  <br/>(20°/1)
            </th>
<th class="text-center" data-chartable="true" data-field="doae" data-reversed="true" data-sortable="true" data-value-type="float1-interval-muted">
            DOA <br/>error (°)
            </th>
<th class="text-center" data-chartable="true" data-field="rde" data-reversed="true" data-sortable="true" data-value-type="float2-interval-muted">
            Relative distance <br/>error
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Du_NERCSLIP_task3a_4</td>
<td>Qing Wang</td>
<td>University of Science and Technology of China</td>
<td>54.4 (48.9 - 59.2)</td>
<td>13.6 (12.4 - 15.0)</td>
<td>0.21 (0.18 - 0.23)</td>
</tr>
<tr>
<td>2</td>
<td>Yu_HYUNDAI_task3a_3</td>
<td>Hogeon Yu</td>
<td>Hyundai Motor Company</td>
<td>29.8 (25.1 - 34.2)</td>
<td>19.8 (18.3 - 21.6)</td>
<td>0.28 (0.25 - 0.32)</td>
</tr>
<tr>
<td>3</td>
<td>Yeow_NTU_task3a_2</td>
<td>Jun Wei Yeow</td>
<td>Nanyang Technological University</td>
<td>26.2 (22.0 - 30.5)</td>
<td>25.1 (23.2 - 27.6)</td>
<td>0.26 (0.22 - 0.28)</td>
</tr>
<tr>
<td>4</td>
<td>Guan_CQUPT_task3a_4</td>
<td>Xin Guan</td>
<td>Chongqing University of Posts and Telecommunications</td>
<td>26.7 (22.7 - 31.1)</td>
<td>18.6 (17.4 - 21.8)</td>
<td>0.36 (0.34 - 0.39)</td>
</tr>
<tr>
<td>5</td>
<td>Vo_DU_task3a_1</td>
<td>Quoc Thinh Vo</td>
<td>Drexel University</td>
<td>24.7 (20.8 - 28.4)</td>
<td>19.3 (17.7 - 21.3)</td>
<td>0.34 (0.30 - 0.37)</td>
</tr>
<tr>
<td>6</td>
<td>Berg_LU_task3a_3</td>
<td>Axel Berg</td>
<td>Lund University, Arm</td>
<td>25.5 (21.8 - 29.6)</td>
<td>23.2 (18.2 - 28.8)</td>
<td>0.39 (0.34 - 0.44)</td>
</tr>
<tr>
<td>7</td>
<td>Sun_JLESS_task3a_1</td>
<td>Wenqiang Sun</td>
<td>Northwestern Polytechnical University</td>
<td>28.5 (24.2 - 33.0)</td>
<td>23.8 (21.5 - 25.9)</td>
<td>0.51 (0.49 - 0.53)</td>
</tr>
<tr>
<td>8</td>
<td>Qian_IASP_task3a_1</td>
<td>Yuanhang Qian</td>
<td>Wuhan University</td>
<td>22.8 (18.6 - 26.8)</td>
<td>27.2 (24.6 - 29.8)</td>
<td>0.36 (0.31 - 0.42)</td>
</tr>
<tr class="info" data-hline="true">
<td>9</td>
<td>AO_Baseline_FOA</td>
<td>Parthasaarathy Sudarsanam</td>
<td>Tampere University</td>
<td>18.0 (14.6 - 21.7)</td>
<td>29.6 (24.6 - 33.3)</td>
<td>0.31 (0.28 - 0.36)</td>
</tr>
<tr>
<td>10</td>
<td>Zhang_BUPT_task3a_1</td>
<td>Zhicheng Zhang</td>
<td>Beijing University of Posts and Telecommunications</td>
<td>19.0 (16.1 - 21.8)</td>
<td>29.6 (26.6 - 32.9)</td>
<td>0.40 (0.32 - 0.48)</td>
</tr>
<tr>
<td>11</td>
<td>Chen_ECUST_task3a_1</td>
<td>Ning Chen</td>
<td>East China University of Science and Technology</td>
<td>15.1 (12.2 - 17.9)</td>
<td>28.3 (25.5 - 30.9)</td>
<td>0.48 (0.39 - 0.59)</td>
</tr>
<tr>
<td>12</td>
<td>Li_BIT_task3a_1</td>
<td>Jiahao Li</td>
<td>Beijing Institution of Technology</td>
<td>16.9 (13.4 - 20.5)</td>
<td>33.5 (30.0 - 42.7)</td>
<td>0.51 (0.26 - 1.25)</td>
</tr>
</tbody>
</table>
<h2 id="track-b-audiovisual">Track B: Audiovisual</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="scatter" data-id-field="anchor" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="f_20" data-scatter-y="team_rank" data-show-chart="false" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="team_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-chartable="true" data-field="team_rank" data-sortable="true" data-value-type="int" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="3">Submission Information</th>
<th class="sep-left-cell" colspan="4">Evaluation dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
            Submission
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
            Corresponding<br/> author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
            Affiliation
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="f_20_1" data-sortable="true" data-value-type="float1-percentage-interval-muted">
            F-score  <br/>(20°/1)
            </th>
<th class="text-center" data-chartable="true" data-field="doae" data-reversed="true" data-sortable="true" data-value-type="float1-interval-muted">
            DOA <br/>error (°)
            </th>
<th class="text-center" data-chartable="true" data-field="rde" data-reversed="true" data-sortable="true" data-value-type="float2-interval-muted">
            Relative distance <br/>error
            </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Du_NERCSLIP_task3b_4</td>
<td>Qing Wang</td>
<td>University of Science and Technology of China</td>
<td>55.8 (51.2 - 60.4)</td>
<td>11.4 (10.4 - 12.5)</td>
<td>0.25 (0.22 - 0.29)</td>
</tr>
<tr>
<td>2</td>
<td>Berghi_SURREY_task3b_4</td>
<td>Davide Berghi</td>
<td>University of Surrey</td>
<td>39.2 (33.9 - 44.3)</td>
<td>15.8 (14.2 - 17.4)</td>
<td>0.29 (0.25 - 0.32)</td>
</tr>
<tr>
<td>3</td>
<td>Li_SHU_task3b_2</td>
<td>Yongbo Li</td>
<td>Shanghai University</td>
<td>34.2 (29.9 - 38.4)</td>
<td>21.5 (19.8 - 23.4)</td>
<td>0.28 (0.25 - 0.31)</td>
</tr>
<tr>
<td>4</td>
<td>Guan_CQUPT_task3b_2</td>
<td>Xin Guan</td>
<td>Chongqing University of Posts and Telecommunications</td>
<td>23.2 (19.2 - 27.2)</td>
<td>18.8 (17.3 - 21.5)</td>
<td>0.32 (0.28 - 0.37)</td>
</tr>
<tr>
<td>5</td>
<td>Berg_LU_task3b_3</td>
<td>Axel Berg</td>
<td>Lund University, Arm</td>
<td>25.9 (22.1 - 30.1)</td>
<td>23.2 (18.2 - 28.8)</td>
<td>0.33 (0.28 - 0.38)</td>
</tr>
<tr>
<td>6</td>
<td>Chen_ECUST_task3b_1</td>
<td>Ning Chen</td>
<td>East China University of Science and Technology</td>
<td>16.3 (13.7 - 19.3)</td>
<td>25.1 (22.3 - 26.9)</td>
<td>0.32 (0.27 - 0.39)</td>
</tr>
<tr class="info" data-hline="true">
<td>7</td>
<td>AV_Baseline_MIC</td>
<td>Parthasaarathy Sudarsanam</td>
<td>Tampere University</td>
<td>16.0 (12.1 - 20.0)</td>
<td>35.9 (31.8 - 39.6)</td>
<td>0.30 (0.27 - 0.33)</td>
</tr>
</tbody>
</table>
<h1 id="baseline-system">Baseline system</h1>
<p>The baselines for both Track A (audio-only inference) and Track B (audiovisual inference) have been unified this year into a common codebase with a flag to switch from one to the other. Additionally, distance estimation has been integrated into the baselines. The output representation of the task objectives is still based on the multi-ACCDOA format as in the previous year, but it is extended to a 4-element vector with the first three elements representing the activity-coupled DOA estimate and the 4th element the estimated distance. For more details on this extended ACCODA representation and using it to train an earlier version of the baseline, you can refer to this recent pre-print:</p>
<div class="btex-item" data-item="krause2024sound" data-source="content/data/challenge2024/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="krause2024sound"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Daniel A. Krause, Archontis Politis, and Annamaria Mesaros.
<em>Sound event detection and localization with distance estimation.</em>
<em>arXiv</em>, 2024.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexkrause2024sound5ade77c57ee24a0184db9557642adbcf" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2403.11827.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsekrause2024sound5ade77c57ee24a0184db9557642adbcf" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsekrause2024sound5ade77c57ee24a0184db9557642adbcf" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingkrause2024sound5ade77c57ee24a0184db9557642adbcf" class="panel-collapse collapse" id="collapsekrause2024sound5ade77c57ee24a0184db9557642adbcf" role="tabpanel">
<h4>Sound Event Detection and Localization with Distance Estimation</h4>
<h5>Abstract</h5>
<p class="text-justify">Sound Event Detection and Localization (SELD) is a combined task of identifying sound events and their corresponding direction-of-arrival (DOA). While this task has numerous applications and has been extensively researched in recent years, it fails to provide full information about the sound source position. In this paper, we overcome this problem by extending the task to Sound Event Detection, Localization with Distance Estimation (3D SELD). We study two ways of integrating distance estimation within the SELD core - a multi-task approach, in which the problem is tackled by a separate model output, and a single-task approach obtained by extending the multi-ACCDOA method to include distance information. We investigate both methods for the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023. Moreover, our study involves experiments on the loss function related to the distance estimation part. Our results show that it is possible to perform 3D SELD without any degradation of performance in sound event detection and DOA estimation.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexkrause2024sound5ade77c57ee24a0184db9557642adbcf" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2403.11827.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexkrause2024sound5ade77c57ee24a0184db9557642adbcflabel" class="modal fade" id="bibtexkrause2024sound5ade77c57ee24a0184db9557642adbcf" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexkrause2024sound5ade77c57ee24a0184db9557642adbcflabel">Sound Event Detection and Localization with Distance Estimation</h4>
</div>
<div class="modal-body">
<pre>@article{krause2024sound,
    author = "Krause, Daniel A. and Politis, Archontis and Mesaros, Annamaria",
    title = "Sound Event Detection and Localization with Distance Estimation",
    abstract = "Sound Event Detection and Localization (SELD) is a combined task of identifying sound events and their corresponding direction-of-arrival (DOA). While this task has numerous applications and has been extensively researched in recent years, it fails to provide full information about the sound source position. In this paper, we overcome this problem by extending the task to Sound Event Detection, Localization with Distance Estimation (3D SELD). We study two ways of integrating distance estimation within the SELD core - a multi-task approach, in which the problem is tackled by a separate model output, and a single-task approach obtained by extending the multi-ACCDOA method to include distance information. We investigate both methods for the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023. Moreover, our study involves experiments on the loss function related to the distance estimation part. Our results show that it is possible to perform 3D SELD without any degradation of performance in sound event detection and DOA estimation.",
    journal = "arXiv",
    year = "2024"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>Details on the multi-ACCDOA representation can be found here:</p>
<div class="btex-item" data-item="Shimada2022" data-source="content/data/external_publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Shimada2022"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Kazuki Shimada, Yuichiro Koyama, Shusuke Takahashi, Naoya Takahashi, Emiru Tsunoo, and Yuki Mitsufuji.
<em>Multi-ACCDOA: localizing and detecting overlapping sounds from the same class with auxiliary duplicating permutation invariant training.</em>
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, May 2022.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexShimada20224bb674b6fa9240489d261e422954640a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2110.07124.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseShimada20224bb674b6fa9240489d261e422954640a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseShimada20224bb674b6fa9240489d261e422954640a" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingShimada20224bb674b6fa9240489d261e422954640a" class="panel-collapse collapse" id="collapseShimada20224bb674b6fa9240489d261e422954640a" role="tabpanel">
<h4>Multi-ACCDOA: Localizing and Detecting Overlapping Sounds from the Same Class with Auxiliary Duplicating Permutation Invariant Training</h4>
<h5>Abstract</h5>
<p class="text-justify">Sound event localization and detection (SELD) involves identifying the direction-of-arrival (DOA) and the event class. The SELD methods with a class-wise output format make the model predict activities of all sound event classes and corresponding locations. The class-wise methods can output activity-coupled Cartesian DOA (ACCDOA) vectors, which enable us to solve a SELD task with a single target using a single network. However, there is still a challenge in detecting the same event class from multiple locations. To overcome this problem while maintaining the advantages of the class-wise format, we extended ACCDOA to a multi one and proposed auxiliary duplicating permutation invariant training (ADPIT). The multi- ACCDOA format (a class- and track-wise output format) enables the model to solve the cases with overlaps from the same class. The class-wise ADPIT scheme enables each track of the multi-ACCDOA format to learn with the same target as the single-ACCDOA format. In evaluations with the DCASE 2021 Task 3 dataset, the model trained with the multi-ACCDOA format and with the class-wise ADPIT detects overlapping events from the same class while maintaining its performance in the other cases. Also, the proposed method performed comparably to state-of-the-art SELD methods with fewer parameters.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexShimada20224bb674b6fa9240489d261e422954640a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2110.07124.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexShimada20224bb674b6fa9240489d261e422954640alabel" class="modal fade" id="bibtexShimada20224bb674b6fa9240489d261e422954640a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexShimada20224bb674b6fa9240489d261e422954640alabel">Multi-ACCDOA: Localizing and Detecting Overlapping Sounds from the Same Class with Auxiliary Duplicating Permutation Invariant Training</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Shimada2022,
    Author = "Shimada, Kazuki and Koyama, Yuichiro and Takahashi, Shusuke and Takahashi, Naoya and Tsunoo, Emiru and Mitsufuji, Yuki",
    title = "{Multi-ACCDOA}: Localizing and Detecting Overlapping Sounds from the Same Class with Auxiliary Duplicating Permutation Invariant Training",
    abstract = "Sound event localization and detection (SELD) involves identifying the direction-of-arrival (DOA) and the event class. The SELD methods with a class-wise output format make the model predict activities of all sound event classes and corresponding locations. The class-wise methods can output activity-coupled Cartesian DOA (ACCDOA) vectors, which enable us to solve a SELD task with a single target using a single network. However, there is still a challenge in detecting the same event class from multiple locations. To overcome this problem while maintaining the advantages of the class-wise format, we extended ACCDOA to a multi one and proposed auxiliary duplicating permutation invariant training (ADPIT). The multi- ACCDOA format (a class- and track-wise output format) enables the model to solve the cases with overlaps from the same class. The class-wise ADPIT scheme enables each track of the multi-ACCDOA format to learn with the same target as the single-ACCDOA format. In evaluations with the DCASE 2021 Task 3 dataset, the model trained with the multi-ACCDOA format and with the class-wise ADPIT detects overlapping events from the same class while maintaining its performance in the other cases. Also, the proposed method performed comparably to state-of-the-art SELD methods with fewer parameters.",
    month = "May",
    year = "2022",
    address = "Singapore, Singapore",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="track-a-audio-only-baseline">Track A: Audio-only baseline</h2>
<p>The baseline for the audio-only inference track remains similar to the DCASE2023 baseline, with the exception of the extended multi-ACCDOA representation. The input features remain the same: namely, mel-band spectra for the FOA and MIC formats, with mel-band aggregated acoustic intensity vectors for FOA and generalized cross-correlation (GCC) sequences for MIC as spatial features. Additionally, for the MIC format there is the option of the SALSA-lite spatial features, without mel-band aggregation in this case.</p>
<p>Details on the SALSA-lite spatial features can be found in:</p>
<div class="btex-item" data-item="Nguyen2022" data-source="content/data/external_publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Nguyen2022"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Thi Ngoc Tho Nguyen, Douglas L. Jones, Karn N. Watcharasupat, Huy Phan, and Woon-Seng Gan.
<em>SALSA-Lite: A fast and effective feature for polyphonic sound event localization and detection with microphone arrays.</em>
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore, Singapore, May 2022.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2111.08192.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" class="panel-collapse collapse" id="collapseNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" role="tabpanel">
<h4>SALSA-Lite: A fast and effective feature for polyphonic sound event localization and detection with microphone arrays</h4>
<h5>Abstract</h5>
<p class="text-justify">Polyphonic sound event localization and detection (SELD) has many practical applications in acoustic sensing and monitoring. However, the development of real-time SELD has been limited by the demanding computational requirement of most recent SELD systems. In this work, we introduce SALSA-Lite, a fast and effective feature for polyphonic SELD using microphone array inputs. SALSA-Lite is a lightweight variation of a previously proposed SALSA feature for polyphonic SELD. SALSA, which stands for Spatial Cue-Augmented Log-Spectrogram, consists of multichannel log-spectrograms stacked channelwise with the normalized principal eigenvectors of the spectrotemporally corresponding spatial covariance matrices. In contrast to SALSA, which uses eigenvector-based spatial features, SALSA-Lite uses normalized inter-channel phase differences as spatial features, allowing a 30-fold speedup compared to the original SALSA feature. Experimental results on the TAU-NIGENS Spatial Sound Events 2021 dataset showed that the SALSA-Lite feature achieved competitive performance compared to the full SALSA feature, and significantly outperformed the traditional feature set of multichannel log-mel spectrograms with generalized cross-correlation spectra. Specifically, using SALSA-Lite features increased localization-dependent F1 score and class-dependent localization recall by 15% and 5%, respectively, compared to using multichannel log-mel spectrograms with generalized cross-correlation spectra.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2111.08192.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aalabel" class="modal fade" id="bibtexNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aa" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexNguyen2022d2b9e5162b104515b85bf6d5e0f0e5aalabel">SALSA-Lite: A fast and effective feature for polyphonic sound event localization and detection with microphone arrays</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Nguyen2022,
    Author = "Nguyen, Thi Ngoc Tho and Jones, Douglas L. and Watcharasupat, Karn N. and Phan, Huy and Gan, Woon-Seng",
    title = "{SALSA-Lite: A fast and effective feature for polyphonic sound event localization and detection with microphone arrays}",
    abstract = "Polyphonic sound event localization and detection (SELD) has many practical applications in acoustic sensing and monitoring. However, the development of real-time SELD has been limited by the demanding computational requirement of most recent SELD systems. In this work, we introduce SALSA-Lite, a fast and effective feature for polyphonic SELD using microphone array inputs. SALSA-Lite is a lightweight variation of a previously proposed SALSA feature for polyphonic SELD. SALSA, which stands for Spatial Cue-Augmented Log-Spectrogram, consists of multichannel log-spectrograms stacked channelwise with the normalized principal eigenvectors of the spectrotemporally corresponding spatial covariance matrices. In contrast to SALSA, which uses eigenvector-based spatial features, SALSA-Lite uses normalized inter-channel phase differences as spatial features, allowing a 30-fold speedup compared to the original SALSA feature. Experimental results on the TAU-NIGENS Spatial Sound Events 2021 dataset showed that the SALSA-Lite feature achieved competitive performance compared to the full SALSA feature, and significantly outperformed the traditional feature set of multichannel log-mel spectrograms with generalized cross-correlation spectra. Specifically, using SALSA-Lite features increased localization-dependent F1 score and class-dependent localization recall by 15\% and 5\%, respectively, compared to using multichannel log-mel spectrograms with generalized cross-correlation spectra.",
    month = "May",
    year = "2022",
    address = "Singapore, Singapore",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="track-b-audiovisual-baseline">Track B: Audiovisual baseline</h2>
<p>While the audio-only baseline system takes only the audio input, the audiovisual baseline system takes both the audio and a visual input. In the previous year's audiovisual baseline an explicit object detector provided bounding boxes for detected objects which were further converted into gaussian spatial vectors and encoded into features before concatenation with the extracted audio features. In this year's AV baseline a pre-trained ResNet-50 network is used to extract video embeddings, inspired by the work of:</p>
<div class="btex-item" data-item="Berghi2024" data-source="content/data/external_publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Berghi2024"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Davide Berghi, Peipei Wu, Jinzheng Zhao, Wenwu Wang, and Philip J.B. Jackson.
<em>Fusion of audio and visual embeddings for sound event localization and detection.</em>
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Seoul, South Korea, April 2024.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexBerghi2024a582ee635acc49dba5e329176e5347f0" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2312.09034.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseBerghi2024a582ee635acc49dba5e329176e5347f0" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseBerghi2024a582ee635acc49dba5e329176e5347f0" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingBerghi2024a582ee635acc49dba5e329176e5347f0" class="panel-collapse collapse" id="collapseBerghi2024a582ee635acc49dba5e329176e5347f0" role="tabpanel">
<h4>Fusion of Audio and Visual Embeddings for Sound Event Localization and Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">Sound event localization and detection (SELD) combines two subtasks: sound event detection (SED) and direction of arrival (DOA) estimation. SELD is usually tackled as an audio-only problem, but visual information has been recently included. Few audio-visual (AV)-SELD works have been published and most employ vision via face/object bounding boxes, or human pose keypoints. In contrast, we explore the integration of audio and visual feature embeddings extracted with pre-trained deep networks. For the visual modality, we tested ResNet50 and Inflated 3D ConvNet (I3D). Our comparison of AV fusion methods includes the AV-Conformer and Cross-Modal Attentive Fusion (CMAF) model. Our best models outperform the DCASE 2023 Task3 audio-only and AV baselines by a wide margin on the development set of the STARSS23 dataset, making them competitive amongst state-of-the-art results of the AV challenge, without model ensembling, heavy data augmentation, or prediction post-processing. Such techniques and further pre-training could be applied as next steps to improve performance.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexBerghi2024a582ee635acc49dba5e329176e5347f0" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2312.09034.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexBerghi2024a582ee635acc49dba5e329176e5347f0label" class="modal fade" id="bibtexBerghi2024a582ee635acc49dba5e329176e5347f0" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexBerghi2024a582ee635acc49dba5e329176e5347f0label">Fusion of Audio and Visual Embeddings for Sound Event Localization and Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Berghi2024,
    Author = "Berghi, Davide and Wu, Peipei and Zhao, Jinzheng and Wang, Wenwu and Jackson, Philip J.B.",
    title = "Fusion of Audio and Visual Embeddings for Sound Event Localization and Detection",
    abstract = "Sound event localization and detection (SELD) combines two subtasks: sound event detection (SED) and direction of arrival (DOA) estimation. SELD is usually tackled as an audio-only problem, but visual information has been recently included. Few audio-visual (AV)-SELD works have been published and most employ vision via face/object bounding boxes, or human pose keypoints. In contrast, we explore the integration of audio and visual feature embeddings extracted with pre-trained deep networks. For the visual modality, we tested ResNet50 and Inflated 3D ConvNet (I3D). Our comparison of AV fusion methods includes the AV-Conformer and Cross-Modal Attentive Fusion (CMAF) model. Our best models outperform the DCASE 2023 Task3 audio-only and AV baselines by a wide margin on the development set of the STARSS23 dataset, making them competitive amongst state-of-the-art results of the AV challenge, without model ensembling, heavy data augmentation, or prediction post-processing. Such techniques and further pre-training could be applied as next steps to improve performance.",
    month = "April",
    year = "2024",
    address = "Seoul, South Korea",
    booktitle = "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>These video features are then fused with the extracted audio features using cross-modal attention blocks. For more details about the feature and the network architecture please check the below baseline repository.</p>
<p>Both baselines and a more detailed description of them can be found in:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/partha2409/DCASE2024_seld_baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/partha2409/DCASE2024_seld_baseline" target="_blank">
<span style="font-size:20px;">DCASE 2024 SELD baseline repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<h2 id="results-for-the-development-dataset">Results for the development dataset</h2>
<p>The evaluation metric scores for the test split of the development dataset are given below. The location-dependent F-score is computed within a 20° angular threshold and a 100% relative distance threshold from the reference.</p>
<h3><strong>Track A: Audio-only baseline</strong></h3>
<div class="table-responsive col-md-12">
<table class="table table-striped">
<thead>
<tr>
<th>Dataset</th>
<th>macro F<sub>20°/1</sub> (%)</th>
<th>DOAE (°)</th>
<th>RDE (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ambisonic</td>
<td>13.1 %</td>
<td>36.9°</td>
<td>33 %</td>
</tr>
<tr>
<td>Microphone array</td>
<td>9.9 %</td>
<td>38.1°</td>
<td>30 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p><strong>Note:</strong> The reported baseline system performance is not exactly reproducible due to varying setups. However, you should be able to obtain very similar results.</p>
<h3><strong>Track B: Audiovisual baseline</strong></h3>
<div class="table-responsive col-md-12">
<table class="table table-striped">
<thead>
<tr>
<th>Dataset</th>
<th>macro F<sub>20°/1</sub> (%)</th>
<th>DOAE (°)</th>
<th>RDE (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ambisonic</td>
<td>11.3 %</td>
<td>38.4°</td>
<td>36 %</td>
</tr>
<td>Microphone array</td>
<td>11.8 %</td>
<td>38.5°</td>
<td>29 %</td>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p><strong>Note:</strong> The reported baseline system performance is not exactly reproducible due to varying setups. However, you should be able to obtain very similar results.</p>
<h1 id="citation">Citation</h1>
<p>If you are participating in this task or using the dataset and code please consider citing the following papers:</p>
<div class="btex-item" data-item="Politis2022starss22" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Politis2022starss22"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Archontis Politis, Kazuki Shimada, Parthasaarathy Sudarsanam, Sharath Adavanne, Daniel Krause, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, Yuki Mitsufuji, and Tuomas Virtanen.
<em>STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events.</em>
In Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022), 125–129. Nancy, France, November 2022.
URL: <a href="https://dcase.community/workshop2022/proceedings">https://dcase.community/workshop2022/proceedings</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexPolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2022/proceedings/DCASE2022Workshop_Politis_51.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapsePolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsePolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingPolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" class="panel-collapse collapse" id="collapsePolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" role="tabpanel">
<h4>STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events</h4>
<h5>Abstract</h5>
<p class="text-justify">This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARSS22) dataset of spatial recordings of real sound scenes collected in various interiors at two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events belonging to 13 target classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. STARSS22 serves as the development and evaluation dataset for Task 3 (Sound Event Localization and Detection) of the DCASE2022 Challenge and it introduces significant new challenges with regard to the previous iterations, which were based on synthetic data. Additionally, the report introduces the baseline system that accompanies the dataset with emphasis on its differences to the baseline of the previous challenge. Baseline results indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in https://zenodo.org/record/6600531.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexPolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2022/proceedings/DCASE2022Workshop_Politis_51.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" href="https://github.com/sharathadavanne/seld-dcase2022" title=""><i class="fa fa-file-code-o"></i> </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexPolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbblabel" class="modal fade" id="bibtexPolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbb" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexPolitis2022starss22e24dc228b5ec42cc8f103b7dae575fbblabel">STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Politis2022starss22,
    author = "Politis, Archontis and Shimada, Kazuki and Sudarsanam, Parthasaarathy and Adavanne, Sharath and Krause, Daniel and Koyama, Yuichiro and Takahashi, Naoya and Takahashi, Shusuke and Mitsufuji, Yuki and Virtanen, Tuomas",
    title = "{STARSS22}: {A} dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events",
    booktitle = "Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)",
    address = "Nancy, France",
    month = "November",
    year = "2022",
    pages = "125--129",
    abstract = "This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARSS22) dataset of spatial recordings of real sound scenes collected in various interiors at two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events belonging to 13 target classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. STARSS22 serves as the development and evaluation dataset for Task 3 (Sound Event Localization and Detection) of the DCASE2022 Challenge and it introduces significant new challenges with regard to the previous iterations, which were based on synthetic data. Additionally, the report introduces the baseline system that accompanies the dataset with emphasis on its differences to the baseline of the previous challenge. Baseline results indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in https://zenodo.org/record/6600531.",
    url = "https://dcase.community/workshop2022/proceedings"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<div class="btex-item" data-item="Shimada2023starss23" data-source="content/data/challenge2023/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Shimada2023starss23"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Kazuki Shimada, Archontis Politis, Parthasaarathy Sudarsanam, Daniel A. Krause, Kengo Uchida, Sharath Adavanne, Aapo Hakala, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, Tuomas Virtanen, and Yuki Mitsufuji.
<em>STARSS23: an audio-visual dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events.</em>
In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, 72931–72957. Curran Associates, Inc., 2023.
URL: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets_and_Benchmarks.html">https://proceedings.neurips.cc/paper_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets_and_Benchmarks.html</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexShimada2023starss233c888ff4314c40d1909b3e315dc823f1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e6c9671ed3b3106b71cafda3ba225c1a-Paper-Datasets_and_Benchmarks.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseShimada2023starss233c888ff4314c40d1909b3e315dc823f1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseShimada2023starss233c888ff4314c40d1909b3e315dc823f1" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingShimada2023starss233c888ff4314c40d1909b3e315dc823f1" class="panel-collapse collapse" id="collapseShimada2023starss233c888ff4314c40d1909b3e315dc823f1" role="tabpanel">
<h4>STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events</h4>
<h5>Abstract</h5>
<p class="text-justify">While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexShimada2023starss233c888ff4314c40d1909b3e315dc823f1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e6c9671ed3b3106b71cafda3ba225c1a-Paper-Datasets_and_Benchmarks.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" href="https://github.com/sony/audio-visual-seld-dcase2023" title=""><i class="fa fa-file-code-o"></i> </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexShimada2023starss233c888ff4314c40d1909b3e315dc823f1label" class="modal fade" id="bibtexShimada2023starss233c888ff4314c40d1909b3e315dc823f1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexShimada2023starss233c888ff4314c40d1909b3e315dc823f1label">STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Shimada2023starss23,
    author = "Shimada, Kazuki and Politis, Archontis and Sudarsanam, Parthasaarathy and Krause, Daniel A. and Uchida, Kengo and Adavanne, Sharath and Hakala, Aapo and Koyama, Yuichiro and Takahashi, Naoya and Takahashi, Shusuke and Virtanen, Tuomas and Mitsufuji, Yuki",
    editor = "Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.",
    booktitle = "Advances in Neural Information Processing Systems",
    pages = "72931--72957",
    publisher = "Curran Associates, Inc.",
    title = "{STARSS23}: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events",
    abstract = "While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.",
    url = "https://proceedings.neurips.cc/paper\_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets\_and\_Benchmarks.html",
    volume = "36",
    year = "2023"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>