<!DOCTYPE html><html lang="en">
<head>
    <title>Acoustic scene classification - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2020/task-acoustic-scene-classification">
        <meta name="author" content="DCASE" />
        <meta name="description" content="The goal of acoustic scene classification is to classify a test recording into one of the provided predefined classes that characterizes the environment in which it was recorded. Challenge has ended. Full results for this task can be found in subtask specific result pages: Task1A Task1B This task comprises two …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2020</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2020/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2020/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2020/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-urban text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2020/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2020/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2020/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2020/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2020/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge awards">
        <a href="/challenge2020/awards"><i class="fa fa-trophy"></i>&nbsp;Awards</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metal-11.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-primary"></i><i class="fa dc-scene fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Scenes</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2020_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Acoustic scene classification</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Annamaria Mesaros" class="img img-circle" src="/images/person/annamaria_mesaros.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Annamaria Mesaros</strong>
<a class="icon" href="mailto:annamaria.mesaros@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Toni Heittola" class="img img-circle" src="/images/person/toni_heittola.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Toni Heittola</strong>
<a class="icon" href="mailto:toni.heittola@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Tuomas Virtanen" class="img img-circle" src="/images/person/tuomas_virtanen.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Tuomas Virtanen</strong>
<a class="icon" href="mailto:tuomas.virtanen@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#subtask-a">Subtask A</a>
<ul>
<li><a href="#audio-dataset">Audio dataset</a></li>
<li><a href="#task-setup">Task setup</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#subtask-b">Subtask B</a>
<ul>
<li><a href="#audio-dataset-1">Audio dataset</a></li>
<li><a href="#task-setup-1">Task setup</a></li>
<li><a href="#download-1">Download</a></li>
</ul>
</li>
<li><a href="#external-data-resources">External data resources</a></li>
<li><a href="#submission">Submission</a>
<ul>
<li><a href="#subtask-a-1">Subtask A</a></li>
<li><a href="#subtask-b-1">Subtask B</a></li>
<li><a href="#package-validator">Package validator</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#subtask-a-2">Subtask A</a></li>
<li><a href="#subtask-b-2">Subtask B</a></li>
<li><a href="#submissions">Submissions</a></li>
</ul>
</li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#repository">Repository</a></li>
<li><a href="#subtask-a-3">Subtask A</a></li>
<li><a href="#subtask-b-3">Subtask B</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="lead">The goal of acoustic scene classification is to classify a test recording into one of the provided predefined classes that characterizes the environment in which it was recorded.</p>
<p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found in subtask specific result pages:
    <a class="btn btn-default btn-xs" href="/challenge2020/task-acoustic-scene-classification-results-a">Task1A <i class="fa fa-caret-right"></i></a>
<a class="btn btn-default btn-xs" href="/challenge2020/task-acoustic-scene-classification-results-b">Task1B <i class="fa fa-caret-right"></i></a>
</p>
<p>This task comprises two different subtasks that involve system development for two different situations:</p>
<div class="row">
<div class="col-md-2 text-center">
<span class="fa-stack fa-3x">
<i class="fa fa-square fa-stack-2x text-primary"></i>
<strong class="fa-stack-1x icon-text">A</strong>
<strong class="fa-stack-1x dcase-icon-top-text">Devices</strong>
<span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span>
</span>
</div>
<div class="col-md-10 col-xs-middle">
<h3>Acoustic Scene Classification with Multiple Devices <br/><small>Subtask A</small></h3>
<p>Classification of data from multiple devices (real and simulated) targeting generalization properties of systems across a number of different devices.</p>
</div>
</div>
<div class="row">
<div class="col-md-2 text-center">
<span class="fa-stack fa-3x">
<i class="fa fa-square fa-stack-2x text-info"></i>
<strong class="fa-stack-1x icon-text">B</strong>
<strong class="fa-stack-1x dcase-icon-top-text">Complexity</strong>
<span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span>
</span>
</div>
<div class="col-md-10 col-xs-middle">
<h3>Low-Complexity Acoustic Scene Classification <br/><small>Subtask B</small></h3>
<p>Classification of data into three higher level classes while focusing on low-complexity solutions.</p>
</div>
</div>
<h1 id="subtask-a">Subtask A</h1>
<div class="row">
<div class="col-md-2 text-center">
<span class="fa-stack fa-3x">
<i class="fa fa-square fa-stack-2x text-primary"></i>
<strong class="fa-stack-1x icon-text">A</strong>
<strong class="fa-stack-1x dcase-icon-top-text">Devices</strong>
<span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span>
</span>
</div>
<div class="col-md-10 col-xs-middle">
<h3>Acoustic Scene Classification with Multiple Devices</h3>
</div>
</div>
<p>This subtask is concerned with the basic problem of acoustic scene classification, in which it is required to classify a test audio recording into one of ten known acoustic scene classes. This task targets <strong>generalization</strong> properties of systems across a number of different devices, and will use audio data recorded and simulated with a variety of devices.</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2020/task1_acoustic_scene_classification.png"/>
<figcaption>Figure 1: Overview of acoustic scene classification system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<h2 id="audio-dataset">Audio dataset</h2>
<p>The dataset for this task is <strong>TAU Urban Acoustic Scenes 2020 Mobile</strong>. The dataset contains recordings from 12 European cities in 10 different acoustic scenes using 4 different devices. Additionally, synthetic data for 11 mobile devices was created based on the original recordings. Of the 12 cities, two are present only in the evaluation set.</p>
<p>Recordings were made using four devices that captured audio simultaneously. The main recording device consists in a Soundman OKM II Klassik/studio A3, electret binaural microphone and a Zoom F8 audio recorder using 48kHz sampling rate and 24-bit resolution, referred to as device A. The other devices are commonly available customer devices: device B is a Samsung Galaxy S7, device C is iPhone SE, and device D is a GoPro Hero5 Session. </p>
<p>Acoustic scenes (10):</p>
<ul>
<li>Airport - <code>airport</code></li>
<li>Indoor shopping mall - <code>shopping_mall</code></li>
<li>Metro station - <code>metro_station</code></li>
<li>Pedestrian street - <code>street_pedestrian</code></li>
<li>Public square - <code>public_square</code></li>
<li>Street with medium level of traffic - <code>street_traffic</code></li>
<li>Travelling by a tram - <code>tram</code></li>
<li>Travelling by a bus - <code>bus</code></li>
<li>Travelling by an underground metro - <code>metro</code></li>
<li>Urban park - <code>park</code></li>
</ul>
<p>Audio data was recorded in Amsterdam, Barcelona, Helsinki, Lisbon, London, Lyon, Madrid, Milan, Prague, Paris, Stockholm and Vienna.</p>
<p>The dataset was collected by Tampere University of Technology between 05/2018 - 11/2018. The data collection received funding from the European Research Council, grant agreement 637422 EVERYSOUND.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="../images/sponsors/erc.jpg" title="ERC"/></a></p>
<p>For complete details on the data recording and processing see </p>
<div class="btex-item" data-item="Mesaros2018_DCASE" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2018_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>A multi-device dataset for urban acoustic scene classification.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018), 9–13. November 2018.
URL: <a href="https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf">https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" class="panel-collapse collapse" id="collapseMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" role="tabpanel">
<h4>A multi-device dataset for urban acoustic scene classification</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic scene classification, DCASE challenge, public datasets, multi-device data</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80flabel" class="modal fade" id="bibtexMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80f" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2018_DCASE23963155bcdc4de4acc0aad04301f80flabel">A multi-device dataset for urban acoustic scene classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2018_DCASE,
    Author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "A multi-device dataset for urban acoustic scene classification",
    year = "2018",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    month = "November",
    pages = "9--13",
    keywords = "Acoustic scene classification, DCASE challenge, public datasets, multi-device data",
    abstract = "This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.",
    url = "https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop\_Mesaros\_8.pdf"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>Additionally, 10 mobile devices S1-S10 are simulated using the audio recorded with device A, impulse responses recorded with real devices, and additional dynamic range compression, in order to simulate realistic recordings. A recording from device A is processed through convolution with the selected Si impulse response, then processed with a selected set of parameters for dynamic range compression (device-specific). The impulse responses are proprietary data and will not be published.</p>
<p>The development dataset comprises 40 hours of data from device A, and smaller amounts from the other devices. Audio is provided in a single-channel 44.1kHz 24-bit format.  </p>
<h2 id="task-setup">Task setup</h2>
<h3>Development dataset</h3>
<p>The development set contains data from 10 cities and 9 devices: 3 real devices (A, B, C) and 6 simulated devices (S1-S6). Data from devices B, C, and S1-S6 consists of randomly selected segments from the simultaneous recordings, therefore all overlap with the data from device A, but not necessarily with each other. The total amount of audio in the development set is <strong>64 hours</strong>.</p>
<p>The dataset is provided with a training/test split in which 70% of the data for each device is included for training, 30% for testing. Some devices appear only in the test subset. In order to create a perfectly balanced test set, a number of segments from various devices are not included in this split. Complete details on the development set and training/test split are provided in the following table.</p>
<table class="table">
<thead>
<tr class="active">
<td class="text-left" colspan="2"><strong>Device</strong></td>
<td class="text-left" colspan="2"><strong>Dataset</strong></td>
<td class="text-left" colspan="3"><strong>Cross-validation setup</strong></td>
</tr>
<tr class="active">
<td class="col-md-2"><strong>Name</strong></td>
<td><strong>Type</strong></td>
<td><strong>Total<br/>duration</strong></td>
<td><strong>Total<br/>segments</strong></td>
<td><strong>Train<br/>segments</strong></td>
<td><strong>Test<br/>segments</strong></td>
<td class="col-md-3"><strong>Notes</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><span class="label label-success">A</span></td>
<td>Real</td>
<td>40h</td>
<td>14400</td>
<td>10215</td>
<td>330</td>
<td>3855 Segments not used in train/test split</td>
</tr>
<tr>
<td><span class="label label-success">B</span> <span class="label label-success">C</span></td>
<td>Real</td>
<td>3h each</td>
<td>1080</td>
<td>750</td>
<td>330</td>
<td></td>
</tr>
<tr class="warning">
<td><span class="label label-warning">S1</span> <span class="label label-warning">S2</span> <span class="label label-warning">S3</span></td>
<td>Simulated</td>
<td>3h each</td>
<td>1080</td>
<td>750</td>
<td>330</td>
<td></td>
</tr>
<tr class="warning">
<td><span class="label label-warning">S4</span> <span class="label label-warning">S5</span> <span class="label label-warning">S6</span></td>
<td>Simulated</td>
<td>3h each</td>
<td>1080</td>
<td>-</td>
<td>330</td>
<td>750 segments not used in train/test split</td>
</tr>
</tbody>
<tfoot>
<tr class="active">
<td><strong>Total</strong></td>
<td></td>
<td><strong>64h</strong></td>
<td><strong>23040</strong></td>
<td><strong>13965</strong></td>
<td><strong>2970</strong></td>
<td></td>
</tr>
</tfoot>
</table>
<p><strong>Participants are required to report the performance of their system using this train/test setup in order to allow a comparison of systems on the development set</strong>. Participants are allowed to create their own cross-validation folds or separate validation set. In this case please pay attention to the segments recorded at the same location. Location identifier can be found from metadata file provided in the dataset or from audio file names:</p>
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="n">scene</span><span class="w"> </span><span class="n">label</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">city</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">location</span><span class="w"> </span><span class="kt">id</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">segment</span><span class="w"> </span><span class="kt">id</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">device</span><span class="w"> </span><span class="kt">id</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<p>Make sure that all the files having the same location id are placed on the same side of the evaluation.</p>
<h3>Evaluation dataset</h3>
<p>The evaluation dataset contains data from 12 cities, 10 acoustic scenes, 11 devices. There are five new devices (not available in the development set): real device D and simulated devices S7-S11. Evaluation data contains 33 hours of audio. The evaluation data contains audio recorded at different locations than the development data.</p>
<p>Device and city information is not provided in the evaluation set. The systems are expected to be robust to different devices.</p>
<h3>Reference labels</h3>
<p>Reference labels are provided only for the development datasets. <strong>Reference labels for evaluation dataset will not be released</strong>. For publications based on the DCASE challenge data, please use the provided training/test setup of the development set, to allow comparisons. After the challenge, if you want to evaluate your proposed system with official challenge evaluation setup, contact the task coordinators. Task coordinators can provide unofficial scoring for a limited amount of system outputs.    </p>
<h2 id="download">Download</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.3819968" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.3819968" target="_blank">
<span style="font-size:20px;">TAU Urban Acoustic Scenes 2020 Mobile, Development dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(30.5 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3819968">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3819968.svg"/>
</a>
<span class="text-muted">
                
                version 2.0
                
                
                </span>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.3685828" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.3685828" target="_blank">
<span style="font-size:20px;">TAU Urban Acoustic Scenes 2020 Mobile, Evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(12.8 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3685828">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3685828.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="subtask-b">Subtask B</h1>
<div class="row">
<div class="col-md-2 text-center">
<span class="fa-stack fa-3x">
<i class="fa fa-square fa-stack-2x text-info"></i>
<strong class="fa-stack-1x icon-text">B</strong>
<strong class="fa-stack-1x dcase-icon-top-text">Complexity</strong>
<span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span>
</span>
</div>
<div class="col-md-10 col-xs-middle">
<h3>Low-Complexity Acoustic Scene Classification <br/><small>Subtask B</small></h3>
<p></p>
</div>
</div>
<p>This subtask is concerned with the classification of audio into three major classes: indoor, outdoor, and transportation. The task targets <strong>low complexity</strong> solutions for the classification problem in terms of model size and uses audio recorded with a single device (device A).</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-8 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2020/task1_acoustic_scene_classification_low_complexity.png"/>
<figcaption>Figure 1: Overview of acoustic scene classification system.</figcaption>
</div>
</div>
</figure>
<p><br/></p>
<h2 id="audio-dataset-1">Audio dataset</h2>
<p>The dataset for this task is <strong>TAU Urban Acoustic Scenes 2020 3Class</strong>. The dataset contains recordings from 12 European cities in 10 different acoustic scenes. </p>
<p>The dataset was collected by Tampere University of Technology between 05/2018 - 11/2018. The data collection received funding from the European Research Council, grant agreement 637422 EVERYSOUND.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="../images/sponsors/erc.jpg" title="ERC"/></a></p>
<p>For complete details on the data recording and processing see </p>
<div class="btex-item" data-item="Mesaros2018_DCASE" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2018_DCASE"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>A multi-device dataset for urban acoustic scene classification.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018), 9–13. November 2018.
URL: <a href="https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf">https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" class="panel-collapse collapse" id="collapseMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" role="tabpanel">
<h4>A multi-device dataset for urban acoustic scene classification</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic scene classification, DCASE challenge, public datasets, multi-device data</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2018_DCASE59476cff349d47eeb35fab007cbb932dlabel" class="modal fade" id="bibtexMesaros2018_DCASE59476cff349d47eeb35fab007cbb932d" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2018_DCASE59476cff349d47eeb35fab007cbb932dlabel">A multi-device dataset for urban acoustic scene classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2018_DCASE,
    Author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "A multi-device dataset for urban acoustic scene classification",
    year = "2018",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    month = "November",
    pages = "9--13",
    keywords = "Acoustic scene classification, DCASE challenge, public datasets, multi-device data",
    abstract = "This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.",
    url = "https://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop\_Mesaros\_8.pdf"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>The 10 acoustic scenes are grouped into <strong>three major classes</strong> as follows:</p>
<ul>
<li>Indoor scenes - <code>indoor</code>: airport, indoor shopping mall, and metro station</li>
<li>Outdoor scenes - <code>outdoor</code>: pedestrian street, public square, a street with a medium level of traffic, and urban park</li>
<li>Transportation related scenes - <code>transportation</code>: traveling by bus, traveling by tram, traveling by underground metro</li>
</ul>
<p>This dataset contains data recorded with a single device (device A). 
Audio is provided in binaural, 48kHz 24-bit format.</p>
<h2 id="task-setup-1">Task setup</h2>
<h3>Development dataset</h3>
<p>The development set contains data from 10 cities. The total amount of audio in the development set is 40 hours.</p>
<p>The dataset is provided with a training/test split. <strong>Participants are required to report the performance of their system using this train/test setup in order to allow comparison of systems on the development set.</strong></p>
<p>Participants are allowed to create their own cross-validation folds or separate validation set. In this case please pay attention to the segments recorded at the same location. Location identifier can be found from metadata file provided in the dataset or from audio file names:</p>
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="n">scene</span><span class="w"> </span><span class="n">label</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">city</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">location</span><span class="w"> </span><span class="kt">id</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">segment</span><span class="w"> </span><span class="kt">id</span><span class="p">]</span><span class="o">-</span><span class="p">[</span><span class="n">device</span><span class="w"> </span><span class="kt">id</span><span class="p">].</span><span class="n">wav</span>
</code></pre></div>
<p>Make sure that all files having the same location id are placed on the same side of the evaluation. In this case, the device id is always A. </p>
<h3>Evaluation dataset</h3>
<p>The evaluation set contains data from 12 cities (2 cities unseen in the development set). Evaluation data contains 30 hours of audio.  </p>
<h3>System complexity requirements</h3>
<p><a name="complexity-requirements"></a></p>
<p>Classifier complexity for this setup is limited to <strong>500KB</strong> size for the <strong>non-zero parameters</strong>. This translates into 128K parameters when using <code>float32</code> (32-bit float) which is often the default data type (<em>128000 parameter values * 32 bits per parameter / 8 bits per byte= 512000 bytes = 500KB</em>).</p>
<p>By limiting the size of the model on disk, we allow participants some flexibility in design, for example, some implementations would prefer to minimize the number of non-zero parameters of the network in order to comply with this size limit, while other implementations may target representation of the model parameters with a low number of bits. There is no requirement nor recommendation on which method to minimize the model size is sought after.</p>
<p>In order to apply the limit strictly on the <strong>classifier size</strong>, the parameter count will exclude the first active layer of the network if this layer is a <strong>feature extraction layer</strong> (Kapre layer in Keras or tf.signal.* layers in Tensorflow). If the feature extraction is done separately, all layers/parameters of the neural network are counted. Layers not used in the classification stage, such as batch normalization layers, are also skipped from the model size calculation. If the system uses embeddings (e.g VGGish, OpenL3, or EdgeL3), the network used to generate the embeddings counts in the number of parameters.</p>
<p>The computational complexity of the feature extraction stage is not included in the system complexity estimation within this task. We acknowledge that feature extraction is an integral part of the system complexity, but since there is no established method for estimating and comparing the complexity of different feature extraction implementations, we estimate the complexity through the size of the classifier models, in order to keep the complexity estimation straightforward across different approaches. </p>
<p>Full information about the model size should be provided in the technical report.</p>
<h4>Model size calculation</h4>
<p>We offer a script for calculating the model size for <a href="https://github.com/toni-heittola/dcase2020_task1_baseline/blob/master/model_size_calculation.py">Keras based models along with the baseline system</a>. If you have any doubts about how to calculate the model size, please contact <em>toni.heittola@tuni.fi</em> or write to the <a href="https://groups.google.com/forum/#!forum/dcase-discussions">DCASE forum</a> for visibility.</p>
<p><strong>Calculation examples</strong></p>
<div aria-multiselectable="true" class="panel-group" id="model-size-examples-accordion" role="tablist">
<div class="panel panel-default">
<div class="panel-heading" id="dcase2020-subtask-a-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed link" data-parent="#model-size-examples-accordion" data-toggle="collapse" href="#dcase2020-subtask-a-example-collapse" role="button">
                    System 1: DCASE2020 Task 1 Baseline, Subtask A
                    <span class="pull-right"><strong>19.12 MB</strong></span>
</a>
</h4>
</div>
<div aria-labelledby="dcase2020-subtask-a-example-header" class="panel-collapse collapse" id="dcase2020-subtask-a-example-collapse" role="tabpanel">
<div class="panel-body">
<p>
Total model size:
17.87 MB (Audio embeddings) + 1.254 MB (Acoustic model) = <strong>19.12 MB</strong>
<h5>Audio embeddings (OpenL3)</h5>
<table class="table table-condensed">
<thead>
<tr>
<th>Layer</th>
<th>Parameters</th>
<th>Non-zero parameters</th>
<th>Size (non-zero)</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>melspectrogram_1</td>
<td>4 460 800</td>
<td>4 196 335</td>
<td>16.01 MB</td>
<td>Skipped</td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_1</td>
<td>4</td>
<td>4</td>
<td>16 bytes</td>
<td>Skipped</td>
</tr>
<tr>
<td>conv2d_1</td>
<td>640</td>
<td>640</td>
<td>2.5 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_2</td>
<td>256</td>
<td>256</td>
<td>1 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_2</td>
<td>36 928</td>
<td>36 928</td>
<td>144.2 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_3</td>
<td>256</td>
<td>256</td>
<td>1 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_2</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>max_pooling2d_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_3</td>
<td>73 856</td>
<td>73 856</td>
<td>288.5 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_4</td>
<td>512</td>
<td>512</td>
<td>2 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_3</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_4</td>
<td>147 584</td>
<td>147 584</td>
<td>576.5 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_5</td>
<td>512</td>
<td>512</td>
<td>2 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_4</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>max_pooling2d_2</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_5</td>
<td>295 168</td>
<td>295 168</td>
<td>1.126 MB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_6</td>
<td>1024</td>
<td>1024</td>
<td>4 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_5</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_6</td>
<td>590 080</td>
<td>590 080</td>
<td>2.251 MB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_7</td>
<td>1024</td>
<td>1024</td>
<td>4 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_6</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>max_pooling2d_3</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_7</td>
<td>1 180 160</td>
<td>1 180 160</td>
<td>4.502 MB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_8</td>
<td>2048</td>
<td>2048</td>
<td>8 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_7</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>audio_embedding_layer</td>
<td>2 359 808</td>
<td>2 359 808</td>
<td>9.002 MB</td>
<td></td>
</tr>
<tr>
<td>max_pooling2d_4</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>flatten_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
</tbody>
<tfoot>
<tr>
<th>Total</th>
<th>4 684 224</th>
<th>4 684 224</th>
<th>17.87 MB</th>
<th></th>
</tr>
</tfoot>
</table>
<h5>Acoustic model</h5>
<table class="table table-condensed">
<thead>
<tr>
<th>Layer</th>
<th>Parameters</th>
<th>Non-zero parameters</th>
<th>Size (non-zero)</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>dense_1</td>
<td>262 656</td>
<td>262 557</td>
<td>1.002 MB</td>
<td></td>
</tr>
<tr>
<td>dense_2</td>
<td>65 664</td>
<td>65 664</td>
<td>256.5 KB</td>
<td></td>
</tr>
<tr>
<td>dense_3</td>
<td>387</td>
<td>387</td>
<td>1.512 KB</td>
<td></td>
</tr>
</tbody>
<tfoot>
<tr>
<th>Total</th>
<th>32 8707</th>
<th>32 8608</th>
<th>1.254 MB</th>
<th></th>
</tr>
</tfoot>
</table>
</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading" id="dcase2020-subtask-b-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed link" data-parent="#model-size-examples-accordion" data-toggle="collapse" href="#dcase2020-subtask-b-example-collapse" role="button">
                    System 2: DCASE2020 Task 1 Baseline, Subtask B
                    <span class="pull-right"><strong>450.1 KB</strong></span>
</a>
</h4>
</div>
<div aria-labelledby="dcase2020-subtask-b-example-header" class="panel-collapse collapse" id="dcase2020-subtask-b-example-collapse" role="tabpanel">
<div class="panel-body">
<p>

Total model size:
0 KB (Audio embeddings) + 450.1 KB (Acoustic model) = <strong>450.1 KB</strong>
<h5>Acoustic model</h5>
<table class="table table-condensed">
<thead>
<tr>
<th>Layer</th>
<th>Parameters</th>
<th>Non-zero parameters</th>
<th>Size (non-zero)</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>conv2d_1</td>
<td>1600</td>
<td>1600</td>
<td>6.25 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_1</td>
<td>128</td>
<td>128</td>
<td>512 bytes</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>max_pooling2d_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>dropout_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>conv2d_2</td>
<td>100 416</td>
<td>100 416</td>
<td>392.2 KB</td>
<td></td>
</tr>
<tr class="active text-muted">
<td>batch_normalization_2</td>
<td>256</td>
<td>256</td>
<td>1 KB</td>
<td>Skipped</td>
</tr>
<tr>
<td>activation_2</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>max_pooling2d_2</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>dropout_2</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>flatten_1</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>dense_1</td>
<td>12 900</td>
<td>12 900</td>
<td>50.39 KB</td>
<td></td>
</tr>
<tr>
<td>dropout_3</td>
<td>0</td>
<td>0</td>
<td>0 KB</td>
<td></td>
</tr>
<tr>
<td>dense_2</td>
<td>303</td>
<td>303</td>
<td>1.184 KB</td>
<td></td>
</tr>
</tbody>
<tfoot>
<tr>
<th>Total</th>
<th>115 219</th>
<th>115 219</th>
<th>450.1 KB</th>
<th></th>
</tr>
</tfoot>
</table>
</p>
</div>
</div>
</div>
</div>
<h3>Reference labels</h3>
<p>Reference labels are provided only for the development datasets. <strong>Reference labels for evaluation dataset will not be released</strong>. For publications based on the DCASE challenge data, please use the provided training/test setup of the development set, to allow comparisons. After the challenge, if you want to evaluate your proposed system with official challenge evaluation setup, contact the task coordinators. Task coordinators can provide unofficial scoring for a limited amount of system outputs.    </p>
<h2 id="download-1">Download</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.3670185" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.3670185" target="_blank">
<span style="font-size:20px;">TAU Urban Acoustic Scenes 2020 3Class, Development dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(41.5 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3670185">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3670185.svg"/>
</a>
</div>
</div>
<p><br/></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://doi.org/10.5281/zenodo.3685835" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://doi.org/10.5281/zenodo.3685835" target="_blank">
<span style="font-size:20px;">TAU Urban Acoustic Scenes 2020 3Class, Evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(19.9 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.3685835">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3685835.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="external-data-resources">External data resources</h1>
<p>Use of external data is allowed in all subtasks under the following conditions:</p>
<ul>
<li>
<p>The used external resource is clearly referenced and freely accessible to any other research group in the world. <strong>External data refers to public datasets, trained models, or impulse responses</strong>. The data must be public and freely available before <strong>1st of April 2020</strong>. </p>
</li>
<li>
<p>Participants submit at least <strong>one system without external training data</strong> so that we can study the contribution of such resources. This condition applies only to cases where external audio datasets are used. In the case of external data being pre-trained models or embeddings, this condition does not apply. The list of external data sources used in training must be clearly indicated in the technical report.</p>
</li>
<li>
<p>Participants <strong>inform the organizers in advance</strong> about such data sources, so that all competitors know about them and have an equal opportunity to use them. Please send an email to the task coordinators; we will update the list of external datasets on the webpage accordingly. Once the evaluation set is published, the list of allowed external data resources is locked (no further external sources allowed).</p>
</li>
<li>
<p><strong>It is not allowed</strong> to use <a href="https://zenodo.org/record/1228142">TUT Urban Acoustic Scenes 2018</a>, <a href="https://zenodo.org/record/2589280">TAU Urban Acoustic Scenes 2019</a> or <a href="https://zenodo.org/record/2589332">TAU Urban Acoustic Scenes 2019 Mobile</a>. These datasets are partially included in the current setup, and additional usage will lead to overfitting.   </p>
</li>
</ul>
<p>List of external data resources allowed:</p>
<table class="datatable table table-hover table-condensed" data-filter-control="false" data-filter-show-clear="false" data-id-field="name" data-pagination="false" data-show-pagination-switch="false" data-sort-name="name" data-sort-order="asc">
<thead>
<tr>
<th data-field="name" data-sortable="true">Dataset name</th>
<th data-field="type" data-filter-control="select" data-sortable="true" data-tag="true">Type</th>
<th data-field="date" data-sortable="true">Added</th>
<th data-field="link" data-value-type="url">Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>LITIS Rouen audio scene dataset</td>
<td>audio</td>
<td>04.03.2019</td>
<td>https://sites.google.com/site/alainrakotomamonjy/home/audio-scene</td>
</tr>
<tr>
<td>DCASE2013 Challenge - Public Dataset for Scene Classification Task</td>
<td>audio</td>
<td>04.03.2019</td>
<td>https://archive.org/details/dcase2013_scene_classification</td>
</tr>
<tr>
<td>DCASE2013 Challenge - Private Dataset for Scene Classification Task</td>
<td>audio</td>
<td>04.03.2019</td>
<td>https://archive.org/details/dcase2013_scene_classification_testset</td>
</tr>
<tr>
<td>AudioSet</td>
<td>audio</td>
<td>04.03.2019</td>
<td>https://research.google.com/audioset/</td>
</tr>
<tr>
<td>OpenL3</td>
<td>model</td>
<td>12.02.2020</td>
<td>https://openl3.readthedocs.io/</td>
</tr>
<tr>
<td>EdgeL3</td>
<td>model</td>
<td>12.02.2020</td>
<td>https://edgel3.readthedocs.io/</td>
</tr>
<tr>
<td>VGGish</td>
<td>model</td>
<td>12.02.2020</td>
<td>https://github.com/tensorflow/models/tree/master/research/audioset/vggish</td>
</tr>
<tr>
<td>SoundNet</td>
<td>model</td>
<td>03.06.2020</td>
<td>http://soundnet.csail.mit.edu/</td>
</tr>
</tbody>
</table>
<p><br/></p>
<h1 id="submission">Submission</h1>
<p>Participants can choose to participate in only one subtask or both.</p>
<p>Official challenge submission consists of:</p>
<ul>
<li>System output file (*.csv)</li>
<li>Metadata file (*.yaml)</li>
<li>Technical report explaining in sufficient detail the method (*.pdf)</li>
</ul>
<p>System output should be presented as a single text-file (in CSV format, with a header row) containing a classification result for each audio file in the evaluation set. In addition, the results file should contain probabilities for each scene class. Result items can be in any order. Multiple system outputs can be submitted (maximum 4 per participant per subtask). </p>
<p>For each system, meta information should be provided in a separate file, containing the task-specific information. 
This meta information enables fast processing of the submissions and analysis of submitted systems. Participants are advised to fill the meta information carefully while making sure all information is correctly provided.  </p>
<p>All files should be packaged into a zip file for submission. Please make a <strong>clear connection between the system name in the submitted yaml, submitted system output, and the technical report!</strong> Instead of system name you can use submission label too. Example package:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="../documents/challenge2020/dcase2020_challenge_submission_package_example.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="../documents/challenge2020/dcase2020_challenge_submission_package_example.zip" target="_blank">
<span style="font-size:20px;">DCASE2020 challenge submission example package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(185 kB)</span>
<br/>
<span class="text-muted">
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/> </p>
<p>Detailed information for submission can be found on the <a href="/challenge2020/submission">submission page</a>.</p>
<h2 id="subtask-a-1">Subtask A</h2>
<h3>System output file</h3>
<p>Row format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">scene label (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">bus probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">metro probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">metro_station probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">park probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">public_square probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">shopping_mall probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">street_pedestrian probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">street_traffic probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">tram probability (float)</span><span class="o">]</span>
</code></pre></div>
<p>Example output:</p>
<pre class="tab18">filename	scene_label	airport	bus	metro	metro_station	park	public_square	shopping_mall	street_pedestrian	street_traffic	tram
0.wav	bus	0.25	0.99	0.12	0.32	0.41	0.42	0.23	0.34	0.12	0.45
1.wav	tram	0.25	0.19	0.12	0.32	0.41	0.42	0.23	0.34	0.12	0.85
</pre>
<h3>Metadata file</h3>
<p>Example meta information file for Task 1 baseline system <code>task1/Heittola_TAU_task1a_1/Heittola_TAU_task1a_1.meta.yaml</code>:</p>
<div aria-multiselectable="true" class="panel-group" id="metadata-A-accordion" role="tablist">
<div class="panel panel-default">
<div class="panel-heading" id="task1a-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#metadata-A-accordion" data-toggle="collapse" href="#task1a-example-collapse" role="button">               
                   Subtask A / Metadata
                </a>
</h4>
</div>
<div aria-labelledby="task1a-example-header" class="panel-collapse collapse" id="task1a-example-collapse" role="tabpanel">
<div class="panel-body" style="padding: 0px">
<pre class="font110" style="padding:0;border:0;border-radius:0;"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Heittola_TAU_task1a_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2020 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Heittola
      firstname: Toni
      email: toni.heittola@tuni.fi                # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Second author
    - lastname: Mesaros
      firstname: Annamaria
      email: annamaria.mesaros@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

    # Third author
    - lastname: Virtanen
      firstname: Tuomas
      email: tuomas.virtanen@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input
    # e.g. 16kHz, 22.05kHz, 44.1kHz
    input_sampling_rate: 44.1kHz

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: !!null

    # Embeddings
    # e.g. VGGish, OpenL3, ...
    embeddings: OpenL3

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    # one or multiple, e.g. GMM, HMM, SVM, MLP, CNN, RNN, CRNN, ResNet, ensemble, ...
    machine_learning_method: MLP

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

    # Decision making methods
    # e.g. average, majority vote, maximum likelihood, ...
    decision_making: !!null

    # External data usage method
    # e.g. directly, embeddings, pre-trained model, ...
    external_data_usage: embeddings

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value.
    total_parameters: 5012931 # embeddings (OpenL2)=4684224, classifier=328707

  # List of external datasets used in the submission.
  # Development dataset is used here only as example, list only external datasets
  external_datasets:
    # Dataset name
    - name: TAU Urban Acoustic Scenes 2020, Development dataset

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.3819968

      # Total audio length in minutes
      total_audio_length: 3840            # minutes

  # URL to the source code of the system [optional]
  source_code: https://github.com/toni-heittola/dcase2020_task1_baseline

# System results
results:
  development_dataset:
    # System results for development dataset with provided the cross-validation setup.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.

    # Overall metrics
    overall:
      accuracy: 51.6    # mean of class-wise accuracies
      logloss: 1.405

    # Class-wise metrics
    class_wise:
      airport:
        accuracy: 36.5
        logloss: 1.989
      bus:
        accuracy: 52.9
        logloss: 1.014
      metro:
        accuracy: 46.8
        logloss: 1.429
      metro_station:
        accuracy: 47.1
        logloss: 1.477
      park:
        accuracy: 72.7
        logloss: 0.971
      public_square:
        accuracy: 59.6
        logloss: 1.182
      shopping_mall:
        accuracy: 42.4
        logloss: 1.714
      street_pedestrian:
        accuracy: 20.9
        logloss: 2.421
      street_traffic:
        accuracy: 74.7
        logloss: 0.861
      tram:
        accuracy: 62.8
        logloss: 0.989

    # Device-wise
    device_wise:
      a:
        accuracy: 68.8
        logloss: 0.946
      b:
        accuracy: 60.2
        logloss: 1.158
      c:
        accuracy: 59.9
        logloss: 1.038
      s1:
        accuracy: 50.3
        logloss: 1.408
      s2:
        accuracy: 50.0
        logloss: 1.405
      s3:
        accuracy: 50.9
        logloss: 1.468
      s4:
        accuracy: 45.2
        logloss: 1.642
      s5:
        accuracy: 44.8
        logloss: 1.646
      s6:
        accuracy: 34.8
        logloss: 1.931
</code></pre>
</div>
</div>
</div>
</div>
<h2 id="subtask-b-1">Subtask B</h2>
<h3>System output file</h3>
<p>Row format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">scene label (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">indoor probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">outdoor probability (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">transportation probability (float)</span><span class="o">]</span>
</code></pre></div>
<p>Example output:</p>
<pre class="tab18">filename	scene_label	indoor	outdoor	transportation
0.wav	outdoor	0.25	0.99	0.12
1.wav	indoor	0.75	0.29	0.12</pre>
<h3>Metadata file</h3>
<p>Example meta information file for Task 1 baseline system <code>task1/Heittola_TAU_task1b_1/Heittola_TAU_task1b_1.meta.yaml</code>:</p>
<div aria-multiselectable="true" class="panel-group" id="metadata-A-accordion" role="tablist">
<div class="panel panel-default">
<div class="panel-heading" id="task1b-example-header" role="tab">
<h4 class="panel-title">
<a aria-controls="collapseOne" aria-expanded="true" class="collapsed accordion-toggle" data-parent="#metadata-A-accordion" data-toggle="collapse" href="#task1b-example-collapse" role="button">                 
                   Subtask B / Metadata 
                </a>
</h4>
</div>
<div aria-labelledby="task1b-example-header" class="panel-collapse collapse" id="task1b-example-collapse" role="tabpanel">
<div class="panel-body" style="padding: 0px">
<pre class="font110" style="padding:0;border:0;border-radius:0;"><code class="yaml"># Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Heittola_TAU_task1b_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2020 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Heittola
      firstname: Toni
      email: toni.heittola@tuni.fi                # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Second author
    - lastname: Mesaros
      firstname: Annamaria
      email: annamaria.mesaros@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

    # Third author
    - lastname: Virtanen
      firstname: Tuomas
      email: tuomas.virtanen@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
    # Audio input / channels
    # one or multiple: e.g. mono, binaural, left, right, mixed, ...
    input_channels: mono

    # Audio input / sampling rate
    # e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 48.0kHz

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: log-mel energies

    # Embeddings
    # e.g. VGGish, OpenL3, ...
    embeddings: !!null

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    # one or multiple, e.g. GMM, HMM, SVM, MLP, CNN, RNN, CRNN, ResNet, ensemble, ...
    machine_learning_method: CNN

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

    # Decision making methods
    # e.g. average, majority vote, maximum likelihood, ...
    decision_making: !!null

    # External data usage method
    # e.g. directly, embeddings, pre-trained model, ...
    external_data_usage: embeddings

    # Method for handling the complexity restrictions
    # e.g. weight quantization, sparsity, ...
    complexity_management: !!null

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value.
    total_parameters: 115219

    # Total amount of non-zero parameters in the acoustic model.
    # Calculated with same principles as "total_parameters".
    # Use numerical value.
    total_parameters_non_zero: 115219

    # Model size calculated as instructed in task description page.
    # Use numerical value, unit is KB
    model_size: 450.1 # KB

  # List of external datasets used in the submission.
  # Development dataset is used here only as example, list only external datasets
  external_datasets:
    # Dataset name
    - name: TAU Urban Acoustic Scenes 2020 3Class, Development dataset

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.3670185

      # Total audio length in minutes
      total_audio_length: 2400            # minutes

  # URL to the source code of the system [optional]
  source_code: https://github.com/toni-heittola/dcase2020_task1_baseline

# System results
results:
  development_dataset:
    # System results for development dataset with provided the cross-validation setup.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.

    # Overall metrics
    overall:
      accuracy: 88.0
      logloss: 0.481

    # Class-wise accuracies
    class_wise:
      indoor:
        accuracy: 83.7
        logloss: 0.746
      outdoor:
        accuracy: 89.5
        logloss: 0.367
      transportation:
        accuracy: 90.7
        logloss: 0.356</code></pre>
</div>
</div>
</div>
</div>
<h2 id="package-validator">Package validator</h2>
<p>This is an automatic validation tool to help challenge participants to prepare a correctly formatted submission package, which in turn will speed up the submission processing in the challenge evaluation stage. Please use this to make sure your submission package follows the given formatting. </p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/toni-heittola/dcase2020_task1_submission_validator" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/toni-heittola/dcase2020_task1_submission_validator" target="_blank">
<span style="font-size:20px;">DCASE2020 Task 1 <strong>submission validator</strong> <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h1 id="task-rules">Task rules</h1>
<p>There are general rules valid for all tasks; these, along with information on technical report and submission requirements can be found here.</p>
<p>Task specific rules:</p>
<ul>
<li>Use of external data is allowed. See conditions for external data usage <a href="#external-data-resources">here</a>.</li>
<li>In subtask B, the model size limit applies. See conditions for the model size <a href="#complexity-requirements">here</a>.</li>
<li>Manipulation of provided training and development data is allowed (e.g. by mixing data sampled from a pdf or using techniques such as pitch shifting or time stretching).</li>
<li>Participants are not allowed to make subjective judgments of the evaluation data, nor to annotate it. The evaluation dataset cannot be used to train the submitted system; the use of statistics about the evaluation data in the decision making is also forbidden. Separately published leaderboard data is considered as evaluation data as well.</li>
<li>Classification decision must be done independently for each test sample.</li>
</ul>
<h1 id="evaluation">Evaluation</h1>
<p>Systems will be ranked by <strong>macro-average accuracy</strong> (average of the class-wise accuracies).</p>
<p>As a secondary metric, we will use <em>multiclass cross-entropy</em> (Log loss), in order to have a metric that is independent of the operating point (see python implementation <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">here</a>). </p>
<h1 id="results">Results</h1>
<h2 id="subtask-a-2">Subtask A</h2>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="10" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="2"></th>
</tr>
<tr>
<th class="sm-cell" data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Classification Accuracy" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% <br/>confidence interval</small>
</th>
<th class="text-center" data-axis-label="Classification logloss" data-chartable="true" data-field="logloss_eval" data-sortable="true" data-value-type="float3">
                Logloss
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Abbasi_ARI_task1a_1</td>
<td>Reyhaneh Abbasi</td>
<td>Mathematics and Signal Processing in Acoustics, acoustic research institute of OEAW, Vienna, Austria</td>
<td>task-acoustic-scene-classification-results-a#Abbasi2020</td>
<td>59.7 (58.8 - 60.6)</td>
<td>1.099</td>
</tr>
<tr>
<td></td>
<td>Abbasi_ARI_task1a_2</td>
<td>Reyhaneh Abbasi</td>
<td>Mathematics and Signal Processing in Acoustics, acoustic research institute of OEAW, Vienna, Austria</td>
<td>task-acoustic-scene-classification-results-a#Abbasi2020</td>
<td>60.6 (59.7 - 61.5)</td>
<td>1.063</td>
</tr>
<tr>
<td></td>
<td>Cao_JNU_task1a_1</td>
<td>Yi Cao</td>
<td>Mechanical engineering, Jiangnan University, Wuxi, China</td>
<td>task-acoustic-scene-classification-results-a#Fei2020</td>
<td>65.7 (64.9 - 66.6)</td>
<td>1.265</td>
</tr>
<tr>
<td></td>
<td>Cao_JNU_task1a_2</td>
<td>Yi Cao</td>
<td>Mechanical engineering, Jiangnan University, Wuxi, China</td>
<td>task-acoustic-scene-classification-results-a#Fei2020</td>
<td>65.7 (64.8 - 66.5)</td>
<td>1.259</td>
</tr>
<tr>
<td></td>
<td>Cao_JNU_task1a_3</td>
<td>Yi Cao</td>
<td>Mechanical engineering, Jiangnan University, Wuxi, China</td>
<td>task-acoustic-scene-classification-results-a#Fei2020</td>
<td>66.0 (65.1 - 66.8)</td>
<td>1.268</td>
</tr>
<tr>
<td></td>
<td>Cao_JNU_task1a_4</td>
<td>Yi Cao</td>
<td>Mechanical engineering, Jiangnan University, Wuxi, China</td>
<td>task-acoustic-scene-classification-results-a#Fei2020</td>
<td>65.9 (65.1 - 66.8)</td>
<td>1.267</td>
</tr>
<tr>
<td></td>
<td>FanVaf__task1a_1</td>
<td>Eleftherios Fanioudakis</td>
<td>Greece</td>
<td>task-acoustic-scene-classification-results-a#Fanioudakis2020</td>
<td>63.4 (62.5 - 64.2)</td>
<td>1.106</td>
</tr>
<tr>
<td></td>
<td>FanVaf__task1a_2</td>
<td>Eleftherios Fanioudakis</td>
<td>Greece</td>
<td>task-acoustic-scene-classification-results-a#Fanioudakis2020</td>
<td>60.7 (59.9 - 61.6)</td>
<td>1.142</td>
</tr>
<tr>
<td></td>
<td>FanVaf__task1a_3</td>
<td>Eleftherios Fanioudakis</td>
<td>Greece</td>
<td>task-acoustic-scene-classification-results-a#Fanioudakis2020</td>
<td>64.8 (63.9 - 65.6)</td>
<td>1.298</td>
</tr>
<tr>
<td></td>
<td>FanVaf__task1a_4</td>
<td>Eleftherios Fanioudakis</td>
<td>Greece</td>
<td>task-acoustic-scene-classification-results-a#Fanioudakis2020</td>
<td>67.5 (66.6 - 68.3)</td>
<td>1.240</td>
</tr>
<tr>
<td></td>
<td>Gao_UNISA_task1a_1</td>
<td>Wei Gao</td>
<td>UniSA STEM, University of South Australia, Adelaide, Australia</td>
<td>task-acoustic-scene-classification-results-a#Gao2020</td>
<td>75.0 (74.3 - 75.8)</td>
<td>1.225</td>
</tr>
<tr>
<td></td>
<td>Gao_UNISA_task1a_2</td>
<td>Wei Gao</td>
<td>UniSA STEM, University of South Australia, Adelaide, Australia</td>
<td>task-acoustic-scene-classification-results-a#Gao2020</td>
<td>74.1 (73.3 - 74.9)</td>
<td>1.242</td>
</tr>
<tr>
<td></td>
<td>Gao_UNISA_task1a_3</td>
<td>Wei Gao</td>
<td>UniSA STEM, University of South Australia, Adelaide, Australia</td>
<td>task-acoustic-scene-classification-results-a#Gao2020</td>
<td>74.7 (73.9 - 75.5)</td>
<td>1.231</td>
</tr>
<tr>
<td></td>
<td>Gao_UNISA_task1a_4</td>
<td>Wei Gao</td>
<td>UniSA STEM, University of South Australia, Adelaide, Australia</td>
<td>task-acoustic-scene-classification-results-a#Gao2020</td>
<td>75.2 (74.4 - 76.0)</td>
<td>1.230</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020 baseline</td>
<td>Toni Heittola</td>
<td>Computing Sciences, Tampere University, Tampere, Finland</td>
<td>task-acoustic-scene-classification-results-a#Heittola2020</td>
<td>51.4 (50.5 - 52.3)</td>
<td>1.902</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1a_1</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020_t1</td>
<td>73.4 (72.6 - 74.2)</td>
<td>0.850</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1a_2</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020_t1</td>
<td>68.4 (67.6 - 69.3)</td>
<td>0.991</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1a_3</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020_t1</td>
<td>73.1 (72.3 - 73.9)</td>
<td>0.889</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1a_4</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020_t1</td>
<td>72.3 (71.5 - 73.1)</td>
<td>0.899</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1a_1</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-a#Hu2020</td>
<td>75.7 (74.9 - 76.4)</td>
<td>0.924</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1a_2</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-a#Hu2020</td>
<td>75.9 (75.1 - 76.7)</td>
<td>0.895</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1a_3</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-a#Hu2020</td>
<td>76.2 (75.4 - 77.0)</td>
<td>0.898</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1a_4</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-a#Hu2020</td>
<td>75.8 (75.0 - 76.5)</td>
<td>0.900</td>
</tr>
<tr>
<td></td>
<td>JHKim_IVS_task1a_1</td>
<td>Jaehun Kim</td>
<td>AI Research Lab, IVS Inc, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Kim2020_t1</td>
<td>67.3 (66.5 - 68.2)</td>
<td>5.219</td>
</tr>
<tr>
<td></td>
<td>JHKim_IVS_task1a_2</td>
<td>Jaehun Kim</td>
<td>AI Research Lab, IVS Inc, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Kim2020_t1</td>
<td>66.2 (65.3 - 67.0)</td>
<td>4.766</td>
</tr>
<tr>
<td></td>
<td>Jie_Maxvision_task1a_1</td>
<td>Liu Jie</td>
<td>Maxvision, Wuhan, China</td>
<td>task-acoustic-scene-classification-results-a#Jie2020</td>
<td>75.0 (74.3 - 75.8)</td>
<td>1.209</td>
</tr>
<tr>
<td></td>
<td>Kim_SGU_task1a_1</td>
<td>Kim Ji-Hwan</td>
<td>Dept. of Computer Scinece and Engineering, Sogang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Changmin2020</td>
<td>71.6 (70.8 - 72.4)</td>
<td>1.309</td>
</tr>
<tr>
<td></td>
<td>Kim_SGU_task1a_2</td>
<td>Kim Ji-Hwan</td>
<td>Dept. of Computer Scinece and Engineering, Sogang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Changmin2020</td>
<td>70.7 (69.9 - 71.6)</td>
<td>1.304</td>
</tr>
<tr>
<td></td>
<td>Kim_SGU_task1a_3</td>
<td>Kim Ji-Hwan</td>
<td>Dept. of Computer Scinece and Engineering, Sogang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Changmin2020</td>
<td>70.7 (69.8 - 71.5)</td>
<td>1.412</td>
</tr>
<tr>
<td></td>
<td>Kim_SGU_task1a_4</td>
<td>Kim Ji-Hwan</td>
<td>Dept. of Computer Scinece and Engineering, Sogang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Changmin2020</td>
<td>66.4 (65.6 - 67.3)</td>
<td>1.428</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1a_1</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-a#Koutini2020</td>
<td>71.9 (71.1 - 72.7)</td>
<td>0.800</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1a_2</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-a#Koutini2020</td>
<td>71.6 (70.8 - 72.4)</td>
<td>0.862</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1a_3</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-a#Koutini2020</td>
<td>73.6 (72.8 - 74.4)</td>
<td>0.796</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1a_4</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-a#Koutini2020</td>
<td>73.4 (72.6 - 74.2)</td>
<td>0.814</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1a_1</td>
<td>Yerin Lee</td>
<td>Statistics Dept., Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Lee2020</td>
<td>69.2 (68.3 - 70.0)</td>
<td>0.885</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1a_2</td>
<td>Yerin Lee</td>
<td>Statistics Dept., Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Lee2020</td>
<td>69.6 (68.8 - 70.5)</td>
<td>0.859</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1a_3</td>
<td>Yerin Lee</td>
<td>Statistics Dept., Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Lee2020</td>
<td>72.0 (71.2 - 72.8)</td>
<td>0.944</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1a_4</td>
<td>Yerin Lee</td>
<td>Statistics Dept., Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Lee2020</td>
<td>72.9 (72.1 - 73.7)</td>
<td>0.919</td>
</tr>
<tr>
<td></td>
<td>Lee_GU_task1a_1</td>
<td>Sang Woong Lee</td>
<td>Gachon University, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Aryal2020</td>
<td>55.9 (55.0 - 56.8)</td>
<td>1.969</td>
</tr>
<tr>
<td></td>
<td>Lee_GU_task1a_2</td>
<td>Sang Woong Lee</td>
<td>Gachon University, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Aryal2020</td>
<td>55.6 (54.7 - 56.5)</td>
<td>1.818</td>
</tr>
<tr>
<td></td>
<td>Lee_GU_task1a_3</td>
<td>Sang Woong Lee</td>
<td>Gachon University, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Aryal2020</td>
<td>55.6 (54.7 - 56.5)</td>
<td>2.987</td>
</tr>
<tr>
<td></td>
<td>Lee_GU_task1a_4</td>
<td>Sang Woong Lee</td>
<td>Gachon University, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Aryal2020</td>
<td>54.9 (54.1 - 55.8)</td>
<td>2.847</td>
</tr>
<tr>
<td></td>
<td>Liu_SHNU_task1a_1</td>
<td>YanHua Long</td>
<td>The College of Information,Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020</td>
<td>69.3 (68.5 - 70.1)</td>
<td>1.396</td>
</tr>
<tr>
<td></td>
<td>Liu_SHNU_task1a_2</td>
<td>YanHua Long</td>
<td>The College of Information,Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020</td>
<td>68.0 (67.2 - 68.9)</td>
<td>4.510</td>
</tr>
<tr>
<td></td>
<td>Liu_SHNU_task1a_3</td>
<td>YanHua Long</td>
<td>The College of Information,Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020</td>
<td>55.7 (54.8 - 56.6)</td>
<td>9.403</td>
</tr>
<tr>
<td></td>
<td>Liu_SHNU_task1a_4</td>
<td>YanHua Long</td>
<td>The College of Information,Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020</td>
<td>72.0 (71.2 - 72.8)</td>
<td>3.165</td>
</tr>
<tr>
<td></td>
<td>Liu_UESTC_task1a_1</td>
<td>Yingzi Liu</td>
<td>School of imformation and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020a</td>
<td>73.2 (72.4 - 74.0)</td>
<td>1.305</td>
</tr>
<tr>
<td></td>
<td>Liu_UESTC_task1a_2</td>
<td>Yingzi Liu</td>
<td>School of imformation and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020a</td>
<td>72.4 (71.6 - 73.2)</td>
<td>1.303</td>
</tr>
<tr>
<td></td>
<td>Liu_UESTC_task1a_3</td>
<td>Yingzi Liu</td>
<td>School of imformation and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020a</td>
<td>72.5 (71.7 - 73.3)</td>
<td>0.755</td>
</tr>
<tr>
<td></td>
<td>Liu_UESTC_task1a_4</td>
<td>Yingzi Liu</td>
<td>School of imformation and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Liu2020a</td>
<td>72.0 (71.2 - 72.8)</td>
<td>0.767</td>
</tr>
<tr>
<td></td>
<td>Lopez-Meyer_IL_task1a_1</td>
<td>Paulo Lopez-Meyer</td>
<td>Intel Labs, Intel Corporation, Jalisco, Mexico</td>
<td>task-acoustic-scene-classification-results-a#Lopez-Meyer2020_t1a</td>
<td>64.3 (63.4 - 65.1)</td>
<td>5.268</td>
</tr>
<tr>
<td></td>
<td>Lopez-Meyer_IL_task1a_2</td>
<td>Paulo Lopez-Meyer</td>
<td>Intel Labs, Intel Corporation, Jalisco, Mexico</td>
<td>task-acoustic-scene-classification-results-a#Lopez-Meyer2020_t1a</td>
<td>64.1 (63.3 - 65.0)</td>
<td>11.870</td>
</tr>
<tr>
<td></td>
<td>Lu_INTC_task1a_1</td>
<td>Lu Hong</td>
<td>Intel Labs, Intel Corporation, Santa Clara, USA</td>
<td>task-acoustic-scene-classification-results-a#Hong2020</td>
<td>71.2 (70.4 - 72.0)</td>
<td>0.809</td>
</tr>
<tr>
<td></td>
<td>Lu_INTC_task1a_2</td>
<td>Lu Hong</td>
<td>Intel Labs, Intel Corporation, Santa Clara, USA</td>
<td>task-acoustic-scene-classification-results-a#Hong2020</td>
<td>64.1 (63.3 - 65.0)</td>
<td>1.383</td>
</tr>
<tr>
<td></td>
<td>Lu_INTC_task1a_3</td>
<td>Lu Hong</td>
<td>Intel Labs, Intel Corporation, Santa Clara, USA</td>
<td>task-acoustic-scene-classification-results-a#Hong2020</td>
<td>66.4 (65.5 - 67.2)</td>
<td>1.192</td>
</tr>
<tr>
<td></td>
<td>Lu_INTC_task1a_4</td>
<td>Lu Hong</td>
<td>Intel Labs, Intel Corporation, Santa Clara, USA</td>
<td>task-acoustic-scene-classification-results-a#Hong2020</td>
<td>71.2 (70.4 - 72.1)</td>
<td>0.806</td>
</tr>
<tr>
<td></td>
<td>Monteiro_INRS_task1a_1</td>
<td>Monteiro Joao</td>
<td>EMT, Institut National de la Recherche Scientifique, Montreal, Canada</td>
<td>task-acoustic-scene-classification-results-a#Joao2020</td>
<td>61.7 (60.8 - 62.6)</td>
<td>5.936</td>
</tr>
<tr>
<td></td>
<td>Monteiro_INRS_task1a_2</td>
<td>Monteiro Joao</td>
<td>EMT, Institut National de la Recherche Scientifique, Montreal, Canada</td>
<td>task-acoustic-scene-classification-results-a#Joao2020</td>
<td>55.9 (55.0 - 56.8)</td>
<td>5.198</td>
</tr>
<tr>
<td></td>
<td>Monteiro_INRS_task1a_3</td>
<td>Monteiro Joao</td>
<td>EMT, Institut National de la Recherche Scientifique, Montreal, Canada</td>
<td>task-acoustic-scene-classification-results-a#Joao2020</td>
<td>50.8 (49.9 - 51.7)</td>
<td>2.766</td>
</tr>
<tr>
<td></td>
<td>Monteiro_INRS_task1a_4</td>
<td>Monteiro Joao</td>
<td>EMT, Institut National de la Recherche Scientifique, Montreal, Canada</td>
<td>task-acoustic-scene-classification-results-a#Joao2020</td>
<td>66.3 (65.5 - 67.2)</td>
<td>2.226</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_Vfy_task1a_1</td>
<td>Javier Naranjo-Alcazar</td>
<td>AI department, Visualfy, Benisano, Spain; Computer Science Department, Universitat de Valencia, Burjassot, Spain</td>
<td>task-acoustic-scene-classification-results-a#Naranjo-Alcazar2020_t1</td>
<td>61.9 (61.0 - 62.7)</td>
<td>1.246</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_Vfy_task1a_2</td>
<td>Javier Naranjo-Alcazar</td>
<td>AI department, Visualfy, Benisano, Spain; Computer Science Department, Universitat de Valencia, Burjassot, Spain</td>
<td>task-acoustic-scene-classification-results-a#Naranjo-Alcazar2020_t1</td>
<td>59.7 (58.8 - 60.6)</td>
<td>1.314</td>
</tr>
<tr>
<td></td>
<td>Paniagua_UPM_task1a_1</td>
<td>Rubén Fraile</td>
<td>CITSEM, Universidad Politéctica de Madrid, Madrid, Spain</td>
<td>task-acoustic-scene-classification-results-a#Paniagua2020</td>
<td>43.8 (42.9 - 44.7)</td>
<td>2.053</td>
</tr>
<tr>
<td></td>
<td>Shim_UOS_task1a_1</td>
<td>Ha-jin Yu</td>
<td>School of Computer Science, University of Seoul, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Shim2020</td>
<td>71.7 (70.9 - 72.5)</td>
<td>1.190</td>
</tr>
<tr>
<td></td>
<td>Shim_UOS_task1a_2</td>
<td>Ha-jin Yu</td>
<td>School of Computer Science, University of Seoul, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Shim2020</td>
<td>71.5 (70.7 - 72.4)</td>
<td>0.897</td>
</tr>
<tr>
<td></td>
<td>Shim_UOS_task1a_3</td>
<td>Ha-jin Yu</td>
<td>School of Computer Science, University of Seoul, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Shim2020</td>
<td>68.5 (67.6 - 69.3)</td>
<td>0.911</td>
</tr>
<tr>
<td></td>
<td>Shim_UOS_task1a_4</td>
<td>Ha-jin Yu</td>
<td>School of Computer Science, University of Seoul, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Shim2020</td>
<td>71.0 (70.2 - 71.8)</td>
<td>0.945</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1a_1</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Suh2020</td>
<td>72.5 (71.7 - 73.3)</td>
<td>1.290</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1a_2</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Suh2020</td>
<td>75.5 (74.7 - 76.2)</td>
<td>1.221</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1a_3</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Suh2020</td>
<td>76.5 (75.8 - 77.3)</td>
<td>1.219</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1a_4</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-a#Suh2020</td>
<td>76.5 (75.7 - 77.2)</td>
<td>1.219</td>
</tr>
<tr>
<td></td>
<td>Swiecicki_NON_task1a_1</td>
<td>Jakub Swiecicki</td>
<td>None, Warsaw, Poland</td>
<td>task-acoustic-scene-classification-results-a#Swiecicki2020</td>
<td>67.1 (66.2 - 67.9)</td>
<td>0.926</td>
</tr>
<tr>
<td></td>
<td>Swiecicki_NON_task1a_2</td>
<td>Jakub Swiecicki</td>
<td>None, Warsaw, Poland</td>
<td>task-acoustic-scene-classification-results-a#Swiecicki2020</td>
<td>69.5 (68.7 - 70.3)</td>
<td>0.851</td>
</tr>
<tr>
<td></td>
<td>Swiecicki_NON_task1a_3</td>
<td>Jakub Swiecicki</td>
<td>None, Warsaw, Poland</td>
<td>task-acoustic-scene-classification-results-a#Swiecicki2020</td>
<td>70.3 (69.4 - 71.1)</td>
<td>0.970</td>
</tr>
<tr>
<td></td>
<td>Swiecicki_NON_task1a_4</td>
<td>Jakub Swiecicki</td>
<td>None, Warsaw, Poland</td>
<td>task-acoustic-scene-classification-results-a#Swiecicki2020</td>
<td>71.8 (71.0 - 72.7)</td>
<td>0.793</td>
</tr>
<tr>
<td></td>
<td>Vilouras_AUTh_task1a_1</td>
<td>Konstantinos Vilouras</td>
<td>Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece</td>
<td>task-acoustic-scene-classification-results-a#Vilouras2020</td>
<td>67.7 (66.8 - 68.5)</td>
<td>0.929</td>
</tr>
<tr>
<td></td>
<td>Vilouras_AUTh_task1a_2</td>
<td>Konstantinos Vilouras</td>
<td>Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece</td>
<td>task-acoustic-scene-classification-results-a#Vilouras2020</td>
<td>67.8 (67.0 - 68.7)</td>
<td>0.931</td>
</tr>
<tr>
<td></td>
<td>Vilouras_AUTh_task1a_3</td>
<td>Konstantinos Vilouras</td>
<td>Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece</td>
<td>task-acoustic-scene-classification-results-a#Vilouras2020</td>
<td>69.3 (68.5 - 70.1)</td>
<td>0.883</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1a_1</td>
<td>Shefali Waldekar</td>
<td>Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India</td>
<td>task-acoustic-scene-classification-results-a#Waldekar2020</td>
<td>58.4 (57.5 - 59.2)</td>
<td>1.427</td>
</tr>
<tr>
<td></td>
<td>Wang_RoyalFlush_task1a_1</td>
<td>Peiyao Wang</td>
<td>Speech Group, Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020a</td>
<td>56.7 (55.8 - 57.6)</td>
<td>1.576</td>
</tr>
<tr>
<td></td>
<td>Wang_RoyalFlush_task1a_2</td>
<td>Peiyao Wang</td>
<td>Speech Group, Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020a</td>
<td>65.2 (64.3 - 66.0)</td>
<td>1.294</td>
</tr>
<tr>
<td></td>
<td>Wang_RoyalFlush_task1a_3</td>
<td>Peiyao Wang</td>
<td>Speech Group, Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020a</td>
<td>64.0 (63.1 - 64.8)</td>
<td>1.239</td>
</tr>
<tr>
<td></td>
<td>Wang_RoyalFlush_task1a_4</td>
<td>Peiyao Wang</td>
<td>Speech Group, Hithink RoyalFlush Information Network Co.,Ltd, Hangzhou, China</td>
<td>task-acoustic-scene-classification-results-a#Wang2020a</td>
<td>45.5 (44.6 - 46.4)</td>
<td>5.880</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1a_1</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-a#Wu2020_t1a</td>
<td>64.7 (63.9 - 65.6)</td>
<td>1.148</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1a_2</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-a#Wu2020_t1a</td>
<td>69.3 (68.4 - 70.1)</td>
<td>1.070</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1a_3</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-a#Wu2020_t1a</td>
<td>67.9 (67.1 - 68.8)</td>
<td>1.100</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1a_4</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-a#Wu2020_t1a</td>
<td>69.4 (68.6 - 70.2)</td>
<td>1.080</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1a_1</td>
<td>Wei-Qiang Zhang</td>
<td>Department of Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-acoustic-scene-classification-results-a#Shao2020</td>
<td>73.0 (72.2 - 73.8)</td>
<td>1.963</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1a_2</td>
<td>Wei-Qiang Zhang</td>
<td>Department of Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-acoustic-scene-classification-results-a#Shao2020</td>
<td>73.2 (72.4 - 74.0)</td>
<td>1.967</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1a_3</td>
<td>Wei-Qiang Zhang</td>
<td>Department of Electronic Engineering, Tsinghua University, Beijing, China</td>
<td>task-acoustic-scene-classification-results-a#Shao2020</td>
<td>72.3 (71.5 - 73.1)</td>
<td>1.958</td>
</tr>
<tr>
<td></td>
<td>Zhang_UESTC_task1a_1</td>
<td>Chi Zhang</td>
<td>Electronic Information Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Zhang2020</td>
<td>50.4 (49.5 - 51.3)</td>
<td>1.899</td>
</tr>
<tr>
<td></td>
<td>Zhang_UESTC_task1a_2</td>
<td>Chi Zhang</td>
<td>Electronic Information Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Zhang2020</td>
<td>51.7 (50.8 - 52.6)</td>
<td>1.805</td>
</tr>
<tr>
<td></td>
<td>Zhang_UESTC_task1a_3</td>
<td>Chi Zhang</td>
<td>Electronic Information Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-a#Zhang2020</td>
<td>47.4 (46.5 - 48.3)</td>
<td>2.068</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2020/task-acoustic-scene-classification-results-a">subtask A results page</a></p>
<h2 id="subtask-b-2">Subtask B</h2>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="10" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="2"></th>
</tr>
<tr>
<th class="sm-cell" data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Classification Accuracy" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% <br/>confidence interval</small>
</th>
<th class="text-center" data-axis-label="Classification logloss" data-chartable="true" data-field="logloss_eval" data-sortable="true" data-value-type="float3">
                Logloss
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Chang_QTI_task1b_1</td>
<td>Simyung Chang</td>
<td>Qualcomm AI Research, Qualcomm Korea YH, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Chang2020</td>
<td>95.0 (94.6 - 95.5)</td>
<td>0.228</td>
</tr>
<tr>
<td></td>
<td>Chang_QTI_task1b_2</td>
<td>Simyung Chang</td>
<td>Qualcomm AI Research, Qualcomm Korea YH, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Chang2020</td>
<td>93.2 (92.9 - 93.5)</td>
<td>0.232</td>
</tr>
<tr>
<td></td>
<td>Chang_QTI_task1b_3</td>
<td>Simyung Chang</td>
<td>Qualcomm AI Research, Qualcomm Korea YH, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Chang2020</td>
<td>94.8 (94.2 - 95.3)</td>
<td>0.224</td>
</tr>
<tr>
<td></td>
<td>Chang_QTI_task1b_4</td>
<td>Simyung Chang</td>
<td>Qualcomm AI Research, Qualcomm Korea YH, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Chang2020</td>
<td>94.4 (93.8 - 95.1)</td>
<td>0.237</td>
</tr>
<tr>
<td></td>
<td>Dat_HCMUni_task1b_1</td>
<td>Ngo Dat</td>
<td>Electrical &amp; Electronic Engineering, Ho Chi Minh University of Technology, Ho Chi Minh, Vietnam</td>
<td>task-acoustic-scene-classification-results-b#Dat2020</td>
<td>89.5 (89.5 - 89.5)</td>
<td>0.648</td>
</tr>
<tr>
<td></td>
<td>Farrugia_IMT-Atlantique-BRAIn_task1b_1</td>
<td>Nicolas Farrugia</td>
<td>Electronics, IMT Atlantique, Brest, France</td>
<td>task-acoustic-scene-classification-results-b#Pajusco2020</td>
<td>85.4 (84.9 - 85.8)</td>
<td>0.379</td>
</tr>
<tr>
<td></td>
<td>Farrugia_IMT-Atlantique-BRAIn_task1b_2</td>
<td>Nicolas Farrugia</td>
<td>Electronics, IMT Atlantique, Brest, France</td>
<td>task-acoustic-scene-classification-results-b#Pajusco2020</td>
<td>90.6 (90.0 - 91.2)</td>
<td>0.270</td>
</tr>
<tr>
<td></td>
<td>Farrugia_IMT-Atlantique-BRAIn_task1b_3</td>
<td>Nicolas Farrugia</td>
<td>Electronics, IMT Atlantique, Brest, France</td>
<td>task-acoustic-scene-classification-results-b#Pajusco2020</td>
<td>86.6 (85.9 - 87.3)</td>
<td>0.384</td>
</tr>
<tr>
<td></td>
<td>Farrugia_IMT-Atlantique-BRAIn_task1b_4</td>
<td>Nicolas Farrugia</td>
<td>Electronics, IMT Atlantique, Brest, France</td>
<td>task-acoustic-scene-classification-results-b#Pajusco2020</td>
<td>88.4 (87.9 - 88.9)</td>
<td>0.286</td>
</tr>
<tr>
<td></td>
<td>Feng_TJU_task1b_1</td>
<td>Guoqing Feng</td>
<td>School of Electrical and Information Engineering, Tianjin University, Tianjin, China</td>
<td>task-acoustic-scene-classification-results-b#Feng2020</td>
<td>72.3 (73.9 - 70.7)</td>
<td>1.728</td>
</tr>
<tr>
<td></td>
<td>Feng_TJU_task1b_2</td>
<td>Guoqing Feng</td>
<td>School of Electrical and Information Engineering, Tianjin University, Tianjin, China</td>
<td>task-acoustic-scene-classification-results-b#Feng2020</td>
<td>81.9 (82.2 - 81.6)</td>
<td>1.189</td>
</tr>
<tr>
<td></td>
<td>Feng_TJU_task1b_3</td>
<td>Guoqing Feng</td>
<td>School of Electrical and Information Engineering, Tianjin University, Tianjin, China</td>
<td>task-acoustic-scene-classification-results-b#Feng2020</td>
<td>80.7 (81.0 - 80.4)</td>
<td>1.302</td>
</tr>
<tr>
<td></td>
<td>Feng_TJU_task1b_4</td>
<td>Guoqing Feng</td>
<td>School of Electrical and Information Engineering, Tianjin University, Tianjin, China</td>
<td>task-acoustic-scene-classification-results-b#Feng2020</td>
<td>79.9 (80.4 - 79.3)</td>
<td>1.281</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2020 baseline</td>
<td>Toni Heittola</td>
<td>Computing Sciences, Tampere University, Tampere, Finland</td>
<td>task-acoustic-scene-classification-results-b#Heittola2020</td>
<td>89.5 (88.8 - 90.2)</td>
<td>0.401</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1b_1</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-b#Wang2020_t1</td>
<td>91.6 (91.1 - 92.0)</td>
<td>0.227</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1b_2</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-b#Wang2020_t1</td>
<td>91.6 (91.2 - 92.0)</td>
<td>0.233</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1b_3</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-b#Wang2020_t1</td>
<td>91.6 (91.1 - 92.0)</td>
<td>0.230</td>
</tr>
<tr>
<td></td>
<td>Helin_ADSPLAB_task1b_4</td>
<td>Yuexian Zou</td>
<td>School of ECE, Peking University, Shenzhen, China</td>
<td>task-acoustic-scene-classification-results-b#Wang2020_t1</td>
<td>91.3 (91.0 - 91.6)</td>
<td>0.264</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1b_1</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-b#Hu2020</td>
<td>95.8 (95.5 - 96.1)</td>
<td>0.357</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1b_2</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-b#Hu2020</td>
<td>95.5 (95.1 - 95.8)</td>
<td>0.367</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1b_3</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-b#Hu2020</td>
<td>96.0 (95.5 - 96.5)</td>
<td>0.122</td>
</tr>
<tr>
<td></td>
<td>Hu_GT_task1b_4</td>
<td>Hu Hu</td>
<td>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA</td>
<td>task-acoustic-scene-classification-results-b#Hu2020</td>
<td>95.8 (95.3 - 96.3)</td>
<td>0.131</td>
</tr>
<tr>
<td></td>
<td>Kalinowski_SRPOL_task1b_4</td>
<td>Beniamin Kalinowski</td>
<td>Audio Intelligence, Samsung R&amp;D Poland, Warsaw, Poland</td>
<td>task-acoustic-scene-classification-results-b#Kalinowski2020</td>
<td>93.1 (92.7 - 93.5)</td>
<td>1.532</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1b_1</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-b#Koutini2020</td>
<td>94.7 (94.5 - 94.9)</td>
<td>0.164</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1b_2</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-b#Koutini2020</td>
<td>96.5 (96.2 - 96.8)</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1b_3</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-b#Koutini2020</td>
<td>95.7 (95.5 - 95.9)</td>
<td>0.113</td>
</tr>
<tr>
<td></td>
<td>Koutini_CPJKU_task1b_4</td>
<td>Khaled Koutini</td>
<td>Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results-b#Koutini2020</td>
<td>96.2 (95.9 - 96.5)</td>
<td>0.105</td>
</tr>
<tr>
<td></td>
<td>Kowaleczko_SRPOL_task1b_3</td>
<td>Pawel Kowaleczko</td>
<td>Audio Intelligence, Samsung R&amp;D Poland, Warsaw, Poland</td>
<td>task-acoustic-scene-classification-results-b#Kalinowski2020</td>
<td>90.1 (89.6 - 90.7)</td>
<td>0.356</td>
</tr>
<tr>
<td></td>
<td>Kwiatkowska_SRPOL_task1b_1</td>
<td>Zuzanna Kwiatkowska</td>
<td>Audio Intelligence, Samsung R&amp;D Institute, Warsaw Poland</td>
<td>task-acoustic-scene-classification-results-b#Kalinowski2020</td>
<td>92.6 (92.0 - 93.2)</td>
<td>0.200</td>
</tr>
<tr>
<td></td>
<td>Kwiatkowska_SRPOL_task1b_2</td>
<td>Zuzanna Kwiatkowska</td>
<td>Audio Intelligence, Samsung R&amp;D Institute, Warsaw Poland</td>
<td>task-acoustic-scene-classification-results-b#Kalinowski2020</td>
<td>93.5 (93.0 - 94.0)</td>
<td>0.168</td>
</tr>
<tr>
<td></td>
<td>LamPham_Kent_task1b_1</td>
<td>Lam Pham</td>
<td>School of Computing, University of Kent, Kent, UK</td>
<td>task-acoustic-scene-classification-results-b#Pham2020</td>
<td>89.4 (89.2 - 89.7)</td>
<td>0.332</td>
</tr>
<tr>
<td></td>
<td>LamPham_Kent_task1b_2</td>
<td>Lam Pham</td>
<td>School of Computing, University of Kent, Kent, UK</td>
<td>task-acoustic-scene-classification-results-b#Pham2020</td>
<td>87.0 (86.1 - 87.8)</td>
<td>0.349</td>
</tr>
<tr>
<td></td>
<td>LamPham_Kent_task1b_3</td>
<td>Lam Pham</td>
<td>School of Computing, University of Kent, Kent, UK</td>
<td>task-acoustic-scene-classification-results-b#Pham2020</td>
<td>84.7 (85.0 - 84.5)</td>
<td>0.402</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1b_1</td>
<td>Yerin Lee</td>
<td>Statistics, Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Lee2020</td>
<td>90.7 (90.7 - 90.7)</td>
<td>0.302</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1b_2</td>
<td>Yerin Lee</td>
<td>Statistics, Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Lee2020</td>
<td>93.9 (93.7 - 94.1)</td>
<td>0.156</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1b_3</td>
<td>Yerin Lee</td>
<td>Statistics, Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Lee2020</td>
<td>91.1 (91.0 - 91.2)</td>
<td>0.246</td>
</tr>
<tr>
<td></td>
<td>Lee_CAU_task1b_4</td>
<td>Yerin Lee</td>
<td>Statistics, Chung-Ang University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Lee2020</td>
<td>91.2 (91.2 - 91.2)</td>
<td>0.864</td>
</tr>
<tr>
<td></td>
<td>Lopez-Meyer_IL_task1b_1</td>
<td>Paulo Lopez-Meyer</td>
<td>Intel Labs, Intel Corporation, Jalisco, Mexico</td>
<td>task-acoustic-scene-classification-results-b#Lopez-Meyer2020_t1b</td>
<td>90.4 (89.6 - 91.1)</td>
<td>0.681</td>
</tr>
<tr>
<td></td>
<td>Lopez-Meyer_IL_task1b_2</td>
<td>Paulo Lopez-Meyer</td>
<td>Intel Labs, Intel Corporation, Jalisco, Mexico</td>
<td>task-acoustic-scene-classification-results-b#Lopez-Meyer2020_t1b</td>
<td>90.1 (89.7 - 90.5)</td>
<td>0.677</td>
</tr>
<tr>
<td></td>
<td>Lopez-Meyer_IL_task1b_3</td>
<td>Paulo Lopez-Meyer</td>
<td>Intel Labs, Intel Corporation, Jalisco, Mexico</td>
<td>task-acoustic-scene-classification-results-b#Lopez-Meyer2020_t1b</td>
<td>90.5 (89.8 - 91.2)</td>
<td>0.276</td>
</tr>
<tr>
<td></td>
<td>Lopez-Meyer_IL_task1b_4</td>
<td>Paulo Lopez-Meyer</td>
<td>Intel Labs, Intel Corporation, Jalisco, Mexico</td>
<td>task-acoustic-scene-classification-results-b#Lopez-Meyer2020_t1b</td>
<td>89.7 (88.8 - 90.5)</td>
<td>0.983</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_1</td>
<td>Mark McDonnell</td>
<td>Computational Learning Systems Laboratory, University of South Australia, Mawson Lakes, Australia</td>
<td>task-acoustic-scene-classification-results-b#McDonnell2020</td>
<td>94.9 (94.9 - 95.0)</td>
<td>0.135</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_2</td>
<td>Mark McDonnell</td>
<td>Computational Learning Systems Laboratory, University of South Australia, Mawson Lakes, Australia</td>
<td>task-acoustic-scene-classification-results-b#McDonnell2020</td>
<td>95.5 (95.3 - 95.7)</td>
<td>0.118</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_3</td>
<td>Mark McDonnell</td>
<td>Computational Learning Systems Laboratory, University of South Australia, Mawson Lakes, Australia</td>
<td>task-acoustic-scene-classification-results-b#McDonnell2020</td>
<td>95.9 (95.7 - 96.1)</td>
<td>0.117</td>
</tr>
<tr>
<td></td>
<td>McDonnell_USA_task1b_4</td>
<td>Mark McDonnell</td>
<td>Computational Learning Systems Laboratory, University of South Australia, Mawson Lakes, Australia</td>
<td>task-acoustic-scene-classification-results-b#McDonnell2020</td>
<td>95.8 (95.6 - 96.0)</td>
<td>0.119</td>
</tr>
<tr>
<td></td>
<td>Monteiro_INRS_task1b_1</td>
<td>Monteiro Joao</td>
<td>EMT, Institut National de la Recherche Scientifique, Montreal, Canada</td>
<td>task-acoustic-scene-classification-results-b#Joao2020</td>
<td>87.4 (86.5 - 88.3)</td>
<td>0.327</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_Vfy_task1b_1</td>
<td>Javier Naranjo-Alcazar</td>
<td>AI department, Visualfy, Benisano, Spain; Computer Science Department, Universitat de Valencia, Burjassot, Spain</td>
<td>task-acoustic-scene-classification-results-b#Naranjo-Alcazar2020_t1</td>
<td>93.6 (93.4 - 93.7)</td>
<td>0.202</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_Vfy_task1b_2</td>
<td>Javier Naranjo-Alcazar</td>
<td>AI department, Visualfy, Benisano, Spain; Computer Science Department, Universitat de Valencia, Burjassot, Spain</td>
<td>task-acoustic-scene-classification-results-b#Naranjo-Alcazar2020_t1</td>
<td>93.6 (93.4 - 93.8)</td>
<td>0.190</td>
</tr>
<tr>
<td></td>
<td>NguyenHongDuc_SU_task1b_1</td>
<td>Paul Nguyen Hong Duc</td>
<td>Institut d’Alembert, Sorbonne Universite, Paris, France</td>
<td>task-acoustic-scene-classification-results-b#Nguyen_Hong_Duc2020</td>
<td>93.1 (92.6 - 93.5)</td>
<td>0.215</td>
</tr>
<tr>
<td></td>
<td>NguyenHongDuc_SU_task1b_2</td>
<td>Paul Nguyen Hong Duc</td>
<td>Institut d’Alembert, Sorbonne Universite, Paris, France</td>
<td>task-acoustic-scene-classification-results-b#Nguyen_Hong_Duc2020</td>
<td>92.3 (91.9 - 92.6)</td>
<td>0.214</td>
</tr>
<tr>
<td></td>
<td>Ooi_NTU_task1b_1</td>
<td>Kenneth Ooi</td>
<td>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore</td>
<td>task-acoustic-scene-classification-results-b#Ooi2020</td>
<td>87.8 (87.1 - 88.6)</td>
<td>0.337</td>
</tr>
<tr>
<td></td>
<td>Ooi_NTU_task1b_2</td>
<td>Kenneth Ooi</td>
<td>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore</td>
<td>task-acoustic-scene-classification-results-b#Ooi2020</td>
<td>87.3 (86.6 - 88.1)</td>
<td>0.367</td>
</tr>
<tr>
<td></td>
<td>Ooi_NTU_task1b_3</td>
<td>Kenneth Ooi</td>
<td>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore</td>
<td>task-acoustic-scene-classification-results-b#Ooi2020</td>
<td>89.8 (89.0 - 90.5)</td>
<td>0.257</td>
</tr>
<tr>
<td></td>
<td>Ooi_NTU_task1b_4</td>
<td>Kenneth Ooi</td>
<td>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore</td>
<td>task-acoustic-scene-classification-results-b#Ooi2020</td>
<td>89.8 (89.1 - 90.5)</td>
<td>0.305</td>
</tr>
<tr>
<td></td>
<td>Paniagua_UPM_task1b_1</td>
<td>Rubén Fraile</td>
<td>CITSEM, Universidad Politéctica de Madrid, Madrid, Spain</td>
<td>task-acoustic-scene-classification-results-b#Paniagua2020</td>
<td>89.4 (89.0 - 89.8)</td>
<td>0.347</td>
</tr>
<tr>
<td></td>
<td>Patki_SELF_task1b_1</td>
<td>Prachi Patki</td>
<td></td>
<td>task-acoustic-scene-classification-results-b#Patki2020</td>
<td>86.0 (85.8 - 86.3)</td>
<td>1.372</td>
</tr>
<tr>
<td></td>
<td>Patki_SELF_task1b_2</td>
<td>Prachi Patki</td>
<td></td>
<td>task-acoustic-scene-classification-results-b#Patki2020</td>
<td>89.4 (89.0 - 89.7)</td>
<td>0.951</td>
</tr>
<tr>
<td></td>
<td>Patki_SELF_task1b_3</td>
<td>Prachi Patki</td>
<td></td>
<td>task-acoustic-scene-classification-results-b#Patki2020</td>
<td>83.7 (81.8 - 85.7)</td>
<td>1.837</td>
</tr>
<tr>
<td></td>
<td>Phan_UIUC_task1b_1</td>
<td>Duc Phan</td>
<td>ECE, University of Illinois at Urbana Champaign, Illinois, USA</td>
<td>task-acoustic-scene-classification-results-b#Phan2020_t1</td>
<td>88.5 (87.8 - 89.2)</td>
<td>0.319</td>
</tr>
<tr>
<td></td>
<td>Phan_UIUC_task1b_2</td>
<td>Duc Phan</td>
<td>ECE, University of Illinois at Urbana Champaign, Illinois, USA</td>
<td>task-acoustic-scene-classification-results-b#Phan2020_t1</td>
<td>89.2 (88.7 - 89.8)</td>
<td>0.283</td>
</tr>
<tr>
<td></td>
<td>Phan_UIUC_task1b_3</td>
<td>Duc Phan</td>
<td>ECE, University of Illinois at Urbana Champaign, Illinois, USA</td>
<td>task-acoustic-scene-classification-results-b#Phan2020_t1</td>
<td>89.0 (88.7 - 89.3)</td>
<td>0.301</td>
</tr>
<tr>
<td></td>
<td>Phan_UIUC_task1b_4</td>
<td>Duc Phan</td>
<td>ECE, University of Illinois at Urbana Champaign, Illinois, USA</td>
<td>task-acoustic-scene-classification-results-b#Phan2020_t1</td>
<td>89.5 (88.9 - 90.0)</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>Sampathkumar_TUC_task1b_1</td>
<td>Arunodhayan Sampathkumar</td>
<td>Juniorprofessur MEDIA COMPUTING, Techniche universität Chemnitz, Chemnitz, Germany</td>
<td>task-acoustic-scene-classification-results-b#Sampathkumar2020</td>
<td>87.5 (87.1 - 87.9)</td>
<td>0.864</td>
</tr>
<tr>
<td></td>
<td>Singh_IITMandi_task1b_1</td>
<td>Arshdeep Singh</td>
<td>School of Computing and Electrical engineering, Indian institute of technology, Mandi, Mandi, India</td>
<td>task-acoustic-scene-classification-results-b#Singh2020</td>
<td>84.5 (83.5 - 85.6)</td>
<td>0.418</td>
</tr>
<tr>
<td></td>
<td>Singh_IITMandi_task1b_2</td>
<td>Arshdeep Singh</td>
<td>School of computing and electrical engineering, Indian institute of technology, Mandi, Mandi, India</td>
<td>task-acoustic-scene-classification-results-b#Singh2020</td>
<td>84.7 (83.5 - 85.9)</td>
<td>0.420</td>
</tr>
<tr>
<td></td>
<td>Singh_IITMandi_task1b_3</td>
<td>Arshdeep Singh</td>
<td>School of Computing and Electrical engineering, Indian institute of technology, Mandi, Mandi, India</td>
<td>task-acoustic-scene-classification-results-b#Singh2020</td>
<td>85.2 (84.6 - 85.8)</td>
<td>0.402</td>
</tr>
<tr>
<td></td>
<td>Singh_IITMandi_task1b_4</td>
<td>Arshdeep Singh</td>
<td>School of computing and electrical engineering, Indian institute of technology, Mandi, Mandi, India</td>
<td>task-acoustic-scene-classification-results-b#Singh2020</td>
<td>86.4 (85.0 - 87.8)</td>
<td>0.385</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1b_1</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Suh2020</td>
<td>93.3 (93.2 - 93.4)</td>
<td>0.302</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1b_2</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Suh2020</td>
<td>94.6 (94.4 - 94.7)</td>
<td>0.270</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1b_3</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Suh2020</td>
<td>95.1 (94.9 - 95.2)</td>
<td>0.277</td>
</tr>
<tr>
<td></td>
<td>Suh_ETRI_task1b_4</td>
<td>Youngho Jeong</td>
<td>Media Coding Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea</td>
<td>task-acoustic-scene-classification-results-b#Suh2020</td>
<td>94.6 (94.5 - 94.8)</td>
<td>0.271</td>
</tr>
<tr>
<td></td>
<td>Vilouras_AUTh_task1b_1</td>
<td>Konstantinos Vilouras</td>
<td>Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece</td>
<td>task-acoustic-scene-classification-results-b#Vilouras2020</td>
<td>91.8 (91.2 - 92.5)</td>
<td>0.215</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>Shefali Waldekar</td>
<td>Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India</td>
<td>task-acoustic-scene-classification-results-b#Waldekar2020</td>
<td>88.6 (88.2 - 89.1)</td>
<td>7.923</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1b_1</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-b#Wu2020_t1b</td>
<td>94.2 (94.0 - 94.3)</td>
<td>0.188</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1b_2</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-b#Wu2020_t1b</td>
<td>94.2 (94.1 - 94.3)</td>
<td>0.201</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1b_3</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-b#Wu2020_t1b</td>
<td>94.3 (94.3 - 94.4)</td>
<td>0.185</td>
</tr>
<tr>
<td></td>
<td>Wu_CUHK_task1b_4</td>
<td>Yuzhong Wu</td>
<td>Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China</td>
<td>task-acoustic-scene-classification-results-b#Wu2020_t1b</td>
<td>94.9 (94.7 - 95.1)</td>
<td>0.218</td>
</tr>
<tr>
<td></td>
<td>Yang_UESTC_task1b_1</td>
<td>Yang Haocong</td>
<td>Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-b#Haocong2020</td>
<td>92.1 (91.7 - 92.4)</td>
<td>0.272</td>
</tr>
<tr>
<td></td>
<td>Yang_UESTC_task1b_2</td>
<td>Yang Haocong</td>
<td>Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-b#Haocong2020</td>
<td>93.5 (93.3 - 93.7)</td>
<td>0.247</td>
</tr>
<tr>
<td></td>
<td>Yang_UESTC_task1b_3</td>
<td>Yang Haocong</td>
<td>Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-b#Haocong2020</td>
<td>93.5 (93.3 - 93.8)</td>
<td>0.228</td>
</tr>
<tr>
<td></td>
<td>Yang_UESTC_task1b_4</td>
<td>Yang Haocong</td>
<td>Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China</td>
<td>task-acoustic-scene-classification-results-b#Haocong2020</td>
<td>90.4 (88.7 - 92.0)</td>
<td>0.327</td>
</tr>
<tr>
<td></td>
<td>Zhang_BUPT_task1b_1</td>
<td>Jiawang Zhang</td>
<td>BUPT, Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-acoustic-scene-classification-results-b#Zhang2020</td>
<td>92.0 (91.6 - 92.4)</td>
<td>0.346</td>
</tr>
<tr>
<td></td>
<td>Zhang_BUPT_task1b_2</td>
<td>Jiawang Zhang</td>
<td>BUPT, Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-acoustic-scene-classification-results-b#Zhang2020</td>
<td>92.7 (92.1 - 93.2)</td>
<td>0.334</td>
</tr>
<tr>
<td></td>
<td>Zhang_BUPT_task1b_3</td>
<td>Jiawang Zhang</td>
<td>BUPT, Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-acoustic-scene-classification-results-b#Zhang2020</td>
<td>92.9 (92.3 - 93.5)</td>
<td>0.316</td>
</tr>
<tr>
<td></td>
<td>Zhang_BUPT_task1b_4</td>
<td>Jiawang Zhang</td>
<td>BUPT, Beijing University of Posts and Telecommunications, Beijing, China</td>
<td>task-acoustic-scene-classification-results-b#Zhang2020</td>
<td>93.0 (92.4 - 93.6)</td>
<td>0.316</td>
</tr>
<tr>
<td></td>
<td>Zhao_JNU_task1b_1</td>
<td>Jingqiao Zhao</td>
<td>Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China</td>
<td>task-acoustic-scene-classification-results-b#Zhao2020</td>
<td>86.6 (86.5 - 86.7)</td>
<td>0.867</td>
</tr>
<tr>
<td></td>
<td>Zhao_JNU_task1b_2</td>
<td>Jingqiao Zhao</td>
<td>Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China</td>
<td>task-acoustic-scene-classification-results-b#Zhao2020</td>
<td>86.9 (86.8 - 87.0)</td>
<td>0.873</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-info" href="/challenge2020/task-acoustic-scene-classification-results-b">subtask B results page</a></p>
<h2 id="submissions">Submissions</h2>
<table class="table">
<thead>
<tr>
<th>Subtask</th>
<th>Teams</th>
<th>Entries</th>
<th>Authors</th>
<th>Affiliations</th>
</tr>
</thead>
<tbody>
<tr>
<td>Subtask A</td>
<td>27</td>
<td>88</td>
<td>92</td>
<td>31</td>
</tr>
<tr>
<td>Subtask B</td>
<td>30</td>
<td>86</td>
<td>110</td>
<td>41</td>
</tr>
<tr>
<td>Overall</td>
<td>45</td>
<td>174</td>
<td>146</td>
<td>54</td>
</tr>
</tbody>
</table>
<h1 id="baseline-system">Baseline system</h1>
<p>The baseline system provides a state of the art approach for the classification in each subtask. The baseline system is built on <code>dcase_util</code> toolbox and has all needed functionality for the dataset handling, acoustic feature extraction, storing and accessing, acoustic model training and storing, and evaluation. The modular structure of the system enables participants to modify the system to their needs.</p>
<h2 id="repository">Repository</h2>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/toni-heittola/dcase2020_task1_baseline" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/toni-heittola/dcase2020_task1_baseline" target="_blank">
<span style="font-size:20px;">DCASE2020 Task 1 <strong>baseline</strong>, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="subtask-a-3">Subtask A</h2>
<p>The baseline system is a modification of the baselines from previous DCASE challenge editions of acoustic scene classification, built on the same skeleton. It replaces use of mel energies with use of <strong>OpenL3</strong> embeddings and replaces the CNN network architecture with two fully-connected feed-forward layers (size 512 and 128) as in the original OpenL3 publication:</p>
<div class="btex-item" data-item="Cramer_2019_ICASSP" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Cramer_2019_ICASSP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            J. Cramer, H-.H. Wu, J. Salamon, and J. P. Bello.
<em>Look, listen and learn more: design choices for deep audio embeddings.</em>
In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 3852–3856. Brighton, UK, May 2019.
URL: <a href="https://ieeexplore.ieee.org/document/8682475">https://ieeexplore.ieee.org/document/8682475</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" class="panel-collapse collapse" id="collapseCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" role="tabpanel">
<h4>Look, Listen and Learn More: Design Choices for Deep Audio Embeddings</h4>
<h5>Abstract</h5>
<p class="text-justify">A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L 3 -Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L 3 -Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3 -Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3 -Net embedding model as well as pre-trained models are made freely available online.</p>
<h5>Keywords</h5>
<p class="text-justify">audio signal processing;learning (artificial intelligence);signal classification;audio collections;labeled datasets;self-supervised learning;audio-visual correspondence;downstream audio classification tasks;downstream audio classifiers;audio-informed choices;deep learning;deep audio embeddings;net embedding model;net design choices;VGGish embeddings;SoundNet embeddings;Videos;Task analysis;Training;Computational modeling;Data models;Training data;Spectrogram;Audio classification;machine listening;deep audio embeddings;deep learning;transfer learning</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447label" class="modal fade" id="bibtexCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexCramer_2019_ICASSP25beaa0f1e614f6db18739893df98447label">Look, Listen and Learn More: Design Choices for Deep Audio Embeddings</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cramer_2019_ICASSP,
    Author = "Cramer, J. and Wu, H-.H. and Salamon, J. and Bello, J. P.",
    title = "Look, Listen and Learn More: Design Choices for Deep Audio Embeddings",
    booktitle = "IEEE Int.\textasciitilde Conf.\textasciitilde on Acoustics, Speech and Signal Processing (ICASSP)",
    year = "2019",
    month = "May",
    pages = "3852--3856",
    address = "Brighton, UK",
    abstract = "A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L 3 -Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L 3 -Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3 -Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3 -Net embedding model as well as pre-trained models are made freely available online.",
    keywords = "audio signal processing;learning (artificial intelligence);signal classification;audio collections;labeled datasets;self-supervised learning;audio-visual correspondence;downstream audio classification tasks;downstream audio classifiers;audio-informed choices;deep learning;deep audio embeddings;net embedding model;net design choices;VGGish embeddings;SoundNet embeddings;Videos;Task analysis;Training;Computational modeling;Data models;Training data;Spectrogram;Audio classification;machine listening;deep audio embeddings;deep learning;transfer learning",
    url = "https://ieeexplore.ieee.org/document/8682475"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h3>Parameters</h3>
<ul>
<li>Audio embeddings: <ul>
<li>OpenL3 embedding</li>
<li>Analysis window size: 1 second, Hop size: 100 ms</li>
<li>Embeddings parameters: <ul>
<li>Input representation: mel256</li>
<li>Content type: music</li>
<li>Embedding size: 512</li>
</ul>
</li>
</ul>
</li>
<li>Neural network:<ul>
<li>Architecture: Two fully connected layers (512 and 128 hidden units)</li>
<li>Learning: 200 epochs (batch size 64), data shuffling between epochs</li>
<li>Optimizer: Adam (learning rate 0.001)</li>
</ul>
</li>
<li>Model selection:<ul>
<li>Approximately 30% of the original training data is assigned to validation set, split done such that training and validation sets do not have segments from the same location and both sets have data from each city</li>
<li>Model performance after each epoch is evaluated on the validation set, and best performing model is selected</li>
</ul>
</li>
</ul>
<h3>Results for the development dataset</h3>
<p>Results are calculated using TensorFlow in GPU mode (using Nvidia Titan XP GPU card). Because results produced with GPU card are generally non-deterministic, the system was trained and tested 10 times; mean and standard deviation of the performance from these 10 independent trials are shown in the results tables.</p>
<p>The system is compared against the 2019 baseline system in order to put its performance into context with respect to the problem:</p>
<div class="table-responsive col-md-12">
<table class="table">
<thead>
<tr class="active">
<th class="col-md-4">System</th>
<th>Accuracy</th>
<th>Log loss</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>DCASE2019 Task 1 Baseline</td>
<td>46.5 %<br/><span class="text-mutes">(± 1.2)</span></td>
<td>1.578<br/><span class="text-muted">(± 0.029)</span></td>
<td><em>Log mel-band energies as features,<br/>two layers of 2D CNN and one fully connected layer as classifier,</em><br/><a class="link" href="http://dcase.community/challenge2019/task-acoustic-scene-classification#baseline-system">See more information</a></td>
</tr>
<tr class="success">
<td><strong>DCASE2020 Task 1 Baseline, Subtask A</strong></td>
<td><strong>54.1 %<br/><span class="text-muted">(± 1.4)</span></strong></td>
<td>1.365<br/><span class="text-muted">(± 0.032)</span></td>
<td><em>OpenL3 as audio embeddings,<br/>two fully connected layers as classifiers</em></td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p>Detailed results for the DCASE2020 baseline:</p>
<div class="table-responsive col-md-12">
<table class="table">
<thead>
<tr class="active">
<th>Scene label</th>
<th>Accuracy</th>
<th class="sep-left" colspan="9">Device-wise accuracies</th>
<th class="sep-left">Log loss</th>
</tr>
<tr class="active">
<th></th>
<th></th>
<th class="sep-left"><span class="label label-success">A</span></th>
<th><span class="label label-success">B</span></th>
<th><span class="label label-success">C</span></th>
<th><span class="label label-warning">S1</span></th>
<th><span class="label label-warning">S2</span></th>
<th><span class="label label-warning">S3</span></th>
<th><span class="label label-warning">S4</span></th>
<th><span class="label label-warning">S5</span></th>
<th><span class="label label-warning">S6</span></th>
<th class="sep-left"></th>
</tr>
</thead>
<tbody>
<tr>
<td>Airport</td>
<td>45.0 %</td>
<td class="sep-left">65.8</td>
<td>61.9</td>
<td>53.6</td>
<td>54.8</td>
<td>34.5</td>
<td>36.7</td>
<td>35.5</td>
<td>32.7</td>
<td>29.7</td>
<td class="sep-left">1.615</td>
</tr>
<tr>
<td>Bus</td>
<td>62.9 %</td>
<td class="sep-left">85.5</td>
<td>76.1</td>
<td>83.3</td>
<td>62.4</td>
<td>67.6</td>
<td>50.3</td>
<td>50.6</td>
<td>41.8</td>
<td>48.8</td>
<td class="sep-left">0.964</td>
</tr>
<tr>
<td>Metro</td>
<td>53.5 %</td>
<td class="sep-left">71.5</td>
<td>50.0</td>
<td>66.4</td>
<td>44.2</td>
<td>45.2</td>
<td>51.8</td>
<td>50.9</td>
<td>37.6</td>
<td>64.2</td>
<td class="sep-left">1.281</td>
</tr>
<tr>
<td>Metro station</td>
<td>53.0 %</td>
<td class="sep-left">63.6</td>
<td>45.5</td>
<td>44.5</td>
<td>49.4</td>
<td>50.3</td>
<td>63.6</td>
<td>50.9</td>
<td>53.0</td>
<td>56.4</td>
<td class="sep-left">1.298</td>
</tr>
<tr>
<td>Park</td>
<td>71.3 %</td>
<td class="sep-left">91.5</td>
<td>94.5</td>
<td>85.5</td>
<td>72.7</td>
<td>79.7</td>
<td>71.5</td>
<td>51.8</td>
<td>55.5</td>
<td>38.8</td>
<td class="sep-left">1.022</td>
</tr>
<tr>
<td>Public square</td>
<td>44.9 %</td>
<td class="sep-left">65.2</td>
<td>40.9</td>
<td>60.3</td>
<td>43.6</td>
<td>41.5</td>
<td>54.5</td>
<td>46.4</td>
<td>39.1</td>
<td>12.7</td>
<td class="sep-left">1.633</td>
</tr>
<tr>
<td>Shopping mall</td>
<td>48.3 %</td>
<td class="sep-left">60.9</td>
<td>63.0</td>
<td>57.9</td>
<td>47.6</td>
<td>57.3</td>
<td>31.8</td>
<td>22.4</td>
<td>51.2</td>
<td>42.4</td>
<td class="sep-left">1.482</td>
</tr>
<tr>
<td>Street, pedestrian</td>
<td>29.8 %</td>
<td class="sep-left">52.1</td>
<td>36.7</td>
<td>30.0</td>
<td>28.2</td>
<td>34.8</td>
<td>29.1</td>
<td>31.2</td>
<td>4.5</td>
<td>21.5</td>
<td class="sep-left">2.277</td>
</tr>
<tr>
<td>Street, traffic</td>
<td>79.9 %</td>
<td class="sep-left">82.1</td>
<td>84.2</td>
<td>86.4</td>
<td>86.4</td>
<td>73.9</td>
<td>77.0</td>
<td>84.2</td>
<td>86.7</td>
<td>58.5</td>
<td class="sep-left">0.731</td>
</tr>
<tr>
<td>Tram</td>
<td>52.2 %</td>
<td class="sep-left">67.9</td>
<td>53.6</td>
<td>58.4</td>
<td>60.3</td>
<td>48.2</td>
<td>50.6</td>
<td>57.9</td>
<td>49.7</td>
<td>23.3</td>
<td class="sep-left">1.350</td>
</tr>
<tr class="active">
<td><strong>Average</strong></td>
<td><strong>54.1 %<br/><span class="text-muted">(± 1.4)</span></strong></td>
<td class="sep-left">70.6</td>
<td>60.6</td>
<td>62.6</td>
<td>55.0</td>
<td>53.3</td>
<td>51.7</td>
<td>48.2</td>
<td>45.2</td>
<td>39.6</td>
<td class="sep-left">1.365<br/><span class="text-muted">(± 0.032)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p>As discussed <a href="#task-setup">here</a>, devices S4-S6 are used only for testing not for training the system.</p>
<p><strong>Note:</strong> The reported baseline system performance is not exactly reproducible due to varying setups. However, you should be able obtain very similar results.</p>
<h2 id="subtask-b-3">Subtask B</h2>
<p>In subtask B, the baseline system is similar to the DCASE2019 baseline. The system implements a convolutional neural network (CNN) based approach. Log mel-band energies are first extracted for each 10-second signal, and a network consisting of two CNN layers and one fully connected layer is trained to assign scene labels to the audio signals. The model size of the system is 450 KB.</p>
<h3>Parameters</h3>
<ul>
<li>Audio features:<ul>
<li>Log mel-band energies (40 bands), analysis frame 40 ms (50% hop size)</li>
</ul>
</li>
<li>Neural network:<ul>
<li>Input shape: 40 * 500 (10 seconds)</li>
<li>Architecture:<ul>
<li>CNN layer #1<ul>
<li>2D Convolutional layer (filters: 32, kernel size: 7) + Batch normalization + ReLu activation</li>
<li>2D max pooling (pool size: (5, 5)) + Dropout (rate: 30%)</li>
</ul>
</li>
<li>CNN layer #2<ul>
<li>2D Convolutional layer (filters: 64, kernel size: 7) + Batch normalization + ReLu activation</li>
<li>2D max pooling (pool size: (4, 100)) + Dropout (rate: 30%)</li>
</ul>
</li>
<li>Flatten</li>
<li>Dense layer #1<ul>
<li>Dense layer (units: 100, activation: ReLu )</li>
<li>Dropout (rate: 30%)</li>
</ul>
</li>
<li>Output layer (activation: softmax)</li>
</ul>
</li>
<li>Learning: 200 epochs (batch size 16), data shuffling between epochs</li>
<li>Optimizer: Adam (learning rate 0.001)</li>
</ul>
</li>
<li>Model selection:<ul>
<li>Approximately 30% of the original training data is assigned to validation set, split done such that training and validation sets do not have segments from the same location and both sets have data from each city</li>
<li>Model performance after each epoch is evaluated on the validation set, and best performing model is selected</li>
</ul>
</li>
</ul>
<h3>Results for the development dataset</h3>
<p>Results are calculated using TensorFlow in GPU mode (using Nvidia Titan XP GPU card). Because results produced with GPU card are generally non-deterministic, the system was trained and tested 10 times; mean and standard deviation of the performance from these 10 independent trials are shown in the results tables.</p>
<p>The system is compared against subtask A baseline system and a minified version of it in order to show performance at different model size levels. The modified version of subtask A baseline replaces the OpenL3 audio embeddings with EdgeL3 embeddings which are a sparse version of OpenL3, intended for low-complexity applications. More about EdgeL3 embeddings can be found here:</p>
<div class="btex-item" data-item="Kumari_2019_IPDPSW" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Kumari_2019_IPDPSW"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            S. <span class="bibtex-protected">Kumari</span>, D. <span class="bibtex-protected">Roy</span>, M. <span class="bibtex-protected">Cartwright</span>, J. P. <span class="bibtex-protected">Bello</span>, and A. <span class="bibtex-protected">Arora</span>.
<em>Edgel^3: compressing l^3-net for mote scale urban noise monitoring.</em>
In 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), volume, 877–884. May 2019.
<a href="https://doi.org/10.1109/IPDPSW.2019.00145">doi:10.1109/IPDPSW.2019.00145</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" class="panel-collapse collapse" id="collapseKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" role="tabpanel">
<h4>EdgeL^3: Compressing L^3-Net for Mote Scale Urban Noise Monitoring</h4>
<h5>Abstract</h5>
<p class="text-justify">Urban noise sensing in deeply embedded devices at the edge of the Internet of Things (IoT) is challenging not only because of the lack of sufficiently labeled training data but also because device resources are quite limited. Look, Listen, and Learn (L3), a recently proposed state-of-the-art transfer learning technique, mitigates the first challenge by training self-supervised deep audio embeddings through binary Audio-Visual Correspondence (AVC), and the resulting embeddings can be used to train a variety of downstream audio classification tasks. However, with close to 4.7 million parameters, the multi-layer L3-Net CNN is still prohibitively expensive to be run on small edge devices, such as "motes" that use a single microcontroller and limited memory to achieve long-lived self-powered operation. In this paper, we comprehensively explore the feasibility of compressing the L3-Net for mote-scale inference. We use pruning, ablation, and knowledge distillation techniques to show that the originally proposed L3-Net architecture is substantially overparameterized, not only for AVC but for the target task of sound classification as evaluated on two popular downstream datasets. Our findings demonstrate the value of fine-tuning and knowledge distillation in regaining the performance lost through aggressive compression strategies. Finally, we present EdgeL3, the first L3-Net reference model compressed by 1-2 orders of magnitude for real-time urban noise monitoring on resource-constrained edge devices, that can fit in just 0.4 MB of memory through half-precision floating point representation.</p>
<h5>Keywords</h5>
<p class="text-justify">audio signal processing;convolutional neural nets;environmental science computing;Internet of Things;learning (artificial intelligence);neural net architecture;noise pollution;signal classification;real-time urban noise monitoring;resource-constrained edge devices;IoT;AVC;downstream audio classification tasks;single microcontroller;mote-scale inference;knowledge distillation techniques;sound classification;compression strategies;binary audio-visual correspondence;self-supervised deep audio embeddings;EdgeL3;downstream datasets;mote scale urban noise monitoring;urban noise sensing;embedded devices;Internet of Things;look listen and learn;transfer learning technique;multilayer L3-Net CNN;L3-Net architecture;memory size 0.4 MByte;Task analysis;Sensors;Training;Convolution;Monitoring;Visualization;Feature extraction;edge network;pruning;convolutional neural nets;deep learning;audio embedding;transfer learning;finetuning;knowledge distillation</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2label" class="modal fade" id="bibtexKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexKumari_2019_IPDPSWe8c6f6de5132459eac7cb03ab6e389f2label">EdgeL^3: Compressing L^3-Net for Mote Scale Urban Noise Monitoring</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kumari_2019_IPDPSW,
    Author = "{Kumari}, S. and {Roy}, D. and {Cartwright}, M. and {Bello}, J. P. and {Arora}, A.",
    booktitle = "2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)",
    title = "EdgeL^3: Compressing L^3-Net for Mote Scale Urban Noise Monitoring",
    year = "2019",
    volume = "",
    number = "",
    pages = "877-884",
    abstract = {Urban noise sensing in deeply embedded devices at the edge of the Internet of Things (IoT) is challenging not only because of the lack of sufficiently labeled training data but also because device resources are quite limited. Look, Listen, and Learn (L3), a recently proposed state-of-the-art transfer learning technique, mitigates the first challenge by training self-supervised deep audio embeddings through binary Audio-Visual Correspondence (AVC), and the resulting embeddings can be used to train a variety of downstream audio classification tasks. However, with close to 4.7 million parameters, the multi-layer L3-Net CNN is still prohibitively expensive to be run on small edge devices, such as "motes" that use a single microcontroller and limited memory to achieve long-lived self-powered operation. In this paper, we comprehensively explore the feasibility of compressing the L3-Net for mote-scale inference. We use pruning, ablation, and knowledge distillation techniques to show that the originally proposed L3-Net architecture is substantially overparameterized, not only for AVC but for the target task of sound classification as evaluated on two popular downstream datasets. Our findings demonstrate the value of fine-tuning and knowledge distillation in regaining the performance lost through aggressive compression strategies. Finally, we present EdgeL3, the first L3-Net reference model compressed by 1-2 orders of magnitude for real-time urban noise monitoring on resource-constrained edge devices, that can fit in just 0.4 MB of memory through half-precision floating point representation.},
    keywords = "audio signal processing;convolutional neural nets;environmental science computing;Internet of Things;learning (artificial intelligence);neural net architecture;noise pollution;signal classification;real-time urban noise monitoring;resource-constrained edge devices;IoT;AVC;downstream audio classification tasks;single microcontroller;mote-scale inference;knowledge distillation techniques;sound classification;compression strategies;binary audio-visual correspondence;self-supervised deep audio embeddings;EdgeL3;downstream datasets;mote scale urban noise monitoring;urban noise sensing;embedded devices;Internet of Things;look listen and learn;transfer learning technique;multilayer L3-Net CNN;L3-Net architecture;memory size 0.4 MByte;Task analysis;Sensors;Training;Convolution;Monitoring;Visualization;Feature extraction;edge network;pruning;convolutional neural nets;deep learning;audio embedding;transfer learning;finetuning;knowledge distillation",
    doi = "10.1109/IPDPSW.2019.00145",
    issn = "null",
    month = "May"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>Systems:</p>
<div class="table-responsive col-md-12">
<table class="table">
<thead>
<tr class="active">
<th class="col-md-4">System</th>
<th>Accuracy</th>
<th>Log loss</th>
<th class="sep-left">Audio embedding</th>
<th>Acoustic model</th>
<th class="sep-left">Total size</th>
</tr>
</thead>
<tbody>
<tr>
<td>DCASE2020 Task 1 Baseline, Subtask A<br/><em>OpenL3 + MLP (2 layers, 512 and 128 units)</em></td>
<td>89.8 %<br/><span class="text-muted">(± 0.3)</span></td>
<td>0.266<br/><span class="text-muted">(± 0.006)</span></td>
<td class="sep-left">17.87 MB</td>
<td>145.2 KB</td>
<td class="sep-left">19.12 MB</td>
</tr>
<tr>
<td>Modified DCASE2020 Task 1 Baseline, Subtask A<br/><em>EdgeL3 + MLP (2 layers, 64 units each)</em></td>
<td>88.9 %<br/><span class="text-muted">(± 0.3)</span></td>
<td>0.298<br/><span class="text-muted">(± 0.003)</span></td>
<td class="sep-left">840.6 KB</td>
<td>145.2 KB</td>
<td class="sep-left">985.8 KB</td>
</tr>
<tr class="success">
<td><strong>DCASE2020 Task 1 Baseline, Subtask B</strong><br/><em>Log mel-band energies + CNN (2 CNN layers and 1 fully-connected)</em></td>
<td>87.3 %<br/><span class="text-muted">(± 0.7)</span></td>
<td>0.437<br/><span class="text-muted">(± 0.045)</span></td>
<td class="sep-left">-</td>
<td>450.1 KB</td>
<td class="sep-left">450 KB</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p>Detailed results for the DCASE2020 Task 1 Baseline, Subtask B:</p>
<div class="table-responsive col-md-12">
<table class="table">
<thead>
<tr class="active">
<th>Class label</th>
<th>Accuracy</th>
<th>Log loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>Indoor</td>
<td>82.0 %</td>
<td>0.680</td>
</tr>
<tr>
<td>Outdoor</td>
<td>88.5 %</td>
<td>0.365</td>
</tr>
<tr>
<td>Transportation</td>
<td>91.5 %</td>
<td>0.282</td>
</tr>
<tr class="active">
<td><strong>Average</strong></td>
<td><strong>87.3 %<br/><span class="text-muted">(± 0.7)</span></strong></td>
<td>0.437<br/><span class="text-muted">(± 0.045)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<p><strong>Note:</strong> The reported baseline system performance is not exactly reproducible due to varying setups. However, you should be able obtain very similar results.</p>
<h1 id="citation">Citation</h1>
<p>If you are participating to this task or using the dataset or baseline code please cite the following paper:</p>
<div class="btex-item" data-item="Heittola2020" data-source="content/data/challenge2020/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Heittola2020"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Toni Heittola, Annamaria Mesaros, and Tuomas Virtanen.
<em>Acoustic scene classification in dcase 2020 challenge: generalization across devices and low complexity solutions.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020), 56–60. 2020.
URL: <a href="https://arxiv.org/abs/2005.14623">https://arxiv.org/abs/2005.14623</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexHeittola2020f4183abf792a4d52b4d75f6908c127b9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://arxiv.org/pdf/2005.14623" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseHeittola2020f4183abf792a4d52b4d75f6908c127b9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseHeittola2020f4183abf792a4d52b4d75f6908c127b9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingHeittola2020f4183abf792a4d52b4d75f6908c127b9" class="panel-collapse collapse" id="collapseHeittola2020f4183abf792a4d52b4d75f6908c127b9" role="tabpanel">
<h4>Acoustic scene classification in DCASE 2020 Challenge: generalization across devices and low complexity solutions</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents the details of Task 1: Acoustic Scene Classification in the DCASE 2020 Challenge. The task consists of two subtasks: classification of data from multiple devices, requiring good generalization properties, and classification using low-complexity solutions. Here we describe the datasets and baseline systems. After the challenge submission deadline, challenge results and analysis of the submissions will be added.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexHeittola2020f4183abf792a4d52b4d75f6908c127b9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://arxiv.org/pdf/2005.14623" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexHeittola2020f4183abf792a4d52b4d75f6908c127b9label" class="modal fade" id="bibtexHeittola2020f4183abf792a4d52b4d75f6908c127b9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexHeittola2020f4183abf792a4d52b4d75f6908c127b9label">Acoustic scene classification in DCASE 2020 Challenge: generalization across devices and low complexity solutions</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Heittola2020,
    author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas",
    title = "Acoustic scene classification in DCASE 2020 Challenge: generalization across devices and low complexity solutions",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)",
    year = "2020",
    pages = "56--60",
    abstract = "This paper presents the details of Task 1: Acoustic Scene Classification in the DCASE 2020 Challenge. The task consists of two subtasks: classification of data from multiple devices, requiring good generalization properties, and classification using low-complexity solutions. Here we describe the datasets and baseline systems. After the challenge submission deadline, challenge results and analysis of the submissions will be added.",
    url = "https://arxiv.org/abs/2005.14623"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/>
<br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>