<!DOCTYPE html><html lang="en">
<head>
    <title>Workshop schedule - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/workshop2024/schedule">
        <meta name="author" content="DCASE2024" />
        <meta name="description" content="Wednesday 23rd October Day 1 Location: Shinagawa Season Terrace 12:00 RegistrationDesk opens 12:00 and will be open until 17:30. 13:00 Welcome 13:10 Keynote IChair: Nobutaka Ono Nancy F. Chen Institute for Infocomm Research (I2R), Agency for Science, Technology, and Research (A*STAR) K1 â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2024</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="/workshop2024/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="btn-group  active">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-list fa-fw"></i>&nbsp;Program&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/workshop2024/schedule"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Schedule</a>
    </li>
            <li class="">
        <a href="/workshop2024/keynotes"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Keynotes</a>
    </li>
            <li class="">
        <a href="/workshop2024/Poster_Session_I"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Poster Session I</a>
    </li>
            <li class="">
        <a href="/workshop2024/Poster_Session_II"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Poster Session II</a>
    </li>
            <li class="">
        <a href="/workshop2024/Poster_Session_III"><i class="fa fa-list-alt fa-fw"></i>&nbsp;Poster Session III</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="/workshop2024/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="">
        <a href="/workshop2024/registration"><i class="fa fa-key fa-fw"></i>&nbsp;Registration</a>
    </li><li class="">
        <a href="/workshop2024/travel"><i class="fa fa-info fa-fw"></i>&nbsp;Venue & Travel</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/workshop2024/submission"><i class="fa fa-upload fa-fw"></i>&nbsp;Submission</a>
    </li>
            <li class="">
        <a href="/workshop2024/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="/workshop2024/presenter-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Presenters</a>
    </li>
            <li class="">
        <a href="/workshop2024/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
        </ul>
    </li><li class="">
        <a href="/workshop2024/reviewer-guidelines"><i class="fa fa-sticky-note-o fa-fw"></i>&nbsp;Reviewers</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="/workshop2024/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/building-bangkok-01.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><h1 class="bold">Workshop schedule</h1></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#wednesday-23rd-october">Wednesday 23rd October</a>
<ul>
<li><a href="#registration">Registration</a></li>
<li><a href="#welcome">Welcome</a></li>
<li><a href="#keynote-i">Keynote I</a></li>
<li><a href="#challenge-task-spotlights">Challenge task Spotlights</a></li>
<li><a href="#coffee-break">Coffee break</a></li>
<li><a href="#oral-session-i">Oral Session I</a></li>
<li><a href="#end-of-day-1">End of day 1</a></li>
</ul>
</li>
<li><a href="#thursday-24th-october">Thursday 24th October</a>
<ul>
<li><a href="#keynote-ii">Keynote II</a></li>
<li><a href="#coffee-break-1">Coffee break</a></li>
<li><a href="#poster-session-i">Poster Session I</a></li>
<li><a href="#lunch">Lunch</a></li>
<li><a href="#oral-session-ii">Oral Session II</a></li>
<li><a href="#coffee-break-2">Coffee break</a></li>
<li><a href="#poster-session-ii">Poster Session II</a></li>
<li><a href="#end-of-day-2">End of day 2</a></li>
</ul>
</li>
<li><a href="#friday-25th-october">Friday 25th October</a>
<ul>
<li><a href="#keynote-iii">Keynote III</a></li>
<li><a href="#coffee-break-3">Coffee break</a></li>
<li><a href="#poster-session-iii">Poster Session III</a></li>
<li><a href="#lunch-1">Lunch</a></li>
<li><a href="#oral-session-iii">Oral Session III</a></li>
<li><a href="#coffee-break-4">Coffee break</a></li>
<li><a href="#town-hall-discussion">Town Hall Discussion</a></li>
<li><a href="#closing-remarks">Closing Remarks</a></li>
</ul>
</li></ul></div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="wednesday-23rd-october">Wednesday 23rd October</h1>
<div class="row">
<div class="panel panel-default">
<div class="panel-heading">Day 1 <span class="pull-right text-muted">Location: <a class="link" href="https://shinagawa-st.jp/en/" target="_blank">Shinagawa Season Terrace</a></span></div>
<table class="table table-condensed">
<tbody>
<tr>
<td>12:00</td>
<td class="active" colspan="2"><h2 id="registration">Registration</h2>Desk opens 12:00 and will be open until 17:30.</td>
</tr>
<tr>
<td>13:00</td>
<td class="warning"><h2 id="welcome">Welcome</h2></td>
<td></td>
</tr>
<tr>
<td style="width:60px;">13:10</td>
<td class="danger" style="width:100px;"><h2 id="keynote-i">Keynote I</h2><br/><em class="text-muted small">Chair: Nobutaka Ono</em></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td>
<a data-toggle="collapse" href="#keynote1" title="Show abstract">Nancy F. Chen<br/>
<span class="text-muted"><em>Institute for Infocomm Research (I2R), Agency for Science, Technology, and Research (A*STAR)</em></span></a>
</td>
<td class="col-md-2">
<div class="btn-group pull-right">
<a class="btn btn-success item-tag2" data-toggle="collapse" href="#keynote1" title="Show abstract">K1</a>
</div>
</td>
</tr>
<tr>
<td colspan="2">
<a data-toggle="collapse" href="#keynote1" title="Show abstract"><strong>Multimodal, Multilingual Generative AI: From Multicultural Contextualization to Empathetic Reasoning</strong></a><br/>
<div class="collapse" id="keynote1">
<strong class="abstract">Abstract</strong>
<p>We will share about MeraLion (Multimodal Empathetic Reasoning and Learning In One Network), our generative AI efforts in Singaporeâ€™s National Multimodal Large Language Model Programme. Speech and audio information is rich in providing more comprehensive understanding of spatial and temporal reasoning in addition to social dynamics that goes beyond semantics derived from text alone. Cultural nuances and multilingual peculiarities add another layer of complexity in understanding human interactions. In addition, we will draw use cases in education to highlight research endeavors, technology deployment experience and application opportunities. </p>
<a class="btn btn-success btn-xs" href="/workshop2024/keynotes#nancy-f-chen" style="">More info <i aria-hidden="true" class="fa fa-chevron-circle-right"></i></a>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td></td>
<td class="active" colspan="2"><h2></h2></td>
</tr>
<tr>
<td>14:20</td>
<td class="tutorial"><h2 id="challenge-task-spotlights">Challenge task Spotlights</h2><br/><em class="text-muted small">Chair: Annamaria Mesaros</em><br/><br/></td>
<td></td>
</tr>
<tr>
<td>15:20</td>
<td class="active" colspan="2"><h2 id="coffee-break">Coffee break</h2></td>
</tr>
<tr>
<td>15:50</td>
<td class="tutorial"><h2 id="oral-session-i">Oral Session I</h2><br/><em class="text-muted small">Chair: Irene Martin Morato</em></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td>
<a data-toggle="collapse" href="#O1-1" title="Show abstract"><strong>Acoustic Scene Classification Across Multiple Devices Through Incremental Learning of Device-Specific Domains</strong>
<div class="authors">Manjunath  Mulimani  (Tampere University ), Annamaria Mesaros (Tampere University)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O1-1">
<strong class="abstract">Abstract</strong>
<p>In this paper, we propose using a domain-incremental learning approach for coping with different devices in acoustic scene classification. While the typical way to handle mismatched training data is through domain adaptation or specific regularization techniques, incremental learning offers a different approach. With this technique, it is possible to learn the characteristics of new devices on-the-go, adding to a previously trained model. This also means that new device data can be introduced at any time, without a need to retrain the original model. In terms of incremental learning, we propose a combination of domain-specific Low-Rank Adaptation (LoRA) parameters and running statistics of Batch Normalization (BN) layers. LoRA adds low-rank decomposition matrices to a convolutional layer with a few trainable parameters for each new device, while domain-specific BN is used to boost performance. Experiments are conducted on the TAU Urban Acoustic Scenes 2020 Mobile development dataset, containing 9 different devices; we train the system using the 40h of data available for the main device, and incrementally learn the domains of the other 8 devices based on 3h of data available for each. We show that the proposed approach outperforms other fine-tuning-based methods, and is outperformed only by joint learning with all data from all devices. </p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O1-2" title="Show abstract"><strong>Leveraging Self-Supervised Audio Representations for Data-Efficient Acoustic Scene Classification</strong>
<div class="authors">Yiqiang Cai (Xi'an Jiaotong-Liverpool University), Shengchen Li (Xi'an Jiaotong-Liverpool University), Xi Shao (Nanjing University of Posts and Telecommunications)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O1-2">
<strong class="abstract">Abstract</strong>
<p>Acoustic scene classification (ASC) predominantly relies on supervised approaches. However, acquiring labeled data for training ASC models is often costly and time-consuming. Recently, self-supervised learning (SSL) has emerged as a powerful method for extracting features from unlabeled audio data, benefiting many downstream audio tasks. This paper proposes a data-efficient and low-complexity ASC system by leveraging self-supervised audio representations extracted from general-purpose audio datasets. We introduce BEATs, an audio SSL pre-trained model, to extract the general representations from AudioSet. Through extensive experiments, it has been demonstrated that the self-supervised audio representations can help to achieve high ASC accuracy with limited labeled fine-tuning data. Furthermore, we find that ensembling the SSL models fine-tuned with different strategies contributes to a further performance improvement. To meet low-complexity requirements, we use knowledge distillation to transfer the self-supervised knowledge from large teacher models to an efficient student model. The experimental results suggest that the self-supervised teachers effectively improve the classification accuracy of the student model. Our best-performing system obtains an average accuracy of 56.7%.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O1-3" title="Show abstract"><strong>Frequency Tracking Features for Data-Efficient Deep Siren Identification</strong>
<div class="authors">Stefano Damiano (KU Leuven), Thomas Dietzen (KU Leuven), Toon van Waterschoot (Department of Electrical Engineering (ESAT-STADIUS/ETC))</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O1-3">
<strong class="abstract">Abstract</strong>
<p>The identification of siren sounds in urban soundscapes is a crucial safety aspect for smart vehicles and has been widely addressed by means of neural networks that ensure robustness to both the diversity of siren signals and the strong and unstructured background noise characterizing traffic. Convolutional neural networks analyzing spectrogram features of incoming signals achieve state-of-the-art performance when enough training data capturing the diversity of the target acoustic scenes is available. In practice, data is usually limited and algorithms should be robust to adapt to unseen acoustic conditions without requiring extensive datasets for re-training. In this work, given the harmonic nature of siren signals, characterized by a periodically evolving fundamental frequency, we propose a low-complexity feature extraction method based on frequency tracking using a single-parameter adaptive notch filter. The features are then used to design a small-scale convolutional network suitable for training with limited data. The evaluation results indicate that the proposed model consistently outperforms the traditional spectrogram-based model when limited training data is available, achieves better cross-domain generalization and has a smaller size.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O1-4" title="Show abstract"><strong>Acoustic-Based Traffic Monitoring with Neural Network Trained by Matching Loss for Ranking Function</strong>
<div class="authors">Tomohiro Takahashi (Tokyo Metropolitan University), Natsuki Ueno (Kumamoto University), Yuma Kinoshita (Tokai University), Yukoh Wakabayashi (Toyohashi University of Technology), Nobutaka Ono (Tokyo Metropolitan University), Makiho Sukekawa (NEXCO-EAST ENGINEERING Company Limited), Seishi Fukuma (NEXCO-EAST ENGINEERING Company Limited), Hiroshi Nakagawa (NEXCO-EAST ENGINEERING Company Limited)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O1-4">
<strong class="abstract">Abstract</strong>
<p>In this study, we propose an effective loss function for training neural networks (NNs) in acoustic-based traffic monitoring. This task involves estimating the number of vehicles from a fixed duration of acoustic input, such as one minute. Since the distribution of the number of passing vehicles depends on the road and can deviate significantly from a normal distribution, using Mean Square Error (MSE) as the loss function may not always lead to efficient learning. To address this, we introduce a matching loss for the ranking function into the loss function. This enhances learning by increasing the rank correlation between true and estimated vehicle counts. We evaluated the effectiveness of this loss function on the development dataset of the DCASE 2024 Challenge Task 10 under various input feature and network architecture conditions. The results demonstrate that the proposed loss function significantly improves Kendall's Tau Rank Correlation (KTRC) and Root Mean Square Error (RMSE), highlighting its potential for improving acoustic-based traffic monitoring systems.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O1-5" title="Show abstract"><strong>The Language of Sound Search: Examining User Queries in Audio Search Engines</strong>
<div class="authors">Benno Weck (Music Technology Group, Universitat Pompeu Fabra (UPF)), Frederic Font (Music Technology Group - Universitat Pompeu Fabra)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O1-5">
<strong class="abstract">Abstract</strong>
<p>This study examines textual, user-written search queries within the context of sound search engines, encompassing various applications such as foley, sound effects, and general audio retrieval.
Current research inadequately addresses real-world user needs and behaviours in designing text-based audio retrieval systems.
To bridge this gap, we analysed search queries from two sources: a custom survey and Freesound website query logs.
The survey was designed to collect queries for an unrestricted, hypothetical sound search engine, resulting in a dataset that captures user intentions without the constraints of existing systems. This dataset is also made available for sharing with the research community.
In contrast, the Freesound query logs encompass approximately 9 million search requests, providing a comprehensive view of real-world usage patterns.
Our findings indicate that survey queries are generally longer than Freesound queries, suggesting users prefer detailed queries when not limited by system constraints.
Both datasets predominantly feature keyword-based queries, with few survey participants using full sentences.
Key factors influencing survey queries include the primary sound source, intended usage, perceived location, and the number of sound sources.
These insights are crucial for developing user-centred, effective text-based audio retrieval systems, enhancing our understanding of user behaviour in sound search contexts.</p>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>17:30</td>
<td class="active" colspan="2"><h2 id="end-of-day-1">End of day 1</h2></td>
</tr>
<tr>
<td>18:00 - 20:00</td>
<td class="social" colspan="2"><h2></h2>Welcome reception<br/>Location: Shinagawa Season Terrace (conference venue)</td>
</tr>
</tbody>
</table>
</div>
</div>
<h1 id="thursday-24th-october">Thursday 24th October</h1>
<div class="row">
<div class="panel panel-default">
<div class="panel-heading">Day 2 <span class="pull-right text-muted">Location: <a class="link" href="https://shinagawa-st.jp/en/" target="_blank">Shinagawa Season Terrace</a></span></div>
<table class="table table-condensed">
<tbody>
<tr>
<td style="width:60px;">9:30</td>
<td class="danger" style="width:100px;"><h2 id="keynote-ii">Keynote II</h2><br/><em class="text-muted small">Chair: Yohei Kawaguchi</em></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td>
<a data-toggle="collapse" href="#keynote2" title="Show abstract">Bourhan Yassin<br/>
<span class="text-muted"><em></em></span></a>
</td>
<td class="col-md-2">
<div class="btn-group pull-right">
<a class="btn btn-success item-tag2" data-toggle="collapse" href="#keynote2" title="Show abstract">K2</a>
</div>
</td>
</tr>
<tr>
<td colspan="2">
<a data-toggle="collapse" href="#keynote2" title="Show abstract"><strong>The Future of Bioacoustics and AI for Large-Scale Biodiversity Monitoring</strong></a><br/>
<div class="collapse" id="keynote2">
<strong class="abstract">Abstract</strong>
<p>Sound is an invaluable tool in the discovery and preservation of species, offering insights that other data collection methodologies often overlook. In the first half of this keynote, we will explore the power of acoustic monitoring, particularly in biodiversity conservation, where AI and sound classification techniques enable near real-time identification of species vocalizations. By leveraging feature embeddings to streamline data processing, these methods allow for accurate species detection and classification, reducing the complexity of handling large numbers of of raw audio files. In the second half, we will focus on the future of ground data collection through the use of Unmanned Aerial Vehicles (UAVs). UAVs present a powerful opportunity to scale ground truth data collection, providing continuous monitoring of rapidly changing ecosystems. This enhanced data gathering, when combined with AI, enables the development of more precise biodiversity indicators and predictions. Together, these innovations expand our ability to monitor ecosystems and protect wildlife at unprecedented scales.</p>
<a class="btn btn-success btn-xs" href="/workshop2024/keynotes#bourhan-yassin" style="">More info <i aria-hidden="true" class="fa fa-chevron-circle-right"></i></a>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>10:30</td>
<td class="active" colspan="2"><h2 id="coffee-break-1">Coffee break</h2></td>
</tr>
<tr>
<td>11:00</td>
<td class="success"><h2 id="poster-session-i">Poster Session I</h2></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td style="border-top: none;padding-top:0px;">
<p><a class="btn btn-danger btn-sm" href="/workshop2024/Poster_Session_I" style="">List of posters</a></p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>12:20</td>
<td class="active" colspan="2"><h2 id="lunch">Lunch</h2></td>
</tr>
<tr>
<td>13:40</td>
<td class="tutorial"><h2 id="oral-session-ii">Oral Session II</h2><br/><em class="text-muted small">Chair: Keisuke Imoto</em><br/><br/></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td>
<a data-toggle="collapse" href="#O2-1" title="Show abstract"><strong>AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks</strong>
<div class="authors">Kevin Wilkinghoff (MERL)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O2-1">
<strong class="abstract">Abstract</strong>
<p>The state-of-the-art approach for semi-supervised anomalous sound detection is to first learn an embedding space by using auxiliary classification tasks based on meta information or self-supervised learning and then estimate the distribution of normal data. In this work, AdaProj a novel loss function for training the embedding model is presented. In contrast to commonly used angular margin losses, which project data of each class as close as possible to their corresponding class centers, AdaProj learns to project data onto class-specific subspaces while still ensuring an angular margin between classes. By doing so, the resulting distributions of the embeddings belonging to normal data are not required to be as restrictive as other loss functions allowing a more detailed view on the data. In experiments conducted on the DCASE2022 and DCASE2023 anomalous sound detection datasets, it is shown that using AdaProj to learn an embedding space significantly outperforms other commonly used loss functions.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O2-2" title="Show abstract"><strong>Improving Domain Generalisation with Diversity-Based Sampling</strong>
<div class="authors">Andrea Napoli (University of Southampton), Paul White (ISVR)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O2-2">
<strong class="abstract">Abstract</strong>
<p>Domain shifts are a major obstacle to the deployment of automated bioacoustic monitoring tools to new recording environments or habitats. Invariance regularisation is one approach for dealing with these shifts, in which the feature distributions of data from different domains are encouraged to match (by minimising some measure of statistical distance). However, in a deep learning setup, the statistical distance is only computed over small minibatches of data at a time. Inevitably, small samples have poor representation of their underlying distributions, resulting in extremely noisy distance estimates. In this paper, we propose that promoting wider distribution coverage, by inducing diversity in each sampled minibatch, would improve these estimates, and hence the generalisation power of the trained model. We describe two options for diversity-based data samplers, based on the k-determinantal point process (k-DPP) and the k-means++ algorithm, which can function as drop-in replacements for a standard random sampler. We then test these on a domain shift task based on humpback whale detection, where we find both options improve the performance of two invariance regularisation algorithms, as well as standard training via empirical risk minimisation (ERM).</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O2-3" title="Show abstract"><strong>Integrating Continuous and Binary Relevances in Audio-Text Relevance Learning</strong>
<div class="authors">Huang Xie (Tampere University), Khazar Khorrami (Tampere University), Okko RÃ¤sÃ¤nen (Tampere University), Tuomas Virtanen (Tampere University)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O2-3">
<strong class="abstract">Abstract</strong>
<p>Audio-text relevance learning refers to learning the shared semantic properties of audio samples and textual descriptions. The standard approach uses binary relevances derived from pairs of audio samples and their human-provided captions, categorizing each pair as either positive or negative. This may result in suboptimal systems due to varying levels of relevance between audio samples and captions. In contrast, a recent study used human-assigned relevance ratings, i.e., continuous relevances, for these pairs but did not obtain performance gains in audio-text relevance learning. This work introduces a relevance learning method that utilizes both human-assigned continuous relevance ratings and binary relevances using a combination of a listwise ranking objective and a contrastive learning objective. Experimental results demonstrate the effectiveness of the proposed method, showing improvements in language-based audio retrieval, a downstream task in audio-text relevance learning. In addition, we analyze how properties of the captions or audio clips contribute to the continuous audio-text relevances provided by humans or learned by the machine.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O2-4" title="Show abstract"><strong>Estimated Audioâ€“Caption Correspondences Improve Language-Based Audio Retrieval</strong>
<div class="authors">Paul Primus (Johannes Kepler University), Florian Schmid (Johannes Kepler University), Gerhard Widmer (Johannes Kepler University)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O2-4">
<strong class="abstract">Abstract</strong>
<p>Dual-encoder-based audio retrieval systems are commonly optimized with contrastive learning on a set of matching and mismatching audioâ€“caption pairs. This leads to a shared embedding space in which corresponding items from the two modalities end up close together. Since audio--caption datasets typically only contain matching pairs of recordings and descriptions, it has become common practice to create mismatching pairs by pairing the audio with a caption randomly drawn from the dataset. This is not ideal because the randomly sampled caption could, just by chance, partly or entirely describe the audio recording. However, correspondence information for all possible pairs is costly to annotate and thus typically unavailable; we, therefore, suggest substituting it with estimated correspondences. To this end, we propose a two-staged training procedure in which multiple retrieval models are first trained as usual, i.e., without estimated correspondences. In the second stage, the audio-caption correspondences predicted by these models then serve as prediction targets. We evaluate our method on the ClothoV2 and the AudioCaps benchmark and show that it improves retrieval performance, even in a restricting self-distillation setting where a single model generates and then learns from the estimated correspondences. We further show that our method outperforms the current state of the art by 1.6 pp. mAP@10 on the ClothoV2 benchmark.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O2-5" title="Show abstract"><strong>Guided Captioning of Audio</strong>
<div class="authors">Irene Martin (Tampere University), James O Afolaranmi (Tampere University), Annamaria Mesaros (Tampere University)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O2-5">
<strong class="abstract">Abstract</strong>
<p>This work introduces a guided captioning system that aims to produce captions focused on different audio content, depending on a guiding text. We show that using keywords guidance results in more diverse captions, even though the usual captioning metrics do not reflect this. We design a system that can be trained using keywords automatically extracted from reference annotations, and which is provided with one keyword at test time. When trained with 5 keywords, the produced captions contain the exact guidance keyword 70% of the time, and results in over 3600 unique sentences for Clotho dataset. In contrast, a baseline without any keywords produces 700 unique captions on the same test set.</p>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>15:20</td>
<td class="active" colspan="2"><h2 id="coffee-break-2">Coffee break</h2></td>
</tr>
<tr>
<td>15:50</td>
<td class="success"><h2 id="poster-session-ii">Poster Session II</h2></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td style="border-top: none;padding-top:0px;">
<p><a class="btn btn-danger btn-sm" href="/workshop2024/Poster_Session_II" style="">List of posters</a></p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>17:10</td>
<td class="active" colspan="2"><h2 id="end-of-day-2">End of day 2</h2></td>
</tr>
<tr>
<td>17:30</td>
<td class="social" colspan="2"><h2>Bus<h2></h2></h2></td>
</tr>
<tr>
<td>18:30 - 21:00</td>
<td class="social" colspan="2"><h2>Banquet<h2>
<a class="link" href="https://www.symphony-cruise.co.jp/" target="_blank">Tokyo Bay Cruise</a> (address: <a class="link" href="https://www.google.com/maps/place/%E3%82%B7%E3%83%B3%E3%83%95%E3%82%A9%E3%83%8B%E3%83%BC%E3%82%AF%E3%83%AB%E3%83%BC%E3%82%BA/@35.650608,139.760413,15z/data=!4m6!3m5!1s0x60188bcdc90d6721:0x2324854e4fa9520c!8m2!3d35.6506084!4d139.7604131!16s%2Fg%2F1tg5zmjn?hl=ja&amp;entry=ttu&amp;g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D" target="_blank">2-7-104 Kaigan, Minato-ku</a>).</h2></h2></td>
</tr>
</tbody>
</table>
</div>
</div>
<h1 id="friday-25th-october">Friday 25th October</h1>
<div class="row">
<div class="panel panel-default">
<div class="panel-heading">Day 3 <span class="pull-right text-muted">Location: <a class="link" href="https://shinagawa-st.jp/en/" target="_blank">Shinagawa Season Terrace</a></span></div>
<table class="table table-condensed">
<tbody>
<tr>
<td style="width:60px;">9:30</td>
<td class="danger" style="width:100px;"><h2 id="keynote-iii">Keynote III</h2><br/><em class="text-muted small">Chair: Yohei Kawaguchi</em></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td>
<a data-toggle="collapse" href="#keynote3" title="Show abstract">Jenelle Feather<br/>
<span class="text-muted"><em>Flatiron Instituteâ€™s Center for Computational Neuroscience</em></span></a>
</td>
<td class="col-md-2">
<div class="btn-group pull-right">
<a class="btn btn-success item-tag2" data-toggle="collapse" href="#keynote3" title="Show abstract">K3</a>
</div>
</td>
</tr>
<tr>
<td colspan="2">
<a data-toggle="collapse" href="#keynote3" title="Show abstract"><strong>Successes and Failures of Machine Learning Models of Sensory Systems
</strong></a><br/>
<div class="collapse" id="keynote3">
<strong>Abstract</strong>
<p>The environment is full of rich sensory information, and our brain can parse this input, understand a scene, and learn from the resulting representations. The past decade has given rise to computational models that transform sensory inputs into representations useful for complex behaviors, such as speech recognition and image classification. These models can improve our understanding of biological sensory systems and serve as a test bed for technology that aids sensory impairments, provided that model representations resemble those in the brain. In this talk, I will detail comparisons of model representations with those of biological systems. In the first study, I will discuss how modifications to a modelâ€™s training environment improve its ability to predict auditory fMRI responses. In the second part of the talk, I will present behavioral experiments using model metamers to reveal divergent invariances between human observers and current computational models of auditory and visual perception. By investigating the similarities and differences between computational models and biological systems, we aim to improve current models and better explain how our brain utilizes robust representations for perception and cognition.</p>
<a class="btn btn-success btn-xs" href="/workshop2024/keynotes#jenelle-feather" style="">More info <i aria-hidden="true" class="fa fa-chevron-circle-right"></i></a>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>10:30</td>
<td class="active" colspan="2"><h2 id="coffee-break-3">Coffee break</h2></td>
</tr>
<tr>
<td>11:00</td>
<td class="success"><h2 id="poster-session-iii">Poster Session III</h2><br/><em class="text-muted small"></em><br/><br/></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td style="border-top: none;padding-top:0px;">
<p><a class="btn btn-danger btn-sm" href="/workshop2024/Poster_Session_III" style="">List of posters</a></p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>12:20</td>
<td class="active" colspan="2"><h2 id="lunch-1">Lunch</h2></td>
</tr>
<tr>
<td>13:40</td>
<td class="tutorial"><h2 id="oral-session-iii">Oral Session III</h2><br/><em class="text-muted small">Chair: Tatsuya Komatsu</em></td>
<td>
<table class="table table-condensed table-borderless table-aligned">
<tr>
<td>
<a data-toggle="collapse" href="#O3-1" title="Show abstract"><strong>From Computation to Consumption: Exploring the Compute-Energy Link for Training and Testing Neural Networks for SED Systems</strong>
<div class="authors">Constance Douwes (Inria), Romain serizel (UniversitÃ© de Lorraine)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O3-1">
<strong class="abstract">Abstract</strong>
<p>The massive use of machine learning models, particularly neural networks, has raised serious concerns about their environmental impact. Indeed, over the last few years we have seen an explosion in the computing costs associated with training and deploying these systems. It is, therefore, crucial to understand their energy requirements in order to better integrate them into the evaluation of models, which has so far focused mainly on performance. In this paper, we study several neural network architectures that are key components of sound event detection systems, using an audio tagging task as an example. We measure the energy consumption for training and testing small to large architectures and establish complex relationships between the energy consumption, the number of floating-point operations, the number of parameters, and the GPU/memory utilization.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O3-2" title="Show abstract"><strong>Self Training and Ensembling Frequency Dependent Networks with Coarse Prediction Pooling and Sound Event Bounding Boxes</strong>
<div class="authors">Hyeonuk Nam (KAIST), Deokki Min (Korea Advanced Institute of Science and Technology (KAIST)), Seung-Deok Choi (KAIST), Inhan Choi (KAIST), Yong-Hwa Park (Kaist)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O3-2">
<strong class="abstract">Abstract</strong>
<p>To tackle sound event detection (SED), we propose frequency dependent networks (FreDNets), which heavily leverage frequency-dependent methods. We apply frequency warping and FilterAugment, which are frequency-dependent data augmentation methods. The model architecture consists of 3 branches: audio teacher-student transformer (ATST) branch, BEATs branch and CNN branch including either partial dilated frequency dynamic convolution (PDFD conv) or squeeze-and-Excitation (SE) with time-frame frequency-wise SE (tfwSE). To train MAESTRO labels with coarse temporal resolution, we applied max pooling on prediction for the MAESTRO dataset. Using best ensemble model, we applied self training to obtain pseudo label from DESED weak set, unlabeled set and AudioSet. AudioSet pseudo labels, filtered to focus on high-confidence labels, are used to train on DESED dataset only. We used change-detection-based sound event bounding boxes (cSEBBs) as post processing for ensemble models on self training and submission models. The resulting FreDNet was ranked 2nd in DCASE 2024 Challenge Task 4.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O3-3" title="Show abstract"><strong>Multi-Iteration Multi-Stage Fine-Tuning of Transformers for Sound Event Detection with Heterogeneous Datasets</strong>
<div class="authors">Florian Schmid (Johannes Kepler University), Paul Primus (Johannes Kepler University), Tobias Morocutti (Johannes Kepler University), Jonathan Greif (     Johannes Kepler University), Gerhard Widmer (Johannes Kepler University)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O3-3">
<strong class="abstract">Abstract</strong>
<p>A central problem in building effective sound event detection systems is the lack of high-quality, strongly annotated sound event datasets. For this reason, Task 4 of the DCASE 2024 challenge proposes learning from two heterogeneous datasets, including audio clips labeled with varying annotation granularity and with different sets of possible events. We propose a multi-iteration, multi-stage procedure for fine-tuning Audio Spectrogram Transformers on the joint DESED and MAESTRO Real datasets. The first stage closely matches the baseline system setup and trains a CRNN model while keeping the pre-trained transformer model frozen. In the second stage, both CRNN and transformer are fine-tuned using heavily weighted self-supervised losses. After the second stage, we compute strong pseudo-labels for all audio clips in the training set using an ensemble of fine-tuned transformers. Then, in a second iteration, we repeat the two-stage training process and include a distillation loss based on the pseudo-labels, achieving a new single-model, state-of-the-art performance on the public evaluation set of DESED with a PSDS1 of 0.692. A single model and an ensemble, both based on our proposed training procedure, ranked first in Task 4 of the DCASE Challenge 2024.</p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O3-4" title="Show abstract"><strong>Learning Multi-Target TDOA Features for Sound Event Localization and Detection</strong>
<div class="authors">Axel Berg (Arm), Johanna Engman (Lund University), Jens Gulin (Sony), Kalle Ã…strÃ¶m (Lund University), Magnus Oskarsson (Lund University)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O3-4">
<strong class="abstract">Abstract</strong>
<p>Sound event localization and detection (SELD) systems using audio recordings from a microphone array rely on spatial cues for determining the location of sound events. As a consequence, the localization performance of such systems is to a large extent determined by the quality of the audio features that are used as inputs to the system. We propose a new feature, based on neural generalized cross-correlations with phase-transform (NGCC-PHAT), that learns audio representations suitable for localization. Using permutation invariant training for the time-difference of arrival (TDOA) estimation problem enables NGCC-PHAT to learn TDOA features for multiple overlapping sound events. These features can be used as a drop-in replacement for GCC-PHAT inputs to a SELD-network. We test our method on the STARSS23 dataset and demonstrate improved localization performance compared to using standard GCC-PHAT or SALSA-Lite input features. </p>
</div>
</td>
</tr>
<tr>
<td>
<a data-toggle="collapse" href="#O3-5" title="Show abstract"><strong>Synthetic Training Set Generation Using Text-to-Audio Models for Environmental Sound Classification</strong>
<div class="authors">Francesca Ronchini (Politecnico di Milano), Luca Comanducci (Politecnico di Milano), Fabio Antonacci (Politecnico di Milano)</div></a>
</td>
</tr>
<tr>
<td colspan="2">
<div class="collapse" id="O3-5">
<strong class="abstract">Abstract</strong>
<p>In recent years, text-to-audio models have revolutionized the field
of automatic audio generation. This paper investigates their ap-
plication in generating synthetic datasets for training data-driven
models. Specifically, this study analyzes the performance of two
environmental sound classification systems trained with data generated from text-to-audio models. We considered three scenarios:
a) augmenting the training dataset with data generated by text-to-
audio models; b) using a mixed training dataset combining real and synthetic text-driven generated data; and c) using a training dataset composed entirely of synthetic audio. In all cases, the performance of the classification models was tested on real data. Results indicate that text-to-audio models are effective for dataset augmentation, with consistent performance when replacing a subset of the recorded dataset. However, the performance of the audio recognition models drops when relying entirely on generated audio. </p>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>15:20</td>
<td class="active" colspan="2"><h2 id="coffee-break-4">Coffee break</h2></td>
</tr>
<tr>
<td>15:50</td>
<td class="warning"><h2 id="town-hall-discussion">Town Hall Discussion</h2><br/><em class="text-muted small"></em><br/><br/></td>
</tr>
<tr>
<td>16:30</td>
<td class="warning" colspan="2"><h2 id="closing-remarks">Closing Remarks</h2></td>
</tr>
</tbody>
</table>
</div>
</div>
<style>
    table.table-aligned td,table.table-aligned th{
        padding-top: 2px !important;
        padding-bottom: 8px !important;
    }
    .item-tag{
        font-size:110%;
        padding: .5em .6em .3em;
    }
    a.item-tag2 {
        font-weight: 700;
        font-size: 16px;
        padding-left:5px;
        padding-right:5px;
        padding-top: 5px;
        padding-bottom: 4px;
    }
    .title {
        font-size: 110%;
    }
    .authors{
        color: #7c7c7c;
        font-style: italic;
        padding-left: 1em;
    }
    .extra{
        padding-top: 0.5em;
        padding-left: 1em;
        padding-bottom: 0;
    }
    .table > tbody > tr.tutorial > td, .table > tbody > tr.tutorial > th, .table > tbody > tr > td.tutorial, .table > tbody > tr > th.tutorial, .table > tfoot > tr.tutorial > td, .table > tfoot > tr.tutorial > th, .table > tfoot > tr > td.tutorial, .table > tfoot > tr > th.tutorial, .table > thead > tr.tutorial > td, .table > thead > tr.tutorial > th, .table > thead > tr > td.tutorial, .table > thead > tr > th.tutorial {
      background-color: rgba(51,121,182,0.4);
    }
    .table > tbody > tr.social > td, .table > tbody > tr.social > th, .table > tbody > tr > td.social, .table > tbody > tr > th.social, .table > tfoot > tr.social > td, .table > tfoot > tr.social > th, .table > tfoot > tr > td.social, .table > tfoot > tr > th.social, .table > thead > tr.social > td, .table > thead > tr.social > th, .table > thead > tr > td.social, .table > thead > tr > th.social {
      background-color: rgba(231,143,2,0.51);
    }
    .text-muted a.link{
        color: #808080;
    }
    .label-task {
        padding-top: 0.3em;
    }
    .extra .abstract{
        padding-top: 0.5em;
    }
    table.table-borderless td,table.table-borderless th{
        border: none !important;
    }
    .table h2{
        font-size: 14px;
        line-height: 20px;
        margin: 0;
        padding: 0;
    }
</style>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>