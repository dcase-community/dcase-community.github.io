<!DOCTYPE html><html lang="en">
<head>
    <title>Sound event detection in real life audio - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2017/task-sound-event-detection-in-real-life-audio">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="Challenge has ended. Full results for this task can be found here Description This task evaluates performance of the sound event detection systems in multisource conditions similar to our everyday life, where the sound sources are rarely heard in isolation. In this task, there is no control over the number …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2017</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2017/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-rare-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-alarm text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-rare-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-rare-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-events text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-large-scale-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-large-scale-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-large-scale-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Download data">
        <a href="/challenge2017/download"><i class="fa fa-download"></i>&nbsp;Download</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2017/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2017/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge organizers">
        <a href="/challenge2017/organizers"><i class="fa fa-users"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/water-madrid-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-warning"></i><i class="fa dc-events fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Events</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 3</span></span><img src="../images/logos/dcase/dcase2017_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound event detection <br>in real life audio</h1><hr class="small right bold"><span class="subheading">Task 3</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Annamaria Mesaros" class="img img-circle" src="/images/person/annamaria_mesaros.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Annamaria Mesaros</strong>
<a class="icon" href="mailto:annamaria.mesaros@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University of Technology
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Toni Heittola" class="img img-circle" src="/images/person/toni_heittola.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Toni Heittola</strong>
<a class="icon" href="mailto:toni.heittola@tuni.fi"><i class="pull-right fa fa-envelope-o"></i></a>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University of Technology
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#recording-and-annotation-procedure">Recording and annotation procedure</a></li>
<li><a href="#download">Download</a></li>
</ul>
</li>
<li><a href="#task-setup">Task setup</a>
<ul>
<li><a href="#development-dataset">Development dataset</a></li>
<li><a href="#evaluation-dataset">Evaluation dataset</a></li>
<li><a href="#submission">Submission</a></li>
</ul>
</li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a>
<ul>
<li><a href="#results-for-tut-sound-events-2017-development-dataset">Results for TUT Sound events 2017, development dataset</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found <a class="btn btn-default btn-xs" href="/challenge2017/task-sound-event-detection-in-real-life-audio-results">here <i class="fa fa-caret-right"></i></a>
</p>
<h1 id="description">Description</h1>
<p>This task evaluates performance of the sound event detection systems in multisource conditions similar to our everyday life, where the sound sources are rarely heard in isolation. In this task, there is no control over the number of overlapping sound events at each time, not in the training nor in the testing audio data.</p>
<figure>
<div class="row-fluid row-centered">
<div class="col-xs-10 col-md-5 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2017/task3_overview.png"/>
<figcaption>Figure 1: Overview of sound event detection system.</figcaption>
</div>
</div>
</figure>
<h1 id="audio-dataset">Audio dataset</h1>
<p><strong>TUT Sound Events 2017</strong> dataset will be used for task 3. Audio in the dataset is a subset of <em>TUT Acoustic scenes 2017</em> dataset (used for <a href="task-acoustic-scene-classification">task 1</a>). The TUT Sound Events 2017 dataset consists of recordings of street acoustic scenes with various levels of traffic and other activity. The scene was selected as representing an environment of interest for detection of sound events related to human activities and hazard situations.</p>
<p>The dataset was collected in Finland by Tampere University of Technology between 06/2015 - 01/2016. The data collection has received funding from the European Research Council.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="../images/sponsors/erc.jpg" title="ERC"/></a></p>
<h2 id="recording-and-annotation-procedure">Recording and annotation procedure</h2>
<p>The recordings were captured each in a different streets. For each recording location, a 3-5 minute long audio recording was captured. The equipment used for recording consists of a binaural <a href="http://www.soundman.de/en/products/">Soundman OKM II Klassik/studio A3</a> electret in-ear microphone and a <a href="http://www.rolandus.com/products/r-09/">Roland Edirol R-09</a> wave recorder using 44.1 kHz sampling rate and 24 bit resolution. For audio material recorded in private places, written consent was obtained from all people involved.</p>
<p>Individual sound events in each recording were annotated by the same person using freely chosen labels for sounds. Nouns were used to characterize the sound source, and verbs to characterize the sound production mechanism,  using a noun-verb pair whenever this was possible. The annotator was instructed to annotate all audible sound events, decide the start time and end time of the sounds as he sees fit, and choose event labels freely. This resulted in a large set of raw labels.</p>
<p><em>Target sound event</em> classes were selected to represent common sounds related to human presence and traffic. Mapping of the raw labels was performed, merging sounds into classes described by their source before selecting target classes. Target sound event classes for the dataset were selected based on the frequency of the obtained labels, resulting in selection of most common sounds for the street acoustic scene, in sufficient numbers for learning acoustic models. Mapping of the raw labels was performed, merging sounds into classes described by their source, for example “car passing by”, “car engine running”, “car idling”, etc into “car”, sounds produced by buses and trucks into “large vehicle”, “children yelling” and ” children talking” into “children”, etc.</p>
<p>Selected sound classes for the task are:</p>
<ul>
<li>brakes squeaking</li>
<li>car</li>
<li>children</li>
<li>large vehicle</li>
<li>people speaking</li>
<li>people walking</li>
</ul>
<p>Due to the high level of subjectivity inherent to the annotation process, a verification of the reference annotation was done using these mapped classes. Three persons (other than the annotator) listened to each audio segment annotated as belonging to one of these classes, marking agreement about the presence of the indicated sound within the segment. Agreement/disagreement did not take into account the sound event onset and offset, only the presence of the sound event within the annotated segment. Event instances that were confirmed by at least one person were kept, resulting in elimination of about 10% of the original event instances in the development set.</p>
<p class="bg-danger">Make sure you are using <strong>version 2</strong> of the dataset as this contains the verified annotations.</p>
<h2 id="download">Download</h2>
<p><em>In case you are using the provided baseline system, there is no need to download the datasets as the system will automatically download needed datasets for you.</em></p>
<p><strong>Development dataset</strong></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/814831" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/814831" target="_blank">
<span style="font-size:20px;">TUT Sound events 2017, <strong>development dataset v2</strong> <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(1.3 GB)</span>
<br/>
<a href="http://doi.org/10.5281/zenodo.814831">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.814831.svg"/>
</a>
<span class="text-muted">
                
                version 2
                
                
                </span>
</div>
</div>
<p><br/></p>
<p><strong>Evaluation dataset</strong></p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/1040179" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/1040179" target="_blank">
<span style="font-size:20px;">TUT Sound events 2017, <strong>evaluation dataset</strong> <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(388.2 MB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.1040179">
<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.1040179.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h1 id="task-setup">Task setup</h1>
<p><em>TUT Sound Events 2017</em> dataset consists of two subsets: <strong>development dataset</strong> and <strong>evaluation dataset</strong>. Partitioning of data into these subsets was done based on the amount of examples available for each sound event class, while also taking into account recording location. Because the event instances belonging to different classes are distributed unevenly within the recordings, the partitioning of individual classes can be controlled only to a certain extent, but so that the majority of events are in the development set.</p>
<p>A detailed description of the data recording and annotation procedure is available in:</p>
<div class="btex-item" data-item="Mesaros2016_EUSIPCO" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_EUSIPCO"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>TUT database for acoustic scene classification and sound event detection.</em>
In 24th European Signal Processing Conference 2016 (EUSIPCO 2016). Budapest, Hungary, 2016.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" class="panel-collapse collapse" id="collapseMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" role="tabpanel">
<h4>TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876alabel" class="modal fade" id="bibtexMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_EUSIPCOa3cbd38fc0494f21ab482bb380d3876alabel">TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2016_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "{TUT} Database for Acoustic Scene Classification and Sound Event Detection",
    abstract = "We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.",
    year = "2016",
    address = "Budapest, Hungary",
    booktitle = "24th European Signal Processing Conference 2016 (EUSIPCO 2016)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="development-dataset">Development dataset</h2>
<p>A cross-validation setup is provided in order to make results reported with this dataset uniform. The setup consists of four folds, and is made so that each recording is used exactly once as test data. While creating the cross-validation folds, the only condition imposed was that the test subset does not contain classes unavailable in training subset. The folds are provided with the dataset.</p>
<h2 id="evaluation-dataset">Evaluation dataset</h2>
<p>Evaluation dataset without ground truth will be released one month before the submission deadline. Full ground truth meta data for it will be published after the DCASE 2017 challenge and workshop are concluded.</p>
<h2 id="submission">Submission</h2>
<p>Detailed information for the challenge submission can found on the <a href="submission">submission page</a>.</p>
<p>System output should be presented as a single text-file (in CSV format) containing a list of detected sound events from each audio file. Events can be in any order. Format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event onset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event offset time in seconds (float)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">event label (string)</span><span class="o">]</span>
</code></pre></div>
<p>Multiple system outputs can be submitted (maximum 4 per participant). If submitting multiple systems, the individual text-files should be packaged into a zip file for submission. Please carefully mark the connection between the submitted files and the corresponding system or system parameters (for example by naming the text file appropriately).</p>
<h1 id="task-rules">Task rules</h1>
<p>These are the general rules valid for all tasks. The same rules and additional information on technical report and submission requirements can be found <a href="rules">here</a>. Task specific rules are highlighted with green.</p>
<ul>
<li>Participants are <strong>not allowed</strong> to use external data for system development. Data from another task is considered external data.</li>
<li>Manipulation of provided training and development data <strong>is allowed</strong>.
    <p class="bg-success">
    The development dataset can be augmented without use of external data (e.g. by mixing data sampled from a pdf or using techniques such as pitch shifting or time stretching).
    </p>
</li>
<li>Participants are <strong>not allowed</strong> to make subjective judgments of the evaluation data, nor to annotate it. The evaluation dataset cannot be used to train the submitted system; the use of statistics about the evaluation data in the decision making is also forbidden.</li>
</ul>
<h1 id="evaluation">Evaluation</h1>
<p>The evaluation metric for this task is <strong>segment-based error rate</strong> calculated in one-second segments over the entire test set. Additionally, segment-based F-score will be calculated. Ranking of submitted systems will be based on segment-based error rate.</p>
<p>Detailed information on metrics calculation is available in:</p>
<div class="btex-item" data-item="Mesaros2016_MDPI" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_MDPI"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>Metrics for polyphonic sound event detection.</em>
<em>Applied Sciences</em>, 6(6):162, 2016.
URL: <a href="http://www.mdpi.com/2076-3417/6/6/162">http://www.mdpi.com/2076-3417/6/6/162</a>, <a href="https://doi.org/10.3390/app6060162">doi:10.3390/app6060162</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://github.com/TUT-ARG/sed_eval" rel="tooltip" title="Toolbox"><i class="fa fa-file-code-o"></i></a>
<button aria-controls="collapseMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" class="panel-collapse collapse" id="collapseMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" role="tabpanel">
<h4>Metrics for Polyphonic Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.mdpi.com/2076-3417/6/6/162/pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" href="http://www.mdpi.com/2076-3417/6/6/162" title="Journal page"><i class="fa fa-book"></i> Web publication</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://github.com/TUT-ARG/sed_eval" rel="tooltip" title="Toolbox"><i class="fa fa-file-code-o"></i> Toolbox</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffblabel" class="modal fade" id="bibtexMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffb" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_MDPI511a9a59ae3648fda17feb4137168ffblabel">Metrics for Polyphonic Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2016_MDPI,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "Metrics for Polyphonic Sound Event Detection",
    journal = "Applied Sciences",
    volume = "6",
    year = "2016",
    number = "6",
    pages = "162",
    url = "http://www.mdpi.com/2076-3417/6/6/162",
    issn = "2076-3417",
    abstract = "This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.",
    doi = "10.3390/app6060162"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p>A short description of metrics can be found <a href="metrics">here</a>.</p>
<p>The evaluation is done automatically in the baseline system. Evaluation is done using <strong>sed_eval toolbox</strong>.</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/sed_eval" target="_blank">
<span style="font-size:20px;">sed_eval - Evaluation toolbox for Sound Event Detection <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<p>In case of using the toolbox directly, use the following parameters for <code>sed_eval.sound_event.SegmentBasedMetrics</code> evaluator to align it with the baseline system:</p>
<ul>
<li>one second segment size <code>time_resolution=1.0</code></li>
</ul>
<p><strong>PLEASE NOTE:</strong> The four cross-validation folds are treated as single experiment, meaning that metrics are calculated only after training and testing all folds, not as average of the individual folds nor as average of individual class performance. Intermediate measures (insertions, deletions, substitutions) from all folds are accumulated before calculating metrics. For more information on why so, please refer to the following paper:</p>
<div class="btex-item" data-item="Forman2010" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Forman2010"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            George Forman and Martin Scholz.
<em>Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement.</em>
<em>SIGKDD Explor. Newsl.</em>, 12(1):49–57, November 2010.
URL: <a href="http://doi.acm.org/10.1145/1882471.1882479">http://doi.acm.org/10.1145/1882471.1882479</a>, <a href="https://doi.org/10.1145/1882471.1882479">doi:10.1145/1882471.1882479</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexForman2010ae3b0be27acc4dcd86318fd8f51e501b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="http://www.kdd.org/exploration_files/v12-1-p49-forman-sigkdd.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseForman2010ae3b0be27acc4dcd86318fd8f51e501b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseForman2010ae3b0be27acc4dcd86318fd8f51e501b" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingForman2010ae3b0be27acc4dcd86318fd8f51e501b" class="panel-collapse collapse" id="collapseForman2010ae3b0be27acc4dcd86318fd8f51e501b" role="tabpanel">
<h4>Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement</h4>
<h5>Abstract</h5>
<p class="text-justify">Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexForman2010ae3b0be27acc4dcd86318fd8f51e501b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="http://www.kdd.org/exploration_files/v12-1-p49-forman-sigkdd.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexForman2010ae3b0be27acc4dcd86318fd8f51e501blabel" class="modal fade" id="bibtexForman2010ae3b0be27acc4dcd86318fd8f51e501b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexForman2010ae3b0be27acc4dcd86318fd8f51e501blabel">Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement</h4>
</div>
<div class="modal-body">
<pre>@article{Forman2010,
    author = "Forman, George and Scholz, Martin",
    title = "Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement",
    abstract = "Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.",
    journal = "SIGKDD Explor. Newsl.",
    issue_date = "June 2010",
    volume = "12",
    number = "1",
    month = "November",
    year = "2010",
    issn = "1931-0145",
    pages = "49--57",
    numpages = "9",
    url = "http://doi.acm.org/10.1145/1882471.1882479",
    doi = "10.1145/1882471.1882479",
    acmid = "1882479",
    publisher = "ACM",
    address = "New York, NY, USA"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="segment_based_ER_overall_eval" data-scatter-y="segment_based_F1_overall_eval" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="segment_based_ER_overall_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="2">Segment-based (overall)</th>
</tr>
<tr>
<th class="sm-cell" data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="segment_based_ER_overall_eval" data-reversed="true" data-sortable="true" data-value-type="float4">
                ER
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="segment_based_F1_overall_eval" data-sortable="true" data-value-type="float1-percentage">
                F1
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Adavanne_TUT_task3_1</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-sound-event-detection-in-real-life-audio-results#Adavanne2017</td>
<td>0.7914</td>
<td>41.7</td>
</tr>
<tr>
<td></td>
<td>Adavanne_TUT_task3_2</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-sound-event-detection-in-real-life-audio-results#Adavanne2017</td>
<td>0.8061</td>
<td>42.9</td>
</tr>
<tr>
<td></td>
<td>Adavanne_TUT_task3_3</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-sound-event-detection-in-real-life-audio-results#Adavanne2017</td>
<td>0.8544</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>Adavanne_TUT_task3_4</td>
<td>Sharath Adavanne</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-sound-event-detection-in-real-life-audio-results#Adavanne2017</td>
<td>0.8716</td>
<td>36.2</td>
</tr>
<tr>
<td></td>
<td>Chen_UR_task3_1</td>
<td>Yukun Chen</td>
<td>Electrical and Computer Engineering, University of Rochester, NY, US</td>
<td>task-sound-event-detection-in-real-life-audio-results#Chen2017</td>
<td>0.8575</td>
<td>30.9</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task3_1</td>
<td>Jia-Ching Wang</td>
<td>Computer Sciene and Information Engineering, National Central University, Taoyuan, Taiwan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Dang2017</td>
<td>0.9529</td>
<td>42.6</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task3_2</td>
<td>Jia-Ching Wang</td>
<td>Computer Sciene and Information Engineering, National Central University, Taoyuan, Taiwan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Dang2017</td>
<td>0.9468</td>
<td>42.8</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task3_3</td>
<td>Jia-Ching Wang</td>
<td>Computer Sciene and Information Engineering, National Central University, Taoyuan, Taiwan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Dang2017</td>
<td>1.0318</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task3_4</td>
<td>Jia-Ching Wang</td>
<td>Computer Sciene and Information Engineering, National Central University, Taoyuan, Taiwan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Dang2017</td>
<td>1.1028</td>
<td>43.5</td>
</tr>
<tr>
<td></td>
<td>Feroze_IST_task3_1</td>
<td>Khizer Feroze</td>
<td>Electrical Engineering, Institute of Space Technology, Islamabad, Pakistan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Feroze2017</td>
<td>1.0942</td>
<td>42.6</td>
</tr>
<tr>
<td></td>
<td>Feroze_IST_task3_2</td>
<td>Khizer Feroze</td>
<td>Electrical Engineering, Institute of Space Technology, Islamabad, Pakistan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Feroze2017</td>
<td>1.0312</td>
<td>39.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2017 baseline</td>
<td>Toni Heittola</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-sound-event-detection-in-real-life-audio-results#Heittola2017</td>
<td>0.9358</td>
<td>42.8</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task3_1</td>
<td>Yuanbo Hou</td>
<td>Embedded Artificial Intelligence Laboratory, Beijing University Of Posts And Telecommunications, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Hou2017</td>
<td>1.0446</td>
<td>29.3</td>
</tr>
<tr>
<td></td>
<td>Hou_BUPT_task3_2</td>
<td>Yuanbo Hou</td>
<td>Embedded Artificial Intelligence Laboratory, Beijing University Of Posts And Telecommunications, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Hou2017</td>
<td>0.9248</td>
<td>34.1</td>
</tr>
<tr>
<td></td>
<td>Kroos_CVSSP_task3_1</td>
<td>Christian Kroos</td>
<td>Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-sound-event-detection-in-real-life-audio-results#Kroos2017</td>
<td>0.8979</td>
<td>44.9</td>
</tr>
<tr>
<td></td>
<td>Kroos_CVSSP_task3_2</td>
<td>Christian Kroos</td>
<td>Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-sound-event-detection-in-real-life-audio-results#Kroos2017</td>
<td>0.8911</td>
<td>41.6</td>
</tr>
<tr>
<td></td>
<td>Kroos_CVSSP_task3_3</td>
<td>Christian Kroos</td>
<td>Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</td>
<td>task-sound-event-detection-in-real-life-audio-results#Kroos2017</td>
<td>1.0141</td>
<td>43.8</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task3_1</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-sound-event-detection-in-real-life-audio-results#Jeong2017</td>
<td>0.9260</td>
<td>42.0</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task3_2</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-sound-event-detection-in-real-life-audio-results#Jeong2017</td>
<td>0.8673</td>
<td>27.9</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task3_3</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-sound-event-detection-in-real-life-audio-results#Jeong2017</td>
<td>0.8080</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Lee_SNU_task3_4</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, Korea</td>
<td>task-sound-event-detection-in-real-life-audio-results#Jeong2017</td>
<td>0.8985</td>
<td>43.6</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task3_1</td>
<td>Yanxiong Li</td>
<td>School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Li2017</td>
<td>0.9920</td>
<td>40.3</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task3_2</td>
<td>Yanxiong Li</td>
<td>School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Li2017</td>
<td>0.9523</td>
<td>41.0</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task3_3</td>
<td>Yanxiong Li</td>
<td>School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Li2017</td>
<td>1.0043</td>
<td>43.4</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task3_4</td>
<td>Yanxiong Li</td>
<td>School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Li2017</td>
<td>0.9878</td>
<td>33.9</td>
</tr>
<tr>
<td></td>
<td>Lu_THU_task3_1</td>
<td>Rui Lu</td>
<td>Department of Automation, Tsinghua University, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Lu2017</td>
<td>0.8251</td>
<td>39.6</td>
</tr>
<tr>
<td></td>
<td>Lu_THU_task3_2</td>
<td>Rui Lu</td>
<td>Department of Automation, Tsinghua University, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Lu2017</td>
<td>0.8306</td>
<td>39.2</td>
</tr>
<tr>
<td></td>
<td>Lu_THU_task3_3</td>
<td>Rui Lu</td>
<td>Department of Automation, Tsinghua University, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Lu2017</td>
<td>0.8361</td>
<td>38.0</td>
</tr>
<tr>
<td></td>
<td>Lu_THU_task3_4</td>
<td>Rui Lu</td>
<td>Department of Automation, Tsinghua University, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Lu2017</td>
<td>0.8373</td>
<td>38.3</td>
</tr>
<tr>
<td></td>
<td>Wang_NTHU_task3_1</td>
<td>Chun-Hao Wang</td>
<td>Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan</td>
<td>task-sound-event-detection-in-real-life-audio-results#Wang2017</td>
<td>0.9749</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Xia_UWA_task3_1</td>
<td>Xianjun Xia</td>
<td>School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Perth, Australia</td>
<td>task-sound-event-detection-in-real-life-audio-results#Xia2017</td>
<td>0.9523</td>
<td>43.5</td>
</tr>
<tr>
<td></td>
<td>Xia_UWA_task3_2</td>
<td>Xianjun Xia</td>
<td>School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Perth, Australia</td>
<td>task-sound-event-detection-in-real-life-audio-results#Xia2017</td>
<td>0.9437</td>
<td>41.1</td>
</tr>
<tr>
<td></td>
<td>Xia_UWA_task3_3</td>
<td>Xianjun Xia</td>
<td>School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Perth, Australia</td>
<td>task-sound-event-detection-in-real-life-audio-results#Xia2017</td>
<td>0.8740</td>
<td>41.7</td>
</tr>
<tr>
<td></td>
<td><a href="javascript:void(0);" title="The system was re-submitted after the deadline. The revised submission yielded substantially lower ER, with the difference in performance attributed to a software bug in the original submission. More details can be found in the technical report.">Yu_FZU_task3_1<sup>*</sup></a></td>
<td>Chun-Yan Yu</td>
<td>College of Mathematics and Computer Science, Fuzhou University, Fuzhou 350108, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Yu2017</td>
<td>1.1963</td>
<td>3.9</td>
</tr>
<tr>
<td></td>
<td>Zhou_PKU_task3_1</td>
<td>Jianchao Zhou</td>
<td>Institute of Computer Science &amp; Technology, Peking University, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Zhou2017</td>
<td>0.8526</td>
<td>39.1</td>
</tr>
<tr>
<td></td>
<td>Zhou_PKU_task3_2</td>
<td>Jianchao Zhou</td>
<td>Institute of Computer Science &amp; Technology, Peking University, Beijing, China</td>
<td>task-sound-event-detection-in-real-life-audio-results#Zhou2017</td>
<td>0.8526</td>
<td>37.3</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found <a href="task-sound-event-detection-in-real-life-audio-results">here</a>.</p>
<h1 id="baseline-system">Baseline system</h1>
<p>The baseline system for the task is provided. The system is meant to implement a basic approach for acoustic scene classification, and provide some comparison point for the participants while developing their systems. The baseline systems for all tasks share the code base, implementing quite similar approach for all tasks. The baseline system will download the needed datasets and produces the results below when ran with the default parameters.</p>
<p>The baseline system is based on a multilayer perceptron architecture using log mel-band energies as features. A 5-frame context is used, resulting in a feature vector length of 200. Using these features, a neural network containing two dense layers of 50 hidden units per layer and 20% dropout is trained for 200 epochs for each class.  Detection decision is based on the network output layer containing sigmoid units that can be active at the same time. A detailed description is available in the baseline system documentation. The baseline system includes evaluation of results using <strong>segment-based error rate</strong> and <strong>segment-based F-score</strong> as metrics.</p>
<p>The baseline system is implemented using Python (version 2.7 and 3.6). Participants are allowed to build their system on top of the given baseline system. The system has all needed functionality for dataset handling, storing / accessing features and models, and evaluating the results, making the adaptation for one's needs rather easy. The baseline system is also a good starting point for entry level researchers.</p>
<h3>Python implementation</h3>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/DCASE2017-baseline-system" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/DCASE2017-baseline-system" target="_blank">
<span style="font-size:20px;">DCASE2017 Baseline, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<p><br/></p>
<h2 id="results-for-tut-sound-events-2017-development-dataset">Results for TUT Sound events 2017, development dataset</h2>
<p><em>Evaluation setup</em></p>
<ul>
<li>4-fold cross-validation, error rate calculated after testing all folds</li>
<li>Python 2.7.13 used</li>
</ul>
<p><em>System parameters</em></p>
<ul>
<li>Frame size: 40 ms (with 50% hop size)</li>
<li>Feature vector: 40 log mel-band energies in 5 consecutive frames = 200 values</li>
<li>MLP: 2 layers x 50 hidden units, 20% dropout, 200 epochs (using early stopping criteria, monitoring started after 100 epoch, 10 epoch patience), learning rate 0.001, sigmoid output layer</li>
<li>Trained and tested on full audio</li>
</ul>
<div class="table-responsive col-md-4">
<table class="table table-striped">
<caption>Segment-based overall metrics</caption>
<tbody>
<tr>
<td class="col-md-6"><strong>ER</strong></td>
<td>0.69</td>
</tr>
<tr>
<td>F-score</td>
<td>56.7 %</td>
</tr>
</tbody>
</table>
</div>
<div class="clearfix"></div>
<h1 id="citation">Citation</h1>
<p>If you are using the <strong>dataset</strong> or <strong>baseline</strong> code, or want to refer <strong>challenge task</strong> please cite the following paper:</p>
<div class="btex-item" data-item="DCASE2017challenge" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="DCASE2017challenge"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            A. Mesaros, T. Heittola, A. Diment, B. Elizalde, A. Shah, E. Vincent, B. Raj, and T. Virtanen.
<em>DCASE 2017 challenge setup: tasks, datasets and baseline system.</em>
In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), 85–92. November 2017.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" class="panel-collapse collapse" id="collapseDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" role="tabpanel">
<h4>DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System</h4>
<h5>Abstract</h5>
<p class="text-justify">DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7label" class="modal fade" id="bibtexDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDCASE2017challenge7da80e3abfbb4540abdc0fcd3b6612b7label">DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{DCASE2017challenge,
    Author = "Mesaros, A. and Heittola, T. and Diment, A. and Elizalde, B. and Shah, A. and Vincent, E. and Raj, B. and Virtanen, T.",
    title = "{DCASE} 2017 Challenge Setup: Tasks, Datasets and Baseline System",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "85--92",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.",
    keywords = "Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<p>When citing <strong>challenge task</strong> and <strong>results</strong> please cite the following paper:</p>
<div class="btex-item" data-item="Mesaros2019_TASLP" data-source="content/data/challenge2017/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2019_TASLP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            A. Mesaros, A. Diment, B. Elizalde, T. Heittola, E. Vincent, B. Raj, and T. Virtanen.
<em>Sound event detection in the DCASE 2017 challenge.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2019.
In press.
<a href="https://doi.org/10.1109/TASLP.2019.2907016">doi:10.1109/TASLP.2019.2907016</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://hal.inria.fr/hal-02067935/document" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" class="panel-collapse collapse" id="collapseMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" role="tabpanel">
<h4>Sound event detection in the DCASE 2017 Challenge</h4>
<h5>Abstract</h5>
<p class="text-justify">Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly-labeled data was available for training. In this paper, we present the three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure, to perform statistical analysis of the challenge results. The analysis indicates that while the 95% confidence intervals for many systems overlap, there are significant difference in performance between the top systems and the baseline for all tasks.</p>
<h5>Keywords</h5>
<p class="text-justify">Event detection;Task analysis;Training;Acoustics;Speech processing;Glass;Hidden Markov models;Sound event detection;weak labels;pattern recognition;jackknife estimates;confidence intervals</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://hal.inria.fr/hal-02067935/document" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222label" class="modal fade" id="bibtexMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2019_TASLP52ac4a3b8ec84ee4a84c8af51a74f222label">Sound event detection in the DCASE 2017 Challenge</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2019_TASLP,
    author = "Mesaros, A. and Diment, A. and Elizalde, B. and Heittola, T. and Vincent, E. and Raj, B. and Virtanen, T.",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    title = "Sound event detection in the {DCASE} 2017 Challenge",
    year = "2019",
    abstract = "Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly-labeled data was available for training. In this paper, we present the three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure, to perform statistical analysis of the challenge results. The analysis indicates that while the 95\% confidence intervals for many systems overlap, there are significant difference in performance between the top systems and the baseline for all tasks.",
    keywords = "Event detection;Task analysis;Training;Acoustics;Speech processing;Glass;Hidden Markov models;Sound event detection;weak labels;pattern recognition;jackknife estimates;confidence intervals",
    doi = "10.1109/TASLP.2019.2907016",
    issn = "2329-9290",
    note = "In press"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>