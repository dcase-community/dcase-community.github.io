<!DOCTYPE html><html lang="en">
<head>
    <title>Acoustic Scene Classification with mismatched recording devices - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2018/task-acoustic-scene-classification-results-b">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description This subtask is concerned with the situation in which an application will be tested with a few different types of devices, possibly not the same as the ones used to record the development data. The development data consists of the same recordings as in subtask A, and a …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2018</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2018/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2018/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class=" active">
        <a href="/challenge2018/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-acoustic-scene-classification-results-c"><i class="fa fa-bar-chart"></i>&nbsp;Subtask C</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-general-purpose-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-general-purpose-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-bird-audio-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-bird-audio-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2018/task-monitoring-domestic-activities" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-home text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2018/task-monitoring-domestic-activities-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2018/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2018/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/dunes-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-info"></i><strong class="fa-stack-1x icon-text">B</strong><strong class="fa-stack-1x dcase-icon-top-text">Mismatch</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2018_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Acoustic Scene Classification with<br> mismatched recording devices</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
<li><a href="#device-wise-performance">Device-wise performance</a></li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>This subtask is concerned with the situation in which an application will be tested with a few different types of devices, possibly not the same as the ones used to record the development data.</p>
<p>The development data consists of the same recordings as in subtask A, and a small amount of parallel data recorded with devices B and C. The amount of data is as follows:</p>
<ul>
<li><strong>Device A</strong>: 24 hours (8640 segments, same as subtask A, but resampled and single-channel)</li>
<li><strong>Device B</strong>: 2 hours (72 segments per acoustic scene)</li>
<li><strong>Device C</strong>: 2 hours (72 segments per acoustic scene)</li>
</ul>
<p>The 2 hours of data recorded with devices B and C is parallel, and also available as recorded with device A. The training/test setup was created such that approximately 70% of recording locations for each city and each scene class are in the training subset, considering only device A. The training subset contains 6122 segments from device A, 540 segments from device B, and 540 segments from device C. The test subset contains 2518 segments from device A, 180 segments from device B, and 180 segments from device C.</p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2018/task-acoustic-scene-classification#subtask-b" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy (B/C)<br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy (B/C)<br/>(Development dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_lb" data-sortable="true" data-value-type="float1-percentage">
                Accuracy (B/C)<br/>(Leaderboard dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_1</td>
<td>SurreyCNN8</td>
<td>Kong2018</td>
<td>59.6 (58.5 - 60.7)</td>
<td>57.2</td>
<td>56.8</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_2</td>
<td>SurreyCNN4</td>
<td>Kong2018</td>
<td>58.8 (57.7 - 59.9)</td>
<td>57.5</td>
<td>57.8</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Heittola2018</td>
<td>46.5 (45.4 - 47.6)</td>
<td>45.6</td>
<td>45.0</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_1</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>41.1 (40.0 - 42.2)</td>
<td>51.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_2</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>39.5 (38.4 - 40.6)</td>
<td>53.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_3</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>42.3 (41.2 - 43.4)</td>
<td>51.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_1</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.0 (66.0 - 68.1)</td>
<td>77.6</td>
<td>65.8</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_2</td>
<td>Xception</td>
<td>Liping2018</td>
<td>63.2 (62.1 - 64.3)</td>
<td>77.6</td>
<td>63.1</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_3</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.7 (66.6 - 68.7)</td>
<td>77.6</td>
<td>67.2</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_4</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.1 (66.1 - 68.2)</td>
<td>77.6</td>
<td>66.5</td>
</tr>
<tr>
<td></td>
<td>Nguyen_TUGraz_task1b_1</td>
<td>NNF_CNNEns</td>
<td>Nguyen2018</td>
<td>69.0 (68.0 - 70.0)</td>
<td>63.6</td>
<td>67.3</td>
</tr>
<tr>
<td></td>
<td>Ren_UAU_task1b_1</td>
<td>ABCNN</td>
<td>Ren2018</td>
<td>60.5 (59.4 - 61.5)</td>
<td>58.3</td>
<td>58.9</td>
</tr>
<tr>
<td></td>
<td>Tchorz_THL_task1b_1</td>
<td>AMS_MFCC</td>
<td>Tchorz2018</td>
<td>54.0 (52.9 - 55.1)</td>
<td>63.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_ABSP_Fusion18</td>
<td>Waldekar2018</td>
<td>56.2 (55.1 - 57.3)</td>
<td>57.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_1</td>
<td>Attention</td>
<td>Jun2018</td>
<td>48.8 (47.7 - 49.9)</td>
<td>69.0</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_2</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.5 (51.4 - 53.6)</td>
<td>69.0</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_3</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.3 (51.2 - 53.4)</td>
<td>69.0</td>
<td>49.4</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Development dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_lb" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Leaderboard dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_1</td>
<td>SurreyCNN8</td>
<td>Kong2018</td>
<td>59.6 (58.5 - 60.7)</td>
<td>57.2</td>
<td>56.8</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Heittola2018</td>
<td>46.5 (45.4 - 47.6)</td>
<td>45.6</td>
<td>45.0</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_3</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>42.3 (41.2 - 43.4)</td>
<td>51.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_3</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.7 (66.6 - 68.7)</td>
<td>77.6</td>
<td>67.2</td>
</tr>
<tr>
<td></td>
<td>Nguyen_TUGraz_task1b_1</td>
<td>NNF_CNNEns</td>
<td>Nguyen2018</td>
<td>69.0 (68.0 - 70.0)</td>
<td>63.6</td>
<td>67.3</td>
</tr>
<tr>
<td></td>
<td>Ren_UAU_task1b_1</td>
<td>ABCNN</td>
<td>Ren2018</td>
<td>60.5 (59.4 - 61.5)</td>
<td>58.3</td>
<td>58.9</td>
</tr>
<tr>
<td></td>
<td>Tchorz_THL_task1b_1</td>
<td>AMS_MFCC</td>
<td>Tchorz2018</td>
<td>54.0 (52.9 - 55.1)</td>
<td>63.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_ABSP_Fusion18</td>
<td>Waldekar2018</td>
<td>56.2 (55.1 - 57.3)</td>
<td>57.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_2</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.5 (51.4 - 53.6)</td>
<td>69.0</td>
<td>49.4</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2018 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Sakashita_TUT_task1a_2" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_bus", "class_accuracy_eval_metro", "class_accuracy_eval_metro_station", "class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_shopping_mall", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic", "class_accuracy_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_metro_station", "class_accuracy_eval_shopping_mall"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_bus","class_accuracy_eval_metro","class_accuracy_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
            Submission<br/>code
        </th>
<th class="sm-cell" data-field="name" data-sortable="true">
            Submission<br/>name
        </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
            Technical<br/>Report
        </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
            Accuracy <br/>(Evaluation dataset)
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="class_accuracy_eval_airport" data-sortable="true" data-value-type="float1-percentage">
            Airport
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_bus" data-sortable="true" data-value-type="float1-percentage">
            Bus
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro" data-sortable="true" data-value-type="float1-percentage">
            Metro
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro_station" data-sortable="true" data-value-type="float1-percentage">
            Metro <br/>station
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_park" data-sortable="true" data-value-type="float1-percentage">
            Park
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_public_square" data-sortable="true" data-value-type="float1-percentage">
            Public <br/>square
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_shopping_mall" data-sortable="true" data-value-type="float1-percentage">
            Shopping <br/>mall
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_pedestrian" data-sortable="true" data-value-type="float1-percentage">
            Street <br/>pedestrian
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_traffic" data-sortable="true" data-value-type="float1-percentage">
            Street <br/>traffic
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_tram" data-sortable="true" data-value-type="float1-percentage">
            Tram
        </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_1</td>
<td>SurreyCNN8</td>
<td>Kong2018</td>
<td>59.6</td>
<td>47.7</td>
<td>65.0</td>
<td>51.0</td>
<td>58.2</td>
<td>86.4</td>
<td>36.7</td>
<td>58.5</td>
<td>49.9</td>
<td>80.2</td>
<td>62.1</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_2</td>
<td>SurreyCNN4</td>
<td>Kong2018</td>
<td>58.8</td>
<td>49.6</td>
<td>63.9</td>
<td>56.9</td>
<td>51.9</td>
<td>74.5</td>
<td>36.7</td>
<td>64.4</td>
<td>41.8</td>
<td>78.4</td>
<td>69.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Heittola2018</td>
<td>46.5</td>
<td>61.6</td>
<td>56.7</td>
<td>45.3</td>
<td>40.0</td>
<td>61.1</td>
<td>15.4</td>
<td>51.8</td>
<td>32.4</td>
<td>69.8</td>
<td>30.4</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_1</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>41.1</td>
<td>33.3</td>
<td>53.0</td>
<td>33.1</td>
<td>30.1</td>
<td>48.9</td>
<td>43.3</td>
<td>38.1</td>
<td>31.4</td>
<td>47.5</td>
<td>52.1</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_2</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>39.5</td>
<td>27.4</td>
<td>66.5</td>
<td>17.3</td>
<td>35.7</td>
<td>52.5</td>
<td>40.7</td>
<td>41.0</td>
<td>23.2</td>
<td>50.6</td>
<td>39.9</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_3</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>42.3</td>
<td>34.7</td>
<td>66.8</td>
<td>27.8</td>
<td>32.2</td>
<td>51.0</td>
<td>52.3</td>
<td>48.4</td>
<td>23.0</td>
<td>47.6</td>
<td>39.3</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_1</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.0</td>
<td>57.6</td>
<td>71.3</td>
<td>65.7</td>
<td>69.6</td>
<td>82.4</td>
<td>57.1</td>
<td>73.4</td>
<td>39.8</td>
<td>82.3</td>
<td>71.1</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_2</td>
<td>Xception</td>
<td>Liping2018</td>
<td>63.2</td>
<td>40.9</td>
<td>73.7</td>
<td>59.8</td>
<td>68.4</td>
<td>84.2</td>
<td>34.8</td>
<td>77.8</td>
<td>42.9</td>
<td>87.5</td>
<td>61.9</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_3</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.7</td>
<td>63.1</td>
<td>72.0</td>
<td>59.5</td>
<td>71.7</td>
<td>86.0</td>
<td>52.3</td>
<td>74.0</td>
<td>42.2</td>
<td>80.4</td>
<td>75.4</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_4</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.1</td>
<td>62.6</td>
<td>71.7</td>
<td>59.8</td>
<td>69.9</td>
<td>88.9</td>
<td>48.2</td>
<td>71.1</td>
<td>44.6</td>
<td>81.7</td>
<td>72.7</td>
</tr>
<tr>
<td></td>
<td>Nguyen_TUGraz_task1b_1</td>
<td>NNF_CNNEns</td>
<td>Nguyen2018</td>
<td>69.0</td>
<td>67.0</td>
<td>86.9</td>
<td>57.6</td>
<td>56.9</td>
<td>93.9</td>
<td>45.6</td>
<td>69.8</td>
<td>53.3</td>
<td>85.1</td>
<td>73.9</td>
</tr>
<tr>
<td></td>
<td>Ren_UAU_task1b_1</td>
<td>ABCNN</td>
<td>Ren2018</td>
<td>60.5</td>
<td>44.6</td>
<td>79.3</td>
<td>52.3</td>
<td>61.4</td>
<td>81.2</td>
<td>29.2</td>
<td>64.0</td>
<td>58.8</td>
<td>81.3</td>
<td>52.7</td>
</tr>
<tr>
<td></td>
<td>Tchorz_THL_task1b_1</td>
<td>AMS_MFCC</td>
<td>Tchorz2018</td>
<td>54.0</td>
<td>44.4</td>
<td>64.5</td>
<td>45.1</td>
<td>43.9</td>
<td>76.6</td>
<td>42.6</td>
<td>57.6</td>
<td>37.2</td>
<td>70.8</td>
<td>57.1</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_ABSP_Fusion18</td>
<td>Waldekar2018</td>
<td>56.2</td>
<td>39.3</td>
<td>62.4</td>
<td>51.1</td>
<td>54.9</td>
<td>73.1</td>
<td>40.2</td>
<td>72.0</td>
<td>41.4</td>
<td>78.4</td>
<td>49.6</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_1</td>
<td>Attention</td>
<td>Jun2018</td>
<td>48.8</td>
<td>37.0</td>
<td>57.2</td>
<td>40.5</td>
<td>60.9</td>
<td>86.9</td>
<td>23.5</td>
<td>50.4</td>
<td>16.2</td>
<td>67.2</td>
<td>48.0</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_2</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.5</td>
<td>70.7</td>
<td>55.4</td>
<td>59.8</td>
<td>44.6</td>
<td>76.3</td>
<td>46.6</td>
<td>48.6</td>
<td>2.1</td>
<td>73.5</td>
<td>47.0</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_3</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.3</td>
<td>51.4</td>
<td>58.5</td>
<td>47.7</td>
<td>59.3</td>
<td>87.8</td>
<td>30.2</td>
<td>52.7</td>
<td>12.8</td>
<td>71.5</td>
<td>50.9</td>
</tr>
</tbody>
</table>
<h1 id="device-wise-performance">Device-wise performance</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_b" data-scatter-y="accuracy_eval_c" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th data-field="code" data-sortable="true" rowspan="2">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true" rowspan="2">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor" rowspan="2">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" colspan="5">Accuracy / Evaluation dataset</th>
</tr>
<tr>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Avg BC) " data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Average<br/> Dev B / Dev C
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev B)" data-chartable="true" data-field="accuracy_eval_b" data-sortable="true" data-value-type="float1-percentage">
                Dev B
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev C)" data-chartable="true" data-field="accuracy_eval_c" data-sortable="true" data-value-type="float1-percentage">
                Dev C
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev A)" data-chartable="true" data-field="accuracy_eval_a" data-sortable="true" data-value-type="float1-percentage">
                Dev A
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Dev D)" data-chartable="true" data-field="accuracy_eval_d" data-sortable="true" data-value-type="float1-percentage">
                Dev D
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_1</td>
<td>SurreyCNN8</td>
<td>Kong2018</td>
<td>59.6</td>
<td>59.5</td>
<td>59.6</td>
<td>69.1</td>
<td>32.1</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_2</td>
<td>SurreyCNN4</td>
<td>Kong2018</td>
<td>58.8</td>
<td>58.7</td>
<td>58.8</td>
<td>70.6</td>
<td>33.8</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Baseline</td>
<td>Heittola2018</td>
<td>46.5</td>
<td>45.9</td>
<td>47.0</td>
<td>63.6</td>
<td>27.5</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_1</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>41.1</td>
<td>42.2</td>
<td>39.9</td>
<td>54.2</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_2</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>39.5</td>
<td>39.8</td>
<td>39.1</td>
<td>55.7</td>
<td>13.2</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_3</td>
<td>Li_SCUT</td>
<td>Li2018</td>
<td>42.3</td>
<td>43.3</td>
<td>41.3</td>
<td>55.7</td>
<td>18.5</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_1</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.0</td>
<td>66.8</td>
<td>67.3</td>
<td>73.7</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_2</td>
<td>Xception</td>
<td>Liping2018</td>
<td>63.2</td>
<td>63.9</td>
<td>62.5</td>
<td>72.2</td>
<td>45.8</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_3</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.7</td>
<td>67.8</td>
<td>67.5</td>
<td>73.9</td>
<td>48.8</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_4</td>
<td>Xception</td>
<td>Liping2018</td>
<td>67.1</td>
<td>67.6</td>
<td>66.7</td>
<td>73.6</td>
<td>47.8</td>
</tr>
<tr>
<td></td>
<td>Nguyen_TUGraz_task1b_1</td>
<td>NNF_CNNEns</td>
<td>Nguyen2018</td>
<td>69.0</td>
<td>68.9</td>
<td>69.1</td>
<td>73.8</td>
<td>37.6</td>
</tr>
<tr>
<td></td>
<td>Ren_UAU_task1b_1</td>
<td>ABCNN</td>
<td>Ren2018</td>
<td>60.5</td>
<td>60.6</td>
<td>60.3</td>
<td>71.2</td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>Tchorz_THL_task1b_1</td>
<td>AMS_MFCC</td>
<td>Tchorz2018</td>
<td>54.0</td>
<td>55.2</td>
<td>52.8</td>
<td>65.1</td>
<td>12.7</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>IITKGP_ABSP_Fusion18</td>
<td>Waldekar2018</td>
<td>56.2</td>
<td>54.8</td>
<td>57.7</td>
<td>58.9</td>
<td>29.5</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_1</td>
<td>Attention</td>
<td>Jun2018</td>
<td>48.8</td>
<td>47.8</td>
<td>49.8</td>
<td>31.1</td>
<td>33.5</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_2</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.5</td>
<td>48.0</td>
<td>56.9</td>
<td>50.1</td>
<td>38.5</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_3</td>
<td>Attention</td>
<td>Jun2018</td>
<td>52.3</td>
<td>49.7</td>
<td>54.8</td>
<td>35.7</td>
<td>36.2</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_1</td>
<td>Kong2018</td>
<td>59.6</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_2</td>
<td>Kong2018</td>
<td>58.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Heittola2018</td>
<td>46.5</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_1</td>
<td>Li2018</td>
<td>41.1</td>
<td>48kHz</td>
<td></td>
<td>MFCC</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_2</td>
<td>Li2018</td>
<td>39.5</td>
<td>48kHz</td>
<td></td>
<td>MFCC</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_3</td>
<td>Li2018</td>
<td>42.3</td>
<td>48kHz</td>
<td></td>
<td>MFCC</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_1</td>
<td>Liping2018</td>
<td>67.0</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_2</td>
<td>Liping2018</td>
<td>63.2</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_3</td>
<td>Liping2018</td>
<td>67.7</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_4</td>
<td>Liping2018</td>
<td>67.1</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>Nguyen_TUGraz_task1b_1</td>
<td>Nguyen2018</td>
<td>69.0</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies and their nearest neighbor filtered version</td>
</tr>
<tr>
<td></td>
<td>Ren_UAU_task1b_1</td>
<td>Ren2018</td>
<td>60.5</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel spectrogram</td>
</tr>
<tr>
<td></td>
<td>Tchorz_THL_task1b_1</td>
<td>Tchorz2018</td>
<td>54.0</td>
<td>44.1kHz</td>
<td></td>
<td>amplitude modulation spectrogram, MFCC</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>Waldekar2018</td>
<td>56.2</td>
<td>48kHz</td>
<td></td>
<td>MFDWC, CQCC</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_1</td>
<td>Jun2018</td>
<td>48.8</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_2</td>
<td>Jun2018</td>
<td>52.5</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_3</td>
<td>Jun2018</td>
<td>52.3</td>
<td>44.1kHz</td>
<td>mixup</td>
<td>log-mel energies</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="system_complexity" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-filter-control="select" data-sortable="true" data-tag="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_1</td>
<td>Kong2018</td>
<td>59.6</td>
<td>4691274</td>
<td>VGGish 8 layer CNN with global max pooling</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Baseline_Surrey_task1b_2</td>
<td>Kong2018</td>
<td>58.8</td>
<td>4309450</td>
<td>VGGish 8 layer CNN with global max pooling</td>
<td></td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2018 baseline</td>
<td>Heittola2018</td>
<td>46.5</td>
<td>116118</td>
<td>CNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_1</td>
<td>Li2018</td>
<td>41.1</td>
<td>116118</td>
<td>LSTM</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_2</td>
<td>Li2018</td>
<td>39.5</td>
<td>116118</td>
<td>LSTM</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1b_3</td>
<td>Li2018</td>
<td>42.3</td>
<td>116118</td>
<td>LSTM</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_1</td>
<td>Liping2018</td>
<td>67.0</td>
<td>22758194</td>
<td>Xception</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_2</td>
<td>Liping2018</td>
<td>63.2</td>
<td>22758194</td>
<td>Xception</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_3</td>
<td>Liping2018</td>
<td>67.7</td>
<td>22758194</td>
<td>Xception</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liping_CQU_task1b_4</td>
<td>Liping2018</td>
<td>67.1</td>
<td>22758194</td>
<td>Xception</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Nguyen_TUGraz_task1b_1</td>
<td>Nguyen2018</td>
<td>69.0</td>
<td>12278040</td>
<td>CNN</td>
<td>12</td>
<td>averaging vote</td>
</tr>
<tr>
<td></td>
<td>Ren_UAU_task1b_1</td>
<td>Ren2018</td>
<td>60.5</td>
<td>616800</td>
<td>CNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Tchorz_THL_task1b_1</td>
<td>Tchorz2018</td>
<td>54.0</td>
<td>15395500</td>
<td>LSTM</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1b_1</td>
<td>Waldekar2018</td>
<td>56.2</td>
<td>20973</td>
<td>SVM</td>
<td>3</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_1</td>
<td>Jun2018</td>
<td>48.8</td>
<td>4634004</td>
<td>CNN,BGRU,self-attention</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_2</td>
<td>Jun2018</td>
<td>52.5</td>
<td>4634004</td>
<td>CNN,BGRU,self-attention</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>WangJun_BUPT_task1b_3</td>
<td>Jun2018</td>
<td>52.3</td>
<td>4634004</td>
<td>CNN,BGRU,self-attention</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2018/technical_reports_task1.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Dang2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Dang2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Ensemble of Convnets
       </h4>
<p style="text-align:left">
        An Dang, Toan Vu and Jia-Ching Wang
       </p>
<p style="text-align:left">
<em>
         Computer Science and Information Engineering, Deep Learning and Media System Laboratory, National Central University, Taoyuan, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dang_NCU_task1a_1</span> <span class="label label-primary">Dang_NCU_task1a_2</span> <span class="label label-primary">Dang_NCU_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dang2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dang2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dang2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dang_104.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dang2018" class="panel-collapse collapse" id="collapse-Dang2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Ensemble of Convnets
      </h4>
<p style="text-align:left">
<small>
        An Dang, Toan Vu and Jia-Ching Wang
       </small>
<br/>
<small>
<em>
         Computer Science and Information Engineering, Deep Learning and Media System Laboratory, National Central University, Taoyuan, Taiwan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents our system for the acoustic scene classification problem in the task 1A of the DCASE2018 challenge whose goal is to classify audio recordings into predefined types of environments. The overall system is an ensemble of ConvNet models working on different audio features separately. Audio signals are processed in both mono channel and two channels before we extract mel-spectrogram and gammatone-based spectrogram features as inputs to models. All models are implemented by almost the same ConvNet structure. Experimental results illustrate that the ensemble system can achieve superior accuracy to the baseline by a large margin of 17% on the test data.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         stereo, mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Ensemble of Convnet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dang2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dang_104.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dang2018label" class="modal fade" id="bibtex-Dang2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDang2018label">
        Acoustic Scene Classification Using Ensemble of Convnets
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dang2018,
    Author = "Dang, An and Vu, Toan and Wang, Jia-Ching",
    title = "Acoustic Scene Classification Using Ensemble of Convnets",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This technical report presents our system for the acoustic scene classification problem in the task 1A of the DCASE2018 challenge whose goal is to classify audio recordings into predefined types of environments. The overall system is an ensemble of ConvNet models working on different audio features separately. Audio signals are processed in both mono channel and two channels before we extract mel-spectrogram and gammatone-based spectrogram features as inputs to models. All models are implemented by almost the same ConvNet structure. Experimental results illustrate that the ensemble system can achieve superior accuracy to the baseline by a large margin of 17\% on the test data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dorfer2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Dorfer2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification with Fully Convolutional Neural Networks and I-Vectors
       </h4>
<p style="text-align:left">
        Matthias Dorfer, Bernhard Lehner, Hamid Eghbal-zadeh, Heindl Christop, Paischer Fabian and Widmer Gerhard
       </p>
<p style="text-align:left">
<em>
         Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dorfer_CPJKU_task1a_1</span> <span class="label label-primary">Dorfer_CPJKU_task1a_2</span> <span class="label label-primary">Dorfer_CPJKU_task1a_3</span> <span class="label label-primary">Dorfer_CPJKU_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dorfer2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dorfer2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dorfer2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dorfer_97.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dorfer2018" class="panel-collapse collapse" id="collapse-Dorfer2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification with Fully Convolutional Neural Networks and I-Vectors
      </h4>
<p style="text-align:left">
<small>
        Matthias Dorfer, Bernhard Lehner, Hamid Eghbal-zadeh, Heindl Christop, Paischer Fabian and Widmer Gerhard
       </small>
<br/>
<small>
<em>
         Institute of Computational Perception, Johannes Kepler University Linz, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the CP-JKU team's submissions for Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge. Our approach is still related to the methodology that achieved ranks 1 and 2 in the 2016 ASC challenge: a fusion of i-vector modelling using MFCC features derived from left and right audio channels, and deep convolutional neural networks (CNNs) trained on spectrograms. However, for our 2018 submission we have put a stronger focus on tuning and pushing the performance of our CNNs. The result of our experiments is a classification system that achieves classification accuracies of around 80% on the public Kaggle-Leaderboard.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, difference; left, right
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.5kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup; pitch shifting; mixup, pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         perceptual weighted power spectrogram; MFCC; perceptual weighted power spectrogram, MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble; i-vector, late fusion; CNN i-vector ensemble; CNN i-vector late fusion ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average; fusion; late calibrated fusion of averaged i-vector and CNN models; late calibrated fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dorfer2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Dorfer_97.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dorfer2018label" class="modal fade" id="bibtex-Dorfer2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDorfer2018label">
        Acoustic Scene Classification with Fully Convolutional Neural Networks and I-Vectors
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dorfer2018,
    Author = "Dorfer, Matthias and Lehner, Bernhard and Eghbal-zadeh, Hamid and Christop, Heindl and Fabian, Paischer and Gerhard, Widmer",
    title = "Acoustic Scene Classification with Fully Convolutional Neural Networks and {I}-Vectors",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This technical report describes the CP-JKU team's submissions for Task 1 - Subtask A (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge. Our approach is still related to the methodology that achieved ranks 1 and 2 in the 2016 ASC challenge: a fusion of i-vector modelling using MFCC features derived from left and right audio channels, and deep convolutional neural networks (CNNs) trained on spectrograms. However, for our 2018 submission we have put a stronger focus on tuning and pushing the performance of our CNNs. The result of our experiments is a classification system that achieves classification accuracies of around 80\% on the public Kaggle-Leaderboard."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fraile2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Fraile2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps
       </h4>
<p style="text-align:left">
        Ruben Fraile, Elena Blanco-Martin, Juana M. Gutierrez-Arriola, Nicolas Saenz-Lechon and Victor J. Osma-Ruiz
       </p>
<p style="text-align:left">
<em>
         Research Center on Software Technologies and Multimedia Systems for Sustainability (CITSEM), Universidad Politecnica de Madrid, Madrid, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Fraile_UPM_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fraile2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fraile2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fraile2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Fraile_84.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fraile2018" class="panel-collapse collapse" id="collapse-Fraile2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps
      </h4>
<p style="text-align:left">
<small>
        Ruben Fraile, Elena Blanco-Martin, Juana M. Gutierrez-Arriola, Nicolas Saenz-Lechon and Victor J. Osma-Ruiz
       </small>
<br/>
<small>
<em>
         Research Center on Software Technologies and Multimedia Systems for Sustainability (CITSEM), Universidad Politecnica de Madrid, Madrid, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       A system for the automatic classification of acoustic scenes is proposed that uses the stereophonic signal captured by a binaural microphone. This system uses one channel for calculating the spectral distribution of energy across auditory-relevant frequency bands. It further obtains some descriptors of the envelope modulation spectrum (EMS) by applying the discrete cosine transform to the logarithm of the EMS. The availability of the two-channel binaural recordings is used for representing the spatial distribution of acoustic sources by means of position-pitch maps. These maps are further parametrized using the two-dimensional Fourier transform. These three types of features (energy spectrum, EMS and positionpitch maps) are used as inputs for a standard multilayer perceptron with two hidden layers.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         LTAS, Modulation spectrum, position-pitch maps
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         sum of log-probabilities
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fraile2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Fraile_84.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fraile2018label" class="modal fade" id="bibtex-Fraile2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFraile2018label">
        Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fraile2018,
    Author = "Fraile, Ruben and Blanco-Martin, Elena and Gutierrez-Arriola, Juana M. and Saenz-Lechon, Nicolas and Osma-Ruiz, Victor J.",
    title = "Classification of Acoustic Scenes Based on Modulation Spectra and Position-Pitch Maps",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "A system for the automatic classification of acoustic scenes is proposed that uses the stereophonic signal captured by a binaural microphone. This system uses one channel for calculating the spectral distribution of energy across auditory-relevant frequency bands. It further obtains some descriptors of the envelope modulation spectrum (EMS) by applying the discrete cosine transform to the logarithm of the EMS. The availability of the two-channel binaural recordings is used for representing the spatial distribution of acoustic sources by means of position-pitch maps. These maps are further parametrized using the two-dimensional Fourier transform. These three types of features (energy spectrum, EMS and positionpitch maps) are used as inputs for a standard multilayer perceptron with two hidden layers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Golubkov2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Golubkov2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Convolutional Neural Networks and Different Channels Representations and Its Fusion
       </h4>
<p style="text-align:left">
        Alexander Golubkov and Alexander Lavrentyev
       </p>
<p style="text-align:left">
<em>
         Saint Petersburg, Russia
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Golubkov_SPCH_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Golubkov2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Golubkov2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Golubkov2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Golubkov_82.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Golubkov2018" class="panel-collapse collapse" id="collapse-Golubkov2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Convolutional Neural Networks and Different Channels Representations and Its Fusion
      </h4>
<p style="text-align:left">
<small>
        Alexander Golubkov and Alexander Lavrentyev
       </small>
<br/>
<small>
<em>
         Saint Petersburg, Russia
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Deep convolutional neural networks has great results in a image classification tasks. In this paper, we used different architectures of DCNN for image classification. As for images we used spectrograms of differenet signal representations, such as MFCC, Melspectrograms and CQT-spectrograms. Result was obtained using goemetric mean of all the models.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, mono, mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT, spectrogram, log-mel, MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Golubkov2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Golubkov_82.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Golubkov2018label" class="modal fade" id="bibtex-Golubkov2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGolubkov2018label">
        Acoustic Scene Classification Using Convolutional Neural Networks and Different Channels Representations and Its Fusion
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Golubkov2018,
    Author = "Golubkov, Alexander and Lavrentyev, Alexander",
    title = "Acoustic Scene Classification Using Convolutional Neural Networks and Different Channels Representations and Its Fusion",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Deep convolutional neural networks has great results in a image classification tasks. In this paper, we used different architectures of DCNN for image classification. As for images we used spectrograms of differenet signal representations, such as MFCC, Melspectrograms and CQT-spectrograms. Result was obtained using goemetric mean of all the models."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hao2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Hao2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2018 Task 1a: Acoustic Scene Classification by Bi-LSTM-CNN-Net Multichannel Fusion
       </h4>
<p style="text-align:left">
        WenJie Hao, Lasheng Zhao, Qiang Zhang, HanYu Zhao and JiaHua Wang
       </p>
<p style="text-align:left">
<em>
         Key Laboratory of Advanced Design and Intelligent Computing(Dalian University), Ministry of Education, Dalian University, Liaoning, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhao_DLU_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hao2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hao2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hao2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Zhao_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hao2018" class="panel-collapse collapse" id="collapse-Hao2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2018 Task 1a: Acoustic Scene Classification by Bi-LSTM-CNN-Net Multichannel Fusion
      </h4>
<p style="text-align:left">
<small>
        WenJie Hao, Lasheng Zhao, Qiang Zhang, HanYu Zhao and JiaHua Wang
       </small>
<br/>
<small>
<em>
         Key Laboratory of Advanced Design and Intelligent Computing(Dalian University), Ministry of Education, Dalian University, Liaoning, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this study, we provide a solution for acoustic scene classification task in the DCASE 2018 challenge. A system consisting of bidirectional long-term memory and convolutional neural networks(BI-LSTM-CNN) is proposed. And, improved logarithmic scaled mel spectra as input to our system. Besides we have adopted a new model fusion mechanism. Finally, to validate the performance of the model and compare it to the baseline system, we used the TUT Acoustic Scene 2018 dataset for training and cross-validation, resulting in an 13.93% improvement over the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         multichannel
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN,Bi-Lstm
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         max of precision
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hao2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Zhao_16.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hao2018label" class="modal fade" id="bibtex-Hao2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHao2018label">
        DCASE 2018 Task 1a: Acoustic Scene Classification by Bi-LSTM-CNN-Net Multichannel Fusion
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hao2018,
    Author = "Hao, WenJie and Zhao, Lasheng and Zhang, Qiang and Zhao, HanYu and Wang, JiaHua",
    title = "{DCASE} 2018 Task 1a: Acoustic Scene Classification by Bi-{LSTM}-{CNN}-Net Multichannel Fusion",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this study, we provide a solution for acoustic scene classification task in the DCASE 2018 challenge. A system consisting of bidirectional long-term memory and convolutional neural networks(BI-LSTM-CNN) is proposed. And, improved logarithmic scaled mel spectra as input to our system. Besides we have adopted a new model fusion mechanism. Finally, to validate the performance of the model and compare it to the baseline system, we used the TUT Acoustic Scene 2018 dataset for training and cross-validation, resulting in an 13.93\% improvement over the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Heittola2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Heittola2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Multi-Device Dataset for Urban Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Toni Heittola, Annamaria Mesaros and Tuomas Virtanen
       </p>
<p style="text-align:left">
<em>
         Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Heittola_TUT_task1a_1</span> <span class="clearfix"></span><span class="label label-info">Heittola_TUT_task1b_1</span> <span class="clearfix"></span><span class="label label-warning">Heittola_TUT_task1c_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Heittola2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Heittola2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Heittola2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="http://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Heittola2018').collapse('show');window.location.hash='#Heittola2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Heittola2018" class="panel-collapse collapse" id="collapse-Heittola2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Multi-Device Dataset for Urban Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Toni Heittola, Annamaria Mesaros and Tuomas Virtanen
       </small>
<br/>
<small>
<em>
         Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Heittola2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="http://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Mesaros_8.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Heittola2018label" class="modal fade" id="bibtex-Heittola2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHeittola2018label">
        A Multi-Device Dataset for Urban Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Heittola2018,
    Author = "Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas",
    title = "A Multi-Device Dataset for Urban Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper introduces the acoustic scene classification task of DCASE 2018 Challenge and the TUT Urban Acoustic Scenes 2018 dataset provided for the task, and evaluates the performance of a baseline system in the task. As in previous years of the challenge, the task is defined for classification of short audio samples into one of predefined acoustic scene classes, using a supervised, closed-set classification setup. The newly recorded TUT Urban Acoustic Scenes 2018 dataset consists of ten different acoustic scenes and was recorded in six large European cities, therefore it has a higher acoustic variability than the previous datasets used for this task, and in addition to high-quality binaural recordings, it also includes data recorded with mobile devices. We also present the baseline system consisting of a convolutional neural network and its performance in the subtasks using the recommended cross-validation setup."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jun2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Jun2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Self-Attention Mechanism Based System for Dcase2018 Challenge Task1 and Task4
       </h4>
<p style="text-align:left">
        Wang Jun<sup>1</sup> and Li Shengchen<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Institute of Information Photonics and Optical Communication, c, Beijing, China, <sup>2</sup>Institute of Information Photonics and Optical Communication, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">WangJun_BUPT_task1a_1</span> <span class="label label-primary">WangJun_BUPT_task1a_2</span> <span class="label label-primary">WangJun_BUPT_task1a_3</span> <span class="clearfix"></span><span class="label label-info">WangJun_BUPT_task1b_1</span> <span class="label label-info">WangJun_BUPT_task1b_2</span> <span class="label label-info">WangJun_BUPT_task1b_3</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jun2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jun2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jun2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wangjun_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jun2018" class="panel-collapse collapse" id="collapse-Jun2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Self-Attention Mechanism Based System for Dcase2018 Challenge Task1 and Task4
      </h4>
<p style="text-align:left">
<small>
        Wang Jun<sup>1</sup> and Li Shengchen<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Institute of Information Photonics and Optical Communication, c, Beijing, China, <sup>2</sup>Institute of Information Photonics and Optical Communication, Beijing University of Posts and Telecommunications, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technique report, we provide self-attention mechanism for the Task1 and Task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE2017) challenge. We take convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) as our basic systems in Task 1 and Task 4. In this convolutional recurrent neural network (CRNN), gated linear units (GLUs) is used for non-linearity which implement a gating mechanism over the output of the network for selecting informative local features. Self-attention mechanism called intra-attention is used for modeling relationship between different positions of a single sequence over the output of the CRNN. Attention-based pooling scheme is used for localizing the specific events in Task 4 and for obtaining the final labels in Task 1. In a summary, we get 70.81% accuracy subtask 1 of Task 1. In the subtask 2 of Task 1, we get 70.1% accuracy for device a, 59.4% accuracy for device b, and 55.6 accuracy for device c. For Task 1, we get 26.98% F1 value for sound event detection in old test data of developmemt data.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN,BGRU,self-attention
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jun2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Wangjun_62.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jun2018label" class="modal fade" id="bibtex-Jun2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJun2018label">
        Self-Attention Mechanism Based System for Dcase2018 Challenge Task1 and Task4
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jun2018,
    Author = "Jun, Wang and Shengchen, Li",
    title = "Self-Attention Mechanism Based System for Dcase2018 Challenge Task1 and Task4",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technique report, we provide self-attention mechanism for the Task1 and Task 4 of Detection and Classification of Acoustic Scenes and Events 2018 (DCASE2017) challenge. We take convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) as our basic systems in Task 1 and Task 4. In this convolutional recurrent neural network (CRNN), gated linear units (GLUs) is used for non-linearity which implement a gating mechanism over the output of the network for selecting informative local features. Self-attention mechanism called intra-attention is used for modeling relationship between different positions of a single sequence over the output of the CRNN. Attention-based pooling scheme is used for localizing the specific events in Task 4 and for obtaining the final labels in Task 1. In a summary, we get 70.81\% accuracy subtask 1 of Task 1. In the subtask 2 of Task 1, we get 70.1\% accuracy for device a, 59.4\% accuracy for device b, and 55.6 accuracy for device c. For Task 1, we get 26.98\% F1 value for sound event detection in old test data of developmemt data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jung2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Jung2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DNN Based Multi-Level Features Ensemble for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Jee-weon Jung, Hee-soo Heo, Hye-jin Shim and Ha-jin Yu
       </p>
<p style="text-align:left">
<em>
         School of Computer Science, University of Seoul, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jung_UOS_task1a_1</span> <span class="label label-primary">Jung_UOS_task1a_2</span> <span class="label label-primary">Jung_UOS_task1a_3</span> <span class="label label-primary">Jung_UOS_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jung2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jung2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jung2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Jung_99.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jung2018" class="panel-collapse collapse" id="collapse-Jung2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DNN Based Multi-Level Features Ensemble for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Jee-weon Jung, Hee-soo Heo, Hye-jin Shim and Ha-jin Yu
       </small>
<br/>
<small>
<em>
         School of Computer Science, University of Seoul, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic scenes are defined by various characteristics such as long-term context or short-term event, making it difficult to select input features or pre-processing methods suitable for acoustic scene classification. In this paper, we propose an ensemble model which exploits various input features that vary in their degree of preprocessing: raw waveform without pre-processing, spectrogram, and i-vector a segment-level low dimensional representation. We tried to effectively perform combination of deep neural networks that handle different types of input features by using a separate scoring phase by using Gaussian models and support vector machines to extract scores from individual system that can be used as a confidence measure. Validity of the proposed framework is tested using the detection and classification of acoustic scenes and events 2018 dataset. The proposed framework showed accuracy of 73.82% using the validation set.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         raw-waveform, spectrogram, i-vector
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, DNN, GMM, SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         score-sum; weighted score-sum
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jung2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Jung_99.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jung2018label" class="modal fade" id="bibtex-Jung2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJung2018label">
        DNN Based Multi-Level Features Ensemble for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jung2018,
    Author = "Jung, Jee-weon and Heo, Hee-soo and Shim, Hye-jin and Yu, Ha-jin",
    title = "{DNN} Based Multi-Level Features Ensemble for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Acoustic scenes are defined by various characteristics such as long-term context or short-term event, making it difficult to select input features or pre-processing methods suitable for acoustic scene classification. In this paper, we propose an ensemble model which exploits various input features that vary in their degree of preprocessing: raw waveform without pre-processing, spectrogram, and i-vector a segment-level low dimensional representation. We tried to effectively perform combination of deep neural networks that handle different types of input features by using a separate scoring phase by using Gaussian models and support vector machines to extract scores from individual system that can be used as a confidence measure. Validity of the proposed framework is tested using the detection and classification of acoustic scenes and events 2018 dataset. The proposed framework showed accuracy of 73.82\% using the validation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Khadkevich2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Khadkevich2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene and Event Detection Systems Submitted to DCASE 2018 Challenge
       </h4>
<p style="text-align:left">
        Maksim Khadkevich
       </p>
<p style="text-align:left">
<em>
         AML, Facebook, Menlo Park, CA, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Khadkevich_FB_task1a_1</span> <span class="label label-primary">Khadkevich_FB_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="label label-warning">Khadkevich_FB_task1c_1</span> <span class="label label-warning">Khadkevich_FB_task1c_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Khadkevich2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Khadkevich2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Khadkevich2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Khadkevich_88.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Khadkevich2018" class="panel-collapse collapse" id="collapse-Khadkevich2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene and Event Detection Systems Submitted to DCASE 2018 Challenge
      </h4>
<p style="text-align:left">
<small>
        Maksim Khadkevich
       </small>
<br/>
<small>
<em>
         AML, Facebook, Menlo Park, CA, USA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report we describe systems that have been submitted to DCASE 2018 [1] challenge. Feature extraction and convolutional neural network (CNN) architecture are outlined. For tasks 1c and 2 we describe transfer learning approach that has been applied. Model training and inference are finally presented.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Khadkevich2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Khadkevich_88.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Khadkevich2018label" class="modal fade" id="bibtex-Khadkevich2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKhadkevich2018label">
        Acoustic Scene and Event Detection Systems Submitted to DCASE 2018 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Khadkevich2018,
    Author = "Khadkevich, Maksim",
    title = "Acoustic Scene and Event Detection Systems Submitted to {DCASE} 2018 Challenge",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this technical report we describe systems that have been submitted to DCASE 2018 [1] challenge. Feature extraction and convolutional neural network (CNN) architecture are outlined. For tasks 1c and 2 we describe transfer learning approach that has been applied. Model training and inference are finally presented."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kong2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Kong2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2018 Challenge Surrey Cross-Task Convolutional Neural Network Baseline
       </h4>
<p style="text-align:left">
        Qiuqiang Kong, Iqbal Turab, Xu Yong, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Baseline_Surrey_task1a_1</span> <span class="label label-primary">Baseline_Surrey_task1a_2</span> <span class="clearfix"></span><span class="label label-info">Baseline_Surrey_task1b_1</span> <span class="label label-info">Baseline_Surrey_task1b_2</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kong2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kong2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Baseline_87.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Kong2018').collapse('show');window.location.hash='#Kong2018';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kong2018" class="panel-collapse collapse" id="collapse-Kong2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2018 Challenge Surrey Cross-Task Convolutional Neural Network Baseline
      </h4>
<p style="text-align:left">
<small>
        Qiuqiang Kong, Iqbal Turab, Xu Yong, Wenwu Wang and Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         Centre for Vission, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The Detection and Classification of Acoustic Scenes and Events (DCASE) consists of five audio classification and sound event detection tasks: 1) Acoustic scene classification, 2) General-purpose audio tagging of Freesound, 3) Bird audio detection, 4) Weaklylabeled semi-supervised sound event detection and 5) Multi-channel audio classification. In this paper, we create a cross-task baseline system for all five tasks based on a convolutional neural network (CNN): a “CNN Baseline” system. We implemented CNNs with 4 layers and 8 layers originating from AlexNet and VGG from computer vision. We investigated how the performance varies from task to task with the same configuration of neural networks. Experiments show that deeper CNN with 8 layers performs better than CNN with 4 layers on all tasks except Task 1. Using CNN with 8 layers, we achieve an accuracy of 0.680 on Task 1, an accuracy of 0.895 and a mean average precision (MAP) of 0.928 on Task 2, an accuracy of 0.751 and an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8% on Task 4, and an F1 score of 87.75% on Task 5. We released the Python source code of the baseline systems under the MIT license for further research.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         VGGish 8 layer CNN with global max pooling; AlexNetish 4 layer CNN with global max pooling
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kong2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Baseline_87.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/qiuqiangkong/dcase2018_task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kong2018label" class="modal fade" id="bibtex-Kong2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKong2018label">
        DCASE 2018 Challenge Surrey Cross-Task Convolutional Neural Network Baseline
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kong2018,
    Author = "Kong, Qiuqiang and Turab, Iqbal and Yong, Xu and Wang, Wenwu and Plumbley, Mark D.",
    title = "{DCASE} 2018 Challenge Surrey Cross-Task Convolutional Neural Network Baseline",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "The Detection and Classification of Acoustic Scenes and Events (DCASE) consists of five audio classification and sound event detection tasks: 1) Acoustic scene classification, 2) General-purpose audio tagging of Freesound, 3) Bird audio detection, 4) Weaklylabeled semi-supervised sound event detection and 5) Multi-channel audio classification. In this paper, we create a cross-task baseline system for all five tasks based on a convolutional neural network (CNN): a “CNN Baseline” system. We implemented CNNs with 4 layers and 8 layers originating from AlexNet and VGG from computer vision. We investigated how the performance varies from task to task with the same configuration of neural networks. Experiments show that deeper CNN with 8 layers performs better than CNN with 4 layers on all tasks except Task 1. Using CNN with 8 layers, we achieve an accuracy of 0.680 on Task 1, an accuracy of 0.895 and a mean average precision (MAP) of 0.928 on Task 2, an accuracy of 0.751 and an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8\% on Task 4, and an F1 score of 87.75\% on Task 5. We released the Python source code of the baseline systems under the MIT license for further research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with CNN and LSTM
       </h4>
<p style="text-align:left">
        Zhitong Li, Liqiang Zhang, Shixuan Du and Wei Liu
       </p>
<p style="text-align:left">
<em>
         Laboratory of Modern Communication, Beijing Institute of Technology, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_BIT_task1a_1</span> <span class="label label-primary">Li_BIT_task1a_2</span> <span class="label label-primary">Li_BIT_task1a_3</span> <span class="label label-primary">Li_BIT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Li_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2018" class="panel-collapse collapse" id="collapse-Li2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with CNN and LSTM
      </h4>
<p style="text-align:left">
<small>
        Zhitong Li, Liqiang Zhang, Shixuan Du and Wei Liu
       </small>
<br/>
<small>
<em>
         Laboratory of Modern Communication, Beijing Institute of Technology, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report presents the solutions proposed by the Beijing Institute of Technology Modern Communications Technology Laboratory for the acoustic scene classification of DCASE2018 task1a. Compared to previous years, the data is more diverse, making such tasks more difficult. In order to solve this problem, we use the Deep Scattering Spectra (DSS) features. The traditional features, such as Mel-frequency Cepstral Coefficients (MFCC), often lose information at high frequencies. DSS is a good way to preserve high frequency information. Based on this feature, we propose a network model of Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) to classify sound scenes. The experimental results show that the proposed feature extraction method and network structure have a good effect on this classification task. From the experimental data, the accuracy increased from 59% to 76%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left,right
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         DSS
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; CNN,DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Li_51.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2018label" class="modal fade" id="bibtex-Li2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2018label">
        Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with CNN and LSTM
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2018,
    Author = "Li, Zhitong and Zhang, Liqiang and Du, Shixuan and Liu, Wei",
    title = "Acoustic Scene Classification Based on Binaural Deep Scattering Spectra with {CNN} and {LSTM}",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This technical report presents the solutions proposed by the Beijing Institute of Technology Modern Communications Technology Laboratory for the acoustic scene classification of DCASE2018 task1a. Compared to previous years, the data is more diverse, making such tasks more difficult. In order to solve this problem, we use the Deep Scattering Spectra (DSS) features. The traditional features, such as Mel-frequency Cepstral Coefficients (MFCC), often lose information at high frequencies. DSS is a good way to preserve high frequency information. Based on this feature, we propose a network model of Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) to classify sound scenes. The experimental results show that the proposed feature extraction method and network structure have a good effect on this classification task. From the experimental data, the accuracy increased from 59\% to 76\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2018a" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2018a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The SEIE-SCUT Systems for Challenge on DCASE 2018: Deep Learning Techniques for Audio Representation and Classification
       </h4>
<p style="text-align:left">
        YangXiong Li, Xianku Li and Yuhan Zhang
       </p>
<p style="text-align:left">
<em>
         Laboratory of Signal Processing, South China University of Technology, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_SCUT_task1a_1</span> <span class="label label-primary">Li_SCUT_task1a_2</span> <span class="label label-primary">Li_SCUT_task1a_3</span> <span class="label label-primary">Li_SCUT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2018a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2018a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Li_12.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2018a" class="panel-collapse collapse" id="collapse-Li2018a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The SEIE-SCUT Systems for Challenge on DCASE 2018: Deep Learning Techniques for Audio Representation and Classification
      </h4>
<p style="text-align:left">
<small>
        YangXiong Li, Xianku Li and Yuhan Zhang
       </small>
<br/>
<small>
<em>
         Laboratory of Signal Processing, South China University of Technology, Guangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our works about one task of challenge on DCASE 2018, i.e. task 1a: Acoustic Scene Classification (ASC). We adopt deep learning techniques to extract Deep Audio Feature (DAF) and classify various acoustic scenes . Specifically, a Deep Neural Network (DNN) is first built for generating the DAF from MelFrequency Cepstral Coefficients (MFCCs), and then a Recurrent Neural Network (RNN) of Bidirectional Long Short Term Memory (BLSTM) fed by the DAF is built for ASC. Evaluated on the development datasets of DCASE 2018, our systems are superior to the corresponding baselines for tasks 1a.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         LSTM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2018a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Li_12.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2018alabel" class="modal fade" id="bibtex-Li2018a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2018alabel">
        The SEIE-SCUT Systems for Challenge on DCASE 2018: Deep Learning Techniques for Audio Representation and Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2018a,
    Author = "Li, YangXiong and Li, Xianku and Zhang, Yuhan",
    title = "The {SEIE-SCUT} Systems for Challenge on {DCASE} 2018: Deep Learning Techniques for Audio Representation and Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this report, we present our works about one task of challenge on DCASE 2018, i.e. task 1a: Acoustic Scene Classification (ASC). We adopt deep learning techniques to extract Deep Audio Feature (DAF) and classify various acoustic scenes . Specifically, a Deep Neural Network (DNN) is first built for generating the DAF from MelFrequency Cepstral Coefficients (MFCCs), and then a Recurrent Neural Network (RNN) of Bidirectional Long Short Term Memory (BLSTM) fed by the DAF is built for ASC. Evaluated on the development datasets of DCASE 2018, our systems are superior to the corresponding baselines for tasks 1a."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2018b" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2018b" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The SEIE-SCUT Systems for Challenge on DCASE 2018: Deep Learning Techniques for Audio Representation and Classification
       </h4>
<p style="text-align:left">
        YangXiong Li, Yuhan Zhang and Xianku Li
       </p>
<p style="text-align:left">
<em>
         Laboratory of Signal Processing, South China University of Technology, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Li_SCUT_task1b_1</span> <span class="label label-info">Li_SCUT_task1b_2</span> <span class="label label-info">Li_SCUT_task1b_3</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2018b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2018b" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Li_27.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2018b" class="panel-collapse collapse" id="collapse-Li2018b" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The SEIE-SCUT Systems for Challenge on DCASE 2018: Deep Learning Techniques for Audio Representation and Classification
      </h4>
<p style="text-align:left">
<small>
        YangXiong Li, Yuhan Zhang and Xianku Li
       </small>
<br/>
<small>
<em>
         Laboratory of Signal Processing, South China University of Technology, Guangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our works about one task of challenge on DCASE 2018, i.e. task 1b:Acoustic Scene Classification with mismatched recording devices (ASC). We adopt deep learning techniques to extract Deep Audio Feature (DAF) and classify various acoustic scenes . Specifically, a Deep Neural Network (DNN) is first built for generating the DAF from Mel-Frequency Cepstral Coefficients (MFCCs), and then a Recurrent Neural Network (RNN) of Bidirectional Long Short Term Memory (BLSTM) fed by the DAF is built for ASC. Evaluated on the development datasets of DCASE 2018, our systems are superior to the corresponding baselines for tasks 1b.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         LSTM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2018b" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Li_27.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2018blabel" class="modal fade" id="bibtex-Li2018b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2018blabel">
        The SEIE-SCUT Systems for Challenge on DCASE 2018: Deep Learning Techniques for Audio Representation and Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2018b,
    Author = "Li, YangXiong and Zhang, Yuhan and Li, Xianku",
    title = "The {SEIE-SCUT} Systems for Challenge on {DCASE} 2018: Deep Learning Techniques for Audio Representation and Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this report, we present our works about one task of challenge on DCASE 2018, i.e. task 1b:Acoustic Scene Classification with mismatched recording devices (ASC). We adopt deep learning techniques to extract Deep Audio Feature (DAF) and classify various acoustic scenes . Specifically, a Deep Neural Network (DNN) is first built for generating the DAF from Mel-Frequency Cepstral Coefficients (MFCCs), and then a Recurrent Neural Network (RNN) of Bidirectional Long Short Term Memory (BLSTM) fed by the DAF is built for ASC. Evaluated on the development datasets of DCASE 2018, our systems are superior to the corresponding baselines for tasks 1b."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liping2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Liping2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Multi-Scale Features
       </h4>
<p style="text-align:left">
        Yang Liping, Chen Xinxing and Tao Lianjie
       </p>
<p style="text-align:left">
<em>
         College of Optoelectronic Engineering, Chongqing University, Chongqing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liping_CQU_task1a_1</span> <span class="label label-primary">Liping_CQU_task1a_2</span> <span class="label label-primary">Liping_CQU_task1a_3</span> <span class="label label-primary">Liping_CQU_task1a_4</span> <span class="clearfix"></span><span class="label label-info">Liping_CQU_task1b_1</span> <span class="label label-info">Liping_CQU_task1b_2</span> <span class="label label-info">Liping_CQU_task1b_3</span> <span class="label label-info">Liping_CQU_task1b_4</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liping2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liping2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liping2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Liping_36.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liping2018" class="panel-collapse collapse" id="collapse-Liping2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Multi-Scale Features
      </h4>
<p style="text-align:left">
<small>
        Yang Liping, Chen Xinxing and Tao Lianjie
       </small>
<br/>
<small>
<em>
         College of Optoelectronic Engineering, Chongqing University, Chongqing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Convolutional neural networks(CNN) has shown tremendous ability in classification problems, because it can extract abstract features for improving classification performance. In this paper, we use CNN to compute feature hierarchy layer by layer. With the layers deepen, the extracted features become more abstract, but the shallow features are also very useful for classification. So we propose a fuse multi-scale features of different layers method, which can improve performance of acoustic scene classification. In our method, the logmel features of audio signal are used as the input of CNN. In order to reduce the parameters' number, we use xception as the foundation network, which is a CNN with depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). And we modify xception to fuse multi-scale features. We also introduce the focal loss, to further improve classification performance. This method can achieve commendable result, whether the audio recordings are collected by same device(subtask A) or by different devices (subtask B).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Xception
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liping2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Liping_36.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liping2018label" class="modal fade" id="bibtex-Liping2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiping2018label">
        Acoustic Scene Classification Using Multi-Scale Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liping2018,
    Author = "Liping, Yang and Xinxing, Chen and Lianjie, Tao",
    title = "Acoustic Scene Classification Using Multi-Scale Features",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Convolutional neural networks(CNN) has shown tremendous ability in classification problems, because it can extract abstract features for improving classification performance. In this paper, we use CNN to compute feature hierarchy layer by layer. With the layers deepen, the extracted features become more abstract, but the shallow features are also very useful for classification. So we propose a fuse multi-scale features of different layers method, which can improve performance of acoustic scene classification. In our method, the logmel features of audio signal are used as the input of CNN. In order to reduce the parameters' number, we use xception as the foundation network, which is a CNN with depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). And we modify xception to fuse multi-scale features. We also introduce the focal loss, to further improve classification performance. This method can achieve commendable result, whether the audio recordings are collected by same device(subtask A) or by different devices (subtask B)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Maka2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Maka2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Auditory Scene Classification Using Ensemble Learning with Small Audio Feature Space
       </h4>
<p style="text-align:left">
        Tomasz Maka
       </p>
<p style="text-align:left">
<em>
         Faculty of Computer Science and Information Technology, West Pomeranian University of Technology, Szczecin, Szczecin, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Maka_ZUT_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Maka2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Maka2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Maka2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Maka_78.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Maka2018" class="panel-collapse collapse" id="collapse-Maka2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Auditory Scene Classification Using Ensemble Learning with Small Audio Feature Space
      </h4>
<p style="text-align:left">
<small>
        Tomasz Maka
       </small>
<br/>
<small>
<em>
         Faculty of Computer Science and Information Technology, West Pomeranian University of Technology, Szczecin, Szczecin, Poland
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The report presents the results of an analysis of audio feature space for auditory scene classification. The final small feature set was determined by the selection of the attributes from various representations. Feature importance was calculated exploiting the Gradient Boosting Machine. A number of classifiers were employed to build the ensemble classification scheme, and majority voting was performed to obtain the final decision. In the result, the proposed solution uses 223 attributes and outperforms the baseline system by over 6 per cent.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Maka2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Maka_78.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Maka2018label" class="modal fade" id="bibtex-Maka2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMaka2018label">
        Auditory Scene Classification Using Ensemble Learning with Small Audio Feature Space
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Maka2018,
    Author = "Maka, Tomasz",
    title = "Auditory Scene Classification Using Ensemble Learning with Small Audio Feature Space",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "The report presents the results of an analysis of audio feature space for auditory scene classification. The final small feature set was determined by the selection of the attributes from various representations. Feature importance was calculated exploiting the Gradient Boosting Machine. A number of classifiers were employed to build the ensemble classification scheme, and majority voting was performed to obtain the final decision. In the result, the proposed solution uses 223 attributes and outperforms the baseline system by over 6 per cent."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mariotti2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Mariotti2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Exploring Deep Vision Models for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Octave Mariotti, Matthieu Cord and Olivier Schwander
       </p>
<p style="text-align:left">
<em>
         Laboratoire d'informatique de Paris 6, Sorbonne Université, Paris, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mariotti_lip6_task1a_1</span> <span class="label label-primary">Mariotti_lip6_task1a_2</span> <span class="label label-primary">Mariotti_lip6_task1a_3</span> <span class="label label-primary">Mariotti_lip6_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mariotti2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mariotti2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mariotti2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Mariotti_85.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mariotti2018" class="panel-collapse collapse" id="collapse-Mariotti2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Exploring Deep Vision Models for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Octave Mariotti, Matthieu Cord and Olivier Schwander
       </small>
<br/>
<small>
<em>
         Laboratoire d'informatique de Paris 6, Sorbonne Université, Paris, France
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report evaluates the application of deep vision models, namely VGG and Resnet, to general audio recognition. In the context of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events 2018, we trained several of these architecture on the task 1 dataset to perform acoustic scene classification. Then, in order to produce more robust predictions, we explored two ensemble methods to aggregate the different model outputs. Our results show a final accuracy of 79% on the development dataset, outperforming the baseline by almost 20%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability; neural network
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mariotti2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Mariotti_85.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mariotti2018label" class="modal fade" id="bibtex-Mariotti2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMariotti2018label">
        Exploring Deep Vision Models for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mariotti2018,
    Author = "Mariotti, Octave and Cord, Matthieu and Schwander, Olivier",
    title = "Exploring Deep Vision Models for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This report evaluates the application of deep vision models, namely VGG and Resnet, to general audio recognition. In the context of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events 2018, we trained several of these architecture on the task 1 dataset to perform acoustic scene classification. Then, in order to produce more robust predictions, we explored two ensemble methods to aggregate the different model outputs. Our results show a final accuracy of 79\% on the development dataset, outperforming the baseline by almost 20\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Nguyen2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters
       </h4>
<p style="text-align:left">
        Truc Nguyen and Franz Pernkopf
       </p>
<p style="text-align:left">
<em>
         Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria/ Europe
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nguyen_TUGraz_task1a_1</span> <span class="clearfix"></span><span class="label label-info">Nguyen_TUGraz_task1b_1</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nguyen2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Nguyen_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen2018" class="panel-collapse collapse" id="collapse-Nguyen2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters
      </h4>
<p style="text-align:left">
<small>
        Truc Nguyen and Franz Pernkopf
       </small>
<br/>
<small>
<em>
         Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria/ Europe
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper proposes Convolutional Neural Network (CNN) ensembles for acoustic scene classification of subtasks 1A and 1B of DCASE 2018 challenge. We introduce a nearest neighbor filter applied on spectrogram, which allows to emphasize and smooth similar patterns of sound events in a scene. We also propose a variety of CNN models for single-input (SI) and multi-input (MI) channels and three different methods for building a network ensemble. The experimental results show that for subtask 1A the combination of the MI-CNN structures using both of log-mel features and their nearest neighbor filtering is slightly more effective than that of single-input channel CNN models using log-mel features only. This statement is opposite for subtask 1B. In addition, the ensemble methods improve the accuracy of the system significantly, in which the best ensemble method is ensemble selection, which achieves 69.3% for subtask 1A and 63.6% for subtask 1B. This improves the baseline system by 8.9% and 14.4% for subtask 1A and 1B, respectively
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz; 44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies and their nearest neighbor filtered version
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         averaging vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Nguyen_57.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen2018label" class="modal fade" id="bibtex-Nguyen2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen2018label">
        Acoustic Scene Classification Using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nguyen2018,
    Author = "Nguyen, Truc and Pernkopf, Franz",
    title = "Acoustic Scene Classification Using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This paper proposes Convolutional Neural Network (CNN) ensembles for acoustic scene classification of subtasks 1A and 1B of DCASE 2018 challenge. We introduce a nearest neighbor filter applied on spectrogram, which allows to emphasize and smooth similar patterns of sound events in a scene. We also propose a variety of CNN models for single-input (SI) and multi-input (MI) channels and three different methods for building a network ensemble. The experimental results show that for subtask 1A the combination of the MI-CNN structures using both of log-mel features and their nearest neighbor filtering is slightly more effective than that of single-input channel CNN models using log-mel features only. This statement is opposite for subtask 1B. In addition, the ensemble methods improve the accuracy of the system significantly, in which the best ensemble method is ensemble selection, which achieves 69.3\% for subtask 1A and 63.6\% for subtask 1B. This improves the baseline system by 8.9\% and 14.4\% for subtask 1A and 1B, respectively"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Purohit2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Purohit2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep CNN on Raw-Waveform
       </h4>
<p style="text-align:left">
        Tilak Purohit and Atul Agarwal
       </p>
<p style="text-align:left">
<em>
         Signal Processing and Pattern Recognition Lab, International Institute of Information Technology, Bangaluru, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Tilak_IIITB_task1a_1</span> <span class="label label-primary">Tilak_IIITB_task1a_2</span> <span class="label label-primary">Tilak_IIITB_task1a_3</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Purohit2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Purohit2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Purohit2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Tilak_73.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Purohit2018" class="panel-collapse collapse" id="collapse-Purohit2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep CNN on Raw-Waveform
      </h4>
<p style="text-align:left">
<small>
        Tilak Purohit and Atul Agarwal
       </small>
<br/>
<small>
<em>
         Signal Processing and Pattern Recognition Lab, International Institute of Information Technology, Bangaluru, India
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       For acoustic scene classification problems, conventionally Convolutional Neural Networks (CNNs) have been used on handcrafted features like Mel Frequency Cepstral Coefficients, filterbank energies, scaled spectrograms etc. However, recently CNNs have been used on raw waveform for acoustic modeling in speech recognition, though the time-scales of these waveforms are short (of the order of typical phoneme durations - 80-120 ms). In this work, we have exploited the representation learning power of CNNs by using them directly on very long raw acoustic sound waveforms (of durations 0.5-10 sec) for the acoustic scene classification (ASC) task of DCASE and have shown that deep CNNs (of 8-34 layers) can outperform CNNs with similar architecture on handcrafted features.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         8kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         raw-waveform
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; DCNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Purohit2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Tilak_73.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Purohit2018label" class="modal fade" id="bibtex-Purohit2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPurohit2018label">
        Acoustic Scene Classification Using Deep CNN on Raw-Waveform
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Purohit2018,
    Author = "Purohit, Tilak and Agarwal, Atul",
    title = "Acoustic Scene Classification Using Deep {CNN} on Raw-Waveform",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "For acoustic scene classification problems, conventionally Convolutional Neural Networks (CNNs) have been used on handcrafted features like Mel Frequency Cepstral Coefficients, filterbank energies, scaled spectrograms etc. However, recently CNNs have been used on raw waveform for acoustic modeling in speech recognition, though the time-scales of these waveforms are short (of the order of typical phoneme durations - 80-120 ms). In this work, we have exploited the representation learning power of CNNs by using them directly on very long raw acoustic sound waveforms (of durations 0.5-10 sec) for the acoustic scene classification (ASC) task of DCASE and have shown that deep CNNs (of 8-34 layers) can outperform CNNs with similar architecture on handcrafted features."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ren2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Ren2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Attention-Based Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Zhao Ren<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Kun Qian<sup>1</sup>, Mark Plumbley<sup>2</sup> and Björn Schuller<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Surrey, UK, <sup>3</sup>ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing / GLAM -- Group on Language, Audio \&amp; Music, University of Augsburg, Imperial College London, Augsburg, Germany / London, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ren_UAU_task1a_1</span> <span class="clearfix"></span><span class="label label-info">Ren_UAU_task1b_1</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ren2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ren2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ren2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Ren_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ren2018" class="panel-collapse collapse" id="collapse-Ren2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Attention-Based Convolutional Neural Networks for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Zhao Ren<sup>1</sup>, Qiuqiang Kong<sup>2</sup>, Kun Qian<sup>1</sup>, Mark Plumbley<sup>2</sup> and Björn Schuller<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Surrey, UK, <sup>3</sup>ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing / GLAM -- Group on Language, Audio \&amp; Music, University of Augsburg, Imperial College London, Augsburg, Germany / London, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       We propose a convolutional neural network (CNN) model based on an attention pooling method to classify ten different acoustic scenes, participating in the acoustic scene classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2018), which includes data from one device (subtask A) and data from three different devices (subtask B). The log mel spectrogram images of the audio waves are first forwarded to convolutional layers, and then fed into an attention pooling layer to reduce the feature dimension and achieve classification. From attention perspective, we build a weighted evaluation of the features, instead of simple max pooling or average pooling. On the official development set of the challenge, the best accuracy of subtask A is 72.6 %, which is an improvement of 12.9 % when compared with the official baseline (p &lt; .001 in a one-tailed z-test). For subtask B, the best result of our attention-based CNN is a significant improvement of the baseline as well, in which the accuracies are 71.8 %, 58.3 %, and 58.3 % for the three devices A to C (p &lt; .001 for device A, p &lt; .01 for device B, and p &lt; .05 for device C).
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ren2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Ren_65.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ren2018label" class="modal fade" id="bibtex-Ren2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRen2018label">
        Attention-Based Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ren2018,
    Author = "Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark and Schuller, Björn",
    title = "Attention-Based Convolutional Neural Networks for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "We propose a convolutional neural network (CNN) model based on an attention pooling method to classify ten different acoustic scenes, participating in the acoustic scene classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2018), which includes data from one device (subtask A) and data from three different devices (subtask B). The log mel spectrogram images of the audio waves are first forwarded to convolutional layers, and then fed into an attention pooling layer to reduce the feature dimension and achieve classification. From attention perspective, we build a weighted evaluation of the features, instead of simple max pooling or average pooling. On the official development set of the challenge, the best accuracy of subtask A is 72.6 \%, which is an improvement of 12.9 \% when compared with the official baseline (p &lt; .001 in a one-tailed z-test). For subtask B, the best result of our attention-based CNN is a significant improvement of the baseline as well, in which the accuracies are 71.8 \%, 58.3 \%, and 58.3 \% for the three devices A to C (p &lt; .001 for device A, p &lt; .01 for device B, and p &lt; .05 for device C)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Roletscheck2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Roletscheck2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Using an Evolutionary Approach to Explore Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Christian Roletscheck and Tobias Watzka
       </p>
<p style="text-align:left">
<em>
         Human Centered Multimedia, Augsburg University, Augsburg, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Roletscheck_UNIA_task1a_1</span> <span class="label label-primary">Roletscheck_UNIA_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Roletscheck2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Roletscheck2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Roletscheck2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Roletscheck_83.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Roletscheck2018" class="panel-collapse collapse" id="collapse-Roletscheck2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Using an Evolutionary Approach to Explore Convolutional Neural Networks for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Christian Roletscheck and Tobias Watzka
       </small>
<br/>
<small>
<em>
         Human Centered Multimedia, Augsburg University, Augsburg, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The successful application of modern deep neural networks is heavily reliant on the chosen architecture and the selection of the appropriate hyperparameters. Due to the large amount of parameters and the complex inner workings of a neural network, finding a suitable configuration for a respective problem turns out to be a rather complex task for a human. In this paper we propose an evolutionary approach to automatically generate a suitable neural network architecture for any given problem. A genetic algorithm is used to generate and evaluate a variety of deep convolutional networks. We take the DCASE 2018 Challenge as an opportunity to evaluate our algorithm on the task of acoustic scene classification. The best accuracy achieved by our approach was 74.7% on the development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Roletscheck2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Roletscheck_83.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Roletscheck2018label" class="modal fade" id="bibtex-Roletscheck2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRoletscheck2018label">
        Using an Evolutionary Approach to Explore Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Roletscheck2018,
    Author = "Roletscheck, Christian and Watzka, Tobias",
    title = "Using an Evolutionary Approach to Explore Convolutional Neural Networks for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "The successful application of modern deep neural networks is heavily reliant on the chosen architecture and the selection of the appropriate hyperparameters. Due to the large amount of parameters and the complex inner workings of a neural network, finding a suitable configuration for a respective problem turns out to be a rather complex task for a human. In this paper we propose an evolutionary approach to automatically generate a suitable neural network architecture for any given problem. A genetic algorithm is used to generate and evaluate a variety of deep convolutional networks. We take the DCASE 2018 Challenge as an opportunity to evaluate our algorithm on the task of acoustic scene classification. The best accuracy achieved by our approach was 74.7\% on the development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sakashita2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Sakashita2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification by Ensemble of Spectrograms Based on Adaptive Temporal Divisions
       </h4>
<p style="text-align:left">
        Yuma Sakashita and Masaki Aono
       </p>
<p style="text-align:left">
<em>
         Knowledge Data Engineering Laboratory, Toyohashi University of Technology, Aichi, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Sakashita_TUT_task1a_1</span> <span class="label label-primary">Sakashita_TUT_task1a_2</span> <span class="label label-primary">Sakashita_TUT_task1a_3</span> <span class="label label-primary">Sakashita_TUT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sakashita2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sakashita2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sakashita2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Sakashita_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sakashita2018" class="panel-collapse collapse" id="collapse-Sakashita2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification by Ensemble of Spectrograms Based on Adaptive Temporal Divisions
      </h4>
<p style="text-align:left">
<small>
        Yuma Sakashita and Masaki Aono
       </small>
<br/>
<small>
<em>
         Knowledge Data Engineering Laboratory, Toyohashi University of Technology, Aichi, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Many classification tasks using deep learning have improved classification accuracy by using a large amount of training data. However, it is difficult to collect audio data and build a large database. Since training data is restricted in DCASE 2018 Task 1a, unknown acoustic scene must be predicted from less training data. From the results of DCASE 2017[1], we determine that using a convolution neural network and ensemble multiple networks is an effective means for classifying acoustic scenes. In our method we generate mel-spectrogram from binaural audio, mono audio, Harmonicpercussive source separation (HPSS) audio, adaptively divide the spectrogram into multiple ways and learn 9 neural networks. We further improve ensemble accuracy by ensemble learning using these outputs. The classification result of the proposed system was 0.769 for Development dataset and 0.796 for Leaderboard dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         random forest
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sakashita2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Sakashita_15.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sakashita2018label" class="modal fade" id="bibtex-Sakashita2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSakashita2018label">
        Acoustic Scene Classification by Ensemble of Spectrograms Based on Adaptive Temporal Divisions
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sakashita2018,
    Author = "Sakashita, Yuma and Aono, Masaki",
    title = "Acoustic Scene Classification by Ensemble of Spectrograms Based on Adaptive Temporal Divisions",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Many classification tasks using deep learning have improved classification accuracy by using a large amount of training data. However, it is difficult to collect audio data and build a large database. Since training data is restricted in DCASE 2018 Task 1a, unknown acoustic scene must be predicted from less training data. From the results of DCASE 2017[1], we determine that using a convolution neural network and ensemble multiple networks is an effective means for classifying acoustic scenes. In our method we generate mel-spectrogram from binaural audio, mono audio, Harmonicpercussive source separation (HPSS) audio, adaptively divide the spectrogram into multiple ways and learn 9 neural networks. We further improve ensemble accuracy by ensemble learning using these outputs. The classification result of the proposed system was 0.769 for Development dataset and 0.796 for Leaderboard dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sangwon2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Sangwon2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CNN Based System for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Lee Sangwon, Kang Seungtae and Jang Gin-jin
       </p>
<p style="text-align:left">
<em>
         School of Electronics Engineering, Kyungpook National University, Daegu, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Gil-jin_KNU_task1a_1</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sangwon2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sangwon2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sangwon2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Sangwon_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sangwon2018" class="panel-collapse collapse" id="collapse-Sangwon2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CNN Based System for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Lee Sangwon, Kang Seungtae and Jang Gin-jin
       </small>
<br/>
<small>
<em>
         School of Electronics Engineering, Kyungpook National University, Daegu, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Convolution neural networks (CNNs) have achieved great successes in many machine learning tasks such as classifying visual objects or various audio sounds. In this report, we describe our system implementation for acoustic scene classification task of DCASE 2018 based on CNN. The classification accuracies of the proposed system are 72.4% and 75.5% on development and leaderboard datasets, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sangwon2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Sangwon_54.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sangwon2018label" class="modal fade" id="bibtex-Sangwon2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSangwon2018label">
        CNN Based System for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sangwon2018,
    Author = "Sangwon, Lee and Seungtae, Kang and Gin-jin, Jang",
    title = "{CNN} Based System for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "Convolution neural networks (CNNs) have achieved great successes in many machine learning tasks such as classifying visual objects or various audio sounds. In this report, we describe our system implementation for acoustic scene classification task of DCASE 2018 based on CNN. The classification accuracies of the proposed system are 72.4\% and 75.5\% on development and leaderboard datasets, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Tchorz2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Tchorz2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Combination of Amplitude Modulation Spectrogram Features and MFCCs for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Juergen Tchorz
       </p>
<p style="text-align:left">
<em>
         Institute for Acoustics, University of Applied Sciences Luebeck, Luebeck, Germany
        </em>
</p>
<p style="text-align:left">
<span class="clearfix"></span><span class="label label-info">Tchorz_THL_task1b_1</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Tchorz2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Tchorz2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Tchorz2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Tchorz_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Tchorz2018" class="panel-collapse collapse" id="collapse-Tchorz2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Combination of Amplitude Modulation Spectrogram Features and MFCCs for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Juergen Tchorz
       </small>
<br/>
<small>
<em>
         Institute for Acoustics, University of Applied Sciences Luebeck, Luebeck, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes an approach for acoustic scene classification and its results for the development data set of the DCASE 2018 challenge. Amplitude modulation spectrograms (AMS), which mimic important aspects of the auditory system are used as features, in combination with mel-scale cepstral coefficients which have shown to be complementary to AMS features. For classification, a long short-term memory deep neural network is used. The proposed system outperforms the baseline system by 6.3-9.3 % for the development data test subset, depending on the recording device.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         amplitude modulation spectrogram, MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         LSTM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Tchorz2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Tchorz_6.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Tchorz2018label" class="modal fade" id="bibtex-Tchorz2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexTchorz2018label">
        Combination of Amplitude Modulation Spectrogram Features and MFCCs for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Tchorz2018,
    Author = "Tchorz, Juergen",
    title = "Combination of Amplitude Modulation Spectrogram Features and {MFCCs} for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This report describes an approach for acoustic scene classification and its results for the development data set of the DCASE 2018 challenge. Amplitude modulation spectrograms (AMS), which mimic important aspects of the auditory system are used as features, in combination with mel-scale cepstral coefficients which have shown to be complementary to AMS features. For classification, a long short-term memory deep neural network is used. The proposed system outperforms the baseline system by 6.3-9.3 \% for the development data test subset, depending on the recording device."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Waldekar2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Waldekar2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Wavelet-Based Audio Features for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Shefali Waldekar and Goutam Saha
       </p>
<p style="text-align:left">
<em>
         Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Waldekar_IITKGP_task1a_1</span> <span class="clearfix"></span><span class="label label-info">Waldekar_IITKGP_task1b_1</span> <span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Waldekar2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Waldekar2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Waldekar2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Waldekar_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Waldekar2018" class="panel-collapse collapse" id="collapse-Waldekar2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Wavelet-Based Audio Features for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Shefali Waldekar and Goutam Saha
       </small>
<br/>
<small>
<em>
         Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes a submission for IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 for Task 1 (acoustic scene classification (ASC)), sub-task A (basic ASC) and sub-task B (ASC with mismatched recording devices). We use two wavelet-based features in a scorefusion framework to achieve the goal. The first feature applies wavelet transform to log mel-band energies, while the second does a high-Q wavelet transformation on the frames of raw signal. The two features are found to be complementary so that the fused system relatively outperforms the deep-learning based baseline system by 17% for sub-task A and 26% for sub-task B with the development dataset provided for the respective sub-tasks.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFDWC, CQCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Waldekar2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Waldekar_52.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Waldekar2018label" class="modal fade" id="bibtex-Waldekar2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWaldekar2018label">
        Wavelet-Based Audio Features for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Waldekar2018,
    Author = "Waldekar, Shefali and Saha, Goutam",
    title = "Wavelet-Based Audio Features for Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This report describes a submission for IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 for Task 1 (acoustic scene classification (ASC)), sub-task A (basic ASC) and sub-task B (ASC with mismatched recording devices). We use two wavelet-based features in a scorefusion framework to achieve the goal. The first feature applies wavelet transform to log mel-band energies, while the second does a high-Q wavelet transformation on the frames of raw signal. The two features are found to be complementary so that the fused system relatively outperforms the deep-learning based baseline system by 17\% for sub-task A and 26\% for sub-task B with the development dataset provided for the respective sub-tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yang2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Yang2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Se-Resnet with Gan-Based Data Augmentation Applied to Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Jeong Hyeon Yang, Nam Kyun Kim and Hong Kook Kim
       </p>
<p style="text-align:left">
<em>
         School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yang_GIST_task1a_1</span> <span class="label label-primary">Yang_GIST_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yang2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yang2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yang2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Yang_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yang2018" class="panel-collapse collapse" id="collapse-Yang2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Se-Resnet with Gan-Based Data Augmentation Applied to Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Jeong Hyeon Yang, Nam Kyun Kim and Hong Kook Kim
       </small>
<br/>
<small>
<em>
         School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes our contribution to the development of audio scene classification methods for the DCASE 2018 Challenge Task 1A. The proposed systems for this task are based on data augmentation through generative adversarial network (GAN)-based data augmentation and various convolutional networks such as residual networks (ResNets) and squeeze-and-excitation residual networks (SE-ResNets). In addition to data augmentation, SEResNets are revised so that they operate on the log-mel spectrogram domain, and the numbers of layers and kernels are adjusted to provide better performance on the task. Finally, the ensemble method is applied using a four-fold cross-validated training dataset. Consequently, the proposed audio scene classification system improves classwise accuracy by 10% compared to the baseline system through the Kaggle competition in acoustic scene classification.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         GAN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yang2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Yang_18.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yang2018label" class="modal fade" id="bibtex-Yang2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYang2018label">
        Se-Resnet with Gan-Based Data Augmentation Applied to Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yang2018,
    Author = "Yang, Jeong Hyeon and Kim, Nam Kyun and Kim, Hong Kook",
    title = "Se-Resnet with Gan-Based Data Augmentation Applied to Acoustic Scene Classification",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "This report describes our contribution to the development of audio scene classification methods for the DCASE 2018 Challenge Task 1A. The proposed systems for this task are based on data augmentation through generative adversarial network (GAN)-based data augmentation and various convolutional networks such as residual networks (ResNets) and squeeze-and-excitation residual networks (SE-ResNets). In addition to data augmentation, SEResNets are revised so that they operate on the log-mel spectrogram domain, and the numbers of layers and kernels are adjusted to provide better performance on the task. Finally, the ensemble method is applied using a four-fold cross-validated training dataset. Consequently, the proposed audio scene classification system improves classwise accuracy by 10\% compared to the baseline system through the Kaggle competition in acoustic scene classification."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zeinali2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Zeinali2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolutional Neural Networks and X-Vector Embedding for Dcase2018 Acoustic Scene Classification Challenge
       </h4>
<p style="text-align:left">
        Hossein Zeinali, Lukas Burget and Honza Cernocky
       </p>
<p style="text-align:left">
<em>
         BUT Speech, Department of Computer Graphics and Multimedia, Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zeinali_BUT_task1a_1</span> <span class="label label-primary">Zeinali_BUT_task1a_2</span> <span class="label label-primary">Zeinali_BUT_task1a_3</span> <span class="label label-primary">Zeinali_BUT_task1a_4</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zeinali2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zeinali2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zeinali2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Zeinali_103.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zeinali2018" class="panel-collapse collapse" id="collapse-Zeinali2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolutional Neural Networks and X-Vector Embedding for Dcase2018 Acoustic Scene Classification Challenge
      </h4>
<p style="text-align:left">
<small>
        Hossein Zeinali, Lukas Burget and Honza Cernocky
       </small>
<br/>
<small>
<em>
         BUT Speech, Department of Computer Graphics and Multimedia, Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, the BUT team submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge is described. Also, the analysis of different method performance on the development set is provided. The proposed approach is a fusion of two different Conventional Neural Network (CNN) topologies. The first one is the common two-dimensional CNNs which mainly is used in image classification task. The second one is one dimensional CNN for extracting embeddings from the neural network which is too common in speech processing, especially for speaker recognition. In addition to the topologies, two types of features were suggested to be used in this task, Mel-spectrogram in log domain and CQT features which explained in detail in the report. Finally, the outputs of different systems are fused using a weighted average.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         block mixing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, x-vector, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zeinali2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Zeinali_103.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zeinali2018label" class="modal fade" id="bibtex-Zeinali2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZeinali2018label">
        Convolutional Neural Networks and X-Vector Embedding for Dcase2018 Acoustic Scene Classification Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zeinali2018,
    Author = "Zeinali, Hossein and Burget, Lukas and Cernocky, Honza",
    title = "Convolutional Neural Networks and X-Vector Embedding for Dcase2018 Acoustic Scene Classification Challenge",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "In this report, the BUT team submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge is described. Also, the analysis of different method performance on the development set is provided. The proposed approach is a fusion of two different Conventional Neural Network (CNN) topologies. The first one is the common two-dimensional CNNs which mainly is used in image classification task. The second one is one dimensional CNN for extracting embeddings from the neural network which is too common in speech processing, especially for speaker recognition. In addition to the topologies, two types of features were suggested to be used in this task, Mel-spectrogram in log domain and CQT features which explained in detail in the report. Finally, the outputs of different systems are fused using a weighted average."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhang2018" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhang2018" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Multi-Layered Temporal Pooling Based on Deep Convolutional Neural Network
       </h4>
<p style="text-align:left">
        Liwen Zhang and Jiqing Han
       </p>
<p style="text-align:left">
<em>
         Laboratory of Speech Signal Processing, Harbin Institute of Technology, Harbin, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_HIT_task1a_1</span> <span class="label label-primary">Zhang_HIT_task1a_2</span> <span class="clearfix"></span><span class="clearfix"></span><span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhang2018" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhang2018" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhang2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Zhang_23.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhang2018" class="panel-collapse collapse" id="collapse-Zhang2018" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Multi-Layered Temporal Pooling Based on Deep Convolutional Neural Network
      </h4>
<p style="text-align:left">
<small>
        Liwen Zhang and Jiqing Han
       </small>
<br/>
<small>
<em>
         Laboratory of Speech Signal Processing, Harbin Institute of Technology, Harbin, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The performance of an Acoustic Scene Classification (ASC) system is highly depending on the latent temporal dynamics of the audio signal. In this paper, we proposed a multiple layers temporal pooling method using CNN feature sequence as input, which can effectively capture the temporal dynamics for an entire audio signal with arbitrary duration by building direct connections between the sequence and its time indexes. We applied our novel framework on DCASE 2018 task 1, ASC. For evaluation, we trained a Support Vector Machine (SVM) with the proposed Multi-Layered Temporal Pooling (MLTP) learned features. Experimental results on the development dataset, usage of the MLTP features significantly improved the ASC performance. The best performance with 75.28% accuracy was achieved by using the optimal setting found in our experiments.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         48kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, SVR, SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         only one SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhang2018" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2018/technical_reports/DCASE2018_Zhang_23.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhang2018label" class="modal fade" id="bibtex-Zhang2018" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhang2018label">
        Acoustic Scene Classification Using Multi-Layered Temporal Pooling Based on Deep Convolutional Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhang2018,
    Author = "Zhang, Liwen and Han, Jiqing",
    title = "Acoustic Scene Classification Using Multi-Layered Temporal Pooling Based on Deep Convolutional Neural Network",
    institution = "DCASE2018 Challenge",
    year = "2018",
    month = "September",
    abstract = "The performance of an Acoustic Scene Classification (ASC) system is highly depending on the latent temporal dynamics of the audio signal. In this paper, we proposed a multiple layers temporal pooling method using CNN feature sequence as input, which can effectively capture the temporal dynamics for an entire audio signal with arbitrary duration by building direct connections between the sequence and its time indexes. We applied our novel framework on DCASE 2018 task 1, ASC. For evaluation, we trained a Support Vector Machine (SVM) with the proposed Multi-Layered Temporal Pooling (MLTP) learned features. Experimental results on the development dataset, usage of the MLTP features significantly improved the ASC performance. The best performance with 75.28\% accuracy was achieved by using the optimal setting found in our experiments."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>