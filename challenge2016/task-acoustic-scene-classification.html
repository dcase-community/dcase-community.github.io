<!DOCTYPE html><html lang="en">
<head>
    <title>Acoustic scene classification - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2016/task-acoustic-scene-classification">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="Challenge has ended. Full results for this task can be found here Description The goal of acoustic scene classification is to classify a test recording into one of predefined classes that characterizes the environment in which it was recorded — for example &#34;park&#34;, &#34;home&#34;, &#34;office&#34;. Figure 1: Overview of acoustic scene …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2016</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2016/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2016/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class=" active">
        <a href="/challenge2016/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2016/task-sound-event-detection-in-synthetic-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-synthetic text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-synthetic-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-synthetic-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2016/task-sound-event-detection-in-real-life-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-events text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-real-life-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-sound-event-detection-in-real-life-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2016/task-audio-tagging" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-tags text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2016/task-audio-tagging"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2016/task-audio-tagging-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Download data">
        <a href="/challenge2016/download"><i class="fa fa-download"></i>&nbsp;Download</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2016/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2016/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge organizers">
        <a href="/challenge2016/organizers"><i class="fa fa-users"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/water-01.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-primary"></i><i class="fa dc-scene fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Scenes</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2016_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Acoustic<br> scene classification</h1><hr class="small right bold"><span class="subheading">Task description</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Coordinators</h3>
</div>
<table class="table bpersonnel-container">
<tr>
<td class="" style="width: 65px;">
<img alt="Annamaria Mesaros" class="img img-circle" src="/images/person/annamaria_mesaros.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Annamaria Mesaros</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University of Technology
                                </a>
</p>
</div>
</div>
</td>
</tr>
<tr>
<td class="" style="width: 65px;">
<img alt="Toni Heittola" class="img img-circle" src="/images/person/toni_heittola.jpg" width="48px"/>
</td>
<td class="">
<div class="row">
<div class="col-md-12">
<strong>Toni Heittola</strong>
</div>
<div class="col-md-12">
<p class="small text-muted">
<a class="text" href="http://arg.cs.tut.fi/">
                                Tampere University of Technology
                                </a>
</p>
</div>
</div>
</td>
</tr>
</table>
</div>

 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#description">Description</a></li>
<li><a href="#audio-dataset">Audio dataset</a>
<ul>
<li><a href="#recording-and-annotation-procedure">Recording and annotation procedure</a></li>
<li><a href="#challenge-setup">Challenge setup</a></li>
<li><a href="#download">Download</a></li>
<li><a href="#cross-validation-with-development-dataset">Cross-validation with development dataset</a></li>
</ul>
</li>
<li><a href="#submission">Submission</a></li>
<li><a href="#task-rules">Task rules</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#baseline-system">Baseline system</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <p class="alert alert-info">
<strong>Challenge has ended.</strong> Full results for this task can be found <a class="btn btn-default btn-xs" href="/challenge2016/task-acoustic-scene-classification-results">here <i class="fa fa-caret-right"></i></a>
</p>
<h1 id="description">Description</h1>
<p>The goal of acoustic scene classification is to classify a test recording into one of predefined classes that characterizes the environment in which it was recorded — for example "park", "home", "office".</p>
<figure>
<div class="row row-centered">
<div class="col-xs-10 col-md-5 col-centered">
<img class="img img-responsive" src="/images/tasks/challenge2016/task1_overview.png"/>
<figcaption>Figure 1: Overview of acoustic scene classification system.</figcaption>
</div>
</div>
</figure>
<h1 id="audio-dataset">Audio dataset</h1>
<p><strong>TUT Acoustic scenes 2016</strong> dataset will be used for the task. The dataset consists of recordings from various acoustic scenes, all having distinct recording locations. For each recording location, 3-5 minute long audio recording was captured. The original recordings were then split into 30-second segments for the challenge.</p>
<p>Acoustic scenes for the task (15):</p>
<ul>
<li>Bus - traveling by bus in the city (vehicle)</li>
<li>Cafe / Restaurant - small cafe/restaurant (indoor)</li>
<li>Car - driving or traveling as a passenger, in the city (vehicle)</li>
<li>City center (outdoor)</li>
<li>Forest path (outdoor)</li>
<li>Grocery store - medium size grocery store (indoor)</li>
<li>Home (indoor)</li>
<li>Lakeside beach (outdoor)</li>
<li>Library (indoor)</li>
<li>Metro station (indoor)</li>
<li>Office  - multiple persons, typical work day (indoor)</li>
<li>Residential area (outdoor)</li>
<li>Train (traveling, vehicle)</li>
<li>Tram (traveling, vehicle)</li>
<li>Urban park (outdoor)</li>
</ul>
<p>Detailed description of acoustic scenes included in the dataset can be found <a href="acoustic-scenes">here</a>.</p>
<p>The dataset was collected in Finland by Tampere University of Technology between 06/2015 - 01/2016. The data collection has received funding from the European Research Council.</p>
<p><a href="https://erc.europa.eu/"><img alt="ERC" src="/images/sponsors/erc.jpg" title="ERC"/></a></p>
<h2 id="recording-and-annotation-procedure">Recording and annotation procedure</h2>
<p>For all acoustic scenes, the recordings were captured each in a different location: different streets, different parks, different homes. Recordings were made using a <a href="http://www.soundman.de/en/products/">Soundman OKM II Klassik/studio A3</a>, electret binaural  microphone and a <a href="http://www.rolandus.com/products/r-09/">Roland Edirol R-09</a> wave recorder using 44.1 kHz sampling rate and 24 bit resolution. The microphones are specifically made to look like headphones, being worn in the ears. As an effect of this, the recorded audio is very similar to the sound that reaches the human auditory system of the person wearing the equipment.</p>
<p>Postprocessing of the recorded data involves aspects related to privacy of recorded individuals, and possible errors in the recording process. For audio material recorded in private places, written consent was obtained from all people involved. Material recorded in public places does not require such consent, but was screened for content, and privacy infringing segments were eliminated. Microphone failure and audio distortions were also annotated and segments containing such errors were also eliminated.</p>
<p>After eliminating the problematic segments, the remaining audio material was cut into segments of 30 seconds length.</p>
<h2 id="challenge-setup">Challenge setup</h2>
<p><em>TUT Acoustic scenes 2016</em> dataset consist of two subsets: <strong>development dataset</strong> and <strong>evaluation dataset</strong>. The partitioning of the data into the subsets was done based on the location of the original recordings. All segments obtained from the same original recording were included into a single subset - either <em>development dataset</em> or <em>evaluation dataset</em>. For each acoustic scene, 78 segments (39 minutes of audio) were included in the development dataset and 26 segments (13 minutes of audio) were kept for evaluation. Development set contains in total 9h 45mins of audio, and evaluation set 3h 15mins.</p>
<p>Participants are <strong>not allowed</strong> to use external data for system development. Manipulation of provided data <strong>is allowed</strong>.</p>
<h2 id="download">Download</h2>
<p>** Development dataset **</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/45739" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/45739" target="_blank">
<span style="font-size:20px;">TUT Acoustic scenes 2016, development dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(7.5 GB)</span>
<br/>
<a href="http://dx.doi.org/10.5281/zenodo.45739">
<img alt="10.5281/zenodo.45739" src="https://zenodo.org/badge/doi/10.5281/zenodo.45739.svg"/>
</a>
</div>
</div>
<p><br/></p>
<p>** Evaluation dataset **</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/165995" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-success"></i>
<i class="fa fa-file-audio-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/165995" target="_blank">
<span style="font-size:20px;">TUT Acoustic scenes 2016, evaluation dataset <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(2.5 GB)</span>
<br/>
<a href="https://doi.org/10.5281/zenodo.165995">
<img alt="10.5281/zenodo.165995" src="https://zenodo.org/badge/DOI/10.5281/zenodo.165995.svg"/>
</a>
</div>
</div>
<p><br/></p>
<p><strong>In publications using the datasets, cite as:</strong></p>
<div class="btex-item" data-item="Mesaros2016_EUSIPCO" data-source="content/data/challenge2016/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_EUSIPCO"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>TUT database for acoustic scene classification and sound event detection.</em>
In 24th European Signal Processing Conference 2016 (EUSIPCO 2016). Budapest, Hungary, 2016.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" class="panel-collapse collapse" id="collapseMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" role="tabpanel">
<h4>TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7label" class="modal fade" id="bibtexMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_EUSIPCOdc29847dc7da418398f38bb56465d5e7label">TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2016_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "{TUT} Database for Acoustic Scene Classification and Sound Event Detection",
    abstract = "We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.",
    year = "2016",
    address = "Budapest, Hungary",
    booktitle = "24th European Signal Processing Conference 2016 (EUSIPCO 2016)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<h2 id="cross-validation-with-development-dataset">Cross-validation with development dataset</h2>
<p>A cross-validation setup is provided for the development dataset in order to make results reported with this dataset uniform. The setup consists of four folds distributing the 78 available segments based on location. The folds are provided with the dataset in the directory <code>evaluation setup</code>.</p>
<p>If not using the provided cross-validation setup, pay attention to the segments extracted from same original recordings. Make sure that all files recorded in same location are placed on the same side of the evaluation.</p>
<h1 id="submission">Submission</h1>
<p>Detailed information for the challenge submission can found from <a href="/challenge2016/submission">submission page</a>.</p>
<p>One should submit single text-file (in CSV format) containing classification result for each audio file in the evaluation set. Result items can be in any order. Format:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span><span class="n">filename (string)</span><span class="o">][</span><span class="n">tab</span><span class="o">][</span><span class="n">scene label (string)</span><span class="o">]</span>
</code></pre></div>
<h1 id="task-rules">Task rules</h1>
<ul>
<li>Only the provided development dataset can be used to train the submitted system.</li>
<li>The development dataset can be augmented only by mixing data sampled from a pdf; use of real recordings is forbidden.</li>
<li>The evaluation dataset cannot be used to train the submitted system; the use of statistics about the evaluation dataset in the decision making is also forbidden.</li>
<li>Technical report with sufficient description of the system has to be submitted along with the system outputs.</li>
</ul>
<p>More information on <a href="/challenge2016/submission">submission process</a>.</p>
<h1 id="evaluation">Evaluation</h1>
<p>The scoring of acoustic scene classification will be based on classification accuracy: the number of correctly classified segments among the total number of segments. Each segment is considered an independent test sample.</p>
<p>Code for evaluation is available with the baseline system:</p>
<ul>
<li>Python implementation <code>from src.evaluation import DCASE2016_SceneClassification_Metrics</code>.</li>
<li>Matlab implementation, use class <code>src/evaluation/DCASE2016_SceneClassification_Metrics.m</code>.</li>
</ul>
<h1 id="results">Results</h1>
<table class="datatable table" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar" data-id-field="code" data-page-list="[10, 25, 50, All]" data-page-size="25" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="1"></th>
</tr>
<tr>
<th class="sm-cell" data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
                Author
            </th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
                Affiliation
            </th>
<th class="sep-left-cell text-center" data-field="external_anchor" data-sortable="false" data-value-type="url">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Classification Accuracy" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Classification<br/>Accuracy
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Aggarwal_task1_1</td>
<td>Naveen Aggarwal</td>
<td>UIET, Panjab University, Chandigarh, India</td>
<td>task-acoustic-scene-classification-results#Vij2016</td>
<td>74.4</td>
</tr>
<tr>
<td></td>
<td>Bae_task1_1</td>
<td>Soo Hyun Bae</td>
<td>Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results#Bae2016</td>
<td>84.1</td>
</tr>
<tr>
<td></td>
<td>Bao_task1_1</td>
<td>Xiao Bao</td>
<td>National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, Anhui, China</td>
<td>task-acoustic-scene-classification-results#Bao2016</td>
<td>83.1</td>
</tr>
<tr>
<td></td>
<td>Battaglino_task1_1</td>
<td>Daniele Battaglino</td>
<td>NXP Software, France; EURECOM, France</td>
<td>task-acoustic-scene-classification-results#Battaglino2016</td>
<td>80.0</td>
</tr>
<tr>
<td></td>
<td>Bisot_task1_1</td>
<td>Victor Bisot</td>
<td>Telecom ParisTech, Paris, France</td>
<td>task-acoustic-scene-classification-results#Bisot2016</td>
<td>87.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2016 baseline</td>
<td>Toni Heittola</td>
<td>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</td>
<td>task-acoustic-scene-classification-results#Heittola2016</td>
<td>77.2</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_1</td>
<td>Quang-Khanh-Ngoc Duong</td>
<td>Technicolor, France</td>
<td>task-acoustic-scene-classification-results#Sena_Mafra2016</td>
<td>76.4</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_2</td>
<td>Quang-Khanh-Ngoc Duong</td>
<td>Technicolor, France</td>
<td>task-acoustic-scene-classification-results#Sena_Mafra2016</td>
<td>80.5</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_3</td>
<td>Quang-Khanh-Ngoc Duong</td>
<td>Technicolor, France</td>
<td>task-acoustic-scene-classification-results#Sena_Mafra2016</td>
<td>73.1</td>
</tr>
<tr>
<td></td>
<td>Duong_task1_4</td>
<td>Quang-Khanh-Ngoc Duong</td>
<td>Technicolor, France</td>
<td>task-acoustic-scene-classification-results#Sena_Mafra2016</td>
<td>62.8</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_1</td>
<td>Hamid Eghbal-Zadeh</td>
<td>Department of Computational Perception, Johannes Kepler University of Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results#Eghbal-Zadeh2016</td>
<td>86.4</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_2</td>
<td>Hamid Eghbal-Zadeh</td>
<td>Department of Computational Perception, Johannes Kepler University of Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results#Eghbal-Zadeh2016</td>
<td>88.7</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_3</td>
<td>Hamid Eghbal-Zadeh</td>
<td>Department of Computational Perception, Johannes Kepler University of Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results#Eghbal-Zadeh2016</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>Eghbal-Zadeh_task1_4</td>
<td>Hamid Eghbal-Zadeh</td>
<td>Department of Computational Perception, Johannes Kepler University of Linz, Linz, Austria</td>
<td>task-acoustic-scene-classification-results#Eghbal-Zadeh2016</td>
<td>89.7</td>
</tr>
<tr>
<td></td>
<td>Foleiss_task1_1</td>
<td>Juliano Henrique Foleiss</td>
<td>Universidade Tecnologica Federal do Parana, Campo Mourao, Brazil</td>
<td>task-acoustic-scene-classification-results#Foleiss2016</td>
<td>76.2</td>
</tr>
<tr>
<td></td>
<td>Hertel_task1_1</td>
<td>Alfred Mertins</td>
<td>Institute for Signal Processing, University of Luebeck, Luebeck, Germany</td>
<td>task-acoustic-scene-classification-results#Hertel2016</td>
<td>79.5</td>
</tr>
<tr>
<td></td>
<td>Kim_task1_1</td>
<td>Alfred Mertins</td>
<td>Institute for Signal Processing, University of Luebeck, Luebeck, Germany</td>
<td>task-acoustic-scene-classification-results#Yun2016</td>
<td>82.1</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_1</td>
<td>Hanseok Ko</td>
<td>School of Electrical Engineering, Korea University, Seoul, South Korea; Department of Visual Information Processing, Korea University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results#Park2016</td>
<td>87.2</td>
</tr>
<tr>
<td></td>
<td>Ko_task1_2</td>
<td>Hanseok Ko</td>
<td>School of Electrical Engineering, Korea University, Seoul, South Korea; Department of Visual Information Processing, Korea University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results#Mun2016</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Kong_task1_1</td>
<td>Qiuqiang Kong</td>
<td>Centre for Vision, Speech and Signal Processing, University of Surrey, Surrey, United Kingdom</td>
<td>task-acoustic-scene-classification-results#Kong2016</td>
<td>81.0</td>
</tr>
<tr>
<td></td>
<td>Kumar_task1_1</td>
<td>Anurag Kumar</td>
<td>Carnegie Mellon University, Pittsburgh, USA</td>
<td>task-acoustic-scene-classification-results#Elizalde2016</td>
<td>85.9</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_1</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results#Han2016</td>
<td>84.6</td>
</tr>
<tr>
<td></td>
<td>Lee_task1_2</td>
<td>Kyogu Lee</td>
<td>Music and Audio Research Group, Seoul National University, Seoul, South Korea</td>
<td>task-acoustic-scene-classification-results#Kim2016</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_1</td>
<td>Jiaming Liu</td>
<td>Department of Control Science and Engineering, Tongji University, Shanghai, China</td>
<td>task-acoustic-scene-classification-results#Liu2016</td>
<td>83.8</td>
</tr>
<tr>
<td></td>
<td>Liu_task1_2</td>
<td>Jiaming Liu</td>
<td>Department of Control Science and Engineering, Tongji University, Shanghai, China</td>
<td>task-acoustic-scene-classification-results#Liu2016</td>
<td>83.6</td>
</tr>
<tr>
<td></td>
<td>Lostanlen_task1_1</td>
<td>Vincent Lostanlen</td>
<td>Departement d’Informatique, Ecole normale superieure, Paris, France</td>
<td>task-acoustic-scene-classification-results#Lostanlen2016</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Marchi_task1_1</td>
<td>Erik Marchi</td>
<td>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany; audEERING GmbH, Gilching, Germany</td>
<td>task-acoustic-scene-classification-results#Marchi2016</td>
<td>86.4</td>
</tr>
<tr>
<td></td>
<td>Marques_task1_1</td>
<td>Gonçalo Marques</td>
<td>Electronic Telecom. and Comp. Dept., Instituto Superior de Engenharia de Lisboa, Lisboa, Portugal</td>
<td>task-acoustic-scene-classification-results#Marques2016</td>
<td>83.1</td>
</tr>
<tr>
<td></td>
<td>Moritz_task1_1</td>
<td>Niko Moritz</td>
<td>Project Group for Hearing, Speech, and Audio Processing, Fraunhofer IDMT, Oldenburg, Germany</td>
<td>task-acoustic-scene-classification-results#Moritz2016</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Mulimani_task1_1</td>
<td>Manjunath Mulimani</td>
<td>Dept. of Computer Science &amp; Engineering, National Institute of Technology, Karnataka, India</td>
<td>task-acoustic-scene-classification-results#Mulimani2016</td>
<td>65.6</td>
</tr>
<tr>
<td></td>
<td>Nogueira_task1_1</td>
<td>Waldo Nogueira</td>
<td>Medical University Hannover, Hannover, Germany; Cluster of Excellence Hearing4all, Hannover, Germany</td>
<td>task-acoustic-scene-classification-results#Nogueira2016</td>
<td>81.0</td>
</tr>
<tr>
<td></td>
<td>Patiyal_task1_1</td>
<td>Rohit Patiyal</td>
<td>School of Computing and Electrical Engineering, Indian Institute of Technology Mandi, Himachal Pradesh, India</td>
<td>task-acoustic-scene-classification-results#Patiyal2016</td>
<td>78.5</td>
</tr>
<tr>
<td></td>
<td>Phan_task1_1</td>
<td>Huy Phan</td>
<td>Institute for Signal Processing, University of Luebeck, Luebeck, Germany; Graduate School for Computing in Medicine and Life Sciences, University of Luebeck, Luebeck, Germany</td>
<td>task-acoustic-scene-classification-results#Phan2016</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>Pugachev_task1_1</td>
<td>Alexei Pugachev</td>
<td>Chair of Speech Information Systems, ITMO University, St. Petersburg, Russia</td>
<td>task-acoustic-scene-classification-results#Pugachev2016</td>
<td>73.1</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_1</td>
<td>Shuhui Qu</td>
<td>Stanford University, Stanford, USA</td>
<td>task-acoustic-scene-classification-results#Dai2016</td>
<td>80.5</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_2</td>
<td>Shuhui Qu</td>
<td>Stanford University, Stanford, USA</td>
<td>task-acoustic-scene-classification-results#Dai2016</td>
<td>84.1</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_3</td>
<td>Shuhui Qu</td>
<td>Stanford University, Stanford, USA</td>
<td>task-acoustic-scene-classification-results#Dai2016</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Qu_task1_4</td>
<td>Shuhui Qu</td>
<td>Stanford University, Stanford, USA</td>
<td>task-acoustic-scene-classification-results#Dai2016</td>
<td>80.5</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_1</td>
<td>Alain Rakotomamonjy</td>
<td>Music and Audio Research Group, Normandie Université, Saint Etienne du Rouvray, France</td>
<td>task-acoustic-scene-classification-results#Rakotomamonjy2016</td>
<td>82.1</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_task1_2</td>
<td>Alain Rakotomamonjy</td>
<td>Music and Audio Research Group, Normandie Université, Saint Etienne du Rouvray, France</td>
<td>task-acoustic-scene-classification-results#Rakotomamonjy2016</td>
<td>79.2</td>
</tr>
<tr>
<td></td>
<td>Santoso_task1_1</td>
<td>Andri Santoso</td>
<td>National Central University, Taiwan</td>
<td>task-acoustic-scene-classification-results#Santoso2016</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_1</td>
<td>Alexander Schindler</td>
<td>Digital Safety and Security, Austrian Institute of Technology, Vienna, Austria</td>
<td>task-acoustic-scene-classification-results#Lidy2016</td>
<td>81.8</td>
</tr>
<tr>
<td></td>
<td>Schindler_task1_2</td>
<td>Alexander Schindler</td>
<td>Digital Safety and Security, Austrian Institute of Technology, Vienna, Austria</td>
<td>task-acoustic-scene-classification-results#Lidy2016</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>Takahashi_task1_1</td>
<td>Gen Takahashi</td>
<td>University of Tsukuba, Tsukuba, Japan</td>
<td>task-acoustic-scene-classification-results#Takahashi2016</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Valenti_task1_1</td>
<td>Michele Valenti</td>
<td>Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy</td>
<td>task-acoustic-scene-classification-results#Valenti2016</td>
<td>86.2</td>
</tr>
<tr>
<td></td>
<td>Vikaskumar_task1_1</td>
<td>Ghodasara Vikaskumar</td>
<td>Electronics &amp; Electrical Communication Engineering Department, Indian Institute of Technology Kharagpur, Kharagpur, India</td>
<td>task-acoustic-scene-classification-results#Vikaskumar2016</td>
<td>81.3</td>
</tr>
<tr>
<td></td>
<td>Vu_task1_1</td>
<td>Toan H. Vu</td>
<td>Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan</td>
<td>task-acoustic-scene-classification-results#Vu2016</td>
<td>80.0</td>
</tr>
<tr>
<td></td>
<td>Xu_task1_1</td>
<td>Yong Xu</td>
<td>Centre for Vision, Speech and Signal Processing, University of Surrey, Surrey, United Kingdom</td>
<td>task-acoustic-scene-classification-results#Xu2016</td>
<td>73.3</td>
</tr>
<tr>
<td></td>
<td>Zoehrer_task1_1</td>
<td>Matthias Zöhrer</td>
<td>Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria</td>
<td>task-acoustic-scene-classification-results#Zoehrer2016</td>
<td>73.1</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>Complete results and technical reports can be found at <a class="btn btn-primary" href="/challenge2016/task-acoustic-scene-classification-results">Task 1 result page</a></p>
<h1 id="baseline-system">Baseline system</h1>
<p>The baseline system for the task is provided. The system is meant to implement basic approach for acoustic scene classification, and provide some comparison point for the participants while developing their systems. The baseline systems for task 1 and <a href="task-sound-event-detection-in-real-life-audio#baseline-system">task 3</a> share the code base, and implements quite similar approach for both tasks. The baseline system will download the needed datasets and produces the results below when ran with the default parameters.</p>
<p>The baseline system is based on <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">MFCC</a> acoustic features and <a href="https://en.wikipedia.org/wiki/Mixture_model">GMM</a> classifier. The acoustic features include MFCC static coefficients (0th coefficient included), delta coefficients and acceleration coefficients. The system learns one acoustic model per acoustic scene class, and does the classification with maximum likelihood classification scheme.</p>
<p>The baseline system provides also reference implementation of evaluation metric. Baseline systems are provided for both Python and Matlab. Python implementation is regarded as the main implementation.</p>
<p>Participants are allowed to build their system on top of the given baseline systems. The systems have all needed functionality for dataset handling, storing / accessing features and models, and evaluating the results, making the adaptation for one's needs rather easy. The baseline systems are also good starting point for entry level researchers.</p>
<p><strong>In publications using the baseline, cite as:</strong></p>
<div class="btex-item" data-item="Mesaros2016_EUSIPCO" data-source="content/data/challenge2016/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_EUSIPCO"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>TUT database for acoustic scene classification and sound event detection.</em>
In 24th European Signal Processing Conference 2016 (EUSIPCO 2016). Budapest, Hungary, 2016.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" class="panel-collapse collapse" id="collapseMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" role="tabpanel">
<h4>TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4label" class="modal fade" id="bibtexMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_EUSIPCO404793bbee45451b827af8561d1e31d4label">TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2016_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "{TUT} Database for Acoustic Scene Classification and Sound Event Detection",
    abstract = "We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.",
    year = "2016",
    address = "Budapest, Hungary",
    booktitle = "24th European Signal Processing Conference 2016 (EUSIPCO 2016)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<h4>Python implementation</h4>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python" target="_blank">
<span style="font-size:20px;">DCASE2016 Task 1&amp;3 Python baseline, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python/archive/v1.0.7.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-warning"></i>
<i class="fa fa-gears fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python/archive/v1.0.7.zip" target="_blank">
<span style="font-size:20px;">DCASE2016 Task 1&amp;3  Python baseline, release <i class="fa fa-download"></i></span>
</a>
<br/>
<span class="text-muted">
                
                version 1.0.7
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/></p>
<h4>Matlab implementation</h4>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/DCASE2016-baseline-system-matlab" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x"></i>
<i class="fa fa-github fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/DCASE2016-baseline-system-matlab" target="_blank">
<span style="font-size:20px;">DCASE2016 Task 1&amp;3 Matlab baseline, repository <i class="fa fa-download"></i></span>
</a>
<br/>
</div>
</div>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://github.com/TUT-ARG/DCASE2016-baseline-system-matlab/archive/v1.0.6.zip" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-info"></i>
<i class="fa fa-gears fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://github.com/TUT-ARG/DCASE2016-baseline-system-matlab/archive/v1.0.6.zip" target="_blank">
<span style="font-size:20px;">DCASE2016 Task 1&amp;3 Matlab baseline, release <i class="fa fa-download"></i></span>
</a>
<br/>
<span class="text-muted">
                
                version 1.0.6
                
                
                (.zip)
                
                </span>
</div>
</div>
<p><br/></p>
<h3>Results for TUT Acoustic scenes 2016, development set</h3>
<p><em>Evaluation setup</em></p>
<ul>
<li>4-fold cross-validation, average classification accuracy over folds</li>
<li>15 acoustic scene classes</li>
<li>Classification unit: one file (30 seconds of audio).</li>
</ul>
<p><em>System parameters</em></p>
<ul>
<li>Frame size: 40 ms (with 50% hop size)</li>
<li>Number of Gaussians per acoustic scene class model: 16</li>
<li>Feature vector: 20 MFCC static coefficients (including 0th) + 20 delta MFCC coefficients + 20 acceleration MFCC coefficients = 60 values</li>
<li>Trained and tested on full audio</li>
<li><a href="https://github.com/TUT-ARG/DCASE2016-baseline-system-python">Python implementation</a></li>
</ul>
<div class="table-responsive col-md-8">
<table class="table table-striped">
<caption>Acoustic scene classification results, averaged over evaluation folds.</caption>
<thead>
<tr>
<th>Acoustic scene</th>
<th class="col-md-3">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beach</td>
<td>69.3 %</td>
</tr>
<tr>
<td>Bus</td>
<td>79.6 %</td>
</tr>
<tr>
<td>Cafe / Restaurant</td>
<td>83.2 %</td>
</tr>
<tr>
<td>Car</td>
<td>87.2 %</td>
</tr>
<tr>
<td>City center</td>
<td>85.5 %</td>
</tr>
<tr>
<td>Forest path</td>
<td>81.0 %</td>
</tr>
<tr>
<td>Grocery store</td>
<td>65.0 %</td>
</tr>
<tr>
<td>Home</td>
<td>82.1 %</td>
</tr>
<tr>
<td>Library</td>
<td>50.4 %</td>
</tr>
<tr>
<td>Metro station</td>
<td>94.7 %</td>
</tr>
<tr>
<td>Office</td>
<td>98.6 %</td>
</tr>
<tr>
<td>Park</td>
<td>13.9 %</td>
</tr>
<tr>
<td>Residential area</td>
<td>77.7 %</td>
</tr>
<tr>
<td>Train</td>
<td>33.6 %</td>
</tr>
<tr>
<td>Tram</td>
<td>85.4 %</td>
</tr>
</tbody>
<tfoot>
<tr>
<td><strong>Overall accuracy</strong></td>
<td><strong>72.5 %</strong></td>
</tr>
</tfoot>
</table>
</div>
<div class="clearfix"></div>
<h1 id="citation">Citation</h1>
<p>If you are using the <strong>dataset</strong> or <strong>baseline</strong> code please cite the following paper:</p>
<div class="btex-item" data-item="Mesaros2016_EUSIPCO" data-source="content/data/challenge2016/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2016_EUSIPCO"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>TUT database for acoustic scene classification and sound event detection.</em>
In 24th European Signal Processing Conference 2016 (EUSIPCO 2016). Budapest, Hungary, 2016.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" class="panel-collapse collapse" id="collapseMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" role="tabpanel">
<h4>TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
<h5>Abstract</h5>
<p class="text-justify">We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://homepages.tuni.fi/annamaria.mesaros/pubs/mesaros_eusipco2016-dcase.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4label" class="modal fade" id="bibtexMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2016_EUSIPCOdb5e946109b549339e509a8e19608ce4label">TUT Database for Acoustic Scene Classification and Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2016_EUSIPCO,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "{TUT} Database for Acoustic Scene Classification and Sound Event Detection",
    abstract = "We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting ofbinaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.",
    year = "2016",
    address = "Budapest, Hungary",
    booktitle = "24th European Signal Processing Conference 2016 (EUSIPCO 2016)"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<p>When citing <strong>challenge task</strong> and <strong>results</strong> please cite the following paper:</p>
<div class="btex-item" data-item="Mesaros2018_TASLP" data-source="content/data/challenge2016/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2018_TASLP"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            A. Mesaros, T. Heittola, E. Benetos, P. Foster, M. Lagrange, T. Virtanen, and M. D. Plumbley.
<em>Detection and classification of acoustic scenes and events: outcome of the DCASE 2016 challenge.</em>
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 26(2):379–393, Feb 2018.
<a href="https://doi.org/10.1109/TASLP.2017.2778423">doi:10.1109/TASLP.2017.2778423</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="https://trepo.tuni.fi//bitstream/handle/10024/126402/dcase2016_taslp.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" class="panel-collapse collapse" id="collapseMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" role="tabpanel">
<h4>Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge</h4>
<h5>Abstract</h5>
<p class="text-justify">Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events (DCASE 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustics;Event detection;Hidden Markov models;Speech;Speech processing;Tagging;Acoustic scene classification;audio datasets;pattern recognition;sound event detection</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="https://trepo.tuni.fi//bitstream/handle/10024/126402/dcase2016_taslp.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-pdf-o fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1label" class="modal fade" id="bibtexMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2018_TASLP3a0beb4b192d4c0a8eecee9b610b57d1label">Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge</h4>
</div>
<div class="modal-body">
<pre>@article{Mesaros2018_TASLP,
    author = "Mesaros, A. and Heittola, T. and Benetos, E. and Foster, P. and Lagrange, M. and Virtanen, T. and Plumbley, M. D.",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    title = "Detection and Classification of Acoustic Scenes and Events: Outcome of the {DCASE} 2016 Challenge",
    year = "2018",
    volume = "26",
    number = "2",
    pages = "379--393",
    abstract = "Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events (DCASE 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.",
    keywords = "Acoustics;Event detection;Hidden Markov models;Speech;Speech processing;Tagging;Acoustic scene classification;audio datasets;pattern recognition;sound event detection",
    doi = "10.1109/TASLP.2017.2778423",
    issn = "2329-9290",
    month = "Feb"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<p>Follow up paper about assessing the human and machine performance for acoustic scene classification task:</p>
<div class="btex-item" data-item="Mesaros2017_WASPAA" data-source="content/data/challenge2016/publications.bib">
<div class="panel panel-default">
<span class="label label-default" style="padding-top:0.4em;margin-left:0em;margin-top:0em;">Publication<a name="Mesaros2017_WASPAA"></a></span>
<div class="panel-body">
<div class="row">
<div class="col-md-9">
<p style="text-align:left">
                            Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen.
<em>Assessment of human and machine performance in acoustic scene classification: DCASE 2016 case study.</em>
In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 319–323. IEEE Computer Society, 2017.
<a href="https://doi.org/10.1109/WASPAA.2017.8170047">doi:10.1109/WASPAA.2017.8170047</a>.
                            
                            
                            </p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button class="btn btn-xs btn-danger" data-target="#bibtexMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bib</button>
<button aria-controls="collapseMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" class="panel-collapse collapse" id="collapseMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" role="tabpanel">
<h4>Assessment of Human and Machine Performance in Acoustic Scene Classification: DCASE 2016 Case Study</h4>
<h5>Abstract</h5>
<p class="text-justify">Human and machine performance in acoustic scene classification is examined through a parallel experiment using TUT Acoustic Scenes 2016 dataset. The machine learning perspective is presented based on the systems submitted for the 2016 challenge on Detection and Classification of Acoustic Scenes and Events. The human performance, assessed through a listening experiment, was found to be significantly lower than machine performance. Test subjects exhibited different behavior throughout the experiment, leading to significant differences in performance between groups of subjects. An expert listener trained for the task obtained similar accuracy to the average of submitted systems, comparable also to previous studies of human abilities in recognizing everyday acoustic scenes.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597label" class="modal fade" id="bibtexMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros2017_WASPAAc554781526cf4fe8b18ded7c1aeed597label">Assessment of Human and Machine Performance in Acoustic Scene Classification: DCASE 2016 Case Study</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2017_WASPAA,
    author = "Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "Assessment of Human and Machine Performance in Acoustic Scene Classification: {DCASE} 2016 Case Study",
    year = "2017",
    pages = "319--323",
    booktitle = "2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
    publisher = "IEEE Computer Society",
    language = "English",
    abstract = "Human and machine performance in acoustic scene classification is examined through a parallel experiment using TUT Acoustic Scenes 2016 dataset. The machine learning perspective is presented based on the systems submitted for the 2016 challenge on Detection and Classification of Acoustic Scenes and Events. The human performance, assessed through a listening experiment, was found to be significantly lower than machine performance. Test subjects exhibited different behavior throughout the experiment, leading to significant differences in performance between groups of subjects. An expert listener trained for the task obtained similar accuracy to the average of submitted systems, comparable also to previous studies of human abilities in recognizing everyday acoustic scenes.",
    doi = "10.1109/WASPAA.2017.8170047"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>