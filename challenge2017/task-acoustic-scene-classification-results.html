<!DOCTYPE html><html lang="en">
<head>
    <title>Acoustic scene classification - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2017/task-acoustic-scene-classification-results">
        <meta name="author" content="Toni Heittola" />
        <meta name="description" content="Task description The goal of acoustic scene classification task was to classify test recordings into one of predefined classes (15) that characterizes the environment in which they were recorded — for example park, home, office. The participants used 4680 10-second audio excerpts (13h of audio) to train their systems, and 1620 …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/font-mfizz.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2017</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2017/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2017/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2017/task-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-rare-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-alarm text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-rare-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-rare-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-events text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-sound-event-detection-in-real-life-audio-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2017/task-large-scale-sound-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2017/task-large-scale-sound-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2017/task-large-scale-sound-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Download data">
        <a href="/challenge2017/download"><i class="fa fa-download"></i>&nbsp;Download</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2017/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2017/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge organizers">
        <a href="/challenge2017/organizers"><i class="fa fa-users"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/dunes-02.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-primary"></i><i class="fa dc-scene fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Scenes</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2017_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Acoustic<br> scene classification</h1><hr class="small right bold"><span class="subheading">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#challenge-results">Challenge results</a>
<ul>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a></li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h2 id="task-description">Task description</h2>
<p>The goal of acoustic scene classification task was to classify test recordings into one of predefined classes (15) that characterizes the environment in which they were recorded — for example <em>park</em>, <em>home</em>, <em>office</em>. The participants used 4680 10-second audio excerpts (13h of audio) to train their systems, and 1620 10-second audio excerpts (4h 30min of audio) were used for the challenge evaluation.</p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2017/task-acoustic-scene-classification">task description page</a></p>
<h2 id="challenge-results">Challenge results</h2>
<p>Here you can find complete information on the submissions for Task 1: results on evaluation and development set (when reported by authors), class-wise results, technical reports and bibtex citations.</p>
<p>System outputs:</p>
<div class="row">
<div class="col-md-1">
<a class="icon" href="https://zenodo.org/record/2598364" target="_blank">
<span class="fa-stack fa-2x">
<i class="fa fa-square fa-stack-2x text-muted"></i>
<i class="fa fa-file-text-o fa-stack-1x fa-inverse"></i>
</span>
</a>
</div>
<div class="col-md-11">
<a href="https://zenodo.org/record/2598364" target="_blank">
<span style="font-size:20px;">DCASE2017 Challenge Submissions Package <i class="fa fa-download"></i></span>
</a>
<span class="text-muted">(28.7 MB)</span>
<br/>
<a href="http://dx.doi.org/10.5281/zenodo.2598364">
<img alt="10.5281/zenodo.2598364" src="https://zenodo.org/badge/doi/10.5281/zenodo.2598364.svg"/>
</a>
</div>
</div>
<p><br/></p>
<h3 id="systems-ranking">Systems ranking</h3>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Abrol_IITM_task1_1</td>
<td>Baseline</td>
<td>Abrol2017</td>
<td>65.7 (63.4 - 68.0)</td>
<td>88.1</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_1</td>
<td>S2S-AE</td>
<td>Amiriparian2017</td>
<td>67.5 (65.3 - 69.8)</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_2</td>
<td>Shahin_APTI</td>
<td>Amiriparian2017a</td>
<td>59.1 (56.7 - 61.5)</td>
<td>90.1</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_1</td>
<td>Biho1</td>
<td>Kim2017</td>
<td>56.5 (54.1 - 59.0)</td>
<td>75.9</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_2</td>
<td>Biho2</td>
<td>Kim2017</td>
<td>60.5 (58.1 - 62.9)</td>
<td>75.9</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_1</td>
<td>TPT1</td>
<td>Bisot2017</td>
<td>69.8 (67.6 - 72.1)</td>
<td>90.1</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_2</td>
<td>TPT2</td>
<td>Bisot2017</td>
<td>69.6 (67.3 - 71.8)</td>
<td>89.1</td>
</tr>
<tr>
<td></td>
<td>Chandrasekhar_IIITH_task1_1</td>
<td></td>
<td>Chandrasekhar2017</td>
<td>45.9 (43.4 - 48.3)</td>
<td>77.6</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_1</td>
<td>TP_CNN_cv1</td>
<td>Chou2017</td>
<td>57.1 (54.7 - 59.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_2</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>61.5 (59.2 - 63.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_3</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>59.8 (57.4 - 62.1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_4</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>57.1 (54.7 - 59.5)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_1</td>
<td>andang1</td>
<td>Dang2017</td>
<td>62.7 (60.4 - 65.1)</td>
<td>82.0</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_2</td>
<td>andang1</td>
<td>Dang2017</td>
<td>62.7 (60.4 - 65.1)</td>
<td>79.1</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_3</td>
<td>andang1</td>
<td>Dang2017</td>
<td>63.7 (61.4 - 66.0)</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_1</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>57.0 (54.6 - 59.4)</td>
<td>79.9</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_2</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>59.9 (57.5 - 62.3)</td>
<td>81.9</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_3</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>64.1 (61.7 - 66.4)</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_4</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>63.0 (60.7 - 65.4)</td>
<td>84.8</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_1</td>
<td>MLPFeats</td>
<td>Foleiss2017</td>
<td>64.5 (62.2 - 66.8)</td>
<td>78.0</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_2</td>
<td>MLPFeatRF</td>
<td>Foleiss2017</td>
<td>66.9 (64.6 - 69.2)</td>
<td>80.0</td>
</tr>
<tr>
<td></td>
<td>Fonseca_MTG_task1_1</td>
<td>MTG</td>
<td>Fonseca2017</td>
<td>67.3 (65.1 - 69.6)</td>
<td>83.0</td>
</tr>
<tr>
<td></td>
<td>Fraile_UPM_task1_1</td>
<td>GAMMA-UPM</td>
<td>Fraile2017</td>
<td>58.3 (55.9 - 60.7)</td>
<td>79.8</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_1</td>
<td>MTG_GBMVGG</td>
<td>Gong2017</td>
<td>61.2 (58.8 - 63.5)</td>
<td>86.8</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_2</td>
<td>MTG_GBM</td>
<td>Gong2017</td>
<td>61.5 (59.1 - 63.9)</td>
<td>86.1</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_3</td>
<td>MTG_VGG</td>
<td>Gong2017</td>
<td>61.9 (59.5 - 64.2)</td>
<td>84.0</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_1</td>
<td>4fEnsemSel</td>
<td>Han2017</td>
<td>79.9 (78.0 - 81.9)</td>
<td>91.9</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_2</td>
<td>4fMeanAll</td>
<td>Han2017</td>
<td>79.6 (77.7 - 81.6)</td>
<td>91.7</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_3</td>
<td>FlEnsemSel</td>
<td>Han2017</td>
<td>80.4 (78.4 - 82.3)</td>
<td>91.9</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_4</td>
<td>flMeanAll</td>
<td>Han2017</td>
<td>80.3 (78.4 - 82.2)</td>
<td>91.7</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_1</td>
<td>BUETBOSCH1</td>
<td>Hyder2017</td>
<td>74.1 (72.0 - 76.3)</td>
<td>88.1</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_2</td>
<td>BUETBOSCH2</td>
<td>Hyder2017</td>
<td>72.2 (70.0 - 74.3)</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_3</td>
<td>BUETBOSCH3</td>
<td>Hyder2017</td>
<td>68.6 (66.3 - 70.8)</td>
<td>89.8</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_4</td>
<td>BUETBOSCH4</td>
<td>Hyder2017</td>
<td>72.0 (69.8 - 74.2)</td>
<td>89.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2017 baseline</td>
<td>Baseline</td>
<td>Heittola2017</td>
<td>61.0 (58.7 - 63.4)</td>
<td>74.8</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_1</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.5 (63.2 - 67.8)</td>
<td>83.4</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_2</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.4 (63.1 - 67.7)</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_1</td>
<td></td>
<td>Hussain2017</td>
<td>56.7 (54.3 - 59.1)</td>
<td>90.7</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_2</td>
<td></td>
<td>Hussain2017</td>
<td>59.5 (57.1 - 61.9)</td>
<td>90.4</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_3</td>
<td></td>
<td>Hussain2017</td>
<td>59.9 (57.5 - 62.3)</td>
<td>90.0</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_4</td>
<td></td>
<td>Hussain2017</td>
<td>55.4 (52.9 - 57.8)</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_1</td>
<td>CRNN-1</td>
<td>Jallet2017</td>
<td>60.7 (58.4 - 63.1)</td>
<td>78.9</td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_2</td>
<td>CRNN-2</td>
<td>Jallet2017</td>
<td>61.2 (58.8 - 63.5)</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Jimenez_CMU_task1_1</td>
<td>LapKernel</td>
<td>Jimenez2017</td>
<td>59.9 (57.6 - 62.3)</td>
<td>78.7</td>
</tr>
<tr>
<td></td>
<td>Kukanov_UEF_task1_1</td>
<td>K-CRNN</td>
<td>Kukanov2017</td>
<td>71.7 (69.5 - 73.9)</td>
<td>85.8</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_1</td>
<td>Wav_SVMs</td>
<td>Kun2017</td>
<td>64.2 (61.9 - 66.5)</td>
<td>83.2</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_2</td>
<td>Wav_GRUs</td>
<td>Kun2017</td>
<td>64.0 (61.7 - 66.3)</td>
<td>82.6</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_1</td>
<td>JKU_IVEC</td>
<td>Lehner2017</td>
<td>68.7 (66.4 - 71.0)</td>
<td>84.5</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_2</td>
<td>JKU_ALL_av</td>
<td>Lehner2017</td>
<td>66.8 (64.5 - 69.1)</td>
<td>87.7</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_3</td>
<td>JKU_CNN</td>
<td>Lehner2017</td>
<td>64.8 (62.5 - 67.1)</td>
<td>89.0</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_4</td>
<td>JKU_All_ca</td>
<td>Lehner2017</td>
<td>73.8 (71.7 - 76.0)</td>
<td>91.3</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_1</td>
<td>LiSCUTt1_1</td>
<td>Li2017</td>
<td>53.7 (51.3 - 56.1)</td>
<td>91.0</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_2</td>
<td>LiSCUTt1_2</td>
<td>Li2017</td>
<td>63.6 (61.3 - 66.0)</td>
<td>83.9</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_3</td>
<td>LiSCUTt1_3</td>
<td>Li2017</td>
<td>61.7 (59.4 - 64.1)</td>
<td>83.1</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_4</td>
<td>LiSCUTt1_4</td>
<td>Li2017</td>
<td>57.8 (55.4 - 60.2)</td>
<td>87.5</td>
</tr>
<tr>
<td></td>
<td>Maka_ZUT_task1_1</td>
<td>ASAWI</td>
<td>Maka2017</td>
<td>47.5 (45.1 - 50.0)</td>
<td>70.6</td>
</tr>
<tr>
<td></td>
<td>Mun_KU_task1_1</td>
<td>GAN_SKMUN</td>
<td>Mun2017</td>
<td>83.3 (81.5 - 85.1)</td>
<td>87.1</td>
</tr>
<tr>
<td></td>
<td>Park_ISPL_task1_1</td>
<td>ISPL</td>
<td>Park2017</td>
<td>72.6 (70.4 - 74.8)</td>
<td>83.6</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_1</td>
<td>CNN</td>
<td>Phan2017</td>
<td>59.0 (56.6 - 61.4)</td>
<td>83.8</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_2</td>
<td>ACNN</td>
<td>Phan2017</td>
<td>55.9 (53.5 - 58.3)</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_3</td>
<td>CNN+</td>
<td>Phan2017</td>
<td>58.3 (55.9 - 60.7)</td>
<td>83.8</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_4</td>
<td>ACNN+</td>
<td>Phan2017</td>
<td>58.0 (55.6 - 60.4)</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_1</td>
<td>amb200</td>
<td>Piczak2017</td>
<td>70.6 (68.4 - 72.8)</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_2</td>
<td>dishes</td>
<td>Piczak2017</td>
<td>69.6 (67.3 - 71.8)</td>
<td>82.7</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_3</td>
<td>amb100</td>
<td>Piczak2017</td>
<td>67.7 (65.4 - 69.9)</td>
<td>80.2</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_4</td>
<td>amb60</td>
<td>Piczak2017</td>
<td>62.0 (59.6 - 64.3)</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_1</td>
<td>HBGS CNN</td>
<td>Rakotomamonjy2017</td>
<td>61.5 (59.2 - 63.9)</td>
<td>85.9</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_2</td>
<td>HBGS CNN-4</td>
<td>Rakotomamonjy2017</td>
<td>62.7 (60.3 - 65.0)</td>
<td>85.3</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_3</td>
<td>HBGS CNN-19</td>
<td>Rakotomamonjy2017</td>
<td>62.8 (60.4 - 65.1)</td>
<td>84.6</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_1</td>
<td>multires</td>
<td>Schindler2017</td>
<td>61.7 (59.4 - 64.1)</td>
<td>87.3</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_2</td>
<td>multires-p</td>
<td>Schindler2017</td>
<td>61.7 (59.4 - 64.1)</td>
<td>90.5</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_1</td>
<td>CERTH_1</td>
<td>Vafeiadis2017</td>
<td>61.0 (58.6 - 63.4)</td>
<td>80.4</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_2</td>
<td>CERTH_2</td>
<td>Vafeiadis2017</td>
<td>49.5 (47.1 - 51.9)</td>
<td>95.9</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_1</td>
<td>Vij_UIET_1</td>
<td>Vij2017</td>
<td>61.2 (58.9 - 63.6)</td>
<td>77.3</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_2</td>
<td>Vij_UIET_2</td>
<td>Vij2017</td>
<td>57.5 (55.1 - 59.9)</td>
<td>79.0</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_3</td>
<td>Vij_UIET_3</td>
<td>Vij2017</td>
<td>59.6 (57.2 - 62.0)</td>
<td>78.0</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_4</td>
<td>Vij_UIET_4</td>
<td>Vij2017</td>
<td>65.0 (62.7 - 67.3)</td>
<td>82.7</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_1</td>
<td>IITKGP_ABSP_Fusion</td>
<td>Waldekar2017</td>
<td>67.0 (64.7 - 69.3)</td>
<td>86.3</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_2</td>
<td>IITKGP_ABSP_Hierarchical</td>
<td>Waldekar2017</td>
<td>64.9 (62.6 - 67.2)</td>
<td>88.8</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_1</td>
<td>DCNN_vote</td>
<td>Weiping2017</td>
<td>74.8 (72.6 - 76.9)</td>
<td>87.6</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_2</td>
<td>DCNN_SVM</td>
<td>Weiping2017</td>
<td>77.7 (75.7 - 79.7)</td>
<td>89.9</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_1</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>68.5 (66.2 - 70.7)</td>
<td>85.3</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_2</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>67.5 (65.3 - 69.8)</td>
<td>87.4</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_1</td>
<td>autolog1</td>
<td>Xu2017a</td>
<td>65.9 (63.6 - 68.2)</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_2</td>
<td>autolog2</td>
<td>Xu2017a</td>
<td>66.7 (64.4 - 69.0)</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_3</td>
<td>autolog3</td>
<td>Xu2017a</td>
<td>64.6 (62.3 - 67.0)</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_1</td>
<td>MFS</td>
<td>Lu2017</td>
<td>61.5 (59.2 - 63.9)</td>
<td>81.3</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_2</td>
<td>STD</td>
<td>Lu2017</td>
<td>65.2 (62.9 - 67.6)</td>
<td>80.3</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_3</td>
<td>MFS+STD</td>
<td>Lu2017</td>
<td>62.8 (60.5 - 65.2)</td>
<td>82.0</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_4</td>
<td>Pre-training</td>
<td>Lu2017</td>
<td>63.6 (61.3 - 66.0)</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_1</td>
<td>UOS_DualIn</td>
<td>Jee-Weon2017</td>
<td>67.0 (64.7 - 69.3)</td>
<td>85.5</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_2</td>
<td>UOS_BalCos</td>
<td>Jee-Weon2017</td>
<td>66.2 (63.9 - 68.5)</td>
<td>85.1</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_3</td>
<td>UOS_DatDup</td>
<td>Jee-Weon2017</td>
<td>67.3 (65.1 - 69.6)</td>
<td>95.4</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_4</td>
<td>UOS_res</td>
<td>Jee-Weon2017</td>
<td>70.6 (68.3 - 72.8)</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_1</td>
<td>MResNet-34</td>
<td>Zhao2017</td>
<td>70.0 (67.8 - 72.2)</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_2</td>
<td>Conv</td>
<td>Zhao2017</td>
<td>67.9 (65.6 - 70.2)</td>
<td>85.4</td>
</tr>
<tr>
<td></td>
<td>Zhao_UAU_UP_task1_1</td>
<td>GRNN</td>
<td>Zhao2017a</td>
<td>63.8 (61.5 - 66.2)</td>
<td>83.3</td>
</tr>
</tbody>
</table>
<h3 id="teams-ranking">Teams ranking</h3>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_confidence" data-scatter-y="accuracy_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission <br/>code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Submission <br/>name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small> <br/>(Evaluation dataset)
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Development dataset)
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Abrol_IITM_task1_1</td>
<td>Baseline</td>
<td>Abrol2017</td>
<td>65.7 (63.4 - 68.0)</td>
<td>88.1</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_1</td>
<td>S2S-AE</td>
<td>Amiriparian2017</td>
<td>67.5 (65.3 - 69.8)</td>
<td>88.0</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_2</td>
<td>Shahin_APTI</td>
<td>Amiriparian2017a</td>
<td>59.1 (56.7 - 61.5)</td>
<td>90.1</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_2</td>
<td>Biho2</td>
<td>Kim2017</td>
<td>60.5 (58.1 - 62.9)</td>
<td>75.9</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_1</td>
<td>TPT1</td>
<td>Bisot2017</td>
<td>69.8 (67.6 - 72.1)</td>
<td>90.1</td>
</tr>
<tr>
<td></td>
<td>Chandrasekhar_IIITH_task1_1</td>
<td></td>
<td>Chandrasekhar2017</td>
<td>45.9 (43.4 - 48.3)</td>
<td>77.6</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_2</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>61.5 (59.2 - 63.9)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_3</td>
<td>andang1</td>
<td>Dang2017</td>
<td>63.7 (61.4 - 66.0)</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_3</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>64.1 (61.7 - 66.4)</td>
<td>81.6</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_2</td>
<td>MLPFeatRF</td>
<td>Foleiss2017</td>
<td>66.9 (64.6 - 69.2)</td>
<td>80.0</td>
</tr>
<tr>
<td></td>
<td>Fonseca_MTG_task1_1</td>
<td>MTG</td>
<td>Fonseca2017</td>
<td>67.3 (65.1 - 69.6)</td>
<td>83.0</td>
</tr>
<tr>
<td></td>
<td>Fraile_UPM_task1_1</td>
<td>GAMMA-UPM</td>
<td>Fraile2017</td>
<td>58.3 (55.9 - 60.7)</td>
<td>79.8</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_3</td>
<td>MTG_VGG</td>
<td>Gong2017</td>
<td>61.9 (59.5 - 64.2)</td>
<td>84.0</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_3</td>
<td>FlEnsemSel</td>
<td>Han2017</td>
<td>80.4 (78.4 - 82.3)</td>
<td>91.9</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_1</td>
<td>BUETBOSCH1</td>
<td>Hyder2017</td>
<td>74.1 (72.0 - 76.3)</td>
<td>88.1</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2017 baseline</td>
<td>Baseline</td>
<td>Heittola2017</td>
<td>61.0 (58.7 - 63.4)</td>
<td>74.8</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_1</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.5 (63.2 - 67.8)</td>
<td>83.4</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_3</td>
<td></td>
<td>Hussain2017</td>
<td>59.9 (57.5 - 62.3)</td>
<td>90.0</td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_2</td>
<td>CRNN-2</td>
<td>Jallet2017</td>
<td>61.2 (58.8 - 63.5)</td>
<td>80.8</td>
</tr>
<tr>
<td></td>
<td>Jimenez_CMU_task1_1</td>
<td>LapKernel</td>
<td>Jimenez2017</td>
<td>59.9 (57.6 - 62.3)</td>
<td>78.7</td>
</tr>
<tr>
<td></td>
<td>Kukanov_UEF_task1_1</td>
<td>K-CRNN</td>
<td>Kukanov2017</td>
<td>71.7 (69.5 - 73.9)</td>
<td>85.8</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_1</td>
<td>Wav_SVMs</td>
<td>Kun2017</td>
<td>64.2 (61.9 - 66.5)</td>
<td>83.2</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_4</td>
<td>JKU_All_ca</td>
<td>Lehner2017</td>
<td>73.8 (71.7 - 76.0)</td>
<td>91.3</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_2</td>
<td>LiSCUTt1_2</td>
<td>Li2017</td>
<td>63.6 (61.3 - 66.0)</td>
<td>83.9</td>
</tr>
<tr>
<td></td>
<td>Maka_ZUT_task1_1</td>
<td>ASAWI</td>
<td>Maka2017</td>
<td>47.5 (45.1 - 50.0)</td>
<td>70.6</td>
</tr>
<tr>
<td></td>
<td>Mun_KU_task1_1</td>
<td>GAN_SKMUN</td>
<td>Mun2017</td>
<td>83.3 (81.5 - 85.1)</td>
<td>87.1</td>
</tr>
<tr>
<td></td>
<td>Park_ISPL_task1_1</td>
<td>ISPL</td>
<td>Park2017</td>
<td>72.6 (70.4 - 74.8)</td>
<td>83.6</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_1</td>
<td>CNN</td>
<td>Phan2017</td>
<td>59.0 (56.6 - 61.4)</td>
<td>83.8</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_1</td>
<td>amb200</td>
<td>Piczak2017</td>
<td>70.6 (68.4 - 72.8)</td>
<td>82.3</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_3</td>
<td>HBGS CNN-19</td>
<td>Rakotomamonjy2017</td>
<td>62.8 (60.4 - 65.1)</td>
<td>84.6</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_1</td>
<td>multires</td>
<td>Schindler2017</td>
<td>61.7 (59.4 - 64.1)</td>
<td>87.3</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_1</td>
<td>CERTH_1</td>
<td>Vafeiadis2017</td>
<td>61.0 (58.6 - 63.4)</td>
<td>80.4</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_4</td>
<td>Vij_UIET_4</td>
<td>Vij2017</td>
<td>65.0 (62.7 - 67.3)</td>
<td>82.7</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_1</td>
<td>IITKGP_ABSP_Fusion</td>
<td>Waldekar2017</td>
<td>67.0 (64.7 - 69.3)</td>
<td>86.3</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_2</td>
<td>DCNN_SVM</td>
<td>Weiping2017</td>
<td>77.7 (75.7 - 79.7)</td>
<td>89.9</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_1</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>68.5 (66.2 - 70.7)</td>
<td>85.3</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_2</td>
<td>autolog2</td>
<td>Xu2017a</td>
<td>66.7 (64.4 - 69.0)</td>
<td>84.4</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_2</td>
<td>STD</td>
<td>Lu2017</td>
<td>65.2 (62.9 - 67.6)</td>
<td>80.3</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_4</td>
<td>UOS_res</td>
<td>Jee-Weon2017</td>
<td>70.6 (68.3 - 72.8)</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_1</td>
<td>MResNet-34</td>
<td>Zhao2017</td>
<td>70.0 (67.8 - 72.2)</td>
<td>85.6</td>
</tr>
<tr>
<td></td>
<td>Zhao_UAU_UP_task1_1</td>
<td>GRNN</td>
<td>Zhao2017a</td>
<td>63.8 (61.5 - 66.2)</td>
<td>83.3</td>
</tr>
</tbody>
</table>
<h3 id="class-wise-performance">Class-wise performance</h3>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2017 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Mun_KU_task1_1" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["class_accuracy_eval_beach", "class_accuracy_eval_bus", "class_accuracy_eval_cafe_restaurant", "class_accuracy_eval_car", "class_accuracy_eval_city_center", "class_accuracy_eval_forest_path", "class_accuracy_eval_grocery_store", "class_accuracy_eval_home", "class_accuracy_eval_library", "class_accuracy_eval_metro_station", "class_accuracy_eval_office", "class_accuracy_eval_park", "class_accuracy_eval_residential_area", "class_accuracy_eval_train", "class_accuracy_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_cafe_restaurant", "class_accuracy_eval_grocery_store", "class_accuracy_eval_home", "class_accuracy_eval_library", "class_accuracy_eval_office"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_beach", "class_accuracy_eval_city_center", "class_accuracy_eval_forest_path", "class_accuracy_eval_park", "class_accuracy_eval_residential_area"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_bus","class_accuracy_eval_car","class_accuracy_eval_metro_station","class_accuracy_eval_train","class_accuracy_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
            Submission<br/>code
        </th>
<th class="sm-cell" data-field="name" data-sortable="true">
            Submission<br/>name
        </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
            Technical<br/>Report
        </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
            Accuracy <br/>(Evaluation dataset)
        </th>
<th class="sep-both-cell text-center" data-chartable="true" data-field="class_accuracy_eval_beach" data-sortable="true" data-value-type="float1-percentage">
            Beach
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_bus" data-sortable="true" data-value-type="float1-percentage">
            Bus
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_cafe_restaurant" data-sortable="true" data-value-type="float1-percentage">
            Cafe / <br/>Restaurant
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_car" data-sortable="true" data-value-type="float1-percentage">
            Car
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_city_center" data-sortable="true" data-value-type="float1-percentage">
            City <br/>center
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_forest_path" data-sortable="true" data-value-type="float1-percentage">
            Forest <br/>path
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_grocery_store" data-sortable="true" data-value-type="float1-percentage">
            Grocery <br/>store
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_home" data-sortable="true" data-value-type="float1-percentage">
            Home
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_library" data-sortable="true" data-value-type="float1-percentage">
            Library
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro_station" data-sortable="true" data-value-type="float1-percentage">
            Metro <br/>station
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_office" data-sortable="true" data-value-type="float1-percentage">
            Office
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_park" data-sortable="true" data-value-type="float1-percentage">
            Park
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_residential_area" data-sortable="true" data-value-type="float1-percentage">
            Residential<br/>area
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_train" data-sortable="true" data-value-type="float1-percentage">
            Train
        </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_tram" data-sortable="true" data-value-type="float1-percentage">
            Tram
        </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Abrol_IITM_task1_1</td>
<td>Baseline</td>
<td>Abrol2017</td>
<td>65.7</td>
<td>73.1</td>
<td>61.1</td>
<td>88.9</td>
<td>81.5</td>
<td>82.4</td>
<td>44.4</td>
<td>73.1</td>
<td>72.2</td>
<td>35.2</td>
<td>75.0</td>
<td>86.1</td>
<td>32.4</td>
<td>49.1</td>
<td>75.0</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_1</td>
<td>S2S-AE</td>
<td>Amiriparian2017</td>
<td>67.5</td>
<td>44.4</td>
<td>75.0</td>
<td>63.0</td>
<td>95.4</td>
<td>94.4</td>
<td>97.2</td>
<td>73.1</td>
<td>60.2</td>
<td>43.5</td>
<td>79.6</td>
<td>62.0</td>
<td>16.7</td>
<td>64.8</td>
<td>82.4</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_2</td>
<td>Shahin_APTI</td>
<td>Amiriparian2017a</td>
<td>59.1</td>
<td>24.1</td>
<td>62.0</td>
<td>58.3</td>
<td>82.4</td>
<td>91.7</td>
<td>97.2</td>
<td>69.4</td>
<td>51.9</td>
<td>39.8</td>
<td>66.7</td>
<td>43.5</td>
<td>7.4</td>
<td>62.0</td>
<td>78.7</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_1</td>
<td>Biho1</td>
<td>Kim2017</td>
<td>56.5</td>
<td>24.1</td>
<td>33.3</td>
<td>33.3</td>
<td>75.9</td>
<td>61.1</td>
<td>80.6</td>
<td>50.9</td>
<td>88.9</td>
<td>27.8</td>
<td>99.1</td>
<td>57.4</td>
<td>17.6</td>
<td>88.0</td>
<td>55.6</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_2</td>
<td>Biho2</td>
<td>Kim2017</td>
<td>60.5</td>
<td>37.0</td>
<td>41.7</td>
<td>30.6</td>
<td>74.1</td>
<td>74.1</td>
<td>88.0</td>
<td>50.9</td>
<td>86.1</td>
<td>39.8</td>
<td>96.3</td>
<td>57.4</td>
<td>41.7</td>
<td>83.3</td>
<td>55.6</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_1</td>
<td>TPT1</td>
<td>Bisot2017</td>
<td>69.8</td>
<td>5.6</td>
<td>81.5</td>
<td>51.9</td>
<td>80.6</td>
<td>76.9</td>
<td>86.1</td>
<td>75.0</td>
<td>88.0</td>
<td>45.4</td>
<td>99.1</td>
<td>85.2</td>
<td>26.9</td>
<td>80.6</td>
<td>95.4</td>
<td>69.4</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_2</td>
<td>TPT2</td>
<td>Bisot2017</td>
<td>69.6</td>
<td>23.1</td>
<td>75.9</td>
<td>54.6</td>
<td>75.9</td>
<td>78.7</td>
<td>84.3</td>
<td>75.0</td>
<td>88.9</td>
<td>39.8</td>
<td>100.0</td>
<td>87.0</td>
<td>27.8</td>
<td>75.9</td>
<td>94.4</td>
<td>62.0</td>
</tr>
<tr>
<td></td>
<td>Chandrasekhar_IIITH_task1_1</td>
<td></td>
<td>Chandrasekhar2017</td>
<td>45.9</td>
<td>6.5</td>
<td>47.2</td>
<td>21.3</td>
<td>88.9</td>
<td>96.3</td>
<td>69.4</td>
<td>42.6</td>
<td>92.6</td>
<td>61.1</td>
<td>68.5</td>
<td>0.0</td>
<td>0.0</td>
<td>3.7</td>
<td>73.1</td>
<td>16.7</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_1</td>
<td>TP_CNN_cv1</td>
<td>Chou2017</td>
<td>57.1</td>
<td>25.9</td>
<td>40.7</td>
<td>48.1</td>
<td>75.0</td>
<td>80.6</td>
<td>88.9</td>
<td>58.3</td>
<td>67.6</td>
<td>19.4</td>
<td>80.6</td>
<td>62.0</td>
<td>21.3</td>
<td>61.1</td>
<td>69.4</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_2</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>61.5</td>
<td>19.4</td>
<td>48.1</td>
<td>66.7</td>
<td>68.5</td>
<td>77.8</td>
<td>86.1</td>
<td>65.7</td>
<td>57.4</td>
<td>25.0</td>
<td>97.2</td>
<td>81.5</td>
<td>28.7</td>
<td>68.5</td>
<td>66.7</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_3</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>59.8</td>
<td>32.4</td>
<td>50.0</td>
<td>49.1</td>
<td>74.1</td>
<td>88.9</td>
<td>88.9</td>
<td>62.0</td>
<td>59.3</td>
<td>36.1</td>
<td>92.6</td>
<td>57.4</td>
<td>20.4</td>
<td>50.0</td>
<td>69.4</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_4</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>57.1</td>
<td>25.9</td>
<td>40.7</td>
<td>48.1</td>
<td>75.0</td>
<td>80.6</td>
<td>88.9</td>
<td>58.3</td>
<td>67.6</td>
<td>19.4</td>
<td>80.6</td>
<td>62.0</td>
<td>21.3</td>
<td>61.1</td>
<td>69.4</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_1</td>
<td>andang1</td>
<td>Dang2017</td>
<td>62.7</td>
<td>32.4</td>
<td>49.1</td>
<td>61.1</td>
<td>65.7</td>
<td>76.9</td>
<td>87.0</td>
<td>57.4</td>
<td>90.7</td>
<td>26.9</td>
<td>95.4</td>
<td>82.4</td>
<td>24.1</td>
<td>75.0</td>
<td>70.4</td>
<td>46.3</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_2</td>
<td>andang1</td>
<td>Dang2017</td>
<td>62.7</td>
<td>24.1</td>
<td>38.9</td>
<td>68.5</td>
<td>66.7</td>
<td>76.9</td>
<td>71.3</td>
<td>65.7</td>
<td>67.6</td>
<td>20.4</td>
<td>99.1</td>
<td>95.4</td>
<td>30.6</td>
<td>77.8</td>
<td>69.4</td>
<td>68.5</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_3</td>
<td>andang1</td>
<td>Dang2017</td>
<td>63.7</td>
<td>28.7</td>
<td>49.1</td>
<td>61.1</td>
<td>71.3</td>
<td>69.4</td>
<td>88.9</td>
<td>59.3</td>
<td>83.3</td>
<td>34.3</td>
<td>100.0</td>
<td>84.3</td>
<td>25.0</td>
<td>83.3</td>
<td>72.2</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_1</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>57.0</td>
<td>13.0</td>
<td>35.2</td>
<td>51.9</td>
<td>88.0</td>
<td>85.2</td>
<td>86.1</td>
<td>52.8</td>
<td>68.5</td>
<td>25.0</td>
<td>28.7</td>
<td>72.2</td>
<td>35.2</td>
<td>82.4</td>
<td>71.3</td>
<td>60.2</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_2</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>59.9</td>
<td>8.3</td>
<td>39.8</td>
<td>57.4</td>
<td>96.3</td>
<td>75.9</td>
<td>88.0</td>
<td>58.3</td>
<td>79.6</td>
<td>34.3</td>
<td>23.1</td>
<td>86.1</td>
<td>40.7</td>
<td>78.7</td>
<td>74.1</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_3</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>64.1</td>
<td>10.2</td>
<td>49.1</td>
<td>45.4</td>
<td>77.8</td>
<td>89.8</td>
<td>85.2</td>
<td>54.6</td>
<td>81.5</td>
<td>38.9</td>
<td>97.2</td>
<td>94.4</td>
<td>25.0</td>
<td>80.6</td>
<td>75.0</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_4</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>63.0</td>
<td>13.9</td>
<td>42.6</td>
<td>57.4</td>
<td>85.2</td>
<td>85.2</td>
<td>87.0</td>
<td>57.4</td>
<td>83.3</td>
<td>35.2</td>
<td>63.9</td>
<td>88.9</td>
<td>31.5</td>
<td>81.5</td>
<td>72.2</td>
<td>60.2</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_1</td>
<td>MLPFeats</td>
<td>Foleiss2017</td>
<td>64.5</td>
<td>18.5</td>
<td>47.2</td>
<td>65.7</td>
<td>75.0</td>
<td>86.1</td>
<td>84.3</td>
<td>63.9</td>
<td>89.8</td>
<td>52.8</td>
<td>99.1</td>
<td>54.6</td>
<td>15.7</td>
<td>77.8</td>
<td>65.7</td>
<td>71.3</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_2</td>
<td>MLPFeatRF</td>
<td>Foleiss2017</td>
<td>66.9</td>
<td>13.9</td>
<td>49.1</td>
<td>68.5</td>
<td>75.9</td>
<td>87.0</td>
<td>91.7</td>
<td>69.4</td>
<td>99.1</td>
<td>50.9</td>
<td>99.1</td>
<td>63.0</td>
<td>18.5</td>
<td>78.7</td>
<td>69.4</td>
<td>69.4</td>
</tr>
<tr>
<td></td>
<td>Fonseca_MTG_task1_1</td>
<td>MTG</td>
<td>Fonseca2017</td>
<td>67.3</td>
<td>36.1</td>
<td>41.7</td>
<td>62.0</td>
<td>75.9</td>
<td>75.0</td>
<td>92.6</td>
<td>57.4</td>
<td>84.3</td>
<td>41.7</td>
<td>99.1</td>
<td>89.8</td>
<td>38.9</td>
<td>76.9</td>
<td>76.9</td>
<td>62.0</td>
</tr>
<tr>
<td></td>
<td>Fraile_UPM_task1_1</td>
<td>GAMMA-UPM</td>
<td>Fraile2017</td>
<td>58.3</td>
<td>61.1</td>
<td>46.3</td>
<td>47.2</td>
<td>76.9</td>
<td>88.9</td>
<td>65.7</td>
<td>48.1</td>
<td>95.4</td>
<td>35.2</td>
<td>63.0</td>
<td>24.1</td>
<td>29.6</td>
<td>63.9</td>
<td>75.0</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_1</td>
<td>MTG_GBMVGG</td>
<td>Gong2017</td>
<td>61.2</td>
<td>50.0</td>
<td>45.4</td>
<td>66.7</td>
<td>67.6</td>
<td>66.7</td>
<td>89.8</td>
<td>62.0</td>
<td>81.5</td>
<td>27.8</td>
<td>85.2</td>
<td>35.2</td>
<td>34.3</td>
<td>68.5</td>
<td>80.6</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_2</td>
<td>MTG_GBM</td>
<td>Gong2017</td>
<td>61.5</td>
<td>41.7</td>
<td>43.5</td>
<td>66.7</td>
<td>70.4</td>
<td>64.8</td>
<td>93.5</td>
<td>51.9</td>
<td>95.4</td>
<td>32.4</td>
<td>88.9</td>
<td>37.0</td>
<td>43.5</td>
<td>67.6</td>
<td>71.3</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_3</td>
<td>MTG_VGG</td>
<td>Gong2017</td>
<td>61.9</td>
<td>64.8</td>
<td>46.3</td>
<td>66.7</td>
<td>71.3</td>
<td>68.5</td>
<td>84.3</td>
<td>71.3</td>
<td>76.9</td>
<td>24.1</td>
<td>55.6</td>
<td>84.3</td>
<td>22.2</td>
<td>57.4</td>
<td>76.9</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_1</td>
<td>4fEnsemSel</td>
<td>Han2017</td>
<td>79.9</td>
<td>75.9</td>
<td>66.7</td>
<td>82.4</td>
<td>92.6</td>
<td>86.1</td>
<td>98.1</td>
<td>80.6</td>
<td>93.5</td>
<td>54.6</td>
<td>100.0</td>
<td>87.0</td>
<td>47.2</td>
<td>75.0</td>
<td>96.3</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_2</td>
<td>4fMeanAll</td>
<td>Han2017</td>
<td>79.6</td>
<td>75.0</td>
<td>65.7</td>
<td>82.4</td>
<td>92.6</td>
<td>86.1</td>
<td>98.1</td>
<td>78.7</td>
<td>92.6</td>
<td>55.6</td>
<td>100.0</td>
<td>85.2</td>
<td>49.1</td>
<td>75.0</td>
<td>96.3</td>
<td>62.0</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_3</td>
<td>FlEnsemSel</td>
<td>Han2017</td>
<td>80.4</td>
<td>78.7</td>
<td>71.3</td>
<td>83.3</td>
<td>93.5</td>
<td>88.9</td>
<td>98.1</td>
<td>79.6</td>
<td>94.4</td>
<td>53.7</td>
<td>100.0</td>
<td>86.1</td>
<td>44.4</td>
<td>75.9</td>
<td>90.7</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_4</td>
<td>flMeanAll</td>
<td>Han2017</td>
<td>80.3</td>
<td>77.8</td>
<td>73.1</td>
<td>82.4</td>
<td>92.6</td>
<td>90.7</td>
<td>98.1</td>
<td>76.9</td>
<td>93.5</td>
<td>52.8</td>
<td>100.0</td>
<td>84.3</td>
<td>48.1</td>
<td>76.9</td>
<td>90.7</td>
<td>66.7</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_1</td>
<td>BUETBOSCH1</td>
<td>Hyder2017</td>
<td>74.1</td>
<td>87.0</td>
<td>59.3</td>
<td>91.7</td>
<td>92.6</td>
<td>94.4</td>
<td>91.7</td>
<td>81.5</td>
<td>97.2</td>
<td>47.2</td>
<td>76.9</td>
<td>49.1</td>
<td>38.0</td>
<td>58.3</td>
<td>81.5</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_2</td>
<td>BUETBOSCH2</td>
<td>Hyder2017</td>
<td>72.2</td>
<td>69.4</td>
<td>61.1</td>
<td>65.7</td>
<td>94.4</td>
<td>81.5</td>
<td>93.5</td>
<td>66.7</td>
<td>91.7</td>
<td>38.9</td>
<td>100.0</td>
<td>83.3</td>
<td>36.1</td>
<td>61.1</td>
<td>77.8</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_3</td>
<td>BUETBOSCH3</td>
<td>Hyder2017</td>
<td>68.6</td>
<td>77.8</td>
<td>70.4</td>
<td>95.4</td>
<td>86.1</td>
<td>86.1</td>
<td>84.3</td>
<td>71.3</td>
<td>98.1</td>
<td>50.0</td>
<td>40.7</td>
<td>22.2</td>
<td>41.7</td>
<td>68.5</td>
<td>83.3</td>
<td>52.8</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_4</td>
<td>BUETBOSCH4</td>
<td>Hyder2017</td>
<td>72.0</td>
<td>83.3</td>
<td>72.2</td>
<td>94.4</td>
<td>85.2</td>
<td>88.0</td>
<td>88.0</td>
<td>71.3</td>
<td>98.1</td>
<td>54.6</td>
<td>60.2</td>
<td>26.9</td>
<td>44.4</td>
<td>75.0</td>
<td>83.3</td>
<td>54.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2017 baseline</td>
<td>Baseline</td>
<td>Heittola2017</td>
<td>61.0</td>
<td>40.7</td>
<td>38.9</td>
<td>43.5</td>
<td>64.8</td>
<td>79.6</td>
<td>85.2</td>
<td>49.1</td>
<td>76.9</td>
<td>30.6</td>
<td>93.5</td>
<td>73.1</td>
<td>32.4</td>
<td>77.8</td>
<td>72.2</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_1</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.5</td>
<td>22.2</td>
<td>50.9</td>
<td>57.4</td>
<td>60.2</td>
<td>77.8</td>
<td>96.3</td>
<td>65.7</td>
<td>90.7</td>
<td>46.3</td>
<td>99.1</td>
<td>77.8</td>
<td>21.3</td>
<td>75.9</td>
<td>73.1</td>
<td>67.6</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_2</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.4</td>
<td>30.6</td>
<td>48.1</td>
<td>63.9</td>
<td>65.7</td>
<td>76.9</td>
<td>95.4</td>
<td>63.9</td>
<td>91.7</td>
<td>37.0</td>
<td>99.1</td>
<td>77.8</td>
<td>10.2</td>
<td>75.9</td>
<td>79.6</td>
<td>64.8</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_1</td>
<td></td>
<td>Hussain2017</td>
<td>56.7</td>
<td>25.9</td>
<td>27.8</td>
<td>49.1</td>
<td>42.6</td>
<td>73.1</td>
<td>88.9</td>
<td>57.4</td>
<td>88.0</td>
<td>4.6</td>
<td>100.0</td>
<td>66.7</td>
<td>29.6</td>
<td>83.3</td>
<td>51.9</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_2</td>
<td></td>
<td>Hussain2017</td>
<td>59.5</td>
<td>28.7</td>
<td>37.0</td>
<td>37.0</td>
<td>73.1</td>
<td>67.6</td>
<td>79.6</td>
<td>55.6</td>
<td>84.3</td>
<td>27.8</td>
<td>100.0</td>
<td>67.6</td>
<td>24.1</td>
<td>85.2</td>
<td>59.3</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_3</td>
<td></td>
<td>Hussain2017</td>
<td>59.9</td>
<td>22.2</td>
<td>36.1</td>
<td>39.8</td>
<td>71.3</td>
<td>74.1</td>
<td>78.7</td>
<td>57.4</td>
<td>85.2</td>
<td>45.4</td>
<td>97.2</td>
<td>67.6</td>
<td>24.1</td>
<td>85.2</td>
<td>55.6</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_4</td>
<td></td>
<td>Hussain2017</td>
<td>55.4</td>
<td>38.9</td>
<td>21.3</td>
<td>59.3</td>
<td>40.7</td>
<td>69.4</td>
<td>92.6</td>
<td>54.6</td>
<td>75.0</td>
<td>14.8</td>
<td>80.6</td>
<td>67.6</td>
<td>20.4</td>
<td>81.5</td>
<td>53.7</td>
<td>60.2</td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_1</td>
<td>CRNN-1</td>
<td>Jallet2017</td>
<td>60.7</td>
<td>15.7</td>
<td>51.9</td>
<td>61.1</td>
<td>75.0</td>
<td>88.0</td>
<td>88.9</td>
<td>56.5</td>
<td>65.7</td>
<td>27.8</td>
<td>87.0</td>
<td>91.7</td>
<td>21.3</td>
<td>55.6</td>
<td>80.6</td>
<td>44.4</td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_2</td>
<td>CRNN-2</td>
<td>Jallet2017</td>
<td>61.2</td>
<td>24.1</td>
<td>55.6</td>
<td>62.0</td>
<td>70.4</td>
<td>88.9</td>
<td>90.7</td>
<td>63.9</td>
<td>70.4</td>
<td>29.6</td>
<td>87.0</td>
<td>84.3</td>
<td>23.1</td>
<td>55.6</td>
<td>72.2</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>Jimenez_CMU_task1_1</td>
<td>LapKernel</td>
<td>Jimenez2017</td>
<td>59.9</td>
<td>69.4</td>
<td>43.5</td>
<td>65.7</td>
<td>72.2</td>
<td>62.0</td>
<td>79.6</td>
<td>47.2</td>
<td>73.1</td>
<td>26.9</td>
<td>76.9</td>
<td>81.5</td>
<td>25.9</td>
<td>63.0</td>
<td>62.0</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Kukanov_UEF_task1_1</td>
<td>K-CRNN</td>
<td>Kukanov2017</td>
<td>71.7</td>
<td>43.5</td>
<td>47.2</td>
<td>77.8</td>
<td>79.6</td>
<td>85.2</td>
<td>99.1</td>
<td>73.1</td>
<td>76.9</td>
<td>35.2</td>
<td>100.0</td>
<td>95.4</td>
<td>46.3</td>
<td>74.1</td>
<td>83.3</td>
<td>59.3</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_1</td>
<td>Wav_SVMs</td>
<td>Kun2017</td>
<td>64.2</td>
<td>61.1</td>
<td>44.4</td>
<td>72.2</td>
<td>68.5</td>
<td>76.9</td>
<td>83.3</td>
<td>48.1</td>
<td>64.8</td>
<td>28.7</td>
<td>92.6</td>
<td>90.7</td>
<td>39.8</td>
<td>56.5</td>
<td>75.9</td>
<td>59.3</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_2</td>
<td>Wav_GRUs</td>
<td>Kun2017</td>
<td>64.0</td>
<td>50.0</td>
<td>49.1</td>
<td>67.6</td>
<td>67.6</td>
<td>89.8</td>
<td>88.0</td>
<td>62.0</td>
<td>81.5</td>
<td>24.1</td>
<td>88.0</td>
<td>65.7</td>
<td>36.1</td>
<td>58.3</td>
<td>73.1</td>
<td>59.3</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_1</td>
<td>JKU_IVEC</td>
<td>Lehner2017</td>
<td>68.7</td>
<td>91.7</td>
<td>65.7</td>
<td>79.6</td>
<td>76.9</td>
<td>70.4</td>
<td>90.7</td>
<td>65.7</td>
<td>88.0</td>
<td>58.3</td>
<td>76.9</td>
<td>50.9</td>
<td>22.2</td>
<td>75.9</td>
<td>71.3</td>
<td>46.3</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_2</td>
<td>JKU_ALL_av</td>
<td>Lehner2017</td>
<td>66.8</td>
<td>57.4</td>
<td>64.8</td>
<td>73.1</td>
<td>80.6</td>
<td>91.7</td>
<td>88.9</td>
<td>79.6</td>
<td>77.8</td>
<td>35.2</td>
<td>64.8</td>
<td>71.3</td>
<td>36.1</td>
<td>38.0</td>
<td>83.3</td>
<td>59.3</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_3</td>
<td>JKU_CNN</td>
<td>Lehner2017</td>
<td>64.8</td>
<td>47.2</td>
<td>59.3</td>
<td>73.1</td>
<td>78.7</td>
<td>88.0</td>
<td>87.0</td>
<td>75.0</td>
<td>74.1</td>
<td>31.5</td>
<td>63.0</td>
<td>69.4</td>
<td>48.1</td>
<td>37.0</td>
<td>83.3</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_4</td>
<td>JKU_All_ca</td>
<td>Lehner2017</td>
<td>73.8</td>
<td>87.0</td>
<td>66.7</td>
<td>88.9</td>
<td>80.6</td>
<td>92.6</td>
<td>92.6</td>
<td>76.9</td>
<td>88.9</td>
<td>49.1</td>
<td>79.6</td>
<td>65.7</td>
<td>45.4</td>
<td>55.6</td>
<td>84.3</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_1</td>
<td>LiSCUTt1_1</td>
<td>Li2017</td>
<td>53.7</td>
<td>14.8</td>
<td>38.0</td>
<td>50.9</td>
<td>55.6</td>
<td>83.3</td>
<td>68.5</td>
<td>60.2</td>
<td>95.4</td>
<td>20.4</td>
<td>80.6</td>
<td>34.3</td>
<td>17.6</td>
<td>70.4</td>
<td>54.6</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_2</td>
<td>LiSCUTt1_2</td>
<td>Li2017</td>
<td>63.6</td>
<td>55.6</td>
<td>45.4</td>
<td>55.6</td>
<td>53.7</td>
<td>87.0</td>
<td>81.5</td>
<td>75.0</td>
<td>99.1</td>
<td>26.9</td>
<td>97.2</td>
<td>62.0</td>
<td>11.1</td>
<td>79.6</td>
<td>56.5</td>
<td>68.5</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_3</td>
<td>LiSCUTt1_3</td>
<td>Li2017</td>
<td>61.7</td>
<td>51.9</td>
<td>33.3</td>
<td>48.1</td>
<td>64.8</td>
<td>83.3</td>
<td>82.4</td>
<td>70.4</td>
<td>99.1</td>
<td>24.1</td>
<td>99.1</td>
<td>50.0</td>
<td>14.8</td>
<td>78.7</td>
<td>53.7</td>
<td>72.2</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_4</td>
<td>LiSCUTt1_4</td>
<td>Li2017</td>
<td>57.8</td>
<td>35.2</td>
<td>38.9</td>
<td>48.1</td>
<td>60.2</td>
<td>84.3</td>
<td>81.5</td>
<td>65.7</td>
<td>97.2</td>
<td>25.9</td>
<td>80.6</td>
<td>38.0</td>
<td>15.7</td>
<td>70.4</td>
<td>55.6</td>
<td>69.4</td>
</tr>
<tr>
<td></td>
<td>Maka_ZUT_task1_1</td>
<td>ASAWI</td>
<td>Maka2017</td>
<td>47.5</td>
<td>60.2</td>
<td>40.7</td>
<td>61.1</td>
<td>57.4</td>
<td>31.5</td>
<td>65.7</td>
<td>44.4</td>
<td>78.7</td>
<td>16.7</td>
<td>33.3</td>
<td>45.4</td>
<td>0.9</td>
<td>69.4</td>
<td>59.3</td>
<td>48.1</td>
</tr>
<tr>
<td></td>
<td>Mun_KU_task1_1</td>
<td>GAN_SKMUN</td>
<td>Mun2017</td>
<td>83.3</td>
<td>83.3</td>
<td>74.1</td>
<td>88.0</td>
<td>93.5</td>
<td>94.4</td>
<td>95.4</td>
<td>82.4</td>
<td>88.0</td>
<td>75.9</td>
<td>88.0</td>
<td>92.6</td>
<td>75.9</td>
<td>86.1</td>
<td>67.6</td>
<td>63.9</td>
</tr>
<tr>
<td></td>
<td>Park_ISPL_task1_1</td>
<td>ISPL</td>
<td>Park2017</td>
<td>72.6</td>
<td>54.6</td>
<td>59.3</td>
<td>71.3</td>
<td>79.6</td>
<td>91.7</td>
<td>85.2</td>
<td>75.0</td>
<td>98.1</td>
<td>44.4</td>
<td>98.1</td>
<td>84.3</td>
<td>23.1</td>
<td>76.9</td>
<td>82.4</td>
<td>64.8</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_1</td>
<td>CNN</td>
<td>Phan2017</td>
<td>59.0</td>
<td>38.9</td>
<td>48.1</td>
<td>61.1</td>
<td>82.4</td>
<td>60.2</td>
<td>80.6</td>
<td>65.7</td>
<td>73.1</td>
<td>38.9</td>
<td>85.2</td>
<td>34.3</td>
<td>32.4</td>
<td>58.3</td>
<td>71.3</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_2</td>
<td>ACNN</td>
<td>Phan2017</td>
<td>55.9</td>
<td>41.7</td>
<td>45.4</td>
<td>51.9</td>
<td>79.6</td>
<td>56.5</td>
<td>67.6</td>
<td>62.0</td>
<td>70.4</td>
<td>35.2</td>
<td>88.9</td>
<td>33.3</td>
<td>31.5</td>
<td>52.8</td>
<td>72.2</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_3</td>
<td>CNN+</td>
<td>Phan2017</td>
<td>58.3</td>
<td>41.7</td>
<td>44.4</td>
<td>68.5</td>
<td>74.1</td>
<td>57.4</td>
<td>94.4</td>
<td>66.7</td>
<td>66.7</td>
<td>27.8</td>
<td>68.5</td>
<td>76.9</td>
<td>21.3</td>
<td>40.7</td>
<td>71.3</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_4</td>
<td>ACNN+</td>
<td>Phan2017</td>
<td>58.0</td>
<td>53.7</td>
<td>47.2</td>
<td>64.8</td>
<td>75.0</td>
<td>59.3</td>
<td>91.7</td>
<td>61.1</td>
<td>70.4</td>
<td>28.7</td>
<td>75.9</td>
<td>69.4</td>
<td>14.8</td>
<td>34.3</td>
<td>68.5</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_1</td>
<td>amb200</td>
<td>Piczak2017</td>
<td>70.6</td>
<td>29.6</td>
<td>66.7</td>
<td>71.3</td>
<td>71.3</td>
<td>91.7</td>
<td>80.6</td>
<td>46.3</td>
<td>88.0</td>
<td>56.5</td>
<td>99.1</td>
<td>69.4</td>
<td>49.1</td>
<td>75.9</td>
<td>81.5</td>
<td>82.4</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_2</td>
<td>dishes</td>
<td>Piczak2017</td>
<td>69.6</td>
<td>32.4</td>
<td>63.9</td>
<td>65.7</td>
<td>77.8</td>
<td>91.7</td>
<td>84.3</td>
<td>49.1</td>
<td>76.9</td>
<td>67.6</td>
<td>99.1</td>
<td>56.5</td>
<td>56.5</td>
<td>67.6</td>
<td>82.4</td>
<td>72.2</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_3</td>
<td>amb100</td>
<td>Piczak2017</td>
<td>67.7</td>
<td>22.2</td>
<td>66.7</td>
<td>65.7</td>
<td>74.1</td>
<td>90.7</td>
<td>86.1</td>
<td>35.2</td>
<td>81.5</td>
<td>59.3</td>
<td>98.1</td>
<td>78.7</td>
<td>41.7</td>
<td>64.8</td>
<td>81.5</td>
<td>68.5</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_4</td>
<td>amb60</td>
<td>Piczak2017</td>
<td>62.0</td>
<td>19.4</td>
<td>63.9</td>
<td>51.9</td>
<td>65.7</td>
<td>89.8</td>
<td>88.9</td>
<td>21.3</td>
<td>67.6</td>
<td>43.5</td>
<td>92.6</td>
<td>81.5</td>
<td>43.5</td>
<td>73.1</td>
<td>63.9</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_1</td>
<td>HBGS CNN</td>
<td>Rakotomamonjy2017</td>
<td>61.5</td>
<td>9.3</td>
<td>74.1</td>
<td>41.7</td>
<td>83.3</td>
<td>84.3</td>
<td>87.0</td>
<td>64.8</td>
<td>96.3</td>
<td>40.7</td>
<td>87.0</td>
<td>26.9</td>
<td>37.0</td>
<td>50.9</td>
<td>81.5</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_2</td>
<td>HBGS CNN-4</td>
<td>Rakotomamonjy2017</td>
<td>62.7</td>
<td>6.5</td>
<td>77.8</td>
<td>47.2</td>
<td>82.4</td>
<td>88.9</td>
<td>87.0</td>
<td>68.5</td>
<td>92.6</td>
<td>38.0</td>
<td>95.4</td>
<td>35.2</td>
<td>33.3</td>
<td>48.1</td>
<td>85.2</td>
<td>53.7</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_3</td>
<td>HBGS CNN-19</td>
<td>Rakotomamonjy2017</td>
<td>62.8</td>
<td>5.6</td>
<td>78.7</td>
<td>48.1</td>
<td>83.3</td>
<td>88.9</td>
<td>84.3</td>
<td>65.7</td>
<td>93.5</td>
<td>38.9</td>
<td>93.5</td>
<td>40.7</td>
<td>29.6</td>
<td>49.1</td>
<td>87.0</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_1</td>
<td>multires</td>
<td>Schindler2017</td>
<td>61.7</td>
<td>47.2</td>
<td>55.6</td>
<td>65.7</td>
<td>69.4</td>
<td>98.1</td>
<td>87.0</td>
<td>46.3</td>
<td>74.1</td>
<td>18.5</td>
<td>47.2</td>
<td>71.3</td>
<td>55.6</td>
<td>74.1</td>
<td>82.4</td>
<td>33.3</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_2</td>
<td>multires-p</td>
<td>Schindler2017</td>
<td>61.7</td>
<td>56.5</td>
<td>56.5</td>
<td>62.0</td>
<td>66.7</td>
<td>99.1</td>
<td>91.7</td>
<td>45.4</td>
<td>75.9</td>
<td>25.0</td>
<td>37.0</td>
<td>79.6</td>
<td>40.7</td>
<td>63.0</td>
<td>88.9</td>
<td>38.0</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_1</td>
<td>CERTH_1</td>
<td>Vafeiadis2017</td>
<td>61.0</td>
<td>23.1</td>
<td>42.6</td>
<td>58.3</td>
<td>66.7</td>
<td>77.8</td>
<td>86.1</td>
<td>64.8</td>
<td>94.4</td>
<td>39.8</td>
<td>92.6</td>
<td>54.6</td>
<td>20.4</td>
<td>72.2</td>
<td>81.5</td>
<td>39.8</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_2</td>
<td>CERTH_2</td>
<td>Vafeiadis2017</td>
<td>49.5</td>
<td>35.2</td>
<td>23.1</td>
<td>58.3</td>
<td>63.0</td>
<td>90.7</td>
<td>90.7</td>
<td>57.4</td>
<td>61.1</td>
<td>20.4</td>
<td>38.0</td>
<td>53.7</td>
<td>25.9</td>
<td>45.4</td>
<td>59.3</td>
<td>20.4</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_1</td>
<td>Vij_UIET_1</td>
<td>Vij2017</td>
<td>61.2</td>
<td>22.2</td>
<td>39.8</td>
<td>43.5</td>
<td>73.1</td>
<td>77.8</td>
<td>90.7</td>
<td>64.8</td>
<td>83.3</td>
<td>43.5</td>
<td>95.4</td>
<td>52.8</td>
<td>28.7</td>
<td>77.8</td>
<td>59.3</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_2</td>
<td>Vij_UIET_2</td>
<td>Vij2017</td>
<td>57.5</td>
<td>21.3</td>
<td>32.4</td>
<td>36.1</td>
<td>64.8</td>
<td>73.1</td>
<td>79.6</td>
<td>50.9</td>
<td>71.3</td>
<td>35.2</td>
<td>99.1</td>
<td>66.7</td>
<td>30.6</td>
<td>83.3</td>
<td>54.6</td>
<td>63.9</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_3</td>
<td>Vij_UIET_3</td>
<td>Vij2017</td>
<td>59.6</td>
<td>10.2</td>
<td>42.6</td>
<td>36.1</td>
<td>53.7</td>
<td>75.0</td>
<td>79.6</td>
<td>54.6</td>
<td>88.0</td>
<td>48.1</td>
<td>98.1</td>
<td>57.4</td>
<td>39.8</td>
<td>88.0</td>
<td>58.3</td>
<td>63.9</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_4</td>
<td>Vij_UIET_4</td>
<td>Vij2017</td>
<td>65.0</td>
<td>16.7</td>
<td>38.9</td>
<td>65.7</td>
<td>74.1</td>
<td>84.3</td>
<td>98.1</td>
<td>64.8</td>
<td>85.2</td>
<td>40.7</td>
<td>98.1</td>
<td>84.3</td>
<td>25.9</td>
<td>69.4</td>
<td>70.4</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_1</td>
<td>IITKGP_ABSP_Fusion</td>
<td>Waldekar2017</td>
<td>67.0</td>
<td>13.9</td>
<td>61.1</td>
<td>76.9</td>
<td>70.4</td>
<td>86.1</td>
<td>90.7</td>
<td>63.0</td>
<td>85.2</td>
<td>49.1</td>
<td>98.1</td>
<td>81.5</td>
<td>19.4</td>
<td>80.6</td>
<td>73.1</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_2</td>
<td>IITKGP_ABSP_Hierarchical</td>
<td>Waldekar2017</td>
<td>64.9</td>
<td>15.7</td>
<td>58.3</td>
<td>78.7</td>
<td>63.9</td>
<td>82.4</td>
<td>84.3</td>
<td>63.0</td>
<td>88.0</td>
<td>50.0</td>
<td>97.2</td>
<td>84.3</td>
<td>15.7</td>
<td>70.4</td>
<td>70.4</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_1</td>
<td>DCNN_vote</td>
<td>Weiping2017</td>
<td>74.8</td>
<td>77.8</td>
<td>88.0</td>
<td>71.3</td>
<td>81.5</td>
<td>78.7</td>
<td>73.1</td>
<td>76.9</td>
<td>67.6</td>
<td>49.1</td>
<td>95.4</td>
<td>82.4</td>
<td>57.4</td>
<td>73.1</td>
<td>88.0</td>
<td>61.1</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_2</td>
<td>DCNN_SVM</td>
<td>Weiping2017</td>
<td>77.7</td>
<td>71.3</td>
<td>84.3</td>
<td>79.6</td>
<td>85.2</td>
<td>82.4</td>
<td>78.7</td>
<td>80.6</td>
<td>73.1</td>
<td>59.3</td>
<td>97.2</td>
<td>81.5</td>
<td>57.4</td>
<td>85.2</td>
<td>92.6</td>
<td>57.4</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_1</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>68.5</td>
<td>27.8</td>
<td>43.5</td>
<td>70.4</td>
<td>84.3</td>
<td>88.0</td>
<td>96.3</td>
<td>66.7</td>
<td>91.7</td>
<td>40.7</td>
<td>100.0</td>
<td>85.2</td>
<td>13.9</td>
<td>82.4</td>
<td>72.2</td>
<td>63.9</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_2</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>67.5</td>
<td>26.9</td>
<td>43.5</td>
<td>68.5</td>
<td>85.2</td>
<td>88.0</td>
<td>94.4</td>
<td>66.7</td>
<td>86.1</td>
<td>42.6</td>
<td>100.0</td>
<td>85.2</td>
<td>11.1</td>
<td>82.4</td>
<td>72.2</td>
<td>60.2</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_1</td>
<td>autolog1</td>
<td>Xu2017a</td>
<td>65.9</td>
<td>29.6</td>
<td>42.6</td>
<td>58.3</td>
<td>80.6</td>
<td>79.6</td>
<td>98.1</td>
<td>67.6</td>
<td>51.9</td>
<td>53.7</td>
<td>100.0</td>
<td>90.7</td>
<td>32.4</td>
<td>70.4</td>
<td>75.0</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_2</td>
<td>autolog2</td>
<td>Xu2017a</td>
<td>66.7</td>
<td>28.7</td>
<td>32.4</td>
<td>59.3</td>
<td>84.3</td>
<td>77.8</td>
<td>99.1</td>
<td>69.4</td>
<td>50.0</td>
<td>36.1</td>
<td>100.0</td>
<td>99.1</td>
<td>38.9</td>
<td>72.2</td>
<td>74.1</td>
<td>79.6</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_3</td>
<td>autolog3</td>
<td>Xu2017a</td>
<td>64.6</td>
<td>25.0</td>
<td>37.0</td>
<td>60.2</td>
<td>84.3</td>
<td>74.1</td>
<td>98.1</td>
<td>64.8</td>
<td>43.5</td>
<td>33.3</td>
<td>100.0</td>
<td>94.4</td>
<td>25.0</td>
<td>68.5</td>
<td>84.3</td>
<td>76.9</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_1</td>
<td>MFS</td>
<td>Lu2017</td>
<td>61.5</td>
<td>10.2</td>
<td>55.6</td>
<td>52.8</td>
<td>76.9</td>
<td>79.6</td>
<td>94.4</td>
<td>50.0</td>
<td>79.6</td>
<td>30.6</td>
<td>94.4</td>
<td>55.6</td>
<td>33.3</td>
<td>68.5</td>
<td>75.9</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_2</td>
<td>STD</td>
<td>Lu2017</td>
<td>65.2</td>
<td>45.4</td>
<td>47.2</td>
<td>57.4</td>
<td>74.1</td>
<td>86.1</td>
<td>88.0</td>
<td>55.6</td>
<td>75.0</td>
<td>49.1</td>
<td>98.1</td>
<td>68.5</td>
<td>29.6</td>
<td>66.7</td>
<td>75.0</td>
<td>63.0</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_3</td>
<td>MFS+STD</td>
<td>Lu2017</td>
<td>62.8</td>
<td>53.7</td>
<td>42.6</td>
<td>54.6</td>
<td>78.7</td>
<td>88.9</td>
<td>88.9</td>
<td>61.1</td>
<td>75.9</td>
<td>47.2</td>
<td>90.7</td>
<td>48.1</td>
<td>15.7</td>
<td>61.1</td>
<td>71.3</td>
<td>63.9</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_4</td>
<td>Pre-training</td>
<td>Lu2017</td>
<td>63.6</td>
<td>42.6</td>
<td>45.4</td>
<td>57.4</td>
<td>71.3</td>
<td>97.2</td>
<td>89.8</td>
<td>51.9</td>
<td>81.5</td>
<td>38.0</td>
<td>99.1</td>
<td>62.0</td>
<td>20.4</td>
<td>67.6</td>
<td>70.4</td>
<td>60.2</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_1</td>
<td>UOS_DualIn</td>
<td>Jee-Weon2017</td>
<td>67.0</td>
<td>53.7</td>
<td>57.4</td>
<td>53.7</td>
<td>73.1</td>
<td>76.9</td>
<td>82.4</td>
<td>65.7</td>
<td>94.4</td>
<td>42.6</td>
<td>99.1</td>
<td>75.0</td>
<td>29.6</td>
<td>79.6</td>
<td>69.4</td>
<td>52.8</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_2</td>
<td>UOS_BalCos</td>
<td>Jee-Weon2017</td>
<td>66.2</td>
<td>55.6</td>
<td>57.4</td>
<td>47.2</td>
<td>72.2</td>
<td>75.9</td>
<td>83.3</td>
<td>65.7</td>
<td>92.6</td>
<td>43.5</td>
<td>99.1</td>
<td>75.0</td>
<td>27.8</td>
<td>77.8</td>
<td>69.4</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_3</td>
<td>UOS_DatDup</td>
<td>Jee-Weon2017</td>
<td>67.3</td>
<td>60.2</td>
<td>58.3</td>
<td>56.5</td>
<td>69.4</td>
<td>76.9</td>
<td>84.3</td>
<td>68.5</td>
<td>90.7</td>
<td>46.3</td>
<td>94.4</td>
<td>72.2</td>
<td>28.7</td>
<td>79.6</td>
<td>72.2</td>
<td>51.9</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_4</td>
<td>UOS_res</td>
<td>Jee-Weon2017</td>
<td>70.6</td>
<td>72.2</td>
<td>51.9</td>
<td>68.5</td>
<td>76.9</td>
<td>77.8</td>
<td>86.1</td>
<td>74.1</td>
<td>93.5</td>
<td>38.9</td>
<td>95.4</td>
<td>77.8</td>
<td>34.3</td>
<td>84.3</td>
<td>68.5</td>
<td>58.3</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_1</td>
<td>MResNet-34</td>
<td>Zhao2017</td>
<td>70.0</td>
<td>41.7</td>
<td>69.4</td>
<td>69.4</td>
<td>93.5</td>
<td>63.9</td>
<td>98.1</td>
<td>71.3</td>
<td>79.6</td>
<td>32.4</td>
<td>100.0</td>
<td>81.5</td>
<td>37.0</td>
<td>84.3</td>
<td>68.5</td>
<td>59.3</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_2</td>
<td>Conv</td>
<td>Zhao2017</td>
<td>67.9</td>
<td>13.0</td>
<td>55.6</td>
<td>67.6</td>
<td>95.4</td>
<td>70.4</td>
<td>100.0</td>
<td>73.1</td>
<td>90.7</td>
<td>45.4</td>
<td>99.1</td>
<td>83.3</td>
<td>20.4</td>
<td>69.4</td>
<td>80.6</td>
<td>54.6</td>
</tr>
<tr>
<td></td>
<td>Zhao_UAU_UP_task1_1</td>
<td>GRNN</td>
<td>Zhao2017a</td>
<td>63.8</td>
<td>47.2</td>
<td>46.3</td>
<td>70.4</td>
<td>66.7</td>
<td>77.8</td>
<td>88.9</td>
<td>65.7</td>
<td>85.2</td>
<td>28.7</td>
<td>86.1</td>
<td>70.4</td>
<td>38.0</td>
<td>56.5</td>
<td>74.1</td>
<td>55.6</td>
</tr>
</tbody>
</table>
<h2 id="system-characteristics">System characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-fields="code:Submission&lt;br&gt;&lt;/br&gt;code:str:visible;sortable;sep-left,
                 name:Submission&lt;br&gt;&lt;/br&gt;name:str:visible;sortable;small,
                 anchor:Tech.&lt;br&gt;&lt;/br&gt;Report:anchor:visible;sep-left;text-center,
                 accuracy_eval:Accuracy &lt;br&gt;&lt;/br&gt;(Eval):float:visible;sortable;chartable;sep-left;text-center:percentage,
                 system_input:Input:tag:visible;sortable;filterable;sep-left,
                 system_features:Features:tag:visible;sortable;filterable,
                 system_classifier:Classifier:tag:visible;sortable;filterable" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="accuracy_eval" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_input" data-filter-control="select" data-sortable="true" data-tag="true">
                Input
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-sortable="true" data-tag="true">
                Features
            </th>
<th class="text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Abrol_IITM_task1_1</td>
<td>Baseline</td>
<td>Abrol2017</td>
<td>65.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>GMM, Archetypal Analysis, SVM</td>
<td>majority vote on audio segments of a file</td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_1</td>
<td>S2S-AE</td>
<td>Amiriparian2017</td>
<td>67.5</td>
<td>mixed</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>MLP</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Amiriparian_AU_task1_2</td>
<td>Shahin_APTI</td>
<td>Amiriparian2017a</td>
<td>59.1</td>
<td>mixed</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>MLP+SVM</td>
<td>weighted late fusion</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_1</td>
<td>Biho1</td>
<td>Kim2017</td>
<td>56.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Biho_Sogang_task1_2</td>
<td>Biho2</td>
<td>Kim2017</td>
<td>60.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_1</td>
<td>TPT1</td>
<td>Bisot2017</td>
<td>69.8</td>
<td>left, right</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>NMF, MLP</td>
<td>average log-probability</td>
</tr>
<tr>
<td></td>
<td>Bisot_TPT_task1_2</td>
<td>TPT2</td>
<td>Bisot2017</td>
<td>69.6</td>
<td>left, right</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>NMF</td>
<td>average log-probability</td>
</tr>
<tr>
<td></td>
<td>Chandrasekhar_IIITH_task1_1</td>
<td></td>
<td>Chandrasekhar2017</td>
<td>45.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, Inverse Melfrequency cepstral coefficients</td>
<td>DNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_1</td>
<td>TP_CNN_cv1</td>
<td>Chou2017</td>
<td>57.1</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_2</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>61.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_3</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>59.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Chou_SINICA_task1_4</td>
<td>SINICA</td>
<td>Chou2017</td>
<td>57.1</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>spectrogram</td>
<td>ensemble</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_1</td>
<td>andang1</td>
<td>Dang2017</td>
<td>62.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC</td>
<td>CRNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_2</td>
<td>andang1</td>
<td>Dang2017</td>
<td>62.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Dang_NCU_task1_3</td>
<td>andang1</td>
<td>Dang2017</td>
<td>63.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, MFCC</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_1</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>57.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_2</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>59.9</td>
<td>mono</td>
<td>16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_3</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>64.1</td>
<td>mono</td>
<td>16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Duppada_Seernet_task1_4</td>
<td>Seernet</td>
<td>Duppada2017</td>
<td>63.0</td>
<td>mono</td>
<td>44.1kHz, 16kHz</td>
<td></td>
<td>log-mel spectrogram</td>
<td>CNN, ensemble</td>
<td>mean</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_1</td>
<td>MLPFeats</td>
<td>Foleiss2017</td>
<td>64.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>STFT</td>
<td>MLP</td>
<td>probability sum</td>
</tr>
<tr>
<td></td>
<td>Foleiss_UTFPR_task1_2</td>
<td>MLPFeatRF</td>
<td>Foleiss2017</td>
<td>66.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>STFT</td>
<td>MLP, random forest</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Fonseca_MTG_task1_1</td>
<td>MTG</td>
<td>Fonseca2017</td>
<td>67.3</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>various</td>
<td>ensemble</td>
<td>max of average score</td>
</tr>
<tr>
<td></td>
<td>Fraile_UPM_task1_1</td>
<td>GAMMA-UPM</td>
<td>Fraile2017</td>
<td>58.3</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>modulation spectrum</td>
<td>MLP</td>
<td>a posteriori probablity</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_1</td>
<td>MTG_GBMVGG</td>
<td>Gong2017</td>
<td>61.2</td>
<td>multichannel</td>
<td>44.1kHz</td>
<td></td>
<td>various</td>
<td>GBM CNN fusion</td>
<td>maximum</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_2</td>
<td>MTG_GBM</td>
<td>Gong2017</td>
<td>61.5</td>
<td>multichannel</td>
<td>44.1kHz</td>
<td></td>
<td>various</td>
<td>GBM fusion</td>
<td>maximum</td>
</tr>
<tr>
<td></td>
<td>Gong_MTG_task1_3</td>
<td>MTG_VGG</td>
<td>Gong2017</td>
<td>61.9</td>
<td>multichannel</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN fusion</td>
<td>maximum</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_1</td>
<td>4fEnsemSel</td>
<td>Han2017</td>
<td>79.9</td>
<td>mono, binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN, ensemble</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_2</td>
<td>4fMeanAll</td>
<td>Han2017</td>
<td>79.6</td>
<td>mono, binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN, ensemble</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_3</td>
<td>FlEnsemSel</td>
<td>Han2017</td>
<td>80.4</td>
<td>mono, binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN, ensemble</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Han_COCAI_task1_4</td>
<td>flMeanAll</td>
<td>Han2017</td>
<td>80.3</td>
<td>mono, binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN, ensemble</td>
<td>mean probability</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_1</td>
<td>BUETBOSCH1</td>
<td>Hyder2017</td>
<td>74.1</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies</td>
<td>GMM-SV, CNN-SV, Multiband CNN-SV</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_2</td>
<td>BUETBOSCH2</td>
<td>Hyder2017</td>
<td>72.2</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN-SV</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_3</td>
<td>BUETBOSCH3</td>
<td>Hyder2017</td>
<td>68.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies</td>
<td>GMM-SV, CNN-SV, Multiband CNN-SV, CNN, Multiband CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Hasan_BUET_task1_4</td>
<td>BUETBOSCH4</td>
<td>Hyder2017</td>
<td>72.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, log-mel energies, different functionals of various spectral and prosodic features</td>
<td>GMM-SV, CNN-SV, Multiband CNN-SV, CNN, Multiband CNN, DNN</td>
<td>majority vote</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2017 baseline</td>
<td>Baseline</td>
<td>Heittola2017</td>
<td>61.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>MLP</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_1</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, CQT</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Huang_THU_task1_2</td>
<td>wjhta</td>
<td>Huang2017</td>
<td>65.4</td>
<td>mono</td>
<td>44.1kHz</td>
<td>pitch shifting</td>
<td>MFCC, CQT</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_1</td>
<td></td>
<td>Hussain2017</td>
<td>56.7</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_2</td>
<td></td>
<td>Hussain2017</td>
<td>59.5</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>DNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_3</td>
<td></td>
<td>Hussain2017</td>
<td>59.9</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>DNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Hussain_NUCES_task1_4</td>
<td></td>
<td>Hussain2017</td>
<td>55.4</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_1</td>
<td>CRNN-1</td>
<td>Jallet2017</td>
<td>60.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRNN</td>
<td>maximum</td>
</tr>
<tr>
<td></td>
<td>Jallet_TUT_task1_2</td>
<td>CRNN-2</td>
<td>Jallet2017</td>
<td>61.2</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Jimenez_CMU_task1_1</td>
<td>LapKernel</td>
<td>Jimenez2017</td>
<td>59.9</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>emo_conf (opensmile)</td>
<td>SVM</td>
<td>highest score</td>
</tr>
<tr>
<td></td>
<td>Kukanov_UEF_task1_1</td>
<td>K-CRNN</td>
<td>Kukanov2017</td>
<td>71.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CRNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_1</td>
<td>Wav_SVMs</td>
<td>Kun2017</td>
<td>64.2</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>wavelets, ComParE (openSMILE)</td>
<td>SVM</td>
<td>margin sampling value</td>
</tr>
<tr>
<td></td>
<td>Kun_TUM_UAU_UP_task1_2</td>
<td>Wav_GRUs</td>
<td>Kun2017</td>
<td>64.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>wavelets, ComParE (openSMILE)</td>
<td>GRNN</td>
<td>margin sampling value</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_1</td>
<td>JKU_IVEC</td>
<td>Lehner2017</td>
<td>68.7</td>
<td>binaural</td>
<td>22.05kHz</td>
<td>pitch shifting</td>
<td>MFCC based i-vectors</td>
<td>i-vector</td>
<td>min. cosine distance</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_2</td>
<td>JKU_ALL_av</td>
<td>Lehner2017</td>
<td>66.8</td>
<td>mono, binaural</td>
<td>22.05kHz</td>
<td>pitch shifting</td>
<td>MFCC, log-scaled spectrogram</td>
<td>CNN, i-vector, ensemble</td>
<td>model averaging</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_3</td>
<td>JKU_CNN</td>
<td>Lehner2017</td>
<td>64.8</td>
<td>mono</td>
<td>22.05kHz</td>
<td></td>
<td>log-scaled spectrogram</td>
<td>CNN, ensemble</td>
<td>fusion w/ logistic linear regression</td>
</tr>
<tr>
<td></td>
<td>Lehner_JKU_task1_4</td>
<td>JKU_All_ca</td>
<td>Lehner2017</td>
<td>73.8</td>
<td>mono, binaural</td>
<td>22.05kHz</td>
<td>pitch shifting</td>
<td>mel-scaled spectrograms, i-vectors</td>
<td>i-vector, CNN, ensemble</td>
<td>fusion w/ logistic linear regression</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_1</td>
<td>LiSCUTt1_1</td>
<td>Li2017</td>
<td>53.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>DNN(MFCC)</td>
<td>Bi-LSTM</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_2</td>
<td>LiSCUTt1_2</td>
<td>Li2017</td>
<td>63.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>DNN(MFCC)</td>
<td>Bi-LSTM</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_3</td>
<td>LiSCUTt1_3</td>
<td>Li2017</td>
<td>61.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>DNN(MFCC)</td>
<td>DNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Li_SCUT_task1_4</td>
<td>LiSCUTt1_4</td>
<td>Li2017</td>
<td>57.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>DNN(MFCC)</td>
<td>Bi-LSTM</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Maka_ZUT_task1_1</td>
<td>ASAWI</td>
<td>Maka2017</td>
<td>47.5</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>cochleagram, onset map, binaural cues, low-level feature contours</td>
<td>random forest</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Mun_KU_task1_1</td>
<td>GAN_SKMUN</td>
<td>Mun2017</td>
<td>83.3</td>
<td>left, right, mixed</td>
<td>22.05kHz</td>
<td>GAN</td>
<td>log-mel energies, spectrogram</td>
<td>MLP, RNN, CNN, SVM</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Park_ISPL_task1_1</td>
<td>ISPL</td>
<td>Park2017</td>
<td>72.6</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>block mixing</td>
<td>covariance of gammachirp energies, double FFT of gammachirp energies</td>
<td>CNN</td>
<td>maximum posterior</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_1</td>
<td>CNN</td>
<td>Phan2017</td>
<td>59.0</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>cross-validation with different data splits</td>
<td>generalized label tree embedding</td>
<td>CNN</td>
<td>entire-signal classification</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_2</td>
<td>ACNN</td>
<td>Phan2017</td>
<td>55.9</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>cross-validation with different data splits</td>
<td>generalized label tree embedding</td>
<td>Attentive CNN</td>
<td>entire-signal classification</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_3</td>
<td>CNN+</td>
<td>Phan2017</td>
<td>58.3</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>cross-validation with different data splits</td>
<td>generalized label tree embedding</td>
<td>CNN</td>
<td>entire-signal classification</td>
</tr>
<tr>
<td></td>
<td>Phan_UniLuebeck_task1_4</td>
<td>ACNN+</td>
<td>Phan2017</td>
<td>58.0</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>cross-validation with different data splits</td>
<td>generalized label tree embedding</td>
<td>Attentive CNN</td>
<td>entire-signal classification</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_1</td>
<td>amb200</td>
<td>Piczak2017</td>
<td>70.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td>time delay, block mixing</td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_2</td>
<td>dishes</td>
<td>Piczak2017</td>
<td>69.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td>time delay, block mixing</td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_3</td>
<td>amb100</td>
<td>Piczak2017</td>
<td>67.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td>time delay, block mixing</td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Piczak_WUT_task1_4</td>
<td>amb60</td>
<td>Piczak2017</td>
<td>62.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td>time delay, block mixing</td>
<td>spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_1</td>
<td>HBGS CNN</td>
<td>Rakotomamonjy2017</td>
<td>61.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>CNN</td>
<td>average prediction</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_2</td>
<td>HBGS CNN-4</td>
<td>Rakotomamonjy2017</td>
<td>62.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>CNN</td>
<td>average prediction over 4  models</td>
</tr>
<tr>
<td></td>
<td>Rakotomamonjy_UROUEN_task1_3</td>
<td>HBGS CNN-19</td>
<td>Rakotomamonjy2017</td>
<td>62.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>CNN</td>
<td>average prediction over 19 models</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_1</td>
<td>multires</td>
<td>Schindler2017</td>
<td>61.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td>time stretching, block mixing, pitch shifting, mixing files of same class, gaussian noise</td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>argmax of average softmax response per file</td>
</tr>
<tr>
<td></td>
<td>Schindler_AIT_task1_2</td>
<td>multires-p</td>
<td>Schindler2017</td>
<td>61.7</td>
<td>mono</td>
<td>44.1kHz</td>
<td>time stretching, block mixing, pitch shifting, mixing files of same class, gaussian noise</td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>argmax of average softmax response per file</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_1</td>
<td>CERTH_1</td>
<td>Vafeiadis2017</td>
<td>61.0</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>MFCC, MFCC delta, MFCC acceleration, centroid, rolloff, ZCR</td>
<td>SVM-HMM</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Vafeiadis_CERTH_task1_2</td>
<td>CERTH_2</td>
<td>Vafeiadis2017</td>
<td>49.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td>speed and pitch change (downsampling), amplitude change (dynamic), gaussian noise</td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_1</td>
<td>Vij_UIET_1</td>
<td>Vij2017</td>
<td>61.2</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>feature frame concatenation</td>
<td>log mel-filter bank</td>
<td>RNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_2</td>
<td>Vij_UIET_2</td>
<td>Vij2017</td>
<td>57.5</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>feature frame concatenation</td>
<td>log mel-filter bank</td>
<td>LSTM</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_3</td>
<td>Vij_UIET_3</td>
<td>Vij2017</td>
<td>59.6</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>feature frame concatenation</td>
<td>log mel-filter bank</td>
<td>GRU</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Vij_UIET_task1_4</td>
<td>Vij_UIET_4</td>
<td>Vij2017</td>
<td>65.0</td>
<td>binaural</td>
<td>44.1kHz</td>
<td>feature frame concatenation</td>
<td>log mel-filter bank</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_1</td>
<td>IITKGP_ABSP_Fusion</td>
<td>Waldekar2017</td>
<td>67.0</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>combination [block-based MFCC; SCFC; CQCC]</td>
<td>SVM</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Waldekar_IITKGP_task1_2</td>
<td>IITKGP_ABSP_Hierarchical</td>
<td>Waldekar2017</td>
<td>64.9</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>combination [block-based MFCC; SCFC; CQCC]</td>
<td>SVM</td>
<td>fusion</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_1</td>
<td>DCNN_vote</td>
<td>Weiping2017</td>
<td>74.8</td>
<td>binaural</td>
<td>22.05kHz</td>
<td></td>
<td>spectrogram, CQT</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Xing_SCNU_task1_2</td>
<td>DCNN_SVM</td>
<td>Weiping2017</td>
<td>77.7</td>
<td>binaural</td>
<td>22.05kHz</td>
<td></td>
<td>spectrogram, CQT</td>
<td>CNN</td>
<td>SVM</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_1</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>68.5</td>
<td>left, right, mixed</td>
<td>44.1kHz</td>
<td>pitch shifting</td>
<td>MFCC, spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Xu_NUDT_task1_2</td>
<td>XuCnnMFCC</td>
<td>Xu2017</td>
<td>67.5</td>
<td>left, right, mixed</td>
<td>44.1kHz</td>
<td>pitch shifting</td>
<td>MFCC, spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_1</td>
<td>autolog1</td>
<td>Xu2017a</td>
<td>65.9</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>Autoencoder and Logistic Regression</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_2</td>
<td>autolog2</td>
<td>Xu2017a</td>
<td>66.7</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>Autoencoder and Logistic Regression</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Xu_PKU_task1_3</td>
<td>autolog3</td>
<td>Xu2017a</td>
<td>64.6</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>CQT</td>
<td>Autoencoder and Logistic Regression</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_1</td>
<td>MFS</td>
<td>Lu2017</td>
<td>61.5</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>logsum</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_2</td>
<td>STD</td>
<td>Lu2017</td>
<td>65.2</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>logsum</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_3</td>
<td>MFS+STD</td>
<td>Lu2017</td>
<td>62.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>logsum</td>
</tr>
<tr>
<td></td>
<td>Yang_WHU_TASK1_4</td>
<td>Pre-training</td>
<td>Lu2017</td>
<td>63.6</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td>CNN</td>
<td>logsum</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_1</td>
<td>UOS_DualIn</td>
<td>Jee-Weon2017</td>
<td>67.0</td>
<td>left, right, mixed</td>
<td>44.1kHz</td>
<td></td>
<td>mel-filterbank features</td>
<td>MLP, ensemble</td>
<td>score sum</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_2</td>
<td>UOS_BalCos</td>
<td>Jee-Weon2017</td>
<td>66.2</td>
<td>left, right, mixed</td>
<td>44.1kHz</td>
<td></td>
<td>mel-filterbank features</td>
<td>MLP, ensemble</td>
<td>score sum</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_3</td>
<td>UOS_DatDup</td>
<td>Jee-Weon2017</td>
<td>67.3</td>
<td>left, right, mixed</td>
<td>44.1kHz</td>
<td>stochastic duplication</td>
<td>mel-filterbank features</td>
<td>MLP, ensemble</td>
<td>score sum</td>
</tr>
<tr>
<td></td>
<td>Yu_UOS_task1_4</td>
<td>UOS_res</td>
<td>Jee-Weon2017</td>
<td>70.6</td>
<td>left, right, mixed</td>
<td>44.1kHz</td>
<td>stochastic duplication</td>
<td>mel-filterbank features</td>
<td>MLP, ensemble</td>
<td>score sum</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_1</td>
<td>MResNet-34</td>
<td>Zhao2017</td>
<td>70.0</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Zhao_ADSC_task1_2</td>
<td>Conv</td>
<td>Zhao2017</td>
<td>67.9</td>
<td>binaural</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel spectrogram</td>
<td>CNN</td>
<td>majority vote</td>
</tr>
<tr>
<td></td>
<td>Zhao_UAU_UP_task1_1</td>
<td>GRNN</td>
<td>Zhao2017a</td>
<td>63.8</td>
<td>mono</td>
<td>44.1kHz</td>
<td></td>
<td>spectrogram, scalogram, wavelets, ComParE (openSMILE)</td>
<td>GRNN</td>
<td>margin sampling value</td>
</tr>
</tbody>
</table>
<h2 id="technical-reports">Technical reports</h2>
<div class="btex" data-source="content/data/challenge2017/technical_reports_task1.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Abrol2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Abrol2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        GMM-AA System for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Vinayak Abrol, Pulkit Sharma and Anshul Thakur
       </p>
<p style="text-align:left">
<em>
         Multimedia Analytics and Systems Lab, SCEE, Indian Institute of Technology Mandi, Mandi, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Abrol_IITM_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Abrol2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Abrol2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Abrol2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Abrol_115.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Abrol2017" class="panel-collapse collapse" id="collapse-Abrol2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       GMM-AA System for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this submission we propose to use Gaussian mixture modelling and Archetypal Analysis based system for DCASE17 acoustic scene classification task. We propose a feature learning approach via decomposing time-frequency (TF) representations with Archetypal Analysis (AA). In order to process large number of TF frames and capture the variations efficiently, firstly a class-specific GMM is build on frames of TF representations, followed by AA on GMM means to build class specific local dictionaries. Next, the TF representations are projected on the concatenated AA dictionary to get the non-negative sparse activations. Finally, the TF frames are reconstructed back using the computed activation vectors, and are then used to train a SVM classifier. The proposed method significantly outperforms the baseline system.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM, Archetypal Analysis, SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote on audio segments of a file
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Abrol2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Abrol_115.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Abrol2017label" class="modal fade" id="bibtex-Abrol2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAbrol2017label">
        GMM-AA System for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Abrol2017,
    Author = "Abrol, Vinayak and Sharma, Pulkit and Thakur, Anshul",
    title = "{GMM}-{AA} System for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this submission we propose to use Gaussian mixture modelling and Archetypal Analysis based system for DCASE17 acoustic scene classification task. We propose a feature learning approach via decomposing time-frequency (TF) representations with Archetypal Analysis (AA). In order to process large number of TF frames and capture the variations efficiently, firstly a class-specific GMM is build on frames of TF representations, followed by AA on GMM means to build class specific local dictionaries. Next, the TF representations are projected on the concatenated AA dictionary to get the non-negative sparse activations. Finally, the TF frames are reconstructed back using the computed activation vectors, and are then used to train a SVM classifier. The proposed method significantly outperforms the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Amiriparian2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Amiriparian2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Sequence to Sequence Autoencoders for Unsupervised Representation Learning From Audio
       </h4>
<p style="text-align:left">
        Shahin Amiriparian<sup>1,2,3</sup>, Michael Freitag<sup>1</sup>, Nicholas Cummins<sup>1,2</sup> and Björn Schuller<sup>2,4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Chair of Complex &amp; Intelligent Systems, Universität Passau, Passau, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care, Augsburg University, Augsburg, Germany, <sup>3</sup>Machine Intelligence &amp; Signal Processing Group, Technische Universität München, München, Germany, <sup>4</sup>Group of Language, Audio &amp; Music, Imperial Collage London, London, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Amiriparian_AU_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Amiriparian2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Amiriparian2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Amiriparian2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Amiriparian_173.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Amiriparian2017').collapse('show');window.location.hash='#Amiriparian2017';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Amiriparian2017" class="panel-collapse collapse" id="collapse-Amiriparian2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Sequence to Sequence Autoencoders for Unsupervised Representation Learning From Audio
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper describes our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). We propose a system for this task using a recurrent sequence to sequence autoencoder for unsupervised representation learning from raw audio files. First, we extract mel-spectrograms from the raw audio files. Second, we train a recurrent sequence to sequence autoencoder on these spectrograms, that are considered as time-dependent frequency vectors. Then, we extract, from a fully connected layer between the decoder and encoder units, the learnt representations of spectrograms as the feature vectors for the corresponding audio instances. Finally, we train a multilayer perceptron neural network on these feature vectors to predict the class labels. An accuracy of 88.0 % is achieved on the official development set of the challenge – a relative improvement of 17.7 % over the challenge baseline.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Amiriparian2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Amiriparian_173.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/auDeep/auDeep" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Amiriparian2017label" class="modal fade" id="bibtex-Amiriparian2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAmiriparian2017label">
        Sequence to Sequence Autoencoders for Unsupervised Representation Learning From Audio
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Amiriparian2017,
    Author = "Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Schuller, Björn",
    title = "Sequence to Sequence Autoencoders for Unsupervised Representation Learning From Audio",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper describes our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). We propose a system for this task using a recurrent sequence to sequence autoencoder for unsupervised representation learning from raw audio files. First, we extract mel-spectrograms from the raw audio files. Second, we train a recurrent sequence to sequence autoencoder on these spectrograms, that are considered as time-dependent frequency vectors. Then, we extract, from a fully connected layer between the decoder and encoder units, the learnt representations of spectrograms as the feature vectors for the corresponding audio instances. Finally, we train a multilayer perceptron neural network on these feature vectors to predict the class labels. An accuracy of 88.0 \% is achieved on the official development set of the challenge – a relative improvement of 17.7 \% over the challenge baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Amiriparian2017a" style="box-shadow: none">
<div class="panel-heading" id="heading-Amiriparian2017a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The Combined Augsburg / Passau / Tum / Icl System for DCASE 2017
       </h4>
<p style="text-align:left">
        Shahin Amiriparian<sup>1,2,3</sup>, Nicholas Cummins<sup>1,2</sup>, Michael Freitag<sup>1</sup>, Andykun Qian<sup>1,2,3</sup>, Ren Zhao<sup>1,2</sup>, Vedhas Pandit<sup>1,2</sup> and Björn Schuller<sup>1,2,4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Chair of Complex &amp; Intelligent Systems, Universität Passau, Passau, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care, Augsburg University, Augsburg, Germany, <sup>3</sup>Machine Intelligence &amp; Signal Processing Group, Technische Universität München, München, Germany, <sup>4</sup>Group of Language, Audio &amp; Music, Imperial Collage London, London, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Amiriparian_AU_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Amiriparian2017a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Amiriparian2017a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Amiriparian2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Amiriparian_182.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Amiriparian2017a').collapse('show');window.location.hash='#Amiriparian2017a';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Amiriparian2017a" class="panel-collapse collapse" id="collapse-Amiriparian2017a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The Combined Augsburg / Passau / Tum / Icl System for DCASE 2017
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This technical report covers the fusion of two approaches towards the Acoustic Scene Classification sub-task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). The first system uses a novel recurrent sequence to sequence autoencoder approach for unsupervised representation learning. The second system is based on the late fusion of support vector machines trained on either wavelet features or an archetypal acoustic feature set. A weighted late-fusion combination of these two systems achieved an accuracy of 90.1 % on the official development set of the challenge – a relative percentage improvement of 20.2 % over the challenge baseline.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP+SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted late fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Amiriparian2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Amiriparian_182.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/auDeep/auDeep" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Amiriparian2017alabel" class="modal fade" id="bibtex-Amiriparian2017a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAmiriparian2017alabel">
        The Combined Augsburg / Passau / Tum / Icl System for DCASE 2017
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Amiriparian2017a,
    Author = "Amiriparian, Shahin and Cummins, Nicholas and Freitag, Michael and Qian, Andykun and Zhao, Ren and Pandit, Vedhas and Schuller, Björn",
    title = "The Combined Augsburg / Passau / Tum / Icl System for {DCASE} 2017",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This technical report covers the fusion of two approaches towards the Acoustic Scene Classification sub-task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). The first system uses a novel recurrent sequence to sequence autoencoder approach for unsupervised representation learning. The second system is based on the late fusion of support vector machines trained on either wavelet features or an archetypal acoustic feature set. A weighted late-fusion combination of these two systems achieved an accuracy of 90.1 \% on the official development set of the challenge – a relative percentage improvement of 20.2 \% over the challenge baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Bisot2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Bisot2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Nonnegative Feature Learning Methods for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Victor Bisot<sup>1</sup>, Romain Serizel<sup>2,3,4</sup>, Slim Essid<sup>1</sup> and Gaël Richard<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Image Data and Signal, Telecom ParisTech, Paris, France, <sup>2</sup>Université de Lorraine, Loria, Nancy, France, <sup>3</sup>Inria, Nancy, France, <sup>4</sup>CNRS, LORIA, Nancy, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bisot_TPT_task1_1</span> <span class="label label-primary">Bisot_TPT_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bisot2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bisot2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bisot2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Bisot_193.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bisot2017" class="panel-collapse collapse" id="collapse-Bisot2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Nonnegative Feature Learning Methods for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper introduces improvements to nonnegative feature learning-based methods for acoustic scene classification. We start by introducing modifications to the task-driven nonnegative matrix factorization algorithm. The proposed adapted scaling algorithm improves the generalization capability of task-driven nonnegative matrix factorization for the task. We then propose to exploit simple deep neural network architecture to classify both low level time-frequency representations and unsupervised nonnegative matrix factorization activation features independently. Moreover, we also propose a deep neural network architecture that exploits jointly unsupervised nonnegative matrix factorization activation features and low-level time frequency representations as inputs. Finally, we present a fusion of proposed systems in order to further improve performance. The resulting systems are our submission for the task 1 of the DCASE 2017 challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         NMF, MLP; NMF
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average log-probability
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bisot2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Bisot_193.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bisot2017label" class="modal fade" id="bibtex-Bisot2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBisot2017label">
        Nonnegative Feature Learning Methods for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bisot2017,
    Author = "Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gaël",
    title = "Nonnegative Feature Learning Methods for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper introduces improvements to nonnegative feature learning-based methods for acoustic scene classification. We start by introducing modifications to the task-driven nonnegative matrix factorization algorithm. The proposed adapted scaling algorithm improves the generalization capability of task-driven nonnegative matrix factorization for the task. We then propose to exploit simple deep neural network architecture to classify both low level time-frequency representations and unsupervised nonnegative matrix factorization activation features independently. Moreover, we also propose a deep neural network architecture that exploits jointly unsupervised nonnegative matrix factorization activation features and low-level time frequency representations as inputs. Finally, we present a fusion of proposed systems in order to further improve performance. The resulting systems are our submission for the task 1 of the DCASE 2017 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chandrasekhar2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Chandrasekhar2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep Neural Network
       </h4>
<p style="text-align:left">
        Paseddula Chandrasekhar<sup>1</sup> and Suryakanth V. Gangashetty<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Speech Processing Lab, International Institute of Information Technology, Hyderabad, Hyderabad, India, <sup>2</sup>Speech processing Lab, International Institute of Information Technology, Hyderabad, Hyderabad, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chandrasekhar_IIITH_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chandrasekhar2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chandrasekhar2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chandrasekhar2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Chandrasekhar_150.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chandrasekhar2017" class="panel-collapse collapse" id="collapse-Chandrasekhar2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep Neural Network
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, deep neural networks (DNN) are applied for acoustic scene classification task provided by DCASE2017 challenge. We perform experiment on a dataset consisting of 15 types of acoustic scenes with a given total development data and evolution data of task1. We propose an DNN architecture for utterance level classification. Evaluation of models were performed on given evolution data of task1 for 4 folds using development data. In this approach MFCC and IMFCC feature vectors are used to train DNN model and their DNN scores were combined to test the system. On the official development data set of the task1 challenge, an accuracy of 81.28% is achieved.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, Inverse Melfrequency cepstral coefficients
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         DNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chandrasekhar2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Chandrasekhar_150.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chandrasekhar2017label" class="modal fade" id="bibtex-Chandrasekhar2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChandrasekhar2017label">
        Acoustic Scene Classification Using Deep Neural Network
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chandrasekhar2017,
    Author = "Chandrasekhar, Paseddula and V. Gangashetty, Suryakanth",
    title = "Acoustic Scene Classification Using Deep Neural Network",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this paper, deep neural networks (DNN) are applied for acoustic scene classification task provided by DCASE2017 challenge. We perform experiment on a dataset consisting of 15 types of acoustic scenes with a given total development data and evolution data of task1. We propose an DNN architecture for utterance level classification. Evaluation of models were performed on given evolution data of task1 for 4 folds using development data. In this approach MFCC and IMFCC feature vectors are used to train DNN model and their DNN scores were combined to test the system. On the official development data set of the task1 challenge, an accuracy of 81.28\% is achieved."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Chou2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Chou2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        FrameCNN: A Weakly-Supervised Learning Framework for Frame-Wise Acoustic Event Detection and Classification
       </h4>
<p style="text-align:left">
        Szu-Yu Chou<sup>1,2</sup>, Jyh-Shing Jang<sup>1</sup> and Yi-Hsuan Yang<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan, <sup>2</sup>Research Center for IT innovation, Academia Sinica, Taipei, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chou_SINICA_task1_1</span> <span class="label label-primary">Chou_SINICA_task1_2</span> <span class="label label-primary">Chou_SINICA_task1_3</span> <span class="label label-primary">Chou_SINICA_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Chou2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Chou2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Chou2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Chou_102.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Chou2017" class="panel-collapse collapse" id="collapse-Chou2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       FrameCNN: A Weakly-Supervised Learning Framework for Frame-Wise Acoustic Event Detection and Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we describe our contribution to the challenge of detection and classification of acoustic scenes and events (DCASE2017).We propose framCNN, a novel weakly supervised learning frame-work that improves the performance of convolutional neural net-work (CNN) for acoustic event detection by attending to details of each sound at various temporal levels. Most existing weakly-supervised frameworks replace fully-connected network with global average pooling after the final convolution layer. Such a method tends to identify only a few discriminative parts, leading to sub-optimal localization and classification accuracy. The key idea of our approach is to consciously classify the sound of each frame given by the corresponding label. The idea is general and can be applied to any network for achieving sound event detection and improving the performance of sound event classification. In acoustic scene classification (Task1), our approach obtained an average accuracy of 99.2% on the four-fold cross-validation for acoustic scene recognition, comparing to the provided baseline of 74.8%. In the large-scale weakly supervised sound event detection for smart cars(Task4), we obtained a F-score 53.8% for sound event audio tagging (subtask A), compared to the baseline of 19.8%, and a F-score32.8% for sound event detection (subtask B), compared to the base-line of 11.4%
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Chou2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Chou_102.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Chou2017label" class="modal fade" id="bibtex-Chou2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexChou2017label">
        FrameCNN: A Weakly-Supervised Learning Framework for Frame-Wise Acoustic Event Detection and Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Chou2017,
    Author = "Chou, Szu-Yu and Jang, Jyh-Shing and Yang, Yi-Hsuan",
    title = "{FrameCNN}: A Weakly-Supervised Learning Framework for Frame-Wise Acoustic Event Detection and Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this paper, we describe our contribution to the challenge of detection and classification of acoustic scenes and events (DCASE2017).We propose framCNN, a novel weakly supervised learning frame-work that improves the performance of convolutional neural net-work (CNN) for acoustic event detection by attending to details of each sound at various temporal levels. Most existing weakly-supervised frameworks replace fully-connected network with global average pooling after the final convolution layer. Such a method tends to identify only a few discriminative parts, leading to sub-optimal localization and classification accuracy. The key idea of our approach is to consciously classify the sound of each frame given by the corresponding label. The idea is general and can be applied to any network for achieving sound event detection and improving the performance of sound event classification. In acoustic scene classification (Task1), our approach obtained an average accuracy of 99.2\% on the four-fold cross-validation for acoustic scene recognition, comparing to the provided baseline of 74.8\%. In the large-scale weakly supervised sound event detection for smart cars(Task4), we obtained a F-score 53.8\% for sound event audio tagging (subtask A), compared to the baseline of 19.8\%, and a F-score32.8\% for sound event detection (subtask B), compared to the base-line of 11.4\%"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dang2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Dang2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Deep Learning for DCASE2017 Challenge
       </h4>
<p style="text-align:left">
        An Dang, Toan Vu and Jia-Ching Wang
       </p>
<p style="text-align:left">
<em>
         Computer Sciene and Information Engineering, National Central University, Taoyuan, Taiwan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dang_NCU_task1_1</span> <span class="label label-primary">Dang_NCU_task1_2</span> <span class="label label-primary">Dang_NCU_task1_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dang2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dang2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dang2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Dang_209.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dang2017" class="panel-collapse collapse" id="collapse-Dang2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Deep Learning for DCASE2017 Challenge
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper reports our results on all tasks of DCASE challenge 2017 which are acoustic scene classification, detection of rare sound events, sound event detection in real life audio, and large-scale weakly supervised sound event detection for smart cars. Our proposed methods are developed based on two favorite neural networks which are convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Experiments show that our proposed methods outperform the baseline.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC; log-mel energies; log-mel energies, MFCC
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN; CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dang2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Dang_209.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dang2017label" class="modal fade" id="bibtex-Dang2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDang2017label">
        Deep Learning for DCASE2017 Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dang2017,
    Author = "Dang, An and Vu, Toan and Wang, Jia-Ching",
    title = "Deep Learning for {DCASE2017} Challenge",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper reports our results on all tasks of DCASE challenge 2017 which are acoustic scene classification, detection of rare sound events, sound event detection in real life audio, and large-scale weakly supervised sound event detection for smart cars. Our proposed methods are developed based on two favorite neural networks which are convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Experiments show that our proposed methods outperform the baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Duppada2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Duppada2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Ensemble of Deep Neural Networks for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Venkatesh Duppada and Sushant Hiray
       </p>
<p style="text-align:left">
<em>
         Data Science, Seernet Technologies, LLC, Mumbai, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Duppada_Seernet_task1_1</span> <span class="label label-primary">Duppada_Seernet_task1_2</span> <span class="label label-primary">Duppada_Seernet_task1_3</span> <span class="label label-primary">Duppada_Seernet_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Duppada2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Duppada2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Duppada2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Duppada_218.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Duppada2017" class="panel-collapse collapse" id="collapse-Duppada2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Ensemble of Deep Neural Networks for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deep neural networks (DNNs) have recently achieved great success in a multitude of classification tasks. Ensembles of DNNs have been shown to improve the performance. In this paper, we explore the recent state-of-the-art DNNs used for image classification. We modified these DNNs and applied them to the task of acoustic scene classification. We conducted a number of experiments on the TUT Acoustic Scenes 2017 dataset to empirically compare these methods. Finally, we show that the ensemble of these DNNs improves the baseline score for DCASE-2017 Task 1 by 10%
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz; 16kHz; 44.1kHz, 16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Duppada2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Duppada_218.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Duppada2017label" class="modal fade" id="bibtex-Duppada2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDuppada2017label">
        Ensemble of Deep Neural Networks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Duppada2017,
    Author = "Duppada, Venkatesh and Hiray, Sushant",
    title = "Ensemble of Deep Neural Networks for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "Deep neural networks (DNNs) have recently achieved great success in a multitude of classification tasks. Ensembles of DNNs have been shown to improve the performance. In this paper, we explore the recent state-of-the-art DNNs used for image classification. We modified these DNNs and applied them to the task of acoustic scene classification. We conducted a number of experiments on the TUT Acoustic Scenes 2017 dataset to empirically compare these methods. Finally, we show that the ensemble of these DNNs improves the baseline score for DCASE-2017 Task 1 by 10\%"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Foleiss2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Foleiss2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MLP-Based Feature Learning for Automatic Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Juliano Foleiss<sup>1</sup> and Tiago Tavares<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Computing Department, Universidade Tecnologica Federal do Parana, Campo Mourao, Brazil, <sup>2</sup>School of Electrical and Computer Engineering, University of Campinas, Campinas, Brazil
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Foleiss_UTFPR_task1_1</span> <span class="label label-primary">Foleiss_UTFPR_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Foleiss2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Foleiss2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Foleiss2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Foleiss_185.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Foleiss2017" class="panel-collapse collapse" id="collapse-Foleiss2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MLP-Based Feature Learning for Automatic Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents an experimental setup for feature learning in the context of Automatic Acoustic Scene Classification. The setup presented in this paper has been successfully used for Automatic Music Genre Classification by Sigtia and Dixon (2014). First a MLP is trained with audio frames calculated from a 2048-sample STFT and one-shot encoding. Then, the activations of each hidden layer of the MLP are stored as learned features for the entire dataset. Such features are then used to train Random Forests in order to increase classification performance. Our results on the DCASE 2017 development dataset reaches 80% accuracy across supplied folds.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         STFT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP; MLP, random forest
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         probability sum; majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Foleiss2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Foleiss_185.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Foleiss2017label" class="modal fade" id="bibtex-Foleiss2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFoleiss2017label">
        MLP-Based Feature Learning for Automatic Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Foleiss2017,
    Author = "Foleiss, Juliano and Tavares, Tiago",
    title = "{MLP}-Based Feature Learning for Automatic Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper presents an experimental setup for feature learning in the context of Automatic Acoustic Scene Classification. The setup presented in this paper has been successfully used for Automatic Music Genre Classification by Sigtia and Dixon (2014). First a MLP is trained with audio frames calculated from a 2048-sample STFT and one-shot encoding. Then, the activations of each hidden layer of the MLP are stored as learned features for the entire dataset. Such features are then used to train Random Forests in order to increase classification performance. Our results on the DCASE 2017 development dataset reaches 80\% accuracy across supplied folds."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fonseca2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Fonseca2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Eduardo Fonseca, Rong Gong, Dmitry Bogdanov, Olga Slizovskaia, Emilia Gomez and Xavier Serra
       </p>
<p style="text-align:left">
<em>
         Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Fonseca_MTG_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fonseca2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fonseca2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fonseca2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Fonseca_180.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fonseca2017" class="panel-collapse collapse" id="collapse-Fonseca2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This work describes our contribution to the acoustic scene classification task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection of hand-crafted features is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled mel-spectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8.2%.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         max of average score
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fonseca2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Fonseca_180.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fonseca2017label" class="modal fade" id="bibtex-Fonseca2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFonseca2017label">
        Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fonseca2017,
    Author = "Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier",
    title = "Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This work describes our contribution to the acoustic scene classification task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection of hand-crafted features is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled mel-spectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8.2\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Fraile2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Fraile2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Classification of Acoustic Scenes Based on the Modulation Spectrum
       </h4>
<p style="text-align:left">
        Ruben Fraile, Juana M. Gutierrez-Arriola, Nicolas Saenz-Lechon and Victor J. Osma-Ruiz
       </p>
<p style="text-align:left">
<em>
         Group on Acoustics and Multimedia Applicationa, Universidad Politecnica de Madrid, Madrid, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Fraile_UPM_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Fraile2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Fraile2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Fraile2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Fraile_127.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Fraile2017" class="panel-collapse collapse" id="collapse-Fraile2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Classification of Acoustic Scenes Based on the Modulation Spectrum
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       A system for the automatic classification of acoustic scenes is proposed. This system calculates the spectral distribution of energy across auditory-relevant frequency bands and obtains some descriptors of the envelope modulation spectrum (EMS) by applying the discrete cosine transform to the logarithm of the EMS. This parametrisation scheme achieves good separation among scene classes, since it gets good classification results with a simple classifier consisting of a multilayer perceptron with only one hidden layer.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         modulation spectrum
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         a posteriori probablity
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Fraile2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Fraile_127.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Fraile2017label" class="modal fade" id="bibtex-Fraile2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexFraile2017label">
        Classification of Acoustic Scenes Based on the Modulation Spectrum
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Fraile2017,
    Author = "Fraile, Ruben and Gutierrez-Arriola, Juana M. and Saenz-Lechon, Nicolas and Osma-Ruiz, Victor J.",
    title = "Classification of Acoustic Scenes Based on the Modulation Spectrum",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "A system for the automatic classification of acoustic scenes is proposed. This system calculates the spectral distribution of energy across auditory-relevant frequency bands and obtains some descriptors of the envelope modulation spectrum (EMS) by applying the discrete cosine transform to the logarithm of the EMS. This parametrisation scheme achieves good separation among scene classes, since it gets good classification results with a simple classifier consisting of a multilayer perceptron with only one hidden layer."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Gong2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Gong2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification by Fusing LightGBM and VGG-Net Multichannel Predictions
       </h4>
<p style="text-align:left">
        Rong Gong, Eduardo Fonseca, Dmitry Bogdanov, Olga Slizovskaia, Emilia Gomez and Xavier Serra
       </p>
<p style="text-align:left">
<em>
         Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Gong_MTG_task1_1</span> <span class="label label-primary">Gong_MTG_task1_2</span> <span class="label label-primary">Gong_MTG_task1_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Gong2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Gong2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Gong2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Gong_189.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Gong2017').collapse('show');window.location.hash='#Gong2017';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Gong2017" class="panel-collapse collapse" id="collapse-Gong2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification by Fusing LightGBM and VGG-Net Multichannel Predictions
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report provides a solution for the task 1 of DCASE 2017 challenge. We build two parallel audio scene classification systems -- LightGBM and VGG-net. The prediction scores are output from the multichannel version of the TUT Acoustic Scenes 2017 dataset. Finally, we perform a linear logistic regression method to fuse the LightGBM, VGG-net and LightGBM+VGG-net scores respectively. The evaluation is done on the development set, and three outputs are submitted for the challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         multichannel
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         various; log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GBM CNN fusion; GBM fusion; CNN fusion
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Gong2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Gong_189.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/ronggong/DCASE2017-task1" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Gong2017label" class="modal fade" id="bibtex-Gong2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexGong2017label">
        Acoustic Scene Classification by Fusing LightGBM and VGG-Net Multichannel Predictions
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Gong2017,
    Author = "Gong, Rong and Fonseca, Eduardo and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier",
    title = "Acoustic Scene Classification by Fusing {LightGBM} and {VGG}-Net Multichannel Predictions",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report provides a solution for the task 1 of DCASE 2017 challenge. We build two parallel audio scene classification systems -- LightGBM and VGG-net. The prediction scores are output from the multichannel version of the TUT Acoustic Scenes 2017 dataset. Finally, we perform a linear logistic regression method to fuse the LightGBM, VGG-net and LightGBM+VGG-net scores respectively. The evaluation is done on the development set, and three outputs are submitted for the challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Han2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Han2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Yoonchang Han<sup>1</sup> and Jeongsoo Park<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Cochlear.ai, Seoul, Korea, <sup>2</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Han_COCAI_task1_1</span> <span class="label label-primary">Han_COCAI_task1_2</span> <span class="label label-primary">Han_COCAI_task1_3</span> <span class="label label-primary">Han_COCAI_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Han2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Han2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Han2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Han_207.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Han2017" class="panel-collapse collapse" id="collapse-Han2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we demonstrate how we applied convolutional neural network for DCASE 2017 task 1, acoustic scene classification. We propose a variety of preprocessing methods that emphasise different acoustic characteristics such as binaural representations, harmonic-percussive source separation, and background subtraction. We also present a network structure that can simultaneously analyse paired input, which makes the system benefit from a spatial information. The experimental results show that the proposed network structure and preprocessing method effectively learn acoustic characteristics from the audio recordings, and combining these with an ensemble model significantly reduces the error rate further, exhibiting an accuracy of 0.917 for 4-fold cross-validation on the development set using a mean ensemble.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono, binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         mean probability
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Han2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Han_207.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Han2017label" class="modal fade" id="bibtex-Han2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHan2017label">
        Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Han2017,
    Author = "Han, Yoonchang and Park, Jeongsoo",
    title = "Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this paper, we demonstrate how we applied convolutional neural network for DCASE 2017 task 1, acoustic scene classification. We propose a variety of preprocessing methods that emphasise different acoustic characteristics such as binaural representations, harmonic-percussive source separation, and background subtraction. We also present a network structure that can simultaneously analyse paired input, which makes the system benefit from a spatial information. The experimental results show that the proposed network structure and preprocessing method effectively learn acoustic characteristics from the audio recordings, and combining these with an ensemble model significantly reduces the error rate further, exhibiting an accuracy of 0.917 for 4-fold cross-validation on the development set using a mean ensemble."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Heittola2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Heittola2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System
       </h4>
<p style="text-align:left">
        Toni Heittola and Annamaria Mesaros
       </p>
<p style="text-align:left">
<em>
         Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Heittola_TUT_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Heittola2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Heittola2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Heittola2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/dcase-2017-challenge-paper.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Heittola2017').collapse('show');window.location.hash='#Heittola2017';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Heittola2017" class="panel-collapse collapse" id="collapse-Heittola2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Heittola2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/dcase-2017-challenge-paper.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/TUT-ARG/DCASE2017-baseline-system" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Heittola2017label" class="modal fade" id="bibtex-Heittola2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHeittola2017label">
        DCASE 2017 Challenge Setup: Tasks, Datasets and Baseline System
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Heittola2017,
    Author = "Heittola, Toni and Mesaros, Annamaria",
    title = "{DCASE} 2017 Challenge Setup: Tasks, Datasets and Baseline System",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A Multi-Scale Deep Convolutional Neural Network for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Taoan Huang and Jianhao Wang
       </p>
<p style="text-align:left">
<em>
         Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Huang_THU_task1_1</span> <span class="label label-primary">Huang_THU_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Huang_183.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang2017" class="panel-collapse collapse" id="collapse-Huang2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A Multi-Scale Deep Convolutional Neural Network for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Deep neural networks have shown great classification performances in numbers of applications. We applied a multi-scale deep convolutional neural network to acoustic scene classification (ASC) which has been submitted to Task 1 of the DCASE-2017 challenge. In this report, we show our model for classifying short sequences of audio, represented by their Mel-Frequency Cepstral Coefficients and Constant-Q value. The system is evaluated on the public dataset provided by the organizers. The best accuracy we obtained on a 4-fold cross-validation setup is 84.4%.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Huang_183.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang2017label" class="modal fade" id="bibtex-Huang2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang2017label">
        A Multi-Scale Deep Convolutional Neural Network for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang2017,
    Author = "Huang, Taoan and Wang, Jianhao",
    title = "A Multi-Scale Deep Convolutional Neural Network for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "Deep neural networks have shown great classification performances in numbers of applications. We applied a multi-scale deep convolutional neural network to acoustic scene classification (ASC) which has been submitted to Task 1 of the DCASE-2017 challenge. In this report, we show our model for classifying short sequences of audio, represented by their Mel-Frequency Cepstral Coefficients and Constant-Q value. The system is evaluated on the public dataset provided by the organizers. The best accuracy we obtained on a 4-fold cross-validation setup is 84.4\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hussain2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Hussain2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Improved Acoustic Scene Classification with DNN and CNN
       </h4>
<p style="text-align:left">
        Khalid Hussain<sup>1</sup>, Mazhar Hussain<sup>2</sup> and Muhammad Khan<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of electrical engineering, National University of computer and emerging sciences, Pakistan, <sup>2</sup>Department of Computer Science, National University of computer and emerging sciences, Pakistan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hussain_NUCES_task1_1</span> <span class="label label-primary">Hussain_NUCES_task1_2</span> <span class="label label-primary">Hussain_NUCES_task1_3</span> <span class="label label-primary">Hussain_NUCES_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hussain2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hussain2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hussain2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Hussain_166.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hussain2017" class="panel-collapse collapse" id="collapse-Hussain2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Improved Acoustic Scene Classification with DNN and CNN
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents the acoustic scene classification (ASC) to differentiate between different acoustic environments corre-sponding to the DCASE 2017 challenge task1. In this contribution we have applied two techniques of classification i.e. Deep Neural Network (DNN) and Convolution Neural Network (CNN). DNN and CNN are widely used in speech recognition, computer vision, and natural language processing applications. These techniques have recently achieved great success in the field of audio classification for the various applications. We achieved higher accuracy than the previous work done on benchmark datasets provided in the DCASE 2016 challenge. We used frame level randomization of the training dataset and log mel energy features to achieve higher accuracy with DNN and CNN. It is observed that DNN achieved 90.41%, 90.03% and CNN achieved 90.71%, 88.86% accuracy on randomized data based on 80 and 60 mel energy features, respectively
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; DNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hussain2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Hussain_166.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hussain2017label" class="modal fade" id="bibtex-Hussain2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHussain2017label">
        Improved Acoustic Scene Classification with DNN and CNN
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hussain2017,
    Author = "Hussain, Khalid and Hussain, Mazhar and Khan, Muhammad",
    title = "Improved Acoustic Scene Classification with {DNN} and {CNN}",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper presents the acoustic scene classification (ASC) to differentiate between different acoustic environments corre-sponding to the DCASE 2017 challenge task1. In this contribution we have applied two techniques of classification i.e. Deep Neural Network (DNN) and Convolution Neural Network (CNN). DNN and CNN are widely used in speech recognition, computer vision, and natural language processing applications. These techniques have recently achieved great success in the field of audio classification for the various applications. We achieved higher accuracy than the previous work done on benchmark datasets provided in the DCASE 2016 challenge. We used frame level randomization of the training dataset and log mel energy features to achieve higher accuracy with DNN and CNN. It is observed that DNN achieved 90.41\%, 90.03\% and CNN achieved 90.71\%, 88.86\% accuracy on randomized data based on 80 and 60 mel energy features, respectively"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hyder2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Hyder2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        BUET Bosch Consortium (B2C) Acoustic Scene Classification Systems for DCASE 2017
       </h4>
<p style="text-align:left">
        Rakib Hyder<sup>1</sup>, Shabnam Ghaffarzadegan<sup>2</sup>, Zhe Feng<sup>2</sup> and Taufiq Hasan<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh, <sup>2</sup>Robert Bosch Research and Technology Center, Robert Bosch Research and Technology Center, Palo Alto, CA, USA, <sup>3</sup>Department of Biomedical Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Hasan_BUET_task1_1</span> <span class="label label-primary">Hasan_BUET_task1_2</span> <span class="label label-primary">Hasan_BUET_task1_3</span> <span class="label label-primary">Hasan_BUET_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hyder2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hyder2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hyder2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Hasan_167.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hyder2017" class="panel-collapse collapse" id="collapse-Hyder2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       BUET Bosch Consortium (B2C) Acoustic Scene Classification Systems for DCASE 2017
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This technical report describes the systems jointly submitted by Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh, and Robert Bosch Research and Technology Center, Palo Alto, CA, USA, for the Acoustic scene classification (ASC) task of the DCASE 2017 challenge. Our sub-systems mainly consist of Convolutional Neural Network (CNN) based models trained on Spectrogram Image Features (SIF) using Mel and Log-scaled filter-banks. We also used a novel multi-band approach that learns the CNN models from different frequency bands separately using a single spectrogram. In a variant of CNN sub-systems, large dimensional audio segment level feature vectors are extracted from the flattening layer of a trained CNN model and later classified utilized a Probabilistic Linear Discriminant Analysis (PLDA) model. This sub-system is termed as the CNN-SuperVector (SV) system. We also implemented a GMM SuperVector system with a PLDA classifier and a feed-forward Neural Network (NN) classifier trained on an acoustic feature ensemble. Finally, we utilized linear score-fusion to combine the class-wise scores obtained from the different sub-systems.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, log-mel energies; log-mel energies; MFCC, log-mel energies, different functionals of various spectral and prosodic features
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GMM-SV, CNN-SV, Multiband CNN-SV; CNN-SV; GMM-SV, CNN-SV, Multiband CNN-SV, CNN, Multiband CNN; GMM-SV, CNN-SV, Multiband CNN-SV, CNN, Multiband CNN, DNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hyder2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Hasan_167.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hyder2017label" class="modal fade" id="bibtex-Hyder2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHyder2017label">
        BUET Bosch Consortium (B2C) Acoustic Scene Classification Systems for DCASE 2017
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hyder2017,
    Author = "Hyder, Rakib and Ghaffarzadegan, Shabnam and Feng, Zhe and Hasan, Taufiq",
    title = "{BUET} Bosch Consortium ({B2C}) Acoustic Scene Classification Systems for {DCASE} 2017",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This technical report describes the systems jointly submitted by Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh, and Robert Bosch Research and Technology Center, Palo Alto, CA, USA, for the Acoustic scene classification (ASC) task of the DCASE 2017 challenge. Our sub-systems mainly consist of Convolutional Neural Network (CNN) based models trained on Spectrogram Image Features (SIF) using Mel and Log-scaled filter-banks. We also used a novel multi-band approach that learns the CNN models from different frequency bands separately using a single spectrogram. In a variant of CNN sub-systems, large dimensional audio segment level feature vectors are extracted from the flattening layer of a trained CNN model and later classified utilized a Probabilistic Linear Discriminant Analysis (PLDA) model. This sub-system is termed as the CNN-SuperVector (SV) system. We also implemented a GMM SuperVector system with a PLDA classifier and a feed-forward Neural Network (NN) classifier trained on an acoustic feature ensemble. Finally, we utilized linear score-fusion to combine the class-wise scores obtained from the different sub-systems."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jallet2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Jallet2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using CRNN
       </h4>
<p style="text-align:left">
        Hugo Jallet, Emre Cakir and Tuomas Virtanen
       </p>
<p style="text-align:left">
<em>
         Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jallet_TUT_task1_1</span> <span class="label label-primary">Jallet_TUT_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jallet2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jallet2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jallet2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Jallet_140.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jallet2017" class="panel-collapse collapse" id="collapse-Jallet2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using CRNN
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents an application of a convolutiona lrecurrent neural network ( CRNN ) for the task of Acoustic Scene Classification ( ASC ). This is the first attempt, to the authors’ knowledge, to use this kind of network for the task of ASC, even though simple convolutional neural networks (CNN ) have already been applied and approved for this specific work. The submitted methods have been developed for the 2017 edition of the ”Detection and Classification of Acoustic Scenes and Events” ( DCASE ) challenge and consequently tested on the datasets provided for the task of ASC. In this paper, we use two based CRNN methods which score an overall accuracy of 78.9% and 80.8%.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum; majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jallet2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Jallet_140.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jallet2017label" class="modal fade" id="bibtex-Jallet2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJallet2017label">
        Acoustic Scene Classification Using CRNN
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jallet2017,
    Author = "Jallet, Hugo and Cakir, Emre and Virtanen, Tuomas",
    title = "Acoustic Scene Classification Using {CRNN}",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper presents an application of a convolutiona lrecurrent neural network ( CRNN ) for the task of Acoustic Scene Classification ( ASC ). This is the first attempt, to the authors’ knowledge, to use this kind of network for the task of ASC, even though simple convolutional neural networks (CNN ) have already been applied and approved for this specific work. The submitted methods have been developed for the 2017 edition of the ”Detection and Classification of Acoustic Scenes and Events” ( DCASE ) challenge and consequently tested on the datasets provided for the task of ASC. In this paper, we use two based CRNN methods which score an overall accuracy of 78.9\% and 80.8\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jee-Weon2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Jee-Weon2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DNN-Based Audio Scene Classification for DCASE 2017: Dual Inputfeatures, Balancing Cost, and Stochastic Data Duplication
       </h4>
<p style="text-align:left">
        Jung Jee-Weon, Heo Hee-Soo, Yang IL-Ho, Yoon Sung-Hyun, Shim Hye-Jin and Yu Ha-Jin
       </p>
<p style="text-align:left">
<em>
         School of Computer Science, University of Seoul, Seoul, Republic of South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yu_UOS_task1_1</span> <span class="label label-primary">Yu_UOS_task1_2</span> <span class="label label-primary">Yu_UOS_task1_3</span> <span class="label label-primary">Yu_UOS_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jee-Weon2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jee-Weon2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jee-Weon2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Yu_188.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jee-Weon2017" class="panel-collapse collapse" id="collapse-Jee-Weon2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DNN-Based Audio Scene Classification for DCASE 2017: Dual Inputfeatures, Balancing Cost, and Stochastic Data Duplication
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this study, we explored DNN-based audio scene classification systems with dual input features. Dual input features take advantage of simultaneously utilizing two features with different levels of abstraction as inputs: a frame-level mel-filterbank feature and utterance-level identity vector. A new fine-tune cost that solves the drawback of dual input features was developed, as well as a data duplication method that enables DNN to clearly discriminate frequently misclassified classes. Combining the proposed methods with the latest DNN techniques such as residual learning achieved a fold-wise accuracy of 95.8% for the validation set provided by the Detection and Classification of Acoustic Scenes and Events community.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         stochastic duplication
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel-filterbank features
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         score sum
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jee-Weon2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Yu_188.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jee-Weon2017label" class="modal fade" id="bibtex-Jee-Weon2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJee-Weon2017label">
        DNN-Based Audio Scene Classification for DCASE 2017: Dual Inputfeatures, Balancing Cost, and Stochastic Data Duplication
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jee-Weon2017,
    Author = "Jee-Weon, Jung and Hee-Soo, Heo and IL-Ho, Yang and Sung-Hyun, Yoon and Hye-Jin, Shim and Ha-Jin, Yu",
    title = "{DNN}-Based Audio Scene Classification for {DCASE} 2017: Dual Inputfeatures, Balancing Cost, and Stochastic Data Duplication",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this study, we explored DNN-based audio scene classification systems with dual input features. Dual input features take advantage of simultaneously utilizing two features with different levels of abstraction as inputs: a frame-level mel-filterbank feature and utterance-level identity vector. A new fine-tune cost that solves the drawback of dual input features was developed, as well as a data duplication method that enables DNN to clearly discriminate frequently misclassified classes. Combining the proposed methods with the latest DNN techniques such as residual learning achieved a fold-wise accuracy of 95.8\% for the validation set provided by the Detection and Classification of Acoustic Scenes and Events community."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Jimenez2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Jimenez2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features
       </h4>
<p style="text-align:left">
        Abelino Jimenez, Benjamin Elizalde and Bhiksha Raj
       </p>
<p style="text-align:left">
<em>
         Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Jimenez_CMU_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Jimenez2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Jimenez2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Jimenez2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Jimenez_186.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Jimenez2017" class="panel-collapse collapse" id="collapse-Jimenez2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       The recordings from acoustic scenes contain information from multiple sound sources that can be captured by different type of handcrafted features. These features can be classified using kernel machines, such as the Support Vector Machines, which can approximate decision boundaries arbitrarily well. However, the complexity of training these methods increases with the dimensionality of the features and the size of the dataset. A solution is to take advantage of shift-invariant kernels to map the input features to a randomized low-dimensional feature space, then used the resulting random features to approximate non-linear kernels with linear kernel computation. In this work, we compared shift-invariant kernels such as Guassian, Laplacian and Cauchy and their corresponding random features. Experiments show that kernels outperformed the DCASE baseline by and absolute 4%. More importantly, the dimensionality of the random features in contrast to the input features is more than three times, from 6,553 to 2,048, with minimal loss of performance and more than 10 times and still outperformed the baseline. Random features approaches provide a strong alternative to perform acoustic scene classification with small or large number of instances. Moreover, they provide other benefits such as privacy preservation.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         emo_conf (opensmile)
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         highest score
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Jimenez2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Jimenez_186.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Jimenez2017label" class="modal fade" id="bibtex-Jimenez2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexJimenez2017label">
        DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Jimenez2017,
    Author = "Jimenez, Abelino and Elizalde, Benjamin and Raj, Bhiksha",
    title = "{DCASE} 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "The recordings from acoustic scenes contain information from multiple sound sources that can be captured by different type of handcrafted features. These features can be classified using kernel machines, such as the Support Vector Machines, which can approximate decision boundaries arbitrarily well. However, the complexity of training these methods increases with the dimensionality of the features and the size of the dataset. A solution is to take advantage of shift-invariant kernels to map the input features to a randomized low-dimensional feature space, then used the resulting random features to approximate non-linear kernels with linear kernel computation. In this work, we compared shift-invariant kernels such as Guassian, Laplacian and Cauchy and their corresponding random features. Experiments show that kernels outperformed the DCASE baseline by and absolute 4\%. More importantly, the dimensionality of the random features in contrast to the input features is more than three times, from 6,553 to 2,048, with minimal loss of performance and more than 10 times and still outperformed the baseline. Random features approaches provide a strong alternative to perform acoustic scene classification with small or large number of instances. Moreover, they provide other benefits such as privacy preservation."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Case 2017 Acoustic Scene Classification Using Convolutional Neural Network in Time Series
       </h4>
<p style="text-align:left">
        Biho Kim
       </p>
<p style="text-align:left">
<em>
         Sogang university, Seoul, Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Biho_Sogang_task1_1</span> <span class="label label-primary">Biho_Sogang_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Biho_116.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2017" class="panel-collapse collapse" id="collapse-Kim2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Case 2017 Acoustic Scene Classification Using Convolutional Neural Network in Time Series
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This technical paper presents our approach for the acoustic scene classification (ASC) task in DACSE2017 challenge. We propose combination of recently deep learning algorithm for classification sequence of audio. We stack dilated causal convolution which is efficient for time series signal without recurrent structure and use SELU activation unit instead batch-normalization. Based on this, various experiments were evaluated on the ASC development dataset. The results were analyzed from different perspectives and the best accuracy score obtained by our system on 75.9% ..
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Biho_116.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2017label" class="modal fade" id="bibtex-Kim2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2017label">
        Case 2017 Acoustic Scene Classification Using Convolutional Neural Network in Time Series
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2017,
    Author = "Kim, Biho",
    title = "Case 2017 Acoustic Scene Classification Using Convolutional Neural Network in Time Series",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This technical paper presents our approach for the acoustic scene classification (ASC) task in DACSE2017 challenge. We propose combination of recently deep learning algorithm for classification sequence of audio. We stack dilated causal convolution which is efficient for time series signal without recurrent structure and use SELU activation unit instead batch-normalization. Based on this, various experiments were evaluated on the ASC development dataset. The results were analyzed from different perspectives and the best accuracy score obtained by our system on 75.9\% .."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kukanov2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Kukanov2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Recurrent Neural Network and Maximal Figure of Merit for Acoustic Event Detection
       </h4>
<p style="text-align:left">
        Ivan Kukanov<sup>1,2</sup>, Ville Hautamäki<sup>1</sup> and Kong Aik Lee<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Computing, University of Eastern Finland, Joensuu, Finland, <sup>2</sup>Institute for Infocomm Research, A*Star, Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kukanov_UEF_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kukanov2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kukanov2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kukanov2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Kukanov_196.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kukanov2017" class="panel-collapse collapse" id="collapse-Kukanov2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Recurrent Neural Network and Maximal Figure of Merit for Acoustic Event Detection
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this report, we describe the systems submitted to the DCASE 2017 challenge. In particular, we explored convolutional recurrent neural network (CRNN) for acoustic scene classification (Task 1). For the weakly supervised sound event detection (Task 4), we utilized CRNN by embedding maximal figure-of-merit (CRNN-MFoM) into the binary cross-entropy objective function. On the development data set, the CRNN model achieves an average 14.7% relative accuracy improvement on the classification Task 1, the CRNN-MFoM improves F1-score from 10.9% to 33.5% on the detection Task 4 compared to the baseline system.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kukanov2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Kukanov_196.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kukanov2017label" class="modal fade" id="bibtex-Kukanov2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKukanov2017label">
        Recurrent Neural Network and Maximal Figure of Merit for Acoustic Event Detection
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kukanov2017,
    Author = "Kukanov, Ivan and Hautamäki, Ville and Lee, Kong Aik",
    title = "Recurrent Neural Network and Maximal Figure of Merit for Acoustic Event Detection",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this report, we describe the systems submitted to the DCASE 2017 challenge. In particular, we explored convolutional recurrent neural network (CRNN) for acoustic scene classification (Task 1). For the weakly supervised sound event detection (Task 4), we utilized CRNN by embedding maximal figure-of-merit (CRNN-MFoM) into the binary cross-entropy objective function. On the development data set, the CRNN model achieves an average 14.7\% relative accuracy improvement on the classification Task 1, the CRNN-MFoM improves F1-score from 10.9\% to 33.5\% on the detection Task 4 compared to the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kun2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Kun2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Wavelets Revisited for the Classification of Acoustic Scenes
       </h4>
<p style="text-align:left">
        Qian Kun<sup>1,2,3</sup>, Ren Zhao<sup>2,3</sup>, Pandit Vedhas<sup>2,3</sup>, Yang Zijiang<sup>2,3</sup>, Zhang Zixing<sup>3</sup> and Schuller Björn<sup>2,3,4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>MISP group, Technische Universität München, Munich, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>3</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>4</sup>Group on Language, Audio and Music, Imperial College London, London, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Kun_TUM_UAU_UP_task1_1</span> <span class="label label-primary">Kun_TUM_UAU_UP_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kun2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kun2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kun2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Kun_197.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kun2017" class="panel-collapse collapse" id="collapse-Kun2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Wavelets Revisited for the Classification of Acoustic Scenes
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       We investigate the effectiveness of wavelet features for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). On the back-end side, gated recurrent neural networks (GRNNs) are compared against traditional support vector machines (SVMs). We observe that, the proposed wavelet features behave comparable to the typically-used temporal and spectral features in the classification of acoustic scenes. Further, a late fusion of trained models with wavelets and typical acoustic features reach the best averaged 4-fold cross validation accuracy of 83.2%, and 82.6% by SVMs, and GRNNs, respectively; both significantly outperform the baseline (74.8%) of the official development set (p&lt;0.001, one-tailed z-test).
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         wavelets, ComParE (openSMILE)
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM; GRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         margin sampling value
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kun2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Kun_197.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kun2017label" class="modal fade" id="bibtex-Kun2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKun2017label">
        Wavelets Revisited for the Classification of Acoustic Scenes
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kun2017,
    Author = "Kun, Qian and Zhao, Ren and Vedhas, Pandit and Zijiang, Yang and Zixing, Zhang and Björn, Schuller",
    title = "Wavelets Revisited for the Classification of Acoustic Scenes",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "We investigate the effectiveness of wavelet features for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). On the back-end side, gated recurrent neural networks (GRNNs) are compared against traditional support vector machines (SVMs). We observe that, the proposed wavelet features behave comparable to the typically-used temporal and spectral features in the classification of acoustic scenes. Further, a late fusion of trained models with wavelets and typical acoustic features reach the best averaged 4-fold cross validation accuracy of 83.2\%, and 82.6\% by SVMs, and GRNNs, respectively; both significantly outperform the baseline (74.8\%) of the official development set (p&lt;0.001, one-tailed z-test)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lehner2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Lehner2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Classifying Short Acoustic Scenes with I-Vectors and CNNs: Challenges and Optimisations for the 2017 DCASE ASC Task
       </h4>
<p style="text-align:left">
        Bernhard Lehner, Hamid Eghbal-Zadeh, Matthias Dorfer, Filip Korzeniowski, Khaled Koutini and Gerhard Widmer
       </p>
<p style="text-align:left">
<em>
         Department of Computational Perception, Johannes Kepler University, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lehner_JKU_task1_1</span> <span class="label label-primary">Lehner_JKU_task1_2</span> <span class="label label-primary">Lehner_JKU_task1_3</span> <span class="label label-primary">Lehner_JKU_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lehner2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lehner2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lehner2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Lehner_142.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lehner2017" class="panel-collapse collapse" id="collapse-Lehner2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Classifying Short Acoustic Scenes with I-Vectors and CNNs: Challenges and Optimisations for the 2017 DCASE ASC Task
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes the CP-JKU team's submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2017 challenge, and discusses some observations we made about the data and the classification setup. Our approach is based on the methodology that achieved ranks 1 and 2 in the 2016 ASC challenge: a fusion of i-vector modelling using MFCC features derived from left and right audio channels, and deep convolutional neural networks (CNNs) trained on raw spectrograms. The data provided for the 2017 ASC task presented some new challenges -- in particular, audio stimuli of very short duration. These will be discussed in detail, and our measures for addressing them will be described. The result of our experiments is a classification system that achieves classification accuracies of around 90% on the provided development data, as estimated via the prescribed four-fold cross-validation scheme (which, we suspect, may be rather optimistic in relation to new data).
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural; mono, binaural; mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC based i-vectors; MFCC, log-scaled spectrogram; log-scaled spectrogram; mel-scaled spectrograms, i-vectors
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         i-vector; CNN, i-vector, ensemble; CNN, ensemble; i-vector, CNN, ensemble
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         min. cosine distance; model averaging; fusion w/ logistic linear regression
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lehner2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Lehner_142.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lehner2017label" class="modal fade" id="bibtex-Lehner2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLehner2017label">
        Classifying Short Acoustic Scenes with I-Vectors and CNNs: Challenges and Optimisations for the 2017 DCASE ASC Task
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lehner2017,
    Author = "Lehner, Bernhard and Eghbal-Zadeh, Hamid and Dorfer, Matthias and Korzeniowski, Filip and Koutini, Khaled and Widmer, Gerhard",
    title = "Classifying Short Acoustic Scenes with {I}-Vectors and {CNNs}: Challenges and Optimisations for the 2017 {DCASE} {ASC} Task",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report describes the CP-JKU team's submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2017 challenge, and discusses some observations we made about the data and the classification setup. Our approach is based on the methodology that achieved ranks 1 and 2 in the 2016 ASC challenge: a fusion of i-vector modelling using MFCC features derived from left and right audio channels, and deep convolutional neural networks (CNNs) trained on raw spectrograms. The data provided for the 2017 ASC task presented some new challenges -- in particular, audio stimuli of very short duration. These will be discussed in detail, and our measures for addressing them will be described. The result of our experiments is a classification system that achieves classification accuracies of around 90\% on the provided development data, as estimated via the prescribed four-fold cross-validation scheme (which, we suspect, may be rather optimistic in relation to new data)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Li2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Li2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The SEIE-SCUT Systems for IEEE AASP Challenge on DCASE 2017: Deep Learning Techniques for Audio Representation and Classification
       </h4>
<p style="text-align:left">
        Yanxiong Li and Xianku Li
       </p>
<p style="text-align:left">
<em>
         School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Li_SCUT_task1_1</span> <span class="label label-primary">Li_SCUT_task1_2</span> <span class="label label-primary">Li_SCUT_task1_3</span> <span class="label label-primary">Li_SCUT_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Li2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Li2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Li2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Li_120.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Li2017" class="panel-collapse collapse" id="collapse-Li2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The SEIE-SCUT Systems for IEEE AASP Challenge on DCASE 2017: Deep Learning Techniques for Audio Representation and Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this report, we present our works about three tasks of IEEE AASP challenge on DCASE 2017, i.e. task 1: Acoustic Scene Classification (ASC), task 2: detection of rare sound events in artificially created mixtures and task 3: sound event detection in real life recordings. Tasks 2 and 3 belong to the same problem, i.e. Sound Event Detection (SED). We adopt deep learning techniques to extract Deep Audio Feature (DAF) and classify various acoustic scenes or sound events. Specifically, a Deep Neural Network (DNN) is first built for generating the DAF from Mel-Frequency Cepstral Coefficients (MFCCs), and then a Recurrent Neural Network (RNN) of Bi-directional Long Short Term Memory (Bi-LSTM) fed by the DAF is built for ASC and SED. Evaluated on the development datasets of DCASE 2017, our systems are superior to the corresponding baselines for tasks 1 and 2, and our system for task 3 performs as good as the baseline in terms of the predominant metrics.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         DNN(MFCC)
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Bi-LSTM; DNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Li2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Li_120.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Li2017label" class="modal fade" id="bibtex-Li2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLi2017label">
        The SEIE-SCUT Systems for IEEE AASP Challenge on DCASE 2017: Deep Learning Techniques for Audio Representation and Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Li2017,
    Author = "Li, Yanxiong and Li, Xianku",
    title = "The {SEIE-SCUT} Systems for {IEEE} {AASP} Challenge on {DCASE} 2017: Deep Learning Techniques for Audio Representation and Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this report, we present our works about three tasks of IEEE AASP challenge on DCASE 2017, i.e. task 1: Acoustic Scene Classification (ASC), task 2: detection of rare sound events in artificially created mixtures and task 3: sound event detection in real life recordings. Tasks 2 and 3 belong to the same problem, i.e. Sound Event Detection (SED). We adopt deep learning techniques to extract Deep Audio Feature (DAF) and classify various acoustic scenes or sound events. Specifically, a Deep Neural Network (DNN) is first built for generating the DAF from Mel-Frequency Cepstral Coefficients (MFCCs), and then a Recurrent Neural Network (RNN) of Bi-directional Long Short Term Memory (Bi-LSTM) fed by the DAF is built for ASC and SED. Evaluated on the development datasets of DCASE 2017, our systems are superior to the corresponding baselines for tasks 1 and 2, and our system for task 3 performs as good as the baseline in terms of the predominant metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lu2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Lu2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classifications
       </h4>
<p style="text-align:left">
        Lu Lu<sup>1,2</sup>, Yuzhi Jiang<sup>1,3</sup>, Huiyu Zhang<sup>1</sup>, Yuhong Yang<sup>1,2</sup>, Ruiming Hu<sup>1,3</sup>, Weiping Tu<sup>1</sup> and Weiyi Huang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>National Engineering Research Center for Multimedia Software, Wuhan University, Hubei, China, <sup>2</sup>Collaborative Innovation Center of Geospatial Technology, Wuhan, China, <sup>3</sup>The Key Laboratory of Multimedia and Network Communication Engineering,Wuhan University, Wuhan University, Hubei, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yang_WHU_TASK1_1</span> <span class="label label-primary">Yang_WHU_TASK1_2</span> <span class="label label-primary">Yang_WHU_TASK1_3</span> <span class="label label-primary">Yang_WHU_TASK1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lu2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lu2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lu2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Yang_169.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lu2017" class="panel-collapse collapse" id="collapse-Lu2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classifications
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper, we present three approaches on Task1 Acoustic Scene Classification(ASC): a simple CNN with low time-complexity, a novelty feature extraction, and feature fusion. First, We propose a simplified CNN architecture with only two convolutional layers to avoid overfitting. The model had a balance between higher accuracy and lower time-complexity. Second, we extract identifiable audio features by a data-driven spectrogram down-sampling. Third, we do feature fusion by combining data-driven features with Mel-Frequency spectrogram(MFS) as the network input. All the three approaches improve classification accuracy, compared with baseline on the development set.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         logsum
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lu2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Yang_169.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lu2017label" class="modal fade" id="bibtex-Lu2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLu2017label">
        Acoustic Scene Classifications
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lu2017,
    Author = "Lu, Lu and Jiang, Yuzhi and Zhang, Huiyu and Yang, Yuhong and Hu, Ruiming and Tu, Weiping and Huang, Weiyi",
    title = "Acoustic Scene Classifications",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this paper, we present three approaches on Task1 Acoustic Scene Classification(ASC): a simple CNN with low time-complexity, a novelty feature extraction, and feature fusion. First, We propose a simplified CNN architecture with only two convolutional layers to avoid overfitting. The model had a balance between higher accuracy and lower time-complexity. Second, we extract identifiable audio features by a data-driven spectrogram down-sampling. Third, we do feature fusion by combining data-driven features with Mel-Frequency spectrogram(MFS) as the network input. All the three approaches improve classification accuracy, compared with baseline on the development set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Maka2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Maka2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Auditory Scene Classification Based on the Spectro-Temporal Structure Analysis
       </h4>
<p style="text-align:left">
        Tomasz Maka
       </p>
<p style="text-align:left">
<em>
         Faculty of Computer Science and Information Technology, West Pomeranian University of Technology, Szczecin, Szczecin, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Maka_ZUT_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Maka2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Maka2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Maka2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Maka_203.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Maka2017" class="panel-collapse collapse" id="collapse-Maka2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Auditory Scene Classification Based on the Spectro-Temporal Structure Analysis
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this report, we present a modular system for acoustic scenes classification. Our proposed system contains four modules to compute the representations describing spectro-temporal properties of audio data. The frequency components are extracted from cochleagram and low-level audio feature contours. An onset map is used to determine the properties of temporal structure, and binaural cues are additional components in the final feature space. Computed features are formed into vector and fed to random forests classifier for the purpose of classification. The results were submitted to the 2017 IEEE AASP DCASE challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         cochleagram, onset map, binaural cues, low-level feature contours
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         random forest
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Maka2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Maka_203.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Maka2017label" class="modal fade" id="bibtex-Maka2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMaka2017label">
        Auditory Scene Classification Based on the Spectro-Temporal Structure Analysis
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Maka2017,
    Author = "Maka, Tomasz",
    title = "Auditory Scene Classification Based on the Spectro-Temporal Structure Analysis",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this report, we present a modular system for acoustic scenes classification. Our proposed system contains four modules to compute the representations describing spectro-temporal properties of audio data. The frequency components are extracted from cochleagram and low-level audio feature contours. An onset map is used to determine the properties of temporal structure, and binaural cues are additional components in the final feature space. Computed features are formed into vector and fed to random forests classifier for the purpose of classification. The results were submitted to the 2017 IEEE AASP DCASE challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Mun2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Mun2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane
       </h4>
<p style="text-align:left">
        Seongkyu Mun<sup>1</sup>, Sangwook Park<sup>1</sup>, David Han<sup>2</sup> and Hanseok Ko<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Intelligent Signal Processing Laboratory, Korea University, Seoul, South Korea, <sup>2</sup>Office of Naval Research, Office of Naval Research, Arlington VA, USA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Mun_KU_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Mun2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Mun2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Mun2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Mun_213.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Mun2017" class="panel-collapse collapse" id="collapse-Mun2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the cross-validated exper-iments on development DB, the usage of the generated features could improve ASC performance.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         GAN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MLP, RNN, CNN, SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Mun2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Mun_213.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Mun2017label" class="modal fade" id="bibtex-Mun2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMun2017label">
        Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Mun2017,
    Author = "Mun, Seongkyu and Park, Sangwook and Han, David and Ko, Hanseok",
    title = "Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using {SVM} Hyper-Plane",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the cross-validated exper-iments on development DB, the usage of the generated features could improve ASC performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Park2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features
       </h4>
<p style="text-align:left">
        Sangwook Park<sup>1</sup>, Seongkyu Mun<sup>2</sup>, Younglo Lee<sup>1</sup> and Hanseok Ko<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Electrical Engineering, Korea University, Seoul, Republic of Korea, <sup>2</sup>Department of Visual Information Processing, Korea University, Seoul, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Park_ISPL_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Park2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Park_212.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park2017" class="panel-collapse collapse" id="collapse-Park2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper proposes new image features for the acoustic scene classification task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In classification of acoustic scenes, identical sounds being observed in different places may affect performance. To resolve this issue, a covariance matrix, which represents energy density for each subband, and a double Fourier transform image, which represents energy variation for each subband, were defined as features. To classify the acoustic scenes with these features, Convolutional Neural Network has been applied with several techniques to reduce training time and to resolve initialization and local optimum problems. According to the experiments which were performed with the DCASE2017 challenge development dataset it is claimed that the proposed method outperformed several baseline methods. Specifically, the class average accuracy is shown as 83.6%, which is an improvement of 8.8%, 9.5%, 8.2% compared to MFCC-MLP, MFCC-GMM, and CepsCom-GMM, respectively.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         block mixing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         covariance of gammachirp energies, double FFT of gammachirp energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum posterior
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Park_212.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park2017label" class="modal fade" id="bibtex-Park2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark2017label">
        Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Park2017,
    Author = "Park, Sangwook and Mun, Seongkyu and Lee, Younglo and Ko, Hanseok",
    title = "Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper proposes new image features for the acoustic scene classification task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In classification of acoustic scenes, identical sounds being observed in different places may affect performance. To resolve this issue, a covariance matrix, which represents energy density for each subband, and a double Fourier transform image, which represents energy variation for each subband, were defined as features. To classify the acoustic scenes with these features, Convolutional Neural Network has been applied with several techniques to reduce training time and to resolve initialization and local optimum problems. According to the experiments which were performed with the DCASE2017 challenge development dataset it is claimed that the proposed method outperformed several baseline methods. Specifically, the class average accuracy is shown as 83.6\%, which is an improvement of 8.8\%, 9.5\%, 8.2\% compared to MFCC-MLP, MFCC-GMM, and CepsCom-GMM, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Phan2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Phan2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Attention-Based CNN with Generalized Label Tree Embedding for Audio Scene Classification
       </h4>
<p style="text-align:left">
        Huy Phan, Philipp Koch, Fabrice Katzberg, Marco Maass, Radoslaw Mazur and Alfred Mertins
       </p>
<p style="text-align:left">
<em>
         Institute for Signal Processing, University of Luebeck, Luebeck, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Phan_UniLuebeck_task1_1</span> <span class="label label-primary">Phan_UniLuebeck_task1_2</span> <span class="label label-primary">Phan_UniLuebeck_task1_3</span> <span class="label label-primary">Phan_UniLuebeck_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Phan2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Phan2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Phan2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Phan_175.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Phan2017" class="panel-collapse collapse" id="collapse-Phan2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Attention-Based CNN with Generalized Label Tree Embedding for Audio Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report presents our audio scene classification systems submitted for Task 1 ("acoustic scene classification") of DCASE 2017 challenge. The systems rely on combinations of generalized label tree embedding representation, convolutional neural networks (CNNs), and attention mechanism. Our experimental results on the development data of the challenge show that our proposed system significantly outperform the challenge's baseline, improving the average classification accuracy from 74.8% of the baseline to 83.8%.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         cross-validation with different data splits
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         generalized label tree embedding
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN; Attentive CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         entire-signal classification
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Phan2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Phan_175.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Phan2017label" class="modal fade" id="bibtex-Phan2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPhan2017label">
        Attention-Based CNN with Generalized Label Tree Embedding for Audio Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Phan2017,
    Author = "Phan, Huy and Koch, Philipp and Katzberg, Fabrice and Maass, Marco and Mazur, Radoslaw and Mertins, Alfred",
    title = "Attention-Based {CNN} with Generalized Label Tree Embedding for Audio Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = {This report presents our audio scene classification systems submitted for Task 1 ("acoustic scene classification") of DCASE 2017 challenge. The systems rely on combinations of generalized label tree embedding representation, convolutional neural networks (CNNs), and attention mechanism. Our experimental results on the development data of the challenge show that our proposed system significantly outperform the challenge's baseline, improving the average classification accuracy from 74.8\% of the baseline to 83.8\%.}
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Piczak2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Piczak2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Karol Piczak
       </p>
<p style="text-align:left">
<em>
         Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Piczak_WUT_task1_1</span> <span class="label label-primary">Piczak_WUT_task1_2</span> <span class="label label-primary">Piczak_WUT_task1_3</span> <span class="label label-primary">Piczak_WUT_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Piczak2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Piczak2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Piczak2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Piczak_208.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Piczak2017').collapse('show');window.location.hash='#Piczak2017';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Piczak2017" class="panel-collapse collapse" id="collapse-Piczak2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This study describes a convolutional neural network model submitted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with different frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolutional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time delay, block mixing
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Piczak2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Piczak_208.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/karoldvl/paper-2017-DCASE" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Piczak2017label" class="modal fade" id="bibtex-Piczak2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPiczak2017label">
        The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Piczak2017,
    Author = "Piczak, Karol",
    title = "The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This study describes a convolutional neural network model submitted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with different frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolutional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Rakotomamonjy2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Rakotomamonjy2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Human-Based Greedy Search of CNN Architecture
       </h4>
<p style="text-align:left">
        Alain Rakotomamonjy
       </p>
<p style="text-align:left">
<em>
         LITIS EA4108, Université de Rouen, Saint Etienne du Rouvray, France
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Rakotomamonjy_UROUEN_task1_1</span> <span class="label label-primary">Rakotomamonjy_UROUEN_task1_2</span> <span class="label label-primary">Rakotomamonjy_UROUEN_task1_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Rakotomamonjy2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Rakotomamonjy2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Rakotomamonjy2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Rakotomamonjy_110.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Rakotomamonjy2017" class="panel-collapse collapse" id="collapse-Rakotomamonjy2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Human-Based Greedy Search of CNN Architecture
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper presents the methodology we have followed for our submission at the DCASE 2017 competition on acoustic scene classification (Task 1). The approach is based convolutional neural networks. There is nothing original about this contribution, as we have just applied a human-based search of the best CNN architecture and hyper-parameters using a 4-fold cross-validation for selecting the best model. We hope that this approach will not reach the top entry of the challenge and that it will be outperformed by clever and beautiful methods.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average prediction; average prediction over 4 models; average prediction over 19 models
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Rakotomamonjy2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Rakotomamonjy_110.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Rakotomamonjy2017label" class="modal fade" id="bibtex-Rakotomamonjy2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexRakotomamonjy2017label">
        Human-Based Greedy Search of CNN Architecture
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Rakotomamonjy2017,
    Author = "Rakotomamonjy, Alain",
    title = "Human-Based Greedy Search of {CNN} Architecture",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper presents the methodology we have followed for our submission at the DCASE 2017 competition on acoustic scene classification (Task 1). The approach is based convolutional neural networks. There is nothing original about this contribution, as we have just applied a human-based search of the best CNN architecture and hyper-parameters using a 4-fold cross-validation for selecting the best model. We hope that this approach will not reach the top entry of the challenge and that it will be outperformed by clever and beautiful methods."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schindler2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Schindler2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Multi-Temporal Resolution Convolutional Neural Networks for the DCASE Acoustic Scene Classification Task
       </h4>
<p style="text-align:left">
        Alexander Schindler<sup>1</sup>, Thomas Lidy<sup>2</sup> and Andreas Rauber<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Center for Digital Safety and Security, Austrian Institute of Technology, Vienna, Austria, <sup>2</sup>Institute for Software and Interactive Systems, Technical University of Vienna, Vienna, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Schindler_AIT_task1_1</span> <span class="label label-primary">Schindler_AIT_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Schindler2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schindler2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schindler2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Schindler_153.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schindler2017" class="panel-collapse collapse" id="collapse-Schindler2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Multi-Temporal Resolution Convolutional Neural Networks for the DCASE Acoustic Scene Classification Task
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       In this paper we present our DCASE 2017 Challenge on Detection and Classification of Acoustic Scenes and Events contributions. We propose a parallel Convolutional Neural Network architecture for the task of classifying acoustic scenes and urban sound scapes. We propose a Deep Neural Network architecture for the task of acoustic scene classification which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolution are chosen to cover fine-grained characteristics of a scene's spectral texture as well as its distribution of acoustic events. The best performing variant of the proposed model scores 90.54% accuracy on the development dataset. This is a 6.81% improvement of the best performing single resolution model and 15.74% of the DCASE 2017 Acoustic Scenes Classification task baseline.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time stretching, block mixing, pitch shifting, mixing files of same class, gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         argmax of average softmax response per file
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schindler2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Schindler_153.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schindler2017label" class="modal fade" id="bibtex-Schindler2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchindler2017label">
        Multi-Temporal Resolution Convolutional Neural Networks for the DCASE Acoustic Scene Classification Task
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Schindler2017,
    Author = "Schindler, Alexander and Lidy, Thomas and Rauber, Andreas",
    title = "Multi-Temporal Resolution Convolutional Neural Networks for the {DCASE} Acoustic Scene Classification Task",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "In this paper we present our DCASE 2017 Challenge on Detection and Classification of Acoustic Scenes and Events contributions. We propose a parallel Convolutional Neural Network architecture for the task of classifying acoustic scenes and urban sound scapes. We propose a Deep Neural Network architecture for the task of acoustic scene classification which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolution are chosen to cover fine-grained characteristics of a scene's spectral texture as well as its distribution of acoustic events. The best performing variant of the proposed model scores 90.54\% accuracy on the development dataset. This is a 6.81\% improvement of the best performing single resolution model and 15.74\% of the DCASE 2017 Acoustic Scenes Classification task baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vafeiadis2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Vafeiadis2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning
       </h4>
<p style="text-align:left">
        Anastasios Vafeiadis<sup>1</sup>, Dimitrios Kalatzis<sup>1</sup>, Konstantinos Votis<sup>1</sup>, Dimitrios Giakoumis<sup>1</sup>, Dimitrios Tzovaras<sup>1</sup>, Liming Chen<sup>2</sup> and Raouf Hamzaoui<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Information Technologies Institute, Center for Research &amp; Technology Hellas, Thessaloniki, Greece, <sup>2</sup>Faculty of Technology, De Montfort University, Leicester, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Vafeiadis_CERTH_task1_1</span> <span class="label label-primary">Vafeiadis_CERTH_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Vafeiadis2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vafeiadis2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vafeiadis2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Vafeiadis_134.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vafeiadis2017" class="panel-collapse collapse" id="collapse-Vafeiadis2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report provides our contribution to the 2017 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. We investigated two approaches for the acoustic scene classification task. Firstly, we used a combination of features in the time and frequency domain and a hybrid Support Vector Machines - Hidden Markov Model (SVM-HMM) classifier to achieve an average accuracy over 4-folds of 80.9%. Secondly, we used the log-mel spectrogram for feature extraction and a Convolutional Neural Network (CNN) to achieve an average accuracy over 4-folds of 83.7%. Moreover, by exploiting data-augmentation techniques and using the whole segment (as opposed to splitting into sub-sequences) as an input, the accuracy of our CNN system was boosted to 95.9%. Our two approaches outperformed the DCASE baseline method, which uses log-mel band energies for feature extraction and a MultiLayer Perceptron (MLP) to achieve an average accuracy over 4-folds of 74.8%
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         speed and pitch change (downsampling), amplitude change (dynamic), gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, MFCC delta, MFCC acceleration, centroid, rolloff, ZCR; log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM-HMM; CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vafeiadis2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Vafeiadis_134.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vafeiadis2017label" class="modal fade" id="bibtex-Vafeiadis2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVafeiadis2017label">
        Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Vafeiadis2017,
    Author = "Vafeiadis, Anastasios and Kalatzis, Dimitrios and Votis, Konstantinos and Giakoumis, Dimitrios and Tzovaras, Dimitrios and Chen, Liming and Hamzaoui, Raouf",
    title = "Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report provides our contribution to the 2017 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. We investigated two approaches for the acoustic scene classification task. Firstly, we used a combination of features in the time and frequency domain and a hybrid Support Vector Machines - Hidden Markov Model (SVM-HMM) classifier to achieve an average accuracy over 4-folds of 80.9\%. Secondly, we used the log-mel spectrogram for feature extraction and a Convolutional Neural Network (CNN) to achieve an average accuracy over 4-folds of 83.7\%. Moreover, by exploiting data-augmentation techniques and using the whole segment (as opposed to splitting into sub-sequences) as an input, the accuracy of our CNN system was boosted to 95.9\%. Our two approaches outperformed the DCASE baseline method, which uses log-mel band energies for feature extraction and a MultiLayer Perceptron (MLP) to achieve an average accuracy over 4-folds of 74.8\%"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Vij2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Vij2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Performance Evaluation of Deep Learning Architectures for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Dinesh Vij and Naveen Aggarwal
       </p>
<p style="text-align:left">
<em>
         Computer Science and Engineering, University Institute of Engineering and Technology, Panjab University, Chandigarh, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Vij_UIET_task1_1</span> <span class="label label-primary">Vij_UIET_task1_2</span> <span class="label label-primary">Vij_UIET_task1_3</span> <span class="label label-primary">Vij_UIET_task1_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Vij2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Vij2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Vij2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Vij_138.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Vij2017" class="panel-collapse collapse" id="collapse-Vij2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Performance Evaluation of Deep Learning Architectures for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This paper is a submission to the sub-task Acoustic Scene Classification of the IEEE Audio and Acoustic Signal Processing challenge: Detection and Classification of Acoustic Scenes and Events 2017. The aim of the sub-task is to correctly detect 15 different acoustic scenes, which consist of indoor, outdoor, and vehicle categories. This work is based on log mel-filter bank features and deep learning. In this short paper, the impact of different parameters while applying a basic Deep Neural Network (DNN) architecture is first analyzed. The accuracy gains obtained by the different types of deep learning architectures such as Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Convolutional Neural Network (CNN) are then reported. It has been observed that the overall best scene classification accuracy was obtained with CNN.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         feature frame concatenation
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log mel-filter bank
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         RNN; LSTM; GRU; CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Vij2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Vij_138.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Vij2017label" class="modal fade" id="bibtex-Vij2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexVij2017label">
        Performance Evaluation of Deep Learning Architectures for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Vij2017,
    Author = "Vij, Dinesh and Aggarwal, Naveen",
    title = "Performance Evaluation of Deep Learning Architectures for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This paper is a submission to the sub-task Acoustic Scene Classification of the IEEE Audio and Acoustic Signal Processing challenge: Detection and Classification of Acoustic Scenes and Events 2017. The aim of the sub-task is to correctly detect 15 different acoustic scenes, which consist of indoor, outdoor, and vehicle categories. This work is based on log mel-filter bank features and deep learning. In this short paper, the impact of different parameters while applying a basic Deep Neural Network (DNN) architecture is first analyzed. The accuracy gains obtained by the different types of deep learning architectures such as Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Convolutional Neural Network (CNN) are then reported. It has been observed that the overall best scene classification accuracy was obtained with CNN."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Waldekar2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Waldekar2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        IIT Kharagpur Submissions for DCASE2017 ASC Task: Audio Features in a Fusion-Based Framework
       </h4>
<p style="text-align:left">
        Shefali Waldekar and Goutam Saha
       </p>
<p style="text-align:left">
<em>
         Electronics and Electrical Communication Engineering Dept., Indian Institute of Technology Kharagpur, Kharagpur, India
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Waldekar_IITKGP_task1_1</span> <span class="label label-primary">Waldekar_IITKGP_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Waldekar2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Waldekar2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Waldekar2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Waldekar_176.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Waldekar2017" class="panel-collapse collapse" id="collapse-Waldekar2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       IIT Kharagpur Submissions for DCASE2017 ASC Task: Audio Features in a Fusion-Based Framework
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes two submissions for Acoustic Scene Classification (ASC) task of the IEEE AASP challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2017. The first system follows an approach based on a score-level fusion of some well-known spectral features of audio processing. The second system uses the first proposed system in a two-stage hierarchical classification framework. On the DCASE 2017 development dataset, the two systems respectively show 18% and 21% better performance relative to that of the MLP-based baseline system.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         combination [block-based MFCC; SCFC; CQCC]
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         SVM
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         fusion
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Waldekar2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Waldekar_176.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Waldekar2017label" class="modal fade" id="bibtex-Waldekar2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWaldekar2017label">
        IIT Kharagpur Submissions for DCASE2017 ASC Task: Audio Features in a Fusion-Based Framework
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Waldekar2017,
    Author = "Waldekar, Shefali and Saha, Goutam",
    title = "{IIT} Kharagpur Submissions for {DCASE2017} {ASC} Task: Audio Features in a Fusion-Based Framework",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report describes two submissions for Acoustic Scene Classification (ASC) task of the IEEE AASP challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2017. The first system follows an approach based on a score-level fusion of some well-known spectral features of audio processing. The second system uses the first proposed system in a two-stage hierarchical classification framework. On the DCASE 2017 development dataset, the two systems respectively show 18\% and 21\% better performance relative to that of the MLP-based baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Weiping2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Weiping2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion
       </h4>
<p style="text-align:left">
        Zheng Weiping<sup>1</sup>, Yi Jiantao<sup>1</sup>, Xing Xiaotao<sup>1</sup>, Liu Xiangtao<sup>2</sup> and Peng Shaohu<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Computer, South China Normal University, Guangzhou, China, <sup>2</sup>Shenzhen Chinasfan Information Technology Co., Ltd., Shenzhen Chinasfan Information Technology Co., Ltd., Shenzhen, China, <sup>3</sup>School of Mechanical and Electrical Engineering,, Guangzhou University, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xing_SCNU_task1_1</span> <span class="label label-primary">Xing_SCNU_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Weiping2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Weiping2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Weiping2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xing_158.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Weiping2017" class="panel-collapse collapse" id="collapse-Weiping2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       Making sense of the environment by sounds is an important re-search in machine learning community. In this work, a Deep Con-volutional Neural Network (DCNN) model is presented to classi-fy acoustic scenes along with a multiple spectrograms fusion method. Firstly, the generations of raw spectrogram and CQT spectrogram are introduced separately. Corresponding features can then be extracted by feeding these spectrogram data into the proposed DCNN model. To fuse these multiple spectrogram fea-tures, two fusing mechanisms, namely the voting and the SVM methods, are designed. By fusing DCNN features of the raw and CQT spectrograms, the accuracy is significantly improved in our experiments, comparing with the single spectrogram schemes. This proves the effectiveness of the proposed multi-spectrograms fusion method.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram, CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote; SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Weiping2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xing_158.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Weiping2017label" class="modal fade" id="bibtex-Weiping2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexWeiping2017label">
        Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Weiping2017,
    Author = "Weiping, Zheng and Jiantao, Yi and Xiaotao, Xing and Xiangtao, Liu and Shaohu, Peng",
    title = "Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "Making sense of the environment by sounds is an important re-search in machine learning community. In this work, a Deep Con-volutional Neural Network (DCNN) model is presented to classi-fy acoustic scenes along with a multiple spectrograms fusion method. Firstly, the generations of raw spectrogram and CQT spectrogram are introduced separately. Corresponding features can then be extracted by feeding these spectrogram data into the proposed DCNN model. To fuse these multiple spectrogram fea-tures, two fusing mechanisms, namely the voting and the SVM methods, are designed. By fusing DCNN features of the raw and CQT spectrograms, the accuracy is significantly improved in our experiments, comparing with the single spectrogram schemes. This proves the effectiveness of the proposed multi-spectrograms fusion method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Xu2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Fusion Model Based on Convolutional Neural Networks with Two Features for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Jinwei Xu, Yang Zhao, jingfei Jiang, Yong Dou, Zhiqiang Liu and Kai Chen
       </p>
<p style="text-align:left">
<em>
         Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xu_NUDT_task1_1</span> <span class="label label-primary">Xu_NUDT_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xu2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xu_131.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2017" class="panel-collapse collapse" id="collapse-Xu2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Fusion Model Based on Convolutional Neural Networks with Two Features for Acoustic Scene Classification
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes two submissions for Task 1 (audio scene classification) of DCASE-2017 challenge of PDL team. We propose two different approaches for Task 1. First, we propose a new convolutional neural network (CNN) architecture trained on frame-level features such as mel-frequency cepstral coefficient (MFCC) of audio data. Second, we propose a late fusion of the proposed CNN trained with two different features, namely, MFCCs and spectrograms. We report the performance of our proposed methods on the cross-validation setup for Task 1 of DCASE-2017 challenge.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         left, right, mixed
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         MFCC, spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xu_131.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2017label" class="modal fade" id="bibtex-Xu2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2017label">
        Fusion Model Based on Convolutional Neural Networks with Two Features for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xu2017,
    Author = "Xu, Jinwei and Zhao, Yang and Jiang, jingfei and Dou, Yong and Liu, Zhiqiang and Chen, Kai",
    title = "Fusion Model Based on Convolutional Neural Networks with Two Features for Acoustic Scene Classification",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report describes two submissions for Task 1 (audio scene classification) of DCASE-2017 challenge of PDL team. We propose two different approaches for Task 1. First, we propose a new convolutional neural network (CNN) architecture trained on frame-level features such as mel-frequency cepstral coefficient (MFCC) of audio data. Second, we propose a late fusion of the proposed CNN trained with two different features, namely, MFCCs and spectrograms. We report the performance of our proposed methods on the cross-validation setup for Task 1 of DCASE-2017 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xu2017a" style="box-shadow: none">
<div class="panel-heading" id="heading-Xu2017a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Using Autoencoder
       </h4>
<p style="text-align:left">
        Xiaoshuo Xu, Xiaoou Chen and Deshun Yang
       </p>
<p style="text-align:left">
<em>
         Institute of Computer Science and Technology, Peking University, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Xu_PKU_task1_1</span> <span class="label label-primary">Xu_PKU_task1_2</span> <span class="label label-primary">Xu_PKU_task1_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xu2017a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xu2017a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xu2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xu_143.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xu2017a" class="panel-collapse collapse" id="collapse-Xu2017a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Using Autoencoder
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes our contribution to the Acoustic Scene Classification (ASC) task of the 2017 IEEE AASP DCASE challenge. We apply an Autoencoder to capture the discriminative information underlying the audio. Then, a Logistic Regression model is employed to recognize different scenes under the compressed representation. In order to boost the performance, we train models based on different channels from the original recordings and simply apply majority voting method on the predictions. Our final system achieves 84.31% on a four-fold cross-validation setting, which outperforms the baseline system by 9.5%.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Autoencoder and Logistic Regression
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xu2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xu_143.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xu2017alabel" class="modal fade" id="bibtex-Xu2017a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXu2017alabel">
        Acoustic Scene Classification Using Autoencoder
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xu2017a,
    Author = "Xu, Xiaoshuo and Chen, Xiaoou and Yang, Deshun",
    title = "Acoustic Scene Classification Using Autoencoder",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report describes our contribution to the Acoustic Scene Classification (ASC) task of the 2017 IEEE AASP DCASE challenge. We apply an Autoencoder to capture the discriminative information underlying the audio. Then, a Logistic Regression model is employed to recognize different scenes under the compressed representation. In order to boost the performance, we train models based on different channels from the original recordings and simply apply majority voting method on the predictions. Our final system achieves 84.31\% on a four-fold cross-validation setting, which outperforms the baseline system by 9.5\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhao2017" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhao2017" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        ADSC Submission for DCASE 2017: Acoustic Scene Classification Using Deep Residual Convolutional Neural Networks
       </h4>
<p style="text-align:left">
        Shengkui Zhao<sup>1</sup>, Thi Ngoc Tho Nguyen<sup>1</sup>, Woon-Seng Gan<sup>2</sup> and Jones Douglas L.<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Illinois at Singapore, Advanced Digital Sciences Center, Singapore, <sup>2</sup>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhao_ADSC_task1_1</span> <span class="label label-primary">Zhao_ADSC_task1_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhao2017" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhao2017" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhao2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Zhao_161.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhao2017" class="panel-collapse collapse" id="collapse-Zhao2017" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       ADSC Submission for DCASE 2017: Acoustic Scene Classification Using Deep Residual Convolutional Neural Networks
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       This report describes our two submissions to the DCASE-2017 challenge for Task 1 (Acoustic scene classification). The first submission is motivated by the superior performance of the deep residual networks for both image and audio classifications. We propose a modified deep residual architecture trained on log-mel spectrogram patches in an end-to-end fashion for acoustic scene classification. We configure the number of layers and kernels for the deep residual nets and find that the modified deep residual net of 34 layers using binaural input features perform well on the DCASE-2017 development dataset. In the second submission, we implement a shallower network that consists of 3 convolutional layers and 2 fully connected layers to benchmark the performance of the residual network. Our two approaches improve the accuracy of the baseline by 10.8% and 10.6% respectively on the 4-fold cross-validation. We suggest that the size of the dataset for Task 1 is relatively small for deep networks to outperform shallower ones.
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         binaural
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         majority vote
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhao2017" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Zhao_161.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhao2017label" class="modal fade" id="bibtex-Zhao2017" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhao2017label">
        ADSC Submission for DCASE 2017: Acoustic Scene Classification Using Deep Residual Convolutional Neural Networks
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhao2017,
    Author = "Zhao, Shengkui and Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Douglas L., Jones",
    title = "{ADSC} Submission for {DCASE} 2017: Acoustic Scene Classification Using Deep Residual Convolutional Neural Networks",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "This report describes our two submissions to the DCASE-2017 challenge for Task 1 (Acoustic scene classification). The first submission is motivated by the superior performance of the deep residual networks for both image and audio classifications. We propose a modified deep residual architecture trained on log-mel spectrogram patches in an end-to-end fashion for acoustic scene classification. We configure the number of layers and kernels for the deep residual nets and find that the modified deep residual net of 34 layers using binaural input features perform well on the DCASE-2017 development dataset. In the second submission, we implement a shallower network that consists of 3 convolutional layers and 2 fully connected layers to benchmark the performance of the residual network. Our two approaches improve the accuracy of the baseline by 10.8\% and 10.6\% respectively on the 4-fold cross-validation. We suggest that the size of the dataset for Task 1 is relatively small for deep networks to outperform shallower ones."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhao2017a" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhao2017a" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A System for 2017 DCASE Challenge Using Deep Sequential Image and Wavelet Features
       </h4>
<p style="text-align:left">
        Ren Zhao<sup>1,2</sup>, Qian Kun<sup>1,2,3</sup>, Pandit Vedhas<sup>1,2</sup>, Zhang Zixing<sup>2</sup>, Yang Zijiang<sup>1,2</sup> and Schuller Björn<sup>1,2,4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>3</sup>MISP group, Technische Universität München, Munich, Germany, <sup>4</sup>Group on Language, Audio and Music, Imperial College London, London, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhao_UAU_UP_task1_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhao2017a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhao2017a" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhao2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Zhao_198.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhao2017a" class="panel-collapse collapse" id="collapse-Zhao2017a" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A System for 2017 DCASE Challenge Using Deep Sequential Image and Wavelet Features
      </h4>
<h5>
       Abstract
      </h5>
<p class="text-justify">
       For the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017), we propose a novel method to classify 15 different acoustic scenes using deep sequential learning for the audio scenes. First, deep representations extracted from the spectrogram and two types of scalograms using Convolutional Neural Networks, the ComparE features and two types of wavelet features are fed into the Gated Recurrent Neural Networks for classification separately. Predictions from the six models are then combined by a margin sampling value strategy. On the official development set of the challenge, the best accuracy on a four-fold cross-validation setup is 83.3%, which increases 8.5% compared with the baseline (p&lt;.001 by one-tailed z-test).
      </p>
<h5>
       System characteristics
      </h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Input
        </td>
<td>
         mono
        </td>
</tr>
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram, scalogram, wavelets, ComParE (openSMILE)
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         GRNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         margin sampling value
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhao2017a" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Zhao_198.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhao2017alabel" class="modal fade" id="bibtex-Zhao2017a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhao2017alabel">
        A System for 2017 DCASE Challenge Using Deep Sequential Image and Wavelet Features
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhao2017a,
    Author = "Zhao, Ren and Kun, Qian and Vedhas, Pandit and Zixing, Zhang and Zijiang, Yang and Björn, Schuller",
    title = "A System for 2017 {DCASE} Challenge Using Deep Sequential Image and Wavelet Features",
    institution = "DCASE2017 Challenge",
    year = "2017",
    month = "September",
    abstract = "For the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017), we propose a novel method to classify 15 different acoustic scenes using deep sequential learning for the audio scenes. First, deep representations extracted from the spectrogram and two types of scalograms using Convolutional Neural Networks, the ComparE features and two types of wavelet features are fed into the Gated Recurrent Neural Networks for classification separately. Predictions from the six models are then combined by a margin sampling value strategy. On the official development set of the challenge, the best accuracy on a four-fold cross-validation setup is 83.3\%, which increases 8.5\% compared with the baseline (p&lt;.001 by one-tailed z-test)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>