<!DOCTYPE html><html lang="en">
<head>
    <title>Low-Complexity Acoustic Scene Classification - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2022/task-low-complexity-acoustic-scene-classification-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The goal of acoustic scene classification is to classify a test recording into one of the predefined ten acoustic scene classes. This targets acoustic scene classification with devices with low computational and memory allowance, which impose certain limits on the model complexity, such as the model’s number …" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2022</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2022/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group  active">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2022/task-low-complexity-acoustic-scene-classification-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-sound-event-detection-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2022/task-automatic-audio-captioning-and-language-based-audio-retrieval" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning-and-language-based-audio-retrieval"><i class="fa fa-info-circle fa-fw"></i>&nbsp;Introduction</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Automatic audio-captioning</strong>
    </li>
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning"><i class="fa dc-captioning fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Language-Based Audio Retrieval</strong>
    </li>
            <li class="">
        <a href="/challenge2022/task-language-based-audio-retrieval"><i class="fa fa-file-text fa-fw"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2022/task-language-based-audio-retrieval-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2022/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2022/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/grid-05.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-primary"></i><i class="fa dc-scene fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text">Scenes</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 1</span></span><img src="../images/logos/dcase/dcase2022_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Low-Complexity Acoustic Scene Classification</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#system-complexity">System complexity</a></li>
<li><a href="#generalization-performance">Generalization performance</a></li>
<li><a href="#class-wise-performance">Class-wise performance</a>
<ul>
<li><a href="#log-loss">Log loss</a></li>
<li><a href="#accuracy">Accuracy</a></li>
</ul>
</li>
<li><a href="#device-wise-performance">Device-wise performance</a>
<ul>
<li><a href="#log-loss-1">Log loss</a></li>
<li><a href="#accuracy-1">Accuracy</a></li>
</ul>
</li>
<li><a href="#system-characteristics">System characteristics</a>
<ul>
<li><a href="#general-characteristics">General characteristics</a></li>
<li><a href="#machine-learning-characteristics">Machine learning characteristics</a></li>
</ul>
</li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The goal of acoustic scene classification is to classify a test recording into one of the predefined ten acoustic scene classes. This targets acoustic scene classification with devices with low computational and memory allowance, which impose certain limits on the model complexity, such as the model’s number of parameters and the multiply-accumulate operations count. In addition to low-complexity, the aim is generalization across a number of different devices. For this purpose, the task will use audio data recorded and simulated with a variety of devices. </p>
<p>The development dataset consists of recordings from 10 European cities using 9 different devices: 3 real devices (A, B, C) and 6 simulated devices (S1-S6). Data from devices B, C, and S1-S6 consists of randomly selected segments from the simultaneous recordings, therefore all overlap with the data from device A, but not necessarily with each other. The total amount of audio in the development set is 64 hours. </p>
<p>The evaluation dataset contains data from 12 cities, 10 acoustic scenes, 11 devices. There are five new devices (not available in the development set): real device D and simulated devices S7-S11. Evaluation data contains 22 hours of audio. </p>
<p>The device A consists in a Soundman OKM II Klassik/studio A3, electret binaural microphone and a Zoom F8 audio recorder using 48kHz sampling rate and 24-bit resolution. The other devices are commonly available customer devices: device B is a Samsung Galaxy S7, device C is iPhone SE, and device D is a GoPro Hero5 Session. </p>
<p>More detailed task description can be found in the <a class="btn btn-primary" href="/challenge2022/task-low-complexity-acoustic-scene-classification" style="">task description page</a></p>
<h1 id="systems-ranking">Systems ranking</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="logloss_dev" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="3">Submission information</th>
<th class="sep-left-cell text-center" colspan="3">Evaluation dataset</th>
<th class="sep-left-cell text-center" colspan="2">Development dataset</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval_confidence" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
                Logloss <br/><small class="text-muted">with 95% confidence interval</small><small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small><small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss (Development dataset)" data-chartable="true" data-field="logloss_dev" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Development dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Development dataset)" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> (Development dataset)</small>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>AI4EDGE_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414 (2.264 - 2.564)</td>
<td>47.0 (46.7 - 47.3)</td>
<td>0.742</td>
<td>75.6</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>AI4EDGE_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365 (2.226 - 2.504)</td>
<td>46.7 (46.4 - 46.9)</td>
<td>0.791</td>
<td>73.5</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>AI4EDGE_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398 (1.343 - 1.454)</td>
<td>49.4 (49.1 - 49.7)</td>
<td>1.347</td>
<td>50.5</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>AI4EDGE_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330 (1.281 - 1.378)</td>
<td>51.6 (51.3 - 51.9)</td>
<td>1.103</td>
<td>60.5</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>AIT_Essex</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636 (1.535 - 1.737)</td>
<td>53.0 (52.7 - 53.3)</td>
<td>1.719</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>AIT_Essex</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787 (1.680 - 1.894)</td>
<td>51.9 (51.6 - 52.2)</td>
<td></td>
<td>51.4</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>AIT_Essex</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808 (1.689 - 1.928)</td>
<td>55.2 (55.0 - 55.5)</td>
<td>1.306</td>
<td>60.1</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>DW_S</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515 (1.454 - 1.575)</td>
<td>47.8 (47.5 - 48.0)</td>
<td>1.578</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>DW</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580 (1.519 - 1.642)</td>
<td>46.4 (46.1 - 46.7)</td>
<td>1.551</td>
<td>45.6</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>DW_AUG_S</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635 (1.566 - 1.704)</td>
<td>45.2 (44.9 - 45.5)</td>
<td>1.437</td>
<td>48.3</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>DW_AUG</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564 (1.501 - 1.627)</td>
<td>48.0 (47.7 - 48.3)</td>
<td>1.327</td>
<td>49.3</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>KDResCG</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795 (2.623 - 2.967)</td>
<td>48.7 (48.4 - 48.9)</td>
<td>1.441</td>
<td>51.1</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>JH_PM_HYU1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147 (1.081 - 1.214)</td>
<td>60.8 (60.6 - 61.1)</td>
<td>0.835</td>
<td>70.1</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>JH_PM_HYU2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187 (1.125 - 1.249)</td>
<td>59.2 (58.9 - 59.5)</td>
<td>1.065</td>
<td>62.6</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>JH_PM_HYU3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190 (1.130 - 1.251)</td>
<td>59.4 (59.1 - 59.6)</td>
<td>1.005</td>
<td>64.9</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>JH_PM_HYU4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187 (1.126 - 1.248)</td>
<td>59.3 (59.1 - 59.6)</td>
<td>1.072</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong1_NCUT</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568 (1.512 - 1.623)</td>
<td>48.0 (47.7 - 48.3)</td>
<td>1.378</td>
<td>53.9</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Houyb_XDU</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481 (1.416 - 1.547)</td>
<td>49.3 (49.0 - 49.5)</td>
<td>1.449</td>
<td>49.7</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>BC-ResNet1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934 (1.830 - 2.038)</td>
<td>41.3 (41.0 - 41.5)</td>
<td>1.263</td>
<td>53.8</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>BC-ResNet2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916 (2.751 - 3.081)</td>
<td>29.9 (29.6 - 30.2)</td>
<td>1.267</td>
<td>55.9</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>BC-ResNet3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701 (2.566 - 2.836)</td>
<td>28.5 (28.2 - 28.7)</td>
<td>1.236</td>
<td>56.2</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>MobileNet2</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612 (1.560 - 1.663)</td>
<td>44.1 (43.8 - 44.4)</td>
<td>1.556</td>
<td>45.9</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td>Baseline</td>
<td></td>
<td></td>
<td>1.532 (1.490 - 1.574)</td>
<td>44.2 (44.0 - 44.5)</td>
<td>1.575</td>
<td>42.9</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>jku_stu_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339 (1.278 - 1.399)</td>
<td>53.8 (53.5 - 54.1)</td>
<td>1.279</td>
<td>53.3</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>jku_stu_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355 (1.296 - 1.414)</td>
<td>53.0 (52.7 - 53.2)</td>
<td>1.280</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>jku_stu_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320 (1.256 - 1.383)</td>
<td>54.7 (54.4 - 55.0)</td>
<td>1.291</td>
<td>52.8</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>jku_stu_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311 (1.253 - 1.369)</td>
<td>54.5 (54.2 - 54.8)</td>
<td>1.288</td>
<td>52.7</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>DepSepConv</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055 (1.991 - 2.119)</td>
<td>36.4 (36.1 - 36.6)</td>
<td>1.878</td>
<td>39.0</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>MConvNet</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504 (1.431 - 1.576)</td>
<td>51.7 (51.4 - 52.0)</td>
<td>1.259</td>
<td>54.0</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>MConvNet</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431 (1.364 - 1.498)</td>
<td>52.7 (52.4 - 53.0)</td>
<td>1.259</td>
<td>54.0</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>t10sec</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092 (1.043 - 1.141)</td>
<td>59.7 (59.5 - 60.0)</td>
<td>1.115</td>
<td>58.6</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>mixstyleR8</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105 (1.057 - 1.153)</td>
<td>59.6 (59.3 - 59.9)</td>
<td>1.110</td>
<td>59.1</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>mixstyleR5</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091 (1.040 - 1.141)</td>
<td>59.6 (59.4 - 59.9)</td>
<td>1.139</td>
<td>58.0</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>audiosetR5</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102 (1.054 - 1.151)</td>
<td>59.4 (59.1 - 59.7)</td>
<td>1.163</td>
<td>57.6</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>final</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731 (1.657 - 1.805)</td>
<td>47.5 (47.2 - 47.8)</td>
<td>1.581</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Surrey_4M</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565 (1.508 - 1.623)</td>
<td>44.6 (44.3 - 44.9)</td>
<td>1.449</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Surrey_5M</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606 (1.547 - 1.664)</td>
<td>44.3 (44.1 - 44.6)</td>
<td>1.475</td>
<td>45.9</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Surrey_19M</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492 (1.441 - 1.544)</td>
<td>45.9 (45.6 - 46.2)</td>
<td>1.392</td>
<td>47.3</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Surrey_20M</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499 (1.447 - 1.551)</td>
<td>45.9 (45.6 - 46.2)</td>
<td>1.389</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>RION1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405 (1.337 - 1.473)</td>
<td>51.5 (51.2 - 51.7)</td>
<td>1.199</td>
<td>56.3</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>RION2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389 (1.325 - 1.454)</td>
<td>51.6 (51.3 - 51.9)</td>
<td>1.179</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>RION3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366 (1.305 - 1.426)</td>
<td>51.7 (51.4 - 51.9)</td>
<td>1.182</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>RION4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397 (1.328 - 1.466)</td>
<td>52.7 (52.5 - 53.0)</td>
<td>1.214</td>
<td>57.1</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>YLSSD</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456 (1.409 - 1.504)</td>
<td>46.2 (46.0 - 46.5)</td>
<td>1.305</td>
<td>51.7</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Conv_Sep_CNN_48</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709 (2.517 - 2.901)</td>
<td>43.8 (43.6 - 44.1)</td>
<td>1.440</td>
<td>50.6</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Conv_Sep_CNN_48</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904 (2.690 - 3.118)</td>
<td>41.9 (41.7 - 42.2)</td>
<td>1.440</td>
<td>50.6</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>THUEE</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096 (1.913 - 2.280)</td>
<td>54.9 (54.7 - 55.2)</td>
<td>1.360</td>
<td>54.1</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>THUEE</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068 (2.775 - 3.361)</td>
<td>54.4 (54.1 - 54.7)</td>
<td>1.360</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>SepCNN</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442 (1.362 - 1.521)</td>
<td>56.3 (56.0 - 56.6)</td>
<td>1.295</td>
<td>60.3</td>
</tr>
</tbody>
</table>
<h1 id="teams-ranking">Teams ranking</h1>
<p>Table including only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="bar" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="logloss_dev" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="logloss_eval_confidence" data-sort-order="desc">
<thead>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="3">Submission information</th>
<th class="sep-left-cell text-center" colspan="4">Evaluation dataset</th>
<th class="sep-left-cell text-center" colspan="2">Development dataset</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sm-cell" data-field="name" data-sortable="true">
                Name
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system rank
            </th>
<th class="text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_team" data-sortable="true" data-value-type="int">
                Team rank
            </th>
<th class="sep-left-cell text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval_confidence" data-reversed="true" data-sortable="true" data-value-type="float3-interval-muted">
                Logloss<small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval_confidence" data-sortable="true" data-value-type="float1-percentage-interval-muted">
                Accuracy <br/><small class="text-muted">with 95% confidence interval</small><small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss (Development dataset)" data-chartable="true" data-field="logloss_dev" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Development dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Development dataset)" data-chartable="true" data-field="accuracy_dev" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> (Development dataset)</small>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>AI4EDGE_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>4</td>
<td>1.330 (1.281 - 1.378)</td>
<td>51.6 (51.3 - 51.9)</td>
<td>1.103</td>
<td>60.5</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>AIT_Essex</td>
<td>Pham2022</td>
<td>34</td>
<td>14</td>
<td>1.636 (1.535 - 1.737)</td>
<td>53.0 (52.7 - 53.3)</td>
<td>1.719</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>DW_S</td>
<td>Cai2022</td>
<td>26</td>
<td>11</td>
<td>1.515 (1.454 - 1.575)</td>
<td>47.8 (47.5 - 48.0)</td>
<td>1.578</td>
<td>46.2</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>KDResCG</td>
<td>Cao2022</td>
<td>45</td>
<td>19</td>
<td>2.795 (2.623 - 2.967)</td>
<td>48.7 (48.4 - 48.9)</td>
<td>1.441</td>
<td>51.1</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>JH_PM_HYU1</td>
<td>Lee2022</td>
<td>5</td>
<td>2</td>
<td>1.147 (1.081 - 1.214)</td>
<td>60.8 (60.6 - 61.1)</td>
<td>0.835</td>
<td>70.1</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong1_NCUT</td>
<td>Dong2022</td>
<td>29</td>
<td>12</td>
<td>1.568 (1.512 - 1.623)</td>
<td>48.0 (47.7 - 48.3)</td>
<td>1.378</td>
<td>53.9</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Houyb_XDU</td>
<td>Hou2022</td>
<td>22</td>
<td>9</td>
<td>1.481 (1.416 - 1.547)</td>
<td>49.3 (49.0 - 49.5)</td>
<td>1.449</td>
<td>49.7</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>MobileNet2</td>
<td>Liang2022</td>
<td>32</td>
<td>13</td>
<td>1.612 (1.560 - 1.663)</td>
<td>44.1 (43.8 - 44.4)</td>
<td>1.556</td>
<td>45.9</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td>Baseline</td>
<td></td>
<td></td>
<td></td>
<td>1.532 (1.490 - 1.574)</td>
<td>44.2 (44.0 - 44.5)</td>
<td>1.575</td>
<td>42.9</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>jku_stu_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>3</td>
<td>1.311 (1.253 - 1.369)</td>
<td>54.5 (54.2 - 54.8)</td>
<td>1.288</td>
<td>52.7</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>DepSepConv</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>16</td>
<td>2.055 (1.991 - 2.119)</td>
<td>36.4 (36.1 - 36.6)</td>
<td>1.878</td>
<td>39.0</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>MConvNet</td>
<td>Kim2022</td>
<td>19</td>
<td>6</td>
<td>1.431 (1.364 - 1.498)</td>
<td>52.7 (52.4 - 53.0)</td>
<td>1.259</td>
<td>54.0</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>mixstyleR5</td>
<td>Schmid2022</td>
<td>1</td>
<td>1</td>
<td>1.091 (1.040 - 1.141)</td>
<td>59.6 (59.4 - 59.9)</td>
<td>1.139</td>
<td>58.0</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>final</td>
<td>Schmidt2022</td>
<td>35</td>
<td>15</td>
<td>1.731 (1.657 - 1.805)</td>
<td>47.5 (47.2 - 47.8)</td>
<td>1.581</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Surrey_19M</td>
<td>Singh2022</td>
<td>23</td>
<td>10</td>
<td>1.492 (1.441 - 1.544)</td>
<td>45.9 (45.6 - 46.2)</td>
<td>1.392</td>
<td>47.3</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>RION3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>5</td>
<td>1.366 (1.305 - 1.426)</td>
<td>51.7 (51.4 - 51.9)</td>
<td>1.182</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>YLSSD</td>
<td>Yu2022</td>
<td>21</td>
<td>8</td>
<td>1.456 (1.409 - 1.504)</td>
<td>46.2 (46.0 - 46.5)</td>
<td>1.305</td>
<td>51.7</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Conv_Sep_CNN_48</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>18</td>
<td>2.709 (2.517 - 2.901)</td>
<td>43.8 (43.6 - 44.1)</td>
<td>1.440</td>
<td>50.6</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>THUEE</td>
<td>Shao2022</td>
<td>40</td>
<td>17</td>
<td>2.096 (1.913 - 2.280)</td>
<td>54.9 (54.7 - 55.2)</td>
<td>1.360</td>
<td>54.1</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>SepCNN</td>
<td>Xin2022</td>
<td>20</td>
<td>7</td>
<td>1.442 (1.362 - 1.521)</td>
<td>56.3 (56.0 - 56.6)</td>
<td>1.295</td>
<td>60.3</td>
</tr>
</tbody>
</table>
<h1 id="system-complexity">System complexity</h1>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="system_complexity_total" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="2">Submission information</th>
<th class="sep-left-cell text-center" colspan="3">Evaluation dataset</th>
<th class="sep-left-cell text-center" colspan="5">Acoustic model</th>
<th class="sep-left-cell text-center">System</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Eval)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Eval)" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> (Eval)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="MACS" data-chartable="true" data-field="system_complexity_macs" data-reversed="true" data-sortable="true" data-value-type="numeric-unit">
                MACS
            </th>
<th class="sep-left-cell text-center" data-axis-label="Memory use" data-chartable="true" data-field="system_complexity_memory_use" data-reversed="true" data-sortable="true" data-value-type="numeric-unit">
                Memory use
            </th>
<th class="sep-left-cell text-center" data-axis-label="Parameters" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total" data-reversed="true" data-sortable="true" data-value-type="numeric-unit">
                Parameters
            </th>
<th class="text-center" data-axis-label="Non-zero parameters" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total_non_zero" data-reversed="true" data-sortable="true" data-value-type="numeric-unit">
                Non-zero <br/>parameters  
            </th>
<th class="text-center" data-axis-label="Sparsity" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_sparsity" data-reversed="true" data-sortable="true" data-value-type="numeric-unit">
                Sparsity
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_complexity_management" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Complexity <br/>management
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414</td>
<td>47.0</td>
<td>21127552</td>
<td>70612</td>
<td>68918</td>
<td>68918</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365</td>
<td>46.7</td>
<td>21127552</td>
<td>70612</td>
<td>68918</td>
<td>68918</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398</td>
<td>49.4</td>
<td>25475456</td>
<td>52852</td>
<td>51986</td>
<td>51986</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330</td>
<td>51.6</td>
<td>25475456</td>
<td>52852</td>
<td>51986</td>
<td>51986</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636</td>
<td>53.0</td>
<td>900000</td>
<td>33822</td>
<td>33822</td>
<td>32382</td>
<td>0.042575838211814765</td>
<td>channel restriction, decomposed convolution, quantization</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787</td>
<td>51.9</td>
<td>750000</td>
<td>31902</td>
<td>31902</td>
<td>30558</td>
<td>0.04212902012413011</td>
<td>channel restriction, decomposed convolution, quantization</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808</td>
<td>55.2</td>
<td>900000</td>
<td>115998</td>
<td>115998</td>
<td>113118</td>
<td>0.024828014276108257</td>
<td>channel restriction, decomposed convolution, quantization</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515</td>
<td>47.8</td>
<td>6287030</td>
<td></td>
<td>25526</td>
<td>25526</td>
<td>0.0</td>
<td>group convolution</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580</td>
<td>46.4</td>
<td>6287030</td>
<td></td>
<td>25526</td>
<td>25526</td>
<td>0.0</td>
<td>group convolution</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635</td>
<td>45.2</td>
<td>7337718</td>
<td></td>
<td>35926</td>
<td>35926</td>
<td>0.0</td>
<td>group convolution</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564</td>
<td>48.0</td>
<td>7337718</td>
<td></td>
<td>35926</td>
<td>35926</td>
<td>0.0</td>
<td>group convolution</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795</td>
<td>48.7</td>
<td>8637250</td>
<td>125330</td>
<td>125330</td>
<td>125330</td>
<td>0.0</td>
<td>weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147</td>
<td>60.8</td>
<td>26763000</td>
<td></td>
<td>126580</td>
<td>126580</td>
<td>0.0</td>
<td>weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187</td>
<td>59.2</td>
<td>26763000</td>
<td></td>
<td>126580</td>
<td>126580</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190</td>
<td>59.4</td>
<td>26763000</td>
<td></td>
<td>126580</td>
<td>126580</td>
<td>0.0</td>
<td>weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187</td>
<td>59.3</td>
<td>26763000</td>
<td></td>
<td>126580</td>
<td>126580</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568</td>
<td>48.0</td>
<td>28461216</td>
<td>540672</td>
<td>70608</td>
<td>70608</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481</td>
<td>49.3</td>
<td>28513000</td>
<td>78140</td>
<td>57957</td>
<td>57957</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934</td>
<td>41.3</td>
<td>20500000</td>
<td>85800</td>
<td>85800</td>
<td>85800</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916</td>
<td>29.9</td>
<td>20500000</td>
<td>85800</td>
<td>85800</td>
<td>85800</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701</td>
<td>28.5</td>
<td>20500000</td>
<td>85800</td>
<td>85800</td>
<td>85800</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612</td>
<td>44.1</td>
<td>11186000</td>
<td>110452</td>
<td>110452</td>
<td>110452</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>1.532</td>
<td>44.2</td>
<td>29234920</td>
<td>65280</td>
<td>46512</td>
<td>46512</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339</td>
<td>53.8</td>
<td>29325000</td>
<td>3510000</td>
<td>65790</td>
<td>65790</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355</td>
<td>53.0</td>
<td>29325000</td>
<td>3510000</td>
<td>65790</td>
<td>65790</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320</td>
<td>54.7</td>
<td>29325000</td>
<td>3510000</td>
<td>65790</td>
<td>65790</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311</td>
<td>54.5</td>
<td>29325000</td>
<td>3510000</td>
<td>65790</td>
<td>65790</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055</td>
<td>36.4</td>
<td>3283692</td>
<td>65280</td>
<td>96473</td>
<td>96473</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504</td>
<td>51.7</td>
<td>29481000</td>
<td>262319</td>
<td>113378</td>
<td>113378</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431</td>
<td>52.7</td>
<td>29481000</td>
<td>262319</td>
<td>113378</td>
<td>113378</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092</td>
<td>59.7</td>
<td>29056324</td>
<td></td>
<td>127046</td>
<td>127046</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105</td>
<td>59.6</td>
<td>29056324</td>
<td></td>
<td>127046</td>
<td>127046</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091</td>
<td>59.6</td>
<td>28240924</td>
<td></td>
<td>121610</td>
<td>121610</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102</td>
<td>59.4</td>
<td>28240924</td>
<td></td>
<td>121610</td>
<td>121610</td>
<td>0.0</td>
<td>knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731</td>
<td>47.5</td>
<td>15163468</td>
<td>5288960</td>
<td>127943</td>
<td>127943</td>
<td>0.0</td>
<td>weight quantization, structured filter pruning</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565</td>
<td>44.6</td>
<td>4129320</td>
<td>261120</td>
<td>13138</td>
<td>13138</td>
<td>0.0</td>
<td>weight quantization, pruning</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606</td>
<td>44.3</td>
<td>5404520</td>
<td>261120</td>
<td>14886</td>
<td>14886</td>
<td>0.0</td>
<td>weight quantization, pruning</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492</td>
<td>45.9</td>
<td>18585480</td>
<td>261120</td>
<td>59570</td>
<td>59570</td>
<td>0.0</td>
<td>weight quantization, pruning</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499</td>
<td>45.9</td>
<td>19831880</td>
<td>261120</td>
<td>60958</td>
<td>60958</td>
<td>0.0</td>
<td>weight quantization, pruning</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405</td>
<td>51.5</td>
<td>26607000</td>
<td></td>
<td>120229</td>
<td>120229</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389</td>
<td>51.6</td>
<td>26607000</td>
<td></td>
<td>120229</td>
<td>120229</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366</td>
<td>51.7</td>
<td>26607000</td>
<td></td>
<td>120229</td>
<td>120229</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397</td>
<td>52.7</td>
<td>26610000</td>
<td></td>
<td>123346</td>
<td>123346</td>
<td>0.0</td>
<td>weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456</td>
<td>46.2</td>
<td>16081000</td>
<td>5934</td>
<td>6306</td>
<td>6306</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709</td>
<td>43.8</td>
<td>28570080</td>
<td>1253376</td>
<td>28320</td>
<td>28320</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904</td>
<td>41.9</td>
<td>28570080</td>
<td>1253376</td>
<td>28320</td>
<td>28320</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096</td>
<td>54.9</td>
<td>28228320</td>
<td>2322560</td>
<td>127160</td>
<td>127160</td>
<td>0.0</td>
<td>pruning, weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068</td>
<td>54.4</td>
<td>28098645</td>
<td>2322560</td>
<td>126078</td>
<td>126078</td>
<td>0.0</td>
<td>pruning, weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442</td>
<td>56.3</td>
<td>28823618</td>
<td>68140</td>
<td>75562</td>
<td>75562</td>
<td>0.0</td>
<td>weight quantization</td>
</tr>
</tbody>
</table>
<p><br/></p>
<h1 id="generalization-performance">Generalization performance</h1>
<p>All results with evaluation dataset.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_source_seen" data-scatter-y="accuracy_eval_source_unseen" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="2">Submission information</th>
<th class="sep-left-cell text-center" colspan="3">Overall</th>
<th class="sep-left-cell text-center" colspan="4">Devices</th>
<th class="sep-left-cell text-center" colspan="4">Cities</th>
</tr>
<tr>
<th></th>
<th class="sep-left-cell text-center" colspan="2"></th>
<th class="sep-left-cell text-center" colspan="3">Evaluation dataset</th>
<th class="sep-left-cell text-center" colspan="2">Unseen</th>
<th class="sep-left-cell text-center" colspan="2">Seen</th>
<th class="sep-left-cell text-center" colspan="2">Unseen</th>
<th class="sep-left-cell text-center" colspan="2">Seen</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy (Evaluation dataset)" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss / unseen devices" data-chartable="true" data-field="logloss_eval_source_unseen" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> / <br/>unseen devices (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy / unseen devices" data-chartable="true" data-field="accuracy_eval_source_unseen" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> / <br/>unseen devices (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss / seen devices" data-chartable="true" data-field="logloss_eval_source_seen" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> / <br/>seen devices (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy / seen devices" data-chartable="true" data-field="accuracy_eval_source_seen" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> / <br/>seen devices (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss / unseen cities" data-chartable="true" data-field="logloss_eval_city_unseen" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> / <br/>unseen cities (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy / unseen cities" data-chartable="true" data-field="accuracy_eval_city_unseen" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> / <br/>unseen cities (Evaluation dataset)</small>
</th>
<th class="sep-left-cell text-center" data-axis-label="Logloss / seen cities" data-chartable="true" data-field="logloss_eval_city_seen" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss<small class="hidden"> / <br/>seen cities (Evaluation dataset)</small>
</th>
<th class="text-center" data-axis-label="Accuracy / seen cities" data-chartable="true" data-field="accuracy_eval_city_seen" data-sortable="true" data-value-type="float1-percentage">
                Accuracy<small class="hidden"> / <br/>seen cities (Evaluation dataset)</small>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414</td>
<td>47.0</td>
<td>2.820</td>
<td>41.6</td>
<td>2.076</td>
<td>51.5</td>
<td>2.218</td>
<td>48.8</td>
<td>2.447</td>
<td>46.8</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365</td>
<td>46.7</td>
<td>2.923</td>
<td>41.2</td>
<td>1.900</td>
<td>51.2</td>
<td>2.211</td>
<td>48.2</td>
<td>2.390</td>
<td>46.7</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398</td>
<td>49.4</td>
<td>1.588</td>
<td>42.5</td>
<td>1.240</td>
<td>55.1</td>
<td>1.337</td>
<td>50.2</td>
<td>1.405</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330</td>
<td>51.6</td>
<td>1.467</td>
<td>46.5</td>
<td>1.215</td>
<td>55.9</td>
<td>1.268</td>
<td>53.0</td>
<td>1.336</td>
<td>51.5</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636</td>
<td>53.0</td>
<td>1.806</td>
<td>50.1</td>
<td>1.494</td>
<td>55.4</td>
<td>1.631</td>
<td>51.5</td>
<td>1.642</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787</td>
<td>51.9</td>
<td>2.138</td>
<td>47.2</td>
<td>1.494</td>
<td>55.8</td>
<td>1.810</td>
<td>50.0</td>
<td>1.792</td>
<td>52.5</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808</td>
<td>55.2</td>
<td>2.258</td>
<td>49.9</td>
<td>1.434</td>
<td>59.7</td>
<td>1.837</td>
<td>53.6</td>
<td>1.805</td>
<td>55.7</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515</td>
<td>47.8</td>
<td>1.847</td>
<td>40.7</td>
<td>1.238</td>
<td>53.7</td>
<td>1.553</td>
<td>45.1</td>
<td>1.500</td>
<td>48.7</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580</td>
<td>46.4</td>
<td>1.920</td>
<td>40.2</td>
<td>1.297</td>
<td>51.6</td>
<td>1.611</td>
<td>44.3</td>
<td>1.567</td>
<td>47.0</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635</td>
<td>45.2</td>
<td>2.059</td>
<td>36.6</td>
<td>1.282</td>
<td>52.3</td>
<td>1.674</td>
<td>42.8</td>
<td>1.617</td>
<td>46.0</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564</td>
<td>48.0</td>
<td>1.916</td>
<td>41.9</td>
<td>1.270</td>
<td>53.1</td>
<td>1.631</td>
<td>46.0</td>
<td>1.546</td>
<td>48.7</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795</td>
<td>48.7</td>
<td>3.746</td>
<td>44.0</td>
<td>2.003</td>
<td>52.5</td>
<td>2.926</td>
<td>47.6</td>
<td>2.775</td>
<td>48.8</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147</td>
<td>60.8</td>
<td>1.377</td>
<td>55.1</td>
<td>0.956</td>
<td>65.6</td>
<td>1.114</td>
<td>60.6</td>
<td>1.153</td>
<td>60.9</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187</td>
<td>59.2</td>
<td>1.426</td>
<td>52.0</td>
<td>0.987</td>
<td>65.2</td>
<td>1.210</td>
<td>58.1</td>
<td>1.175</td>
<td>59.9</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190</td>
<td>59.4</td>
<td>1.428</td>
<td>52.6</td>
<td>0.992</td>
<td>65.0</td>
<td>1.224</td>
<td>57.6</td>
<td>1.183</td>
<td>60.0</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187</td>
<td>59.3</td>
<td>1.433</td>
<td>51.9</td>
<td>0.982</td>
<td>65.5</td>
<td>1.207</td>
<td>58.4</td>
<td>1.176</td>
<td>60.0</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568</td>
<td>48.0</td>
<td>1.872</td>
<td>38.8</td>
<td>1.314</td>
<td>55.6</td>
<td>1.638</td>
<td>45.2</td>
<td>1.555</td>
<td>48.8</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481</td>
<td>49.3</td>
<td>1.740</td>
<td>42.8</td>
<td>1.265</td>
<td>54.6</td>
<td>1.547</td>
<td>47.1</td>
<td>1.468</td>
<td>49.9</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934</td>
<td>41.3</td>
<td>2.289</td>
<td>36.2</td>
<td>1.637</td>
<td>45.5</td>
<td>1.919</td>
<td>42.4</td>
<td>1.944</td>
<td>41.1</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916</td>
<td>29.9</td>
<td>3.346</td>
<td>26.4</td>
<td>2.557</td>
<td>32.9</td>
<td>2.928</td>
<td>30.6</td>
<td>2.929</td>
<td>29.7</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701</td>
<td>28.5</td>
<td>3.063</td>
<td>24.6</td>
<td>2.400</td>
<td>31.7</td>
<td>2.670</td>
<td>29.2</td>
<td>2.709</td>
<td>28.3</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612</td>
<td>44.1</td>
<td>1.690</td>
<td>41.8</td>
<td>1.546</td>
<td>46.1</td>
<td>1.654</td>
<td>43.3</td>
<td>1.608</td>
<td>44.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>1.532</td>
<td>44.2</td>
<td>1.725</td>
<td>38.1</td>
<td>1.372</td>
<td>49.4</td>
<td>1.552</td>
<td>43.4</td>
<td>1.530</td>
<td>44.7</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339</td>
<td>53.8</td>
<td>1.509</td>
<td>48.6</td>
<td>1.197</td>
<td>58.1</td>
<td>1.328</td>
<td>54.3</td>
<td>1.338</td>
<td>53.9</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355</td>
<td>53.0</td>
<td>1.512</td>
<td>47.8</td>
<td>1.224</td>
<td>57.3</td>
<td>1.342</td>
<td>53.7</td>
<td>1.360</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320</td>
<td>54.7</td>
<td>1.508</td>
<td>48.8</td>
<td>1.162</td>
<td>59.5</td>
<td>1.310</td>
<td>54.7</td>
<td>1.321</td>
<td>54.9</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311</td>
<td>54.5</td>
<td>1.480</td>
<td>48.8</td>
<td>1.170</td>
<td>59.2</td>
<td>1.302</td>
<td>55.4</td>
<td>1.311</td>
<td>54.5</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055</td>
<td>36.4</td>
<td>2.515</td>
<td>28.4</td>
<td>1.671</td>
<td>43.0</td>
<td>2.180</td>
<td>32.8</td>
<td>2.021</td>
<td>37.3</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504</td>
<td>51.7</td>
<td>1.768</td>
<td>46.3</td>
<td>1.284</td>
<td>56.1</td>
<td>1.551</td>
<td>49.8</td>
<td>1.487</td>
<td>52.3</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431</td>
<td>52.7</td>
<td>1.624</td>
<td>48.4</td>
<td>1.270</td>
<td>56.2</td>
<td>1.442</td>
<td>51.9</td>
<td>1.417</td>
<td>53.2</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092</td>
<td>59.7</td>
<td>1.236</td>
<td>54.5</td>
<td>0.972</td>
<td>64.1</td>
<td>1.122</td>
<td>58.3</td>
<td>1.085</td>
<td>60.3</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105</td>
<td>59.6</td>
<td>1.218</td>
<td>55.3</td>
<td>1.011</td>
<td>63.2</td>
<td>1.126</td>
<td>58.6</td>
<td>1.103</td>
<td>59.9</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091</td>
<td>59.6</td>
<td>1.231</td>
<td>54.8</td>
<td>0.974</td>
<td>63.7</td>
<td>1.113</td>
<td>58.9</td>
<td>1.087</td>
<td>60.1</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102</td>
<td>59.4</td>
<td>1.229</td>
<td>54.9</td>
<td>0.997</td>
<td>63.1</td>
<td>1.129</td>
<td>58.4</td>
<td>1.097</td>
<td>59.7</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731</td>
<td>47.5</td>
<td>2.139</td>
<td>40.7</td>
<td>1.390</td>
<td>53.2</td>
<td>1.775</td>
<td>46.7</td>
<td>1.720</td>
<td>47.9</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565</td>
<td>44.6</td>
<td>1.835</td>
<td>37.8</td>
<td>1.341</td>
<td>50.3</td>
<td>1.640</td>
<td>42.6</td>
<td>1.545</td>
<td>45.1</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606</td>
<td>44.3</td>
<td>1.898</td>
<td>37.8</td>
<td>1.362</td>
<td>49.8</td>
<td>1.672</td>
<td>43.1</td>
<td>1.585</td>
<td>45.0</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492</td>
<td>45.9</td>
<td>1.728</td>
<td>39.1</td>
<td>1.296</td>
<td>51.5</td>
<td>1.532</td>
<td>44.3</td>
<td>1.480</td>
<td>46.5</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499</td>
<td>45.9</td>
<td>1.754</td>
<td>38.8</td>
<td>1.287</td>
<td>51.9</td>
<td>1.540</td>
<td>44.5</td>
<td>1.486</td>
<td>46.6</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405</td>
<td>51.5</td>
<td>1.576</td>
<td>46.9</td>
<td>1.261</td>
<td>55.3</td>
<td>1.444</td>
<td>49.4</td>
<td>1.401</td>
<td>51.9</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389</td>
<td>51.6</td>
<td>1.534</td>
<td>47.8</td>
<td>1.269</td>
<td>54.8</td>
<td>1.431</td>
<td>49.6</td>
<td>1.388</td>
<td>52.1</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366</td>
<td>51.7</td>
<td>1.496</td>
<td>47.9</td>
<td>1.257</td>
<td>54.8</td>
<td>1.405</td>
<td>49.6</td>
<td>1.364</td>
<td>52.1</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397</td>
<td>52.7</td>
<td>1.691</td>
<td>46.8</td>
<td>1.152</td>
<td>57.7</td>
<td>1.463</td>
<td>50.0</td>
<td>1.379</td>
<td>53.6</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456</td>
<td>46.2</td>
<td>1.619</td>
<td>40.6</td>
<td>1.321</td>
<td>51.0</td>
<td>1.473</td>
<td>46.0</td>
<td>1.454</td>
<td>46.5</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709</td>
<td>43.8</td>
<td>3.181</td>
<td>40.1</td>
<td>2.315</td>
<td>47.0</td>
<td>2.665</td>
<td>43.4</td>
<td>2.733</td>
<td>44.0</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904</td>
<td>41.9</td>
<td>3.254</td>
<td>38.8</td>
<td>2.613</td>
<td>44.6</td>
<td>2.824</td>
<td>42.1</td>
<td>2.930</td>
<td>42.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096</td>
<td>54.9</td>
<td>2.435</td>
<td>47.0</td>
<td>1.814</td>
<td>61.5</td>
<td>2.293</td>
<td>53.1</td>
<td>2.056</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068</td>
<td>54.4</td>
<td>4.008</td>
<td>45.7</td>
<td>2.284</td>
<td>61.6</td>
<td>3.149</td>
<td>51.5</td>
<td>2.986</td>
<td>55.2</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442</td>
<td>56.3</td>
<td>1.842</td>
<td>48.6</td>
<td>1.108</td>
<td>62.7</td>
<td>1.530</td>
<td>53.0</td>
<td>1.409</td>
<td>57.5</td>
</tr>
</tbody>
</table>
<h1 id="class-wise-performance">Class-wise performance</h1>
<h2 id="log-loss">Log loss</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2022 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Schmid_CPJKU_task1_3" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Log loss",
        "fields": ["class_logloss_eval_airport", "class_logloss_eval_bus", "class_logloss_eval_metro", "class_logloss_eval_metro_station", "class_logloss_eval_park", "class_logloss_eval_public_square", "class_logloss_eval_shopping_mall", "class_logloss_eval_street_pedestrian", "class_logloss_eval_street_traffic", "class_logloss_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Log loss", "fields": ["class_logloss_eval_airport", "class_logloss_eval_metro_station", "class_logloss_eval_shopping_mall"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Log loss", "fields": ["class_logloss_eval_park", "class_logloss_eval_public_square", "class_logloss_eval_street_pedestrian", "class_logloss_eval_street_traffic"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Log loss", "fields": ["class_logloss_eval_bus","class_logloss_eval_metro","class_logloss_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="class_logloss_eval_airport" data-reversed="true" data-sortable="true" data-value-type="float3">
                Airport
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_bus" data-reversed="true" data-sortable="true" data-value-type="float3">
                Bus
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_metro" data-reversed="true" data-sortable="true" data-value-type="float3">
                Metro
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_metro_station" data-reversed="true" data-sortable="true" data-value-type="float3">
                Metro <br/>station
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_park" data-reversed="true" data-sortable="true" data-value-type="float3">
                Park
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_public_square" data-reversed="true" data-sortable="true" data-value-type="float3">
                Public <br/>square
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_shopping_mall" data-reversed="true" data-sortable="true" data-value-type="float3">
                Shopping <br/>mall
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_street_pedestrian" data-reversed="true" data-sortable="true" data-value-type="float3">
                Street <br/>pedestrian
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_street_traffic" data-reversed="true" data-sortable="true" data-value-type="float3">
                Street <br/>traffic
            </th>
<th class="text-center" data-chartable="true" data-field="class_logloss_eval_tram" data-reversed="true" data-sortable="true" data-value-type="float3">
                Tram
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414</td>
<td>2.559</td>
<td>1.332</td>
<td>1.923</td>
<td>2.808</td>
<td>0.996</td>
<td>4.097</td>
<td>2.567</td>
<td>4.302</td>
<td>1.554</td>
<td>2.003</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365</td>
<td>2.293</td>
<td>1.227</td>
<td>1.818</td>
<td>2.520</td>
<td>1.026</td>
<td>4.030</td>
<td>3.000</td>
<td>4.196</td>
<td>1.628</td>
<td>1.912</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398</td>
<td>1.587</td>
<td>0.806</td>
<td>1.519</td>
<td>1.992</td>
<td>0.814</td>
<td>1.662</td>
<td>1.537</td>
<td>1.879</td>
<td>1.213</td>
<td>0.975</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330</td>
<td>1.429</td>
<td>0.959</td>
<td>1.193</td>
<td>1.549</td>
<td>0.901</td>
<td>1.877</td>
<td>1.314</td>
<td>1.722</td>
<td>1.160</td>
<td>1.192</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636</td>
<td>1.557</td>
<td>0.777</td>
<td>1.535</td>
<td>2.490</td>
<td>0.603</td>
<td>2.772</td>
<td>1.033</td>
<td>2.702</td>
<td>1.434</td>
<td>1.456</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787</td>
<td>1.810</td>
<td>0.544</td>
<td>2.138</td>
<td>2.715</td>
<td>1.037</td>
<td>2.748</td>
<td>1.077</td>
<td>2.948</td>
<td>1.564</td>
<td>1.288</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808</td>
<td>1.767</td>
<td>0.711</td>
<td>1.660</td>
<td>2.431</td>
<td>0.805</td>
<td>3.070</td>
<td>1.449</td>
<td>3.163</td>
<td>1.921</td>
<td>1.106</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515</td>
<td>1.590</td>
<td>1.258</td>
<td>1.274</td>
<td>1.845</td>
<td>1.217</td>
<td>2.132</td>
<td>1.300</td>
<td>1.986</td>
<td>1.414</td>
<td>1.131</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580</td>
<td>1.591</td>
<td>1.457</td>
<td>1.342</td>
<td>1.837</td>
<td>1.356</td>
<td>2.090</td>
<td>1.317</td>
<td>2.113</td>
<td>1.624</td>
<td>1.078</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635</td>
<td>1.583</td>
<td>1.091</td>
<td>1.290</td>
<td>1.979</td>
<td>1.346</td>
<td>2.552</td>
<td>1.521</td>
<td>2.157</td>
<td>1.761</td>
<td>1.070</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564</td>
<td>1.409</td>
<td>1.318</td>
<td>1.307</td>
<td>1.954</td>
<td>1.852</td>
<td>2.006</td>
<td>1.153</td>
<td>1.997</td>
<td>1.559</td>
<td>1.083</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795</td>
<td>3.614</td>
<td>1.835</td>
<td>2.234</td>
<td>2.986</td>
<td>2.026</td>
<td>4.087</td>
<td>2.443</td>
<td>3.969</td>
<td>2.277</td>
<td>2.480</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147</td>
<td>1.647</td>
<td>0.539</td>
<td>1.055</td>
<td>1.100</td>
<td>0.555</td>
<td>1.867</td>
<td>1.291</td>
<td>1.837</td>
<td>0.820</td>
<td>0.763</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187</td>
<td>1.549</td>
<td>0.564</td>
<td>1.033</td>
<td>1.079</td>
<td>0.701</td>
<td>1.900</td>
<td>1.458</td>
<td>1.936</td>
<td>0.835</td>
<td>0.812</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190</td>
<td>1.504</td>
<td>0.645</td>
<td>1.041</td>
<td>1.205</td>
<td>0.610</td>
<td>1.903</td>
<td>1.375</td>
<td>1.889</td>
<td>0.907</td>
<td>0.822</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187</td>
<td>1.645</td>
<td>0.542</td>
<td>1.049</td>
<td>1.084</td>
<td>0.698</td>
<td>1.849</td>
<td>1.510</td>
<td>1.889</td>
<td>0.827</td>
<td>0.777</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568</td>
<td>1.976</td>
<td>0.936</td>
<td>1.390</td>
<td>1.595</td>
<td>0.909</td>
<td>2.288</td>
<td>1.607</td>
<td>2.043</td>
<td>1.441</td>
<td>1.489</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481</td>
<td>1.827</td>
<td>1.100</td>
<td>1.508</td>
<td>1.937</td>
<td>0.824</td>
<td>1.669</td>
<td>2.015</td>
<td>1.801</td>
<td>1.166</td>
<td>0.963</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934</td>
<td>3.818</td>
<td>1.085</td>
<td>1.029</td>
<td>1.451</td>
<td>1.927</td>
<td>2.831</td>
<td>2.010</td>
<td>2.775</td>
<td>1.460</td>
<td>0.950</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916</td>
<td>5.660</td>
<td>2.051</td>
<td>0.736</td>
<td>1.248</td>
<td>3.257</td>
<td>3.867</td>
<td>3.809</td>
<td>3.728</td>
<td>3.736</td>
<td>1.064</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701</td>
<td>3.537</td>
<td>2.557</td>
<td>0.693</td>
<td>0.817</td>
<td>4.344</td>
<td>3.929</td>
<td>2.739</td>
<td>3.305</td>
<td>3.690</td>
<td>1.403</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612</td>
<td>2.008</td>
<td>1.598</td>
<td>1.933</td>
<td>1.602</td>
<td>1.009</td>
<td>2.357</td>
<td>1.261</td>
<td>1.772</td>
<td>1.158</td>
<td>1.417</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>1.532</td>
<td>1.596</td>
<td>1.368</td>
<td>1.489</td>
<td>1.692</td>
<td>1.635</td>
<td>1.943</td>
<td>1.289</td>
<td>1.891</td>
<td>1.219</td>
<td>1.202</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339</td>
<td>1.409</td>
<td>0.841</td>
<td>1.335</td>
<td>1.714</td>
<td>0.736</td>
<td>1.742</td>
<td>1.607</td>
<td>1.953</td>
<td>1.107</td>
<td>0.943</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355</td>
<td>1.487</td>
<td>0.985</td>
<td>1.305</td>
<td>1.677</td>
<td>0.682</td>
<td>1.718</td>
<td>1.460</td>
<td>1.734</td>
<td>1.289</td>
<td>1.212</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320</td>
<td>1.537</td>
<td>0.673</td>
<td>1.247</td>
<td>1.701</td>
<td>0.712</td>
<td>1.715</td>
<td>1.427</td>
<td>1.959</td>
<td>1.309</td>
<td>0.916</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311</td>
<td>1.426</td>
<td>0.747</td>
<td>1.337</td>
<td>1.587</td>
<td>0.701</td>
<td>1.738</td>
<td>1.637</td>
<td>1.874</td>
<td>1.168</td>
<td>0.894</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055</td>
<td>1.970</td>
<td>2.535</td>
<td>1.331</td>
<td>2.020</td>
<td>2.054</td>
<td>2.868</td>
<td>1.795</td>
<td>2.608</td>
<td>1.604</td>
<td>1.765</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504</td>
<td>1.615</td>
<td>1.048</td>
<td>1.761</td>
<td>1.982</td>
<td>1.153</td>
<td>2.345</td>
<td>1.229</td>
<td>1.631</td>
<td>1.554</td>
<td>0.721</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431</td>
<td>1.166</td>
<td>1.093</td>
<td>1.448</td>
<td>2.117</td>
<td>1.210</td>
<td>1.734</td>
<td>1.198</td>
<td>2.071</td>
<td>1.416</td>
<td>0.855</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092</td>
<td>1.435</td>
<td>0.773</td>
<td>0.997</td>
<td>1.173</td>
<td>0.593</td>
<td>1.549</td>
<td>1.099</td>
<td>1.628</td>
<td>0.901</td>
<td>0.770</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105</td>
<td>1.358</td>
<td>0.759</td>
<td>1.043</td>
<td>1.217</td>
<td>0.451</td>
<td>1.609</td>
<td>1.254</td>
<td>1.616</td>
<td>0.896</td>
<td>0.851</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091</td>
<td>1.430</td>
<td>0.790</td>
<td>0.932</td>
<td>1.120</td>
<td>0.502</td>
<td>1.521</td>
<td>1.129</td>
<td>1.709</td>
<td>0.953</td>
<td>0.822</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102</td>
<td>1.349</td>
<td>0.839</td>
<td>0.918</td>
<td>1.246</td>
<td>0.534</td>
<td>1.557</td>
<td>1.120</td>
<td>1.726</td>
<td>0.956</td>
<td>0.781</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731</td>
<td>2.207</td>
<td>1.025</td>
<td>1.621</td>
<td>1.923</td>
<td>1.726</td>
<td>2.125</td>
<td>1.938</td>
<td>2.518</td>
<td>1.157</td>
<td>1.068</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565</td>
<td>1.919</td>
<td>1.420</td>
<td>1.175</td>
<td>1.754</td>
<td>1.398</td>
<td>2.065</td>
<td>1.270</td>
<td>2.082</td>
<td>1.350</td>
<td>1.219</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606</td>
<td>1.834</td>
<td>1.619</td>
<td>1.275</td>
<td>1.713</td>
<td>1.414</td>
<td>2.020</td>
<td>1.394</td>
<td>2.216</td>
<td>1.464</td>
<td>1.109</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492</td>
<td>1.656</td>
<td>1.604</td>
<td>1.242</td>
<td>1.696</td>
<td>1.289</td>
<td>1.893</td>
<td>1.239</td>
<td>1.898</td>
<td>1.363</td>
<td>1.041</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499</td>
<td>1.660</td>
<td>1.496</td>
<td>1.242</td>
<td>1.700</td>
<td>1.370</td>
<td>1.895</td>
<td>1.281</td>
<td>1.966</td>
<td>1.359</td>
<td>1.023</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405</td>
<td>1.421</td>
<td>1.028</td>
<td>1.349</td>
<td>1.426</td>
<td>0.352</td>
<td>2.227</td>
<td>1.235</td>
<td>2.167</td>
<td>1.676</td>
<td>1.161</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389</td>
<td>1.468</td>
<td>1.220</td>
<td>1.377</td>
<td>1.324</td>
<td>0.365</td>
<td>1.918</td>
<td>1.311</td>
<td>2.078</td>
<td>1.559</td>
<td>1.274</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366</td>
<td>1.440</td>
<td>1.201</td>
<td>1.410</td>
<td>1.326</td>
<td>0.379</td>
<td>1.830</td>
<td>1.276</td>
<td>2.045</td>
<td>1.476</td>
<td>1.273</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397</td>
<td>1.545</td>
<td>0.723</td>
<td>1.148</td>
<td>1.447</td>
<td>1.024</td>
<td>2.212</td>
<td>1.118</td>
<td>2.483</td>
<td>1.150</td>
<td>1.122</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456</td>
<td>1.732</td>
<td>1.177</td>
<td>1.425</td>
<td>1.789</td>
<td>1.008</td>
<td>1.808</td>
<td>1.380</td>
<td>1.792</td>
<td>1.255</td>
<td>1.200</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709</td>
<td>1.110</td>
<td>2.356</td>
<td>3.714</td>
<td>4.442</td>
<td>2.247</td>
<td>1.790</td>
<td>3.617</td>
<td>3.139</td>
<td>2.755</td>
<td>1.919</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904</td>
<td>0.995</td>
<td>2.967</td>
<td>4.176</td>
<td>5.117</td>
<td>2.167</td>
<td>1.582</td>
<td>4.277</td>
<td>3.045</td>
<td>2.690</td>
<td>2.026</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096</td>
<td>1.557</td>
<td>1.105</td>
<td>2.007</td>
<td>1.513</td>
<td>1.942</td>
<td>1.812</td>
<td>1.438</td>
<td>1.618</td>
<td>5.762</td>
<td>2.208</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068</td>
<td>2.807</td>
<td>2.400</td>
<td>1.595</td>
<td>1.378</td>
<td>3.292</td>
<td>10.393</td>
<td>1.345</td>
<td>1.799</td>
<td>4.168</td>
<td>1.503</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442</td>
<td>1.971</td>
<td>0.724</td>
<td>1.329</td>
<td>1.486</td>
<td>0.639</td>
<td>2.221</td>
<td>1.269</td>
<td>2.379</td>
<td>1.247</td>
<td>1.150</td>
</tr>
</tbody>
</table>
<h2 id="accuracy">Accuracy</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2022 baseline" data-comparison-active-set="Class-wise performance (all)" data-comparison-b-row="Schmid_CPJKU_task1_3" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title": "Class-wise performance (all)",
        "data_axis_title": "Accuracy",
        "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_bus", "class_accuracy_eval_metro", "class_accuracy_eval_metro_station", "class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_shopping_mall", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic", "class_accuracy_eval_tram"]
        },
        {"title": "Class-wise performance (indoor)","data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_airport", "class_accuracy_eval_metro_station", "class_accuracy_eval_shopping_mall"]
        },
        {"title": "Class-wise performance (outdoor)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_park", "class_accuracy_eval_public_square", "class_accuracy_eval_street_pedestrian", "class_accuracy_eval_street_traffic"]
        },
        {"title": "Class-wise performance (transport)", "data_axis_title": "Accuracy", "fields": ["class_accuracy_eval_bus","class_accuracy_eval_metro","class_accuracy_eval_tram"]
        }]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval" data-scatter-y="accuracy_eval" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="class_accuracy_eval_airport" data-sortable="true" data-value-type="float1-percentage">
                Airport
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_bus" data-sortable="true" data-value-type="float1-percentage">
                Bus
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro" data-sortable="true" data-value-type="float1-percentage">
                Metro
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_metro_station" data-sortable="true" data-value-type="float1-percentage">
                Metro <br/>station
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_park" data-sortable="true" data-value-type="float1-percentage">
                Park
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_public_square" data-sortable="true" data-value-type="float1-percentage">
                Public <br/>square
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_shopping_mall" data-sortable="true" data-value-type="float1-percentage">
                Shopping <br/>mall
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_pedestrian" data-sortable="true" data-value-type="float1-percentage">
                Street <br/>pedestrian
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_street_traffic" data-sortable="true" data-value-type="float1-percentage">
                Street <br/>traffic
            </th>
<th class="text-center" data-chartable="true" data-field="class_accuracy_eval_tram" data-sortable="true" data-value-type="float1-percentage">
                Tram
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>47.0</td>
<td>41.3</td>
<td>66.9</td>
<td>51.3</td>
<td>35.3</td>
<td>75.2</td>
<td>26.7</td>
<td>40.3</td>
<td>22.3</td>
<td>60.4</td>
<td>50.4</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>46.7</td>
<td>40.8</td>
<td>68.5</td>
<td>53.2</td>
<td>39.4</td>
<td>74.9</td>
<td>24.8</td>
<td>34.7</td>
<td>21.0</td>
<td>61.2</td>
<td>48.1</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>49.4</td>
<td>38.2</td>
<td>72.9</td>
<td>41.2</td>
<td>33.2</td>
<td>75.2</td>
<td>38.4</td>
<td>44.1</td>
<td>25.6</td>
<td>59.2</td>
<td>65.8</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>51.6</td>
<td>42.1</td>
<td>68.9</td>
<td>56.2</td>
<td>45.8</td>
<td>73.8</td>
<td>25.5</td>
<td>52.7</td>
<td>34.7</td>
<td>61.4</td>
<td>54.8</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>53.0</td>
<td>48.6</td>
<td>78.3</td>
<td>49.0</td>
<td>31.3</td>
<td>84.2</td>
<td>30.3</td>
<td>65.9</td>
<td>26.0</td>
<td>66.1</td>
<td>50.2</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>51.9</td>
<td>44.8</td>
<td>84.7</td>
<td>38.5</td>
<td>29.1</td>
<td>76.0</td>
<td>32.6</td>
<td>66.9</td>
<td>26.6</td>
<td>65.3</td>
<td>54.0</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>55.2</td>
<td>51.2</td>
<td>81.4</td>
<td>49.2</td>
<td>38.2</td>
<td>83.1</td>
<td>31.8</td>
<td>61.7</td>
<td>29.4</td>
<td>60.0</td>
<td>66.4</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>47.8</td>
<td>37.7</td>
<td>57.4</td>
<td>46.8</td>
<td>33.9</td>
<td>69.8</td>
<td>23.4</td>
<td>60.2</td>
<td>36.3</td>
<td>56.5</td>
<td>55.5</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>46.4</td>
<td>39.7</td>
<td>48.8</td>
<td>41.7</td>
<td>38.0</td>
<td>63.9</td>
<td>28.3</td>
<td>62.7</td>
<td>28.8</td>
<td>50.2</td>
<td>62.0</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>45.2</td>
<td>40.4</td>
<td>61.5</td>
<td>44.5</td>
<td>28.2</td>
<td>68.1</td>
<td>17.4</td>
<td>53.6</td>
<td>32.2</td>
<td>48.3</td>
<td>57.6</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>48.0</td>
<td>46.2</td>
<td>53.3</td>
<td>46.2</td>
<td>32.8</td>
<td>61.8</td>
<td>26.0</td>
<td>62.6</td>
<td>33.3</td>
<td>56.5</td>
<td>61.5</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>48.7</td>
<td>40.1</td>
<td>58.5</td>
<td>42.1</td>
<td>38.5</td>
<td>72.3</td>
<td>24.2</td>
<td>52.0</td>
<td>35.7</td>
<td>69.0</td>
<td>54.1</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>60.8</td>
<td>39.7</td>
<td>81.8</td>
<td>61.4</td>
<td>62.3</td>
<td>83.5</td>
<td>35.1</td>
<td>58.0</td>
<td>39.5</td>
<td>74.4</td>
<td>72.5</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>59.2</td>
<td>42.3</td>
<td>82.3</td>
<td>62.4</td>
<td>62.8</td>
<td>79.5</td>
<td>32.6</td>
<td>51.3</td>
<td>32.6</td>
<td>74.0</td>
<td>72.2</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>59.4</td>
<td>43.0</td>
<td>79.9</td>
<td>63.5</td>
<td>59.6</td>
<td>81.9</td>
<td>34.2</td>
<td>52.1</td>
<td>35.3</td>
<td>72.2</td>
<td>71.9</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>59.3</td>
<td>37.6</td>
<td>83.2</td>
<td>62.3</td>
<td>63.2</td>
<td>80.0</td>
<td>34.6</td>
<td>49.4</td>
<td>34.8</td>
<td>74.5</td>
<td>73.9</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>48.0</td>
<td>30.8</td>
<td>70.4</td>
<td>50.3</td>
<td>48.3</td>
<td>72.5</td>
<td>24.7</td>
<td>45.5</td>
<td>35.3</td>
<td>56.6</td>
<td>45.6</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>49.3</td>
<td>33.2</td>
<td>59.9</td>
<td>44.8</td>
<td>39.1</td>
<td>74.3</td>
<td>40.1</td>
<td>33.7</td>
<td>37.7</td>
<td>64.1</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>41.3</td>
<td>8.2</td>
<td>59.0</td>
<td>58.0</td>
<td>43.0</td>
<td>49.5</td>
<td>21.8</td>
<td>35.1</td>
<td>22.9</td>
<td>56.6</td>
<td>58.5</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>29.9</td>
<td>3.3</td>
<td>35.8</td>
<td>71.6</td>
<td>48.8</td>
<td>23.8</td>
<td>16.2</td>
<td>9.0</td>
<td>14.8</td>
<td>20.0</td>
<td>55.9</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>28.5</td>
<td>8.5</td>
<td>26.4</td>
<td>72.3</td>
<td>64.4</td>
<td>13.8</td>
<td>12.5</td>
<td>13.0</td>
<td>14.4</td>
<td>18.8</td>
<td>40.5</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>44.1</td>
<td>25.2</td>
<td>44.2</td>
<td>29.7</td>
<td>43.1</td>
<td>69.3</td>
<td>17.1</td>
<td>57.1</td>
<td>39.9</td>
<td>63.7</td>
<td>51.7</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>44.2</td>
<td>32.2</td>
<td>50.6</td>
<td>37.9</td>
<td>39.8</td>
<td>52.2</td>
<td>25.7</td>
<td>58.2</td>
<td>27.9</td>
<td>64.4</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>53.8</td>
<td>49.5</td>
<td>72.6</td>
<td>49.6</td>
<td>42.7</td>
<td>77.7</td>
<td>37.3</td>
<td>45.9</td>
<td>25.6</td>
<td>69.2</td>
<td>67.8</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>53.0</td>
<td>41.1</td>
<td>68.7</td>
<td>51.4</td>
<td>43.4</td>
<td>80.3</td>
<td>36.3</td>
<td>48.8</td>
<td>33.2</td>
<td>66.0</td>
<td>60.5</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>54.7</td>
<td>42.7</td>
<td>77.4</td>
<td>52.3</td>
<td>42.8</td>
<td>78.5</td>
<td>39.8</td>
<td>54.2</td>
<td>26.0</td>
<td>63.9</td>
<td>69.0</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>54.5</td>
<td>48.5</td>
<td>75.0</td>
<td>48.0</td>
<td>47.0</td>
<td>79.4</td>
<td>37.5</td>
<td>43.5</td>
<td>27.3</td>
<td>68.3</td>
<td>70.1</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>36.4</td>
<td>31.8</td>
<td>27.1</td>
<td>50.5</td>
<td>30.9</td>
<td>44.5</td>
<td>17.2</td>
<td>46.9</td>
<td>20.1</td>
<td>54.0</td>
<td>40.5</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>51.7</td>
<td>44.0</td>
<td>64.2</td>
<td>42.5</td>
<td>39.8</td>
<td>67.1</td>
<td>25.5</td>
<td>58.0</td>
<td>42.2</td>
<td>58.5</td>
<td>74.9</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>52.7</td>
<td>57.5</td>
<td>64.3</td>
<td>49.8</td>
<td>36.4</td>
<td>64.1</td>
<td>38.6</td>
<td>55.2</td>
<td>29.0</td>
<td>61.0</td>
<td>71.1</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>59.7</td>
<td>48.0</td>
<td>76.8</td>
<td>63.8</td>
<td>58.3</td>
<td>82.7</td>
<td>43.2</td>
<td>57.1</td>
<td>32.0</td>
<td>68.9</td>
<td>66.6</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>59.6</td>
<td>51.2</td>
<td>78.2</td>
<td>62.6</td>
<td>58.3</td>
<td>88.5</td>
<td>41.3</td>
<td>52.1</td>
<td>30.8</td>
<td>69.5</td>
<td>63.7</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>59.6</td>
<td>47.4</td>
<td>75.5</td>
<td>66.1</td>
<td>60.4</td>
<td>85.4</td>
<td>43.6</td>
<td>55.1</td>
<td>31.8</td>
<td>66.8</td>
<td>64.4</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>59.4</td>
<td>53.1</td>
<td>74.7</td>
<td>67.5</td>
<td>53.8</td>
<td>85.8</td>
<td>42.6</td>
<td>56.9</td>
<td>27.4</td>
<td>66.5</td>
<td>65.7</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>47.5</td>
<td>31.6</td>
<td>66.4</td>
<td>48.1</td>
<td>40.9</td>
<td>53.5</td>
<td>33.3</td>
<td>44.4</td>
<td>26.2</td>
<td>66.1</td>
<td>64.2</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>44.6</td>
<td>23.9</td>
<td>43.7</td>
<td>54.1</td>
<td>36.6</td>
<td>61.4</td>
<td>32.0</td>
<td>62.4</td>
<td>24.2</td>
<td>59.5</td>
<td>48.3</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>44.3</td>
<td>29.3</td>
<td>37.3</td>
<td>48.8</td>
<td>39.5</td>
<td>62.9</td>
<td>37.2</td>
<td>58.3</td>
<td>17.9</td>
<td>56.8</td>
<td>55.1</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>45.9</td>
<td>29.1</td>
<td>33.0</td>
<td>49.8</td>
<td>38.6</td>
<td>63.3</td>
<td>36.7</td>
<td>62.4</td>
<td>24.6</td>
<td>60.0</td>
<td>61.2</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>45.9</td>
<td>29.8</td>
<td>37.3</td>
<td>49.2</td>
<td>39.0</td>
<td>61.5</td>
<td>37.1</td>
<td>61.7</td>
<td>22.7</td>
<td>60.0</td>
<td>60.8</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>51.5</td>
<td>45.4</td>
<td>66.0</td>
<td>49.8</td>
<td>48.8</td>
<td>90.8</td>
<td>24.2</td>
<td>53.4</td>
<td>31.6</td>
<td>44.5</td>
<td>60.1</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>51.6</td>
<td>42.5</td>
<td>61.0</td>
<td>52.3</td>
<td>53.0</td>
<td>90.4</td>
<td>29.0</td>
<td>52.5</td>
<td>33.6</td>
<td>48.8</td>
<td>53.0</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>51.7</td>
<td>42.5</td>
<td>61.1</td>
<td>52.1</td>
<td>53.0</td>
<td>90.5</td>
<td>29.1</td>
<td>52.7</td>
<td>33.5</td>
<td>48.9</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>52.7</td>
<td>36.3</td>
<td>76.5</td>
<td>54.1</td>
<td>47.8</td>
<td>73.7</td>
<td>26.7</td>
<td>65.4</td>
<td>23.9</td>
<td>63.5</td>
<td>59.8</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>46.2</td>
<td>39.5</td>
<td>59.9</td>
<td>43.8</td>
<td>32.6</td>
<td>68.7</td>
<td>37.0</td>
<td>51.8</td>
<td>24.7</td>
<td>62.7</td>
<td>42.0</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>43.8</td>
<td>64.2</td>
<td>48.8</td>
<td>29.9</td>
<td>30.8</td>
<td>64.9</td>
<td>54.4</td>
<td>23.3</td>
<td>14.1</td>
<td>58.9</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>41.9</td>
<td>67.5</td>
<td>41.5</td>
<td>27.5</td>
<td>26.0</td>
<td>64.2</td>
<td>56.9</td>
<td>15.2</td>
<td>12.1</td>
<td>58.7</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>54.9</td>
<td>39.9</td>
<td>74.9</td>
<td>55.4</td>
<td>47.3</td>
<td>82.2</td>
<td>36.0</td>
<td>48.2</td>
<td>37.9</td>
<td>62.6</td>
<td>64.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>54.4</td>
<td>42.1</td>
<td>80.4</td>
<td>50.1</td>
<td>47.7</td>
<td>81.3</td>
<td>34.2</td>
<td>45.6</td>
<td>39.1</td>
<td>64.0</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>56.3</td>
<td>37.6</td>
<td>74.9</td>
<td>55.9</td>
<td>56.2</td>
<td>81.6</td>
<td>33.1</td>
<td>62.1</td>
<td>34.3</td>
<td>67.0</td>
<td>60.1</td>
</tr>
</tbody>
</table>
<h1 id="device-wise-performance">Device-wise performance</h1>
<h2 id="log-loss-1">Log loss</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2022 baseline" data-comparison-active-set="Device-wise performance (all)" data-comparison-b-row="Schmid_CPJKU_task1_3" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title":"Device-wise performance (all)","data_axis_title":"Log loss","fields":["device_logloss_eval_a","device_logloss_eval_b","device_logloss_eval_c","device_logloss_eval_d","device_logloss_eval_s1","device_logloss_eval_s2","device_logloss_eval_s3","device_logloss_eval_s7","device_logloss_eval_s8","device_logloss_eval_s9","device_logloss_eval_s10"]},
        {"title":"Device-wise performance / Real","data_axis_title":"Log loss","fields":["device_logloss_eval_a","device_logloss_eval_b","device_logloss_eval_c","device_logloss_eval_d"]},
        {"title":"Device-wise performance / Simulated","data_axis_title":"Accuracy","fields":["device_logloss_eval_s1","device_logloss_eval_s2","device_logloss_eval_s3","device_logloss_eval_s7","device_logloss_eval_s8","device_logloss_eval_s9","device_logloss_eval_s10"]},
        {"title":"Device-wise performance / Unseen devices","data_axis_title":"Log loss","fields":["device_logloss_eval_d","device_logloss_eval_s7","device_logloss_eval_s8","device_logloss_eval_s9","device_logloss_eval_s10"]},
        {"title":"Device-wise performance / Seen devices","data_axis_title":"Accuracy","fields":["device_logloss_eval_a","device_logloss_eval_b","device_logloss_eval_c","device_logloss_eval_s1","device_logloss_eval_s2","device_logloss_eval_s3"]}]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="logloss_eval_source_seen" data-scatter-y="logloss_eval_source_unseen" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell"></th>
<th class="sep-right-cell" colspan="2"></th>
<th class="sep-right-cell" colspan="4"></th>
<th class="sep-right-cell text-center" colspan="5">Unseen devices</th>
<th class="sep-right-cell text-center" colspan="6">Seen devices</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Log loss
            </th>
<th class="text-center" data-chartable="true" data-field="logloss_eval_source_unseen" data-reversed="true" data-sortable="true" data-value-type="float3">
                Log loss / <br/>Unseen
            </th>
<th class="text-center" data-chartable="true" data-field="logloss_eval_source_seen" data-reversed="true" data-sortable="true" data-value-type="float3">
                Log loss / <br/>Seen
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="device_logloss_eval_d" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-success">D</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s7" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S7</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s8" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S8</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s9" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S9</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s10" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S10</span>
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="device_logloss_eval_a" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-success">A</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_b" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-success">B</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_c" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-success">C</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s1" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S1</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s2" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S2</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_logloss_eval_s3" data-reversed="true" data-sortable="true" data-value-type="float3">
<span class="label label-warning">S3</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414</td>
<td>2.820</td>
<td>2.076</td>
<td>3.553</td>
<td>2.482</td>
<td>2.701</td>
<td>2.691</td>
<td>2.672</td>
<td>1.734</td>
<td>2.025</td>
<td>1.789</td>
<td>2.325</td>
<td>2.385</td>
<td>2.200</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365</td>
<td>2.923</td>
<td>1.900</td>
<td>3.623</td>
<td>2.602</td>
<td>2.706</td>
<td>2.707</td>
<td>2.977</td>
<td>1.582</td>
<td>1.797</td>
<td>1.684</td>
<td>2.160</td>
<td>2.182</td>
<td>1.995</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398</td>
<td>1.588</td>
<td>1.240</td>
<td>1.782</td>
<td>1.511</td>
<td>1.592</td>
<td>1.556</td>
<td>1.500</td>
<td>1.168</td>
<td>1.220</td>
<td>1.154</td>
<td>1.255</td>
<td>1.359</td>
<td>1.286</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330</td>
<td>1.467</td>
<td>1.215</td>
<td>1.775</td>
<td>1.350</td>
<td>1.465</td>
<td>1.374</td>
<td>1.370</td>
<td>1.123</td>
<td>1.214</td>
<td>1.153</td>
<td>1.211</td>
<td>1.378</td>
<td>1.213</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636</td>
<td>1.806</td>
<td>1.494</td>
<td>2.479</td>
<td>1.380</td>
<td>1.653</td>
<td>1.980</td>
<td>1.541</td>
<td>1.355</td>
<td>1.775</td>
<td>1.513</td>
<td>1.357</td>
<td>1.671</td>
<td>1.292</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787</td>
<td>2.138</td>
<td>1.494</td>
<td>2.958</td>
<td>1.699</td>
<td>1.937</td>
<td>2.136</td>
<td>1.961</td>
<td>1.323</td>
<td>1.580</td>
<td>1.331</td>
<td>1.532</td>
<td>1.713</td>
<td>1.485</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808</td>
<td>2.258</td>
<td>1.434</td>
<td>2.959</td>
<td>1.692</td>
<td>1.897</td>
<td>2.581</td>
<td>2.159</td>
<td>1.205</td>
<td>1.559</td>
<td>1.481</td>
<td>1.492</td>
<td>1.470</td>
<td>1.395</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515</td>
<td>1.847</td>
<td>1.238</td>
<td>2.264</td>
<td>1.361</td>
<td>1.539</td>
<td>2.193</td>
<td>1.876</td>
<td>1.025</td>
<td>1.324</td>
<td>1.125</td>
<td>1.288</td>
<td>1.355</td>
<td>1.313</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580</td>
<td>1.920</td>
<td>1.297</td>
<td>2.219</td>
<td>1.372</td>
<td>1.600</td>
<td>2.504</td>
<td>1.906</td>
<td>1.089</td>
<td>1.366</td>
<td>1.175</td>
<td>1.346</td>
<td>1.422</td>
<td>1.386</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635</td>
<td>2.059</td>
<td>1.282</td>
<td>2.418</td>
<td>1.414</td>
<td>1.759</td>
<td>2.704</td>
<td>1.998</td>
<td>1.049</td>
<td>1.397</td>
<td>1.149</td>
<td>1.329</td>
<td>1.400</td>
<td>1.368</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564</td>
<td>1.916</td>
<td>1.270</td>
<td>2.789</td>
<td>1.323</td>
<td>1.454</td>
<td>2.081</td>
<td>1.932</td>
<td>1.045</td>
<td>1.372</td>
<td>1.112</td>
<td>1.341</td>
<td>1.429</td>
<td>1.322</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795</td>
<td>3.746</td>
<td>2.003</td>
<td>9.920</td>
<td>2.145</td>
<td>2.105</td>
<td>2.184</td>
<td>2.375</td>
<td>1.774</td>
<td>2.110</td>
<td>1.902</td>
<td>2.059</td>
<td>2.011</td>
<td>2.159</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147</td>
<td>1.377</td>
<td>0.956</td>
<td>1.744</td>
<td>1.065</td>
<td>1.185</td>
<td>1.418</td>
<td>1.475</td>
<td>0.839</td>
<td>0.977</td>
<td>0.879</td>
<td>1.008</td>
<td>1.015</td>
<td>1.016</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187</td>
<td>1.426</td>
<td>0.987</td>
<td>1.823</td>
<td>1.126</td>
<td>1.127</td>
<td>1.444</td>
<td>1.610</td>
<td>0.884</td>
<td>0.978</td>
<td>0.927</td>
<td>1.054</td>
<td>1.038</td>
<td>1.043</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190</td>
<td>1.428</td>
<td>0.992</td>
<td>1.976</td>
<td>1.080</td>
<td>1.186</td>
<td>1.374</td>
<td>1.526</td>
<td>0.882</td>
<td>0.985</td>
<td>0.928</td>
<td>1.028</td>
<td>1.083</td>
<td>1.045</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187</td>
<td>1.433</td>
<td>0.982</td>
<td>1.846</td>
<td>1.132</td>
<td>1.137</td>
<td>1.440</td>
<td>1.611</td>
<td>0.856</td>
<td>0.965</td>
<td>0.936</td>
<td>1.051</td>
<td>1.042</td>
<td>1.043</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568</td>
<td>1.872</td>
<td>1.314</td>
<td>2.012</td>
<td>1.547</td>
<td>1.630</td>
<td>2.315</td>
<td>1.857</td>
<td>1.083</td>
<td>1.239</td>
<td>1.170</td>
<td>1.433</td>
<td>1.494</td>
<td>1.463</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481</td>
<td>1.740</td>
<td>1.265</td>
<td>2.361</td>
<td>1.557</td>
<td>1.451</td>
<td>1.724</td>
<td>1.609</td>
<td>1.028</td>
<td>1.262</td>
<td>1.132</td>
<td>1.317</td>
<td>1.476</td>
<td>1.375</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934</td>
<td>2.289</td>
<td>1.637</td>
<td>3.535</td>
<td>1.861</td>
<td>1.950</td>
<td>1.726</td>
<td>2.373</td>
<td>1.291</td>
<td>1.428</td>
<td>1.493</td>
<td>2.011</td>
<td>1.789</td>
<td>1.813</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916</td>
<td>3.346</td>
<td>2.557</td>
<td>4.393</td>
<td>2.989</td>
<td>2.820</td>
<td>2.730</td>
<td>3.798</td>
<td>1.890</td>
<td>2.140</td>
<td>2.204</td>
<td>3.056</td>
<td>3.097</td>
<td>2.956</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701</td>
<td>3.063</td>
<td>2.400</td>
<td>3.483</td>
<td>2.931</td>
<td>2.782</td>
<td>2.601</td>
<td>3.516</td>
<td>1.805</td>
<td>2.093</td>
<td>1.973</td>
<td>2.992</td>
<td>2.757</td>
<td>2.780</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612</td>
<td>1.690</td>
<td>1.546</td>
<td>2.074</td>
<td>1.588</td>
<td>1.600</td>
<td>1.527</td>
<td>1.660</td>
<td>1.448</td>
<td>1.663</td>
<td>1.509</td>
<td>1.569</td>
<td>1.524</td>
<td>1.566</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>1.532</td>
<td>1.725</td>
<td>1.372</td>
<td>1.894</td>
<td>1.485</td>
<td>1.573</td>
<td>1.864</td>
<td>1.807</td>
<td>1.108</td>
<td>1.360</td>
<td>1.299</td>
<td>1.478</td>
<td>1.528</td>
<td>1.460</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339</td>
<td>1.509</td>
<td>1.197</td>
<td>1.884</td>
<td>1.259</td>
<td>1.379</td>
<td>1.602</td>
<td>1.420</td>
<td>1.028</td>
<td>1.320</td>
<td>1.156</td>
<td>1.215</td>
<td>1.275</td>
<td>1.189</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355</td>
<td>1.512</td>
<td>1.224</td>
<td>1.907</td>
<td>1.340</td>
<td>1.467</td>
<td>1.424</td>
<td>1.420</td>
<td>1.025</td>
<td>1.351</td>
<td>1.186</td>
<td>1.220</td>
<td>1.314</td>
<td>1.250</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320</td>
<td>1.508</td>
<td>1.162</td>
<td>1.982</td>
<td>1.232</td>
<td>1.372</td>
<td>1.541</td>
<td>1.414</td>
<td>0.995</td>
<td>1.263</td>
<td>1.107</td>
<td>1.192</td>
<td>1.235</td>
<td>1.182</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311</td>
<td>1.480</td>
<td>1.170</td>
<td>1.937</td>
<td>1.240</td>
<td>1.337</td>
<td>1.511</td>
<td>1.376</td>
<td>1.004</td>
<td>1.275</td>
<td>1.116</td>
<td>1.191</td>
<td>1.259</td>
<td>1.175</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055</td>
<td>2.515</td>
<td>1.671</td>
<td>2.871</td>
<td>2.236</td>
<td>2.247</td>
<td>2.652</td>
<td>2.567</td>
<td>1.389</td>
<td>1.583</td>
<td>1.488</td>
<td>1.822</td>
<td>1.915</td>
<td>1.832</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504</td>
<td>1.768</td>
<td>1.284</td>
<td>1.942</td>
<td>1.504</td>
<td>1.562</td>
<td>1.860</td>
<td>1.970</td>
<td>1.020</td>
<td>1.326</td>
<td>1.140</td>
<td>1.331</td>
<td>1.405</td>
<td>1.480</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431</td>
<td>1.624</td>
<td>1.270</td>
<td>1.979</td>
<td>1.339</td>
<td>1.418</td>
<td>1.657</td>
<td>1.728</td>
<td>1.060</td>
<td>1.323</td>
<td>1.081</td>
<td>1.387</td>
<td>1.456</td>
<td>1.313</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092</td>
<td>1.236</td>
<td>0.972</td>
<td>1.431</td>
<td>1.058</td>
<td>1.042</td>
<td>1.305</td>
<td>1.343</td>
<td>0.783</td>
<td>1.048</td>
<td>0.877</td>
<td>1.026</td>
<td>1.086</td>
<td>1.010</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105</td>
<td>1.218</td>
<td>1.011</td>
<td>1.548</td>
<td>1.075</td>
<td>1.048</td>
<td>1.171</td>
<td>1.248</td>
<td>0.847</td>
<td>1.077</td>
<td>0.932</td>
<td>1.076</td>
<td>1.098</td>
<td>1.039</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091</td>
<td>1.231</td>
<td>0.974</td>
<td>1.507</td>
<td>1.042</td>
<td>1.038</td>
<td>1.272</td>
<td>1.299</td>
<td>0.792</td>
<td>1.046</td>
<td>0.915</td>
<td>1.020</td>
<td>1.084</td>
<td>0.984</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102</td>
<td>1.229</td>
<td>0.997</td>
<td>1.464</td>
<td>1.075</td>
<td>1.047</td>
<td>1.248</td>
<td>1.311</td>
<td>0.817</td>
<td>1.075</td>
<td>0.900</td>
<td>1.047</td>
<td>1.108</td>
<td>1.035</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731</td>
<td>2.139</td>
<td>1.390</td>
<td>3.108</td>
<td>1.589</td>
<td>1.652</td>
<td>2.011</td>
<td>2.337</td>
<td>1.150</td>
<td>1.435</td>
<td>1.364</td>
<td>1.428</td>
<td>1.486</td>
<td>1.480</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565</td>
<td>1.835</td>
<td>1.341</td>
<td>1.909</td>
<td>1.511</td>
<td>1.505</td>
<td>2.079</td>
<td>2.168</td>
<td>1.081</td>
<td>1.313</td>
<td>1.230</td>
<td>1.465</td>
<td>1.488</td>
<td>1.468</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606</td>
<td>1.898</td>
<td>1.362</td>
<td>2.122</td>
<td>1.493</td>
<td>1.516</td>
<td>2.151</td>
<td>2.207</td>
<td>1.090</td>
<td>1.355</td>
<td>1.233</td>
<td>1.483</td>
<td>1.516</td>
<td>1.496</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492</td>
<td>1.728</td>
<td>1.296</td>
<td>1.808</td>
<td>1.413</td>
<td>1.445</td>
<td>1.972</td>
<td>1.999</td>
<td>1.040</td>
<td>1.300</td>
<td>1.166</td>
<td>1.407</td>
<td>1.448</td>
<td>1.416</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499</td>
<td>1.754</td>
<td>1.287</td>
<td>1.820</td>
<td>1.413</td>
<td>1.453</td>
<td>2.040</td>
<td>2.042</td>
<td>1.030</td>
<td>1.282</td>
<td>1.165</td>
<td>1.406</td>
<td>1.437</td>
<td>1.404</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405</td>
<td>1.576</td>
<td>1.261</td>
<td>1.866</td>
<td>1.461</td>
<td>1.216</td>
<td>1.378</td>
<td>1.960</td>
<td>1.032</td>
<td>1.353</td>
<td>1.180</td>
<td>1.505</td>
<td>1.236</td>
<td>1.263</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389</td>
<td>1.534</td>
<td>1.269</td>
<td>1.798</td>
<td>1.373</td>
<td>1.204</td>
<td>1.414</td>
<td>1.880</td>
<td>1.016</td>
<td>1.422</td>
<td>1.174</td>
<td>1.431</td>
<td>1.275</td>
<td>1.298</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366</td>
<td>1.496</td>
<td>1.257</td>
<td>1.757</td>
<td>1.344</td>
<td>1.196</td>
<td>1.396</td>
<td>1.785</td>
<td>1.011</td>
<td>1.409</td>
<td>1.172</td>
<td>1.401</td>
<td>1.269</td>
<td>1.280</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397</td>
<td>1.691</td>
<td>1.152</td>
<td>2.857</td>
<td>1.225</td>
<td>1.235</td>
<td>1.467</td>
<td>1.670</td>
<td>1.028</td>
<td>1.248</td>
<td>1.124</td>
<td>1.166</td>
<td>1.178</td>
<td>1.171</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456</td>
<td>1.619</td>
<td>1.321</td>
<td>1.708</td>
<td>1.383</td>
<td>1.470</td>
<td>1.974</td>
<td>1.561</td>
<td>1.040</td>
<td>1.388</td>
<td>1.143</td>
<td>1.419</td>
<td>1.540</td>
<td>1.396</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709</td>
<td>3.181</td>
<td>2.315</td>
<td>3.420</td>
<td>2.402</td>
<td>2.540</td>
<td>4.638</td>
<td>2.906</td>
<td>1.861</td>
<td>2.791</td>
<td>1.976</td>
<td>2.152</td>
<td>2.471</td>
<td>2.641</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904</td>
<td>3.254</td>
<td>2.613</td>
<td>3.502</td>
<td>2.669</td>
<td>2.861</td>
<td>4.235</td>
<td>3.004</td>
<td>2.122</td>
<td>3.252</td>
<td>2.281</td>
<td>2.326</td>
<td>2.752</td>
<td>2.943</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096</td>
<td>2.435</td>
<td>1.814</td>
<td>2.891</td>
<td>1.896</td>
<td>2.103</td>
<td>2.839</td>
<td>2.444</td>
<td>1.589</td>
<td>1.909</td>
<td>1.585</td>
<td>1.927</td>
<td>1.942</td>
<td>1.934</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068</td>
<td>4.008</td>
<td>2.284</td>
<td>4.883</td>
<td>2.930</td>
<td>3.463</td>
<td>5.471</td>
<td>3.294</td>
<td>2.096</td>
<td>2.320</td>
<td>2.148</td>
<td>2.332</td>
<td>2.533</td>
<td>2.277</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442</td>
<td>1.842</td>
<td>1.108</td>
<td>2.401</td>
<td>1.314</td>
<td>1.401</td>
<td>2.214</td>
<td>1.882</td>
<td>0.899</td>
<td>1.053</td>
<td>0.932</td>
<td>1.194</td>
<td>1.288</td>
<td>1.281</td>
</tr>
</tbody>
</table>
<h2 id="accuracy-1">Accuracy</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar,scatter,comparison" data-chart-tooltip-fields="code" data-comparison-a-row="DCASE2022 baseline" data-comparison-active-set="Device-wise performance (all)" data-comparison-b-row="Schmid_CPJKU_task1_3" data-comparison-row-id-field="code" data-comparison-sets-json='[
        {"title":"Device-wise performance (all)","data_axis_title":"Accuracy","fields":["device_accuracy_eval_a","device_accuracy_eval_b","device_accuracy_eval_c","device_accuracy_eval_d","device_accuracy_eval_s1","device_accuracy_eval_s2","device_accuracy_eval_s3","device_accuracy_eval_s7","device_accuracy_eval_s8","device_accuracy_eval_s9","device_accuracy_eval_s10"]},
        {"title":"Device-wise performance / Real","data_axis_title":"Accuracy","fields":["device_accuracy_eval_a","device_accuracy_eval_b","device_accuracy_eval_c","device_accuracy_eval_d"]},
        {"title":"Device-wise performance / Simulated","data_axis_title":"Accuracy","fields":["device_accuracy_eval_s1","device_accuracy_eval_s2","device_accuracy_eval_s3","device_accuracy_eval_s7","device_accuracy_eval_s8","device_accuracy_eval_s9","device_accuracy_eval_s10"]},
        {"title":"Device-wise performance / Unseen devices","data_axis_title":"Accuracy","fields":["device_accuracy_eval_d","device_accuracy_eval_s7","device_accuracy_eval_s8","device_accuracy_eval_s9","device_accuracy_eval_s10"]},
        {"title":"Device-wise performance / Seen devices","data_axis_title":"Accuracy","fields":["device_accuracy_eval_a","device_accuracy_eval_b","device_accuracy_eval_c","device_accuracy_eval_s1","device_accuracy_eval_s2","device_accuracy_eval_s3"]}]' data-filter-control="false" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval_source_seen" data-scatter-y="accuracy_eval_source_unseen" data-show-chart="true" data-show-pagination-switch="yes" data-show-rank="true" data-sort-name="accuracy_eval" data-sort-order="desc">
<thead>
<tr>
<th class="sep-right-cell"></th>
<th class="sep-right-cell" colspan="2"></th>
<th class="sep-right-cell" colspan="4"></th>
<th class="sep-right-cell text-center" colspan="5">Unseen devices</th>
<th class="sep-right-cell text-center" colspan="6">Seen devices</th>
</tr>
<tr>
<th class="sep-right-cell" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval_source_unseen" data-sortable="true" data-value-type="float1-percentage">
                Accuracy / <br/>Unseen
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval_source_seen" data-sortable="true" data-value-type="float1-percentage">
                Accuracy / <br/>Seen
            </th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="device_accuracy_eval_d" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-success">D</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s7" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S7</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s8" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S8</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s9" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S9</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s10" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S10</span>
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="device_accuracy_eval_a" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-success">A</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_b" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-success">B</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_c" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-success">C</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s1" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S1</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s2" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S2</span>
</th>
<th class="text-center" data-chartable="true" data-field="device_accuracy_eval_s3" data-sortable="true" data-value-type="float1-percentage">
<span class="label label-warning">S3</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>47.0</td>
<td>41.6</td>
<td>51.5</td>
<td>36.2</td>
<td>43.3</td>
<td>43.1</td>
<td>41.8</td>
<td>43.4</td>
<td>54.4</td>
<td>52.1</td>
<td>53.8</td>
<td>48.8</td>
<td>49.0</td>
<td>51.0</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>46.7</td>
<td>41.2</td>
<td>51.2</td>
<td>35.6</td>
<td>43.6</td>
<td>43.6</td>
<td>41.3</td>
<td>42.1</td>
<td>54.1</td>
<td>52.1</td>
<td>53.4</td>
<td>49.5</td>
<td>47.3</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>49.4</td>
<td>42.5</td>
<td>55.1</td>
<td>34.1</td>
<td>44.9</td>
<td>44.4</td>
<td>43.9</td>
<td>45.2</td>
<td>58.1</td>
<td>55.6</td>
<td>58.2</td>
<td>54.7</td>
<td>50.7</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>51.6</td>
<td>46.5</td>
<td>55.9</td>
<td>35.5</td>
<td>49.6</td>
<td>47.6</td>
<td>50.2</td>
<td>49.4</td>
<td>60.3</td>
<td>55.5</td>
<td>57.7</td>
<td>56.1</td>
<td>49.6</td>
<td>56.0</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>53.0</td>
<td>50.1</td>
<td>55.4</td>
<td>42.2</td>
<td>56.7</td>
<td>52.2</td>
<td>45.8</td>
<td>53.6</td>
<td>62.6</td>
<td>49.1</td>
<td>56.2</td>
<td>56.7</td>
<td>50.1</td>
<td>57.8</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>51.9</td>
<td>47.2</td>
<td>55.8</td>
<td>39.8</td>
<td>52.3</td>
<td>49.1</td>
<td>45.7</td>
<td>49.0</td>
<td>63.0</td>
<td>52.0</td>
<td>60.7</td>
<td>53.6</td>
<td>50.9</td>
<td>54.4</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>55.2</td>
<td>49.9</td>
<td>59.7</td>
<td>42.9</td>
<td>56.4</td>
<td>52.6</td>
<td>46.0</td>
<td>51.5</td>
<td>67.3</td>
<td>56.0</td>
<td>61.1</td>
<td>58.4</td>
<td>56.3</td>
<td>59.2</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>47.8</td>
<td>40.7</td>
<td>53.7</td>
<td>34.4</td>
<td>49.5</td>
<td>45.9</td>
<td>33.2</td>
<td>40.3</td>
<td>62.0</td>
<td>50.6</td>
<td>58.3</td>
<td>50.8</td>
<td>49.3</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>46.4</td>
<td>40.2</td>
<td>51.6</td>
<td>37.5</td>
<td>49.3</td>
<td>45.1</td>
<td>29.5</td>
<td>39.6</td>
<td>60.0</td>
<td>49.2</td>
<td>56.0</td>
<td>49.1</td>
<td>46.0</td>
<td>49.2</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>45.2</td>
<td>36.6</td>
<td>52.3</td>
<td>33.2</td>
<td>46.0</td>
<td>40.6</td>
<td>26.0</td>
<td>37.3</td>
<td>62.8</td>
<td>46.8</td>
<td>56.4</td>
<td>49.3</td>
<td>48.8</td>
<td>49.7</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>48.0</td>
<td>41.9</td>
<td>53.1</td>
<td>35.2</td>
<td>50.3</td>
<td>49.4</td>
<td>35.9</td>
<td>38.9</td>
<td>62.2</td>
<td>49.3</td>
<td>58.0</td>
<td>49.7</td>
<td>48.2</td>
<td>51.1</td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>48.7</td>
<td>44.0</td>
<td>52.5</td>
<td>29.2</td>
<td>49.4</td>
<td>47.5</td>
<td>47.4</td>
<td>46.6</td>
<td>61.2</td>
<td>48.6</td>
<td>55.2</td>
<td>50.9</td>
<td>50.4</td>
<td>48.9</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>60.8</td>
<td>55.1</td>
<td>65.6</td>
<td>46.4</td>
<td>63.2</td>
<td>59.5</td>
<td>54.5</td>
<td>52.1</td>
<td>70.3</td>
<td>64.8</td>
<td>67.2</td>
<td>64.0</td>
<td>63.3</td>
<td>63.7</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>59.2</td>
<td>52.0</td>
<td>65.2</td>
<td>39.7</td>
<td>60.3</td>
<td>59.3</td>
<td>51.8</td>
<td>48.9</td>
<td>68.8</td>
<td>65.1</td>
<td>66.8</td>
<td>63.4</td>
<td>63.6</td>
<td>63.5</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>59.4</td>
<td>52.6</td>
<td>65.0</td>
<td>37.0</td>
<td>62.3</td>
<td>58.5</td>
<td>54.4</td>
<td>50.7</td>
<td>69.2</td>
<td>64.4</td>
<td>67.0</td>
<td>64.6</td>
<td>61.8</td>
<td>63.1</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>59.3</td>
<td>51.9</td>
<td>65.5</td>
<td>39.9</td>
<td>60.2</td>
<td>59.4</td>
<td>52.1</td>
<td>48.1</td>
<td>70.2</td>
<td>65.4</td>
<td>66.8</td>
<td>63.4</td>
<td>63.4</td>
<td>63.6</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>48.0</td>
<td>38.8</td>
<td>55.6</td>
<td>31.9</td>
<td>48.5</td>
<td>44.7</td>
<td>28.6</td>
<td>40.6</td>
<td>64.7</td>
<td>57.8</td>
<td>60.3</td>
<td>51.0</td>
<td>49.9</td>
<td>50.0</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>49.3</td>
<td>42.8</td>
<td>54.6</td>
<td>31.4</td>
<td>46.9</td>
<td>49.4</td>
<td>43.6</td>
<td>42.8</td>
<td>62.7</td>
<td>53.9</td>
<td>59.2</td>
<td>52.4</td>
<td>48.9</td>
<td>50.7</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>41.3</td>
<td>36.2</td>
<td>45.5</td>
<td>25.4</td>
<td>41.0</td>
<td>40.8</td>
<td>44.2</td>
<td>29.6</td>
<td>55.8</td>
<td>49.7</td>
<td>49.1</td>
<td>36.7</td>
<td>41.1</td>
<td>40.4</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>29.9</td>
<td>26.4</td>
<td>32.9</td>
<td>22.4</td>
<td>27.6</td>
<td>29.6</td>
<td>30.7</td>
<td>21.5</td>
<td>43.3</td>
<td>36.3</td>
<td>37.0</td>
<td>25.9</td>
<td>26.9</td>
<td>27.8</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>28.5</td>
<td>24.6</td>
<td>31.7</td>
<td>21.4</td>
<td>25.0</td>
<td>26.7</td>
<td>28.8</td>
<td>21.0</td>
<td>41.4</td>
<td>34.3</td>
<td>37.2</td>
<td>24.4</td>
<td>26.7</td>
<td>26.4</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>44.1</td>
<td>41.8</td>
<td>46.1</td>
<td>31.6</td>
<td>44.4</td>
<td>44.1</td>
<td>46.6</td>
<td>42.1</td>
<td>49.1</td>
<td>42.6</td>
<td>47.2</td>
<td>45.7</td>
<td>46.5</td>
<td>45.2</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>44.2</td>
<td>38.1</td>
<td>49.4</td>
<td>33.2</td>
<td>45.0</td>
<td>42.6</td>
<td>34.0</td>
<td>35.4</td>
<td>59.9</td>
<td>50.3</td>
<td>53.5</td>
<td>44.3</td>
<td>43.4</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>53.8</td>
<td>48.6</td>
<td>58.1</td>
<td>37.8</td>
<td>54.9</td>
<td>52.1</td>
<td>48.2</td>
<td>49.8</td>
<td>64.3</td>
<td>53.2</td>
<td>60.6</td>
<td>57.0</td>
<td>54.9</td>
<td>58.5</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>53.0</td>
<td>47.8</td>
<td>57.3</td>
<td>35.9</td>
<td>52.4</td>
<td>51.1</td>
<td>50.3</td>
<td>49.3</td>
<td>65.9</td>
<td>52.1</td>
<td>58.3</td>
<td>56.2</td>
<td>53.5</td>
<td>57.6</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>54.7</td>
<td>48.8</td>
<td>59.5</td>
<td>35.1</td>
<td>55.3</td>
<td>53.7</td>
<td>48.9</td>
<td>51.2</td>
<td>66.0</td>
<td>55.8</td>
<td>61.2</td>
<td>58.5</td>
<td>56.8</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>54.5</td>
<td>48.8</td>
<td>59.2</td>
<td>36.6</td>
<td>55.1</td>
<td>53.5</td>
<td>48.5</td>
<td>50.1</td>
<td>65.4</td>
<td>55.0</td>
<td>61.7</td>
<td>57.7</td>
<td>56.6</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>36.4</td>
<td>28.4</td>
<td>43.0</td>
<td>24.9</td>
<td>33.7</td>
<td>34.7</td>
<td>23.6</td>
<td>25.3</td>
<td>54.1</td>
<td>44.4</td>
<td>48.0</td>
<td>37.7</td>
<td>36.1</td>
<td>37.5</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>51.7</td>
<td>46.3</td>
<td>56.1</td>
<td>40.5</td>
<td>51.6</td>
<td>52.1</td>
<td>44.1</td>
<td>43.4</td>
<td>64.8</td>
<td>54.2</td>
<td>60.6</td>
<td>53.1</td>
<td>52.6</td>
<td>51.4</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>52.7</td>
<td>48.4</td>
<td>56.2</td>
<td>40.9</td>
<td>54.8</td>
<td>53.5</td>
<td>46.1</td>
<td>46.8</td>
<td>62.9</td>
<td>53.4</td>
<td>62.6</td>
<td>52.4</td>
<td>51.4</td>
<td>54.7</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>59.7</td>
<td>54.5</td>
<td>64.1</td>
<td>46.4</td>
<td>61.1</td>
<td>61.4</td>
<td>51.1</td>
<td>52.5</td>
<td>71.5</td>
<td>61.2</td>
<td>69.1</td>
<td>62.0</td>
<td>58.7</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>59.6</td>
<td>55.3</td>
<td>63.2</td>
<td>43.8</td>
<td>60.7</td>
<td>61.7</td>
<td>56.0</td>
<td>54.5</td>
<td>69.5</td>
<td>60.3</td>
<td>66.7</td>
<td>61.0</td>
<td>59.7</td>
<td>61.9</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>59.6</td>
<td>54.8</td>
<td>63.7</td>
<td>46.4</td>
<td>61.7</td>
<td>61.1</td>
<td>52.0</td>
<td>52.9</td>
<td>71.0</td>
<td>60.9</td>
<td>66.7</td>
<td>61.9</td>
<td>58.9</td>
<td>62.7</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>59.4</td>
<td>54.9</td>
<td>63.1</td>
<td>47.3</td>
<td>60.6</td>
<td>61.9</td>
<td>52.2</td>
<td>52.8</td>
<td>70.5</td>
<td>60.0</td>
<td>67.2</td>
<td>61.1</td>
<td>58.4</td>
<td>61.2</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>47.5</td>
<td>40.7</td>
<td>53.2</td>
<td>30.6</td>
<td>48.2</td>
<td>47.9</td>
<td>41.4</td>
<td>35.3</td>
<td>61.4</td>
<td>50.7</td>
<td>53.9</td>
<td>51.9</td>
<td>49.9</td>
<td>51.1</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>44.6</td>
<td>37.8</td>
<td>50.3</td>
<td>36.9</td>
<td>44.4</td>
<td>44.5</td>
<td>32.9</td>
<td>30.5</td>
<td>61.0</td>
<td>49.9</td>
<td>56.4</td>
<td>45.6</td>
<td>44.1</td>
<td>44.6</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>44.3</td>
<td>37.8</td>
<td>49.8</td>
<td>34.9</td>
<td>45.1</td>
<td>44.8</td>
<td>32.6</td>
<td>31.6</td>
<td>60.8</td>
<td>49.0</td>
<td>55.7</td>
<td>45.4</td>
<td>42.4</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>45.9</td>
<td>39.1</td>
<td>51.5</td>
<td>37.7</td>
<td>46.5</td>
<td>46.0</td>
<td>32.4</td>
<td>32.8</td>
<td>62.1</td>
<td>50.0</td>
<td>57.5</td>
<td>47.3</td>
<td>45.1</td>
<td>47.2</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>45.9</td>
<td>38.8</td>
<td>51.9</td>
<td>37.9</td>
<td>46.5</td>
<td>45.7</td>
<td>31.8</td>
<td>32.0</td>
<td>62.2</td>
<td>50.8</td>
<td>58.1</td>
<td>47.5</td>
<td>45.2</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>51.5</td>
<td>46.9</td>
<td>55.3</td>
<td>39.3</td>
<td>50.7</td>
<td>55.3</td>
<td>49.6</td>
<td>39.5</td>
<td>63.0</td>
<td>51.8</td>
<td>56.9</td>
<td>49.1</td>
<td>56.0</td>
<td>54.9</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>51.6</td>
<td>47.8</td>
<td>54.8</td>
<td>42.1</td>
<td>51.9</td>
<td>55.2</td>
<td>49.1</td>
<td>40.9</td>
<td>63.3</td>
<td>50.6</td>
<td>58.0</td>
<td>48.8</td>
<td>54.1</td>
<td>54.1</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>51.7</td>
<td>47.9</td>
<td>54.8</td>
<td>42.1</td>
<td>51.9</td>
<td>55.2</td>
<td>49.1</td>
<td>41.0</td>
<td>63.3</td>
<td>50.6</td>
<td>58.0</td>
<td>48.8</td>
<td>54.1</td>
<td>54.2</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>52.7</td>
<td>46.8</td>
<td>57.7</td>
<td>29.1</td>
<td>55.5</td>
<td>55.8</td>
<td>49.7</td>
<td>44.2</td>
<td>61.7</td>
<td>55.2</td>
<td>59.0</td>
<td>56.5</td>
<td>57.1</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>46.2</td>
<td>40.6</td>
<td>51.0</td>
<td>34.9</td>
<td>48.3</td>
<td>44.5</td>
<td>32.0</td>
<td>43.1</td>
<td>62.1</td>
<td>47.7</td>
<td>58.1</td>
<td>47.3</td>
<td>43.1</td>
<td>47.6</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>43.8</td>
<td>40.1</td>
<td>47.0</td>
<td>42.8</td>
<td>42.8</td>
<td>41.9</td>
<td>34.0</td>
<td>38.8</td>
<td>54.5</td>
<td>42.8</td>
<td>52.6</td>
<td>45.9</td>
<td>42.7</td>
<td>43.5</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>41.9</td>
<td>38.8</td>
<td>44.6</td>
<td>41.4</td>
<td>40.5</td>
<td>40.0</td>
<td>35.0</td>
<td>37.1</td>
<td>51.8</td>
<td>40.1</td>
<td>49.7</td>
<td>44.3</td>
<td>40.8</td>
<td>40.7</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>54.9</td>
<td>47.0</td>
<td>61.5</td>
<td>40.6</td>
<td>51.8</td>
<td>53.7</td>
<td>37.9</td>
<td>51.1</td>
<td>69.1</td>
<td>60.5</td>
<td>65.3</td>
<td>58.0</td>
<td>58.5</td>
<td>57.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>54.4</td>
<td>45.7</td>
<td>61.6</td>
<td>38.4</td>
<td>53.1</td>
<td>51.0</td>
<td>35.0</td>
<td>51.2</td>
<td>67.5</td>
<td>60.0</td>
<td>65.7</td>
<td>57.8</td>
<td>58.9</td>
<td>59.9</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>56.3</td>
<td>48.6</td>
<td>62.7</td>
<td>39.8</td>
<td>58.5</td>
<td>55.4</td>
<td>42.3</td>
<td>47.1</td>
<td>69.9</td>
<td>62.8</td>
<td>68.1</td>
<td>59.1</td>
<td>58.2</td>
<td>57.8</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<h2 id="general-characteristics">General characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="off" data-chart-modes="bar" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Submission label 
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss <br/>(Eval)
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_sampling_rate" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Sampling <br/>rate
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_data_augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Data <br/>augmentation
            </th>
<th class="text-center narrow-col" data-field="system_features" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Features
            </th>
<th class="text-center narrow-col" data-field="system_embeddings" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Embeddings
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414</td>
<td>47.0</td>
<td>8kHz</td>
<td>pitch shifting, time stretching, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365</td>
<td>46.7</td>
<td>8kHz</td>
<td>pitch shifting, time stretching, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398</td>
<td>49.4</td>
<td>8kHz</td>
<td>pitch shifting, time stretching, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330</td>
<td>51.6</td>
<td>8kHz</td>
<td>pitch shifting, time stretching, mixup, time masking, frequency masking</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636</td>
<td>53.0</td>
<td>44.1kHz</td>
<td>mixup, random cropping, SpecAugment</td>
<td>CQT, Gammatonegram, Mel</td>
<td></td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787</td>
<td>51.9</td>
<td>44.1kHz</td>
<td>mixup, random cropping, SpecAugment</td>
<td>CQT, Gammatonegram, Mel</td>
<td></td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808</td>
<td>55.2</td>
<td>44.1kHz</td>
<td>mixup, random cropping, SpecAugment</td>
<td>CQT, Gammatonegram, Mel</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515</td>
<td>47.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580</td>
<td>46.4</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635</td>
<td>45.2</td>
<td>44.1kHz</td>
<td>mixup, pitch shifting, spectrum correction</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564</td>
<td>48.0</td>
<td>44.1kHz</td>
<td>mixup, pitch shifting, spectrum correction</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795</td>
<td>48.7</td>
<td>44.1kHz</td>
<td>mixup, time stretching,pitch shifting,spectrum correction</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147</td>
<td>60.8</td>
<td>16kHz</td>
<td>mixup, SpecAugment, time masking, frequency masking, temporal shuffle</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187</td>
<td>59.2</td>
<td>16kHz</td>
<td>mixup, SpecAugment, time masking, frequency masking, temporal shuffle</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190</td>
<td>59.4</td>
<td>16kHz</td>
<td>mixup, SpecAugment, time masking, frequency masking, temporal shuffle</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187</td>
<td>59.3</td>
<td>16kHz</td>
<td>mixup, SpecAugment, time masking, frequency masking, temporal shuffle</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568</td>
<td>48.0</td>
<td>44.1kHz</td>
<td>mixup, SpecAugment</td>
<td>log-mel energies,delta and delta-delta</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481</td>
<td>49.3</td>
<td>44.1kHz</td>
<td>SpecAugment, mixup</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934</td>
<td>41.3</td>
<td>44.1kHz</td>
<td>time masking, frequency masking, time warping, mixup</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916</td>
<td>29.9</td>
<td>44.1kHz</td>
<td>time masking, frequency masking, time warping, mixup</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701</td>
<td>28.5</td>
<td>44.1kHz</td>
<td>time masking, frequency masking, time warping, frequency warping, mixup</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612</td>
<td>44.1</td>
<td>44.1kHz</td>
<td>noise addition, pitch shifting, speed changing, time masking, mixup</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>1.532</td>
<td>44.2</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339</td>
<td>53.8</td>
<td>22.05kHz</td>
<td>mixup, pitch shifting, time stretching, shifting, adding gaussian noise</td>
<td>mel-spectrogram</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355</td>
<td>53.0</td>
<td>22.05kHz</td>
<td>mixup, pitch shifting, time stretching, shifting, adding gaussian noise</td>
<td>mel-spectrogram</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320</td>
<td>54.7</td>
<td>22.05kHz</td>
<td>mixup, pitch shifting, time stretching, shifting, adding gaussian noise</td>
<td>mel-spectrogram</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311</td>
<td>54.5</td>
<td>22.05kHz</td>
<td>mixup, pitch shifting, time stretching, shifting, adding gaussian noise</td>
<td>mel-spectrogram</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055</td>
<td>36.4</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504</td>
<td>51.7</td>
<td>22.05kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431</td>
<td>52.7</td>
<td>22.05kHz</td>
<td>SpecAugment</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092</td>
<td>59.7</td>
<td>32.0kHz</td>
<td>mixup, mixstyle, pitch shifting</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105</td>
<td>59.6</td>
<td>32.0kHz</td>
<td>mixstyle, pitch shifting</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091</td>
<td>59.6</td>
<td>32.0kHz</td>
<td>mixstyle, pitch shifting</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102</td>
<td>59.4</td>
<td>32.0kHz</td>
<td>mixup, mixstyle, pitch shifting</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731</td>
<td>47.5</td>
<td>16kHz</td>
<td>mixup, rolling, SpecAugment</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565</td>
<td>44.6</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606</td>
<td>44.3</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492</td>
<td>45.9</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499</td>
<td>45.9</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405</td>
<td>51.5</td>
<td>44.1kHz</td>
<td>mixup, SpecAugment, spectrum modulation</td>
<td>log-mel energies, deltas</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389</td>
<td>51.6</td>
<td>44.1kHz</td>
<td>mixup, SpecAugment, spectrum modulation</td>
<td>log-mel energies, deltas</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366</td>
<td>51.7</td>
<td>44.1kHz</td>
<td>mixup, SpecAugment, spectrum modulation</td>
<td>log-mel energies, deltas</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397</td>
<td>52.7</td>
<td>44.1kHz</td>
<td>mixup, SpecAugment, spectrum modulation</td>
<td>log-mel energies, deltas</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456</td>
<td>46.2</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies, spectral entropy, spectral flatness</td>
<td>dilated-CNN</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709</td>
<td>43.8</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904</td>
<td>41.9</td>
<td>44.1kHz</td>
<td></td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096</td>
<td>54.9</td>
<td>44.1kHz</td>
<td>mixup, ImageDataGenerator, temporal crop, Auto levels, pix2pix</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068</td>
<td>54.4</td>
<td>44.1kHz</td>
<td>mixup, ImageDataGenerator, temporal crop, Auto levels, pix2pix</td>
<td>log-mel energies</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442</td>
<td>56.3</td>
<td>44.1kHz</td>
<td>SpecAugment++, time shifting</td>
<td>spectrogram</td>
<td>CNN6</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h2 id="machine-learning-characteristics">Machine learning characteristics</h2>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="code" data-pagination="true" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="accuracy_eval" data-scatter-y="system_complexity_total" data-show-bar-chart-xaxis="false" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="logloss_eval" data-sort-order="desc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th class="sep-right-cell text-center" data-rank="true">Rank</th>
<th data-field="code" data-sortable="true">
                Code
            </th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
                Technical<br/>Report
            </th>
<th class="sep-left-cell text-center" data-axis-label="Official rank" data-chartable="true" data-field="rank_entry" data-sortable="true" data-value-type="int">
                Official <br/>system <br/>rank
            </th>
<th class="text-center" data-axis-label="Logloss (Evaluation dataset)" data-chartable="true" data-field="logloss_eval" data-reversed="true" data-sortable="true" data-value-type="float3">
                Logloss <br/>(Eval)
            </th>
<th class="text-center" data-chartable="true" data-field="accuracy_eval" data-sortable="true" data-value-type="float1-percentage">
                Accuracy <br/>(Eval)
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_external_data_usage" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                External <br/>data usage
            </th>
<th class="text-center narrow-col" data-field="external_data_sources" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                External <br/>data sources
            </th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-chartable="true" data-field="system_complexity_total" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>complexity
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_complexity_macs" data-sortable="true" data-value-type="numeric-unit">
                Model <br/>MACS
            </th>
<th class="sep-left-cell text-center narrow-col" data-field="system_classifier" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Classifier
            </th>
<th class="text-center narrow-col" data-chartable="true" data-field="system_ensemble_method_subsystem_count" data-sortable="true" data-value-type="int">
                Ensemble <br/>subsystems
            </th>
<th class="text-center narrow-col" data-field="system_decision_making" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Decision <br/>making
            </th>
<th class="text-center narrow-col" data-field="system_framework" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Framework
            </th>
<th class="text-center narrow-col" data-field="system_pipeline" data-filter-control="select" data-filter-strict-search="true" data-sortable="true" data-tag="true">
                Pipeline
            </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_1</td>
<td>Anastcio2022</td>
<td>42</td>
<td>2.414</td>
<td>47.0</td>
<td></td>
<td></td>
<td>68918</td>
<td>21127552</td>
<td>CNN, ensemble</td>
<td>2</td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining,  ensemble, training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_2</td>
<td>Anastcio2022</td>
<td>41</td>
<td>2.365</td>
<td>46.7</td>
<td></td>
<td></td>
<td>68918</td>
<td>21127552</td>
<td>CNN, ensemble</td>
<td>2</td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining,  ensemble, training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_3</td>
<td>Anastcio2022</td>
<td>17</td>
<td>1.398</td>
<td>49.4</td>
<td></td>
<td></td>
<td>51986</td>
<td>25475456</td>
<td>CNN, ensemble</td>
<td>10</td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining,  ensemble, training, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>AI4EDGE_IPL_task1_4</td>
<td>Anastcio2022</td>
<td>11</td>
<td>1.330</td>
<td>51.6</td>
<td></td>
<td></td>
<td>51986</td>
<td>25475456</td>
<td>CNN, ensemble</td>
<td>10</td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining, ensemble, training, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_1</td>
<td>Pham2022</td>
<td>34</td>
<td>1.636</td>
<td>53.0</td>
<td></td>
<td></td>
<td>33822</td>
<td>900000</td>
<td>CNN</td>
<td>3</td>
<td>late fusion of predicted probabilities</td>
<td>tensorflow</td>
<td>training</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_2</td>
<td>Pham2022</td>
<td>36</td>
<td>1.787</td>
<td>51.9</td>
<td></td>
<td></td>
<td>31902</td>
<td>750000</td>
<td>CNN</td>
<td>3</td>
<td>late fusion of predicted probabilities</td>
<td>tensorflow</td>
<td>training</td>
</tr>
<tr>
<td></td>
<td>AIT_Essex_task1_3</td>
<td>Pham2022</td>
<td>37</td>
<td>1.808</td>
<td>55.2</td>
<td></td>
<td></td>
<td>115998</td>
<td>900000</td>
<td>CNN</td>
<td>3</td>
<td>late fusion of predicted probabilities</td>
<td>tensorflow</td>
<td>training</td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_1</td>
<td>Cai2022</td>
<td>26</td>
<td>1.515</td>
<td>47.8</td>
<td></td>
<td></td>
<td>25526</td>
<td>6287030</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_2</td>
<td>Cai2022</td>
<td>30</td>
<td>1.580</td>
<td>46.4</td>
<td></td>
<td></td>
<td>25526</td>
<td>6287030</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_3</td>
<td>Cai2022</td>
<td>33</td>
<td>1.635</td>
<td>45.2</td>
<td></td>
<td></td>
<td>35926</td>
<td>7337718</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cai_XJTLU_task1_4</td>
<td>Cai2022</td>
<td>27</td>
<td>1.564</td>
<td>48.0</td>
<td></td>
<td></td>
<td>35926</td>
<td>7337718</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Cao_SCUT_task1_1</td>
<td>Cao2022</td>
<td>45</td>
<td>2.795</td>
<td>48.7</td>
<td>embeddings, pre-trained model</td>
<td></td>
<td>125330</td>
<td>8637250</td>
<td>BC-ResNet, CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>pretraining, training, adaptation</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_1</td>
<td>Lee2022</td>
<td>5</td>
<td>1.147</td>
<td>60.8</td>
<td>directly</td>
<td></td>
<td>126580</td>
<td>26763000</td>
<td>CNN, BC-Res2Net</td>
<td></td>
<td>categorical cross entropy</td>
<td>pytorch</td>
<td>pretraining, weight quantization, fine tuning, data-random-drop</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_2</td>
<td>Lee2022</td>
<td>6</td>
<td>1.187</td>
<td>59.2</td>
<td>directly</td>
<td></td>
<td>126580</td>
<td>26763000</td>
<td>CNN, BC-Res2Net</td>
<td></td>
<td>categorical cross entropy</td>
<td>pytorch</td>
<td>pretraining, weight quantization, fine tuning, data-random-drop</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_3</td>
<td>Lee2022</td>
<td>8</td>
<td>1.190</td>
<td>59.4</td>
<td>directly</td>
<td></td>
<td>126580</td>
<td>26763000</td>
<td>CNN, BC-Res2Net</td>
<td></td>
<td>categorical cross entropy</td>
<td>pytorch</td>
<td>pretraining, weight quantization, fine tuning, data-random-drop</td>
</tr>
<tr>
<td></td>
<td>Chang_HYU_task1_4</td>
<td>Lee2022a</td>
<td>7</td>
<td>1.187</td>
<td>59.3</td>
<td>directly</td>
<td></td>
<td>126580</td>
<td>26763000</td>
<td>CNN, BC-Res2Net</td>
<td></td>
<td>categorical cross entropy</td>
<td>pytorch</td>
<td>pretraining, weight quantization, fine tuning, data-random-drop</td>
</tr>
<tr>
<td></td>
<td>Dong_NCUT_task1_1</td>
<td>Dong2022</td>
<td>29</td>
<td>1.568</td>
<td>48.0</td>
<td></td>
<td></td>
<td>70608</td>
<td>28461216</td>
<td>FHR_Mobilenet</td>
<td></td>
<td>average</td>
<td>keras/tensorflow</td>
<td>training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Houyb_XDU_task1_1</td>
<td>Hou2022</td>
<td>22</td>
<td>1.481</td>
<td>49.3</td>
<td>embeddings</td>
<td></td>
<td>57957</td>
<td>28513000</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>data augment, training, adaptation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_1</td>
<td>Liang2022</td>
<td>38</td>
<td>1.934</td>
<td>41.3</td>
<td></td>
<td></td>
<td>85800</td>
<td>20500000</td>
<td>BC-ResNet</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_2</td>
<td>Liang2022</td>
<td>47</td>
<td>2.916</td>
<td>29.9</td>
<td></td>
<td></td>
<td>85800</td>
<td>20500000</td>
<td>BC-ResNet</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_3</td>
<td>Liang2022</td>
<td>43</td>
<td>2.701</td>
<td>28.5</td>
<td></td>
<td></td>
<td>85800</td>
<td>20500000</td>
<td>BC-ResNet</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Liang_UESTC_task1_4</td>
<td>Liang2022</td>
<td>32</td>
<td>1.612</td>
<td>44.1</td>
<td></td>
<td></td>
<td>110452</td>
<td>11186000</td>
<td>MobileNetV2</td>
<td></td>
<td></td>
<td>keras/tensorflow</td>
<td>training, adaptation, pruning, weight quantization</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>DCASE2022 baseline</td>
<td></td>
<td></td>
<td>1.532</td>
<td>44.2</td>
<td></td>
<td></td>
<td>46512</td>
<td>29234920</td>
<td>CNN</td>
<td></td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining, training, adaptation, pruning, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_1</td>
<td>Morocutti2022</td>
<td>12</td>
<td>1.339</td>
<td>53.8</td>
<td></td>
<td></td>
<td>65790</td>
<td>29325000</td>
<td>ensemble, CNN</td>
<td>3</td>
<td>average</td>
<td>pytorch</td>
<td>preprocessing, training teacher, training student, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_2</td>
<td>Morocutti2022</td>
<td>13</td>
<td>1.355</td>
<td>53.0</td>
<td></td>
<td></td>
<td>65790</td>
<td>29325000</td>
<td>ensemble, CNN</td>
<td>3</td>
<td>average</td>
<td>pytorch</td>
<td>preprocessing, training teacher, training student, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_3</td>
<td>Morocutti2022</td>
<td>10</td>
<td>1.320</td>
<td>54.7</td>
<td></td>
<td></td>
<td>65790</td>
<td>29325000</td>
<td>ensemble, CNN</td>
<td>3</td>
<td>average</td>
<td>pytorch</td>
<td>preprocessing, training teacher, training student, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Morocutti_JKU_task1_4</td>
<td>Morocutti2022</td>
<td>9</td>
<td>1.311</td>
<td>54.5</td>
<td></td>
<td></td>
<td>65790</td>
<td>29325000</td>
<td>ensemble, CNN</td>
<td>3</td>
<td>average</td>
<td>pytorch</td>
<td>preprocessing, training teacher, training student, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Olisaemeka_ARU_task1_1</td>
<td>Olisaemeka2022</td>
<td>39</td>
<td>2.055</td>
<td>36.4</td>
<td></td>
<td></td>
<td>96473</td>
<td>3283692</td>
<td>CNN</td>
<td></td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining, training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_1</td>
<td>Kim2022</td>
<td>25</td>
<td>1.504</td>
<td>51.7</td>
<td></td>
<td></td>
<td>113378</td>
<td>29481000</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Park_KT_task1_2</td>
<td>Kim2022</td>
<td>19</td>
<td>1.431</td>
<td>52.7</td>
<td></td>
<td></td>
<td>113378</td>
<td>29481000</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_1</td>
<td>Schmid2022</td>
<td>2</td>
<td>1.092</td>
<td>59.7</td>
<td>pre-trained model</td>
<td>PaSST</td>
<td>127046</td>
<td>29056324</td>
<td>RF-regularized CNNs, PaSST transformer</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training teacher, training student, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_2</td>
<td>Schmid2022</td>
<td>4</td>
<td>1.105</td>
<td>59.6</td>
<td>pre-trained model</td>
<td>PaSST</td>
<td>127046</td>
<td>29056324</td>
<td>RF-regularized CNNs, PaSST transformer</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training teacher, training student, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_3</td>
<td>Schmid2022</td>
<td>1</td>
<td>1.091</td>
<td>59.6</td>
<td>pre-trained model</td>
<td>PaSST</td>
<td>121610</td>
<td>28240924</td>
<td>RF-regularized CNNs, PaSST transformer</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training teacher, training student, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmid_CPJKU_task1_4</td>
<td>Schmid2022</td>
<td>3</td>
<td>1.102</td>
<td>59.4</td>
<td>pre-trained model</td>
<td>PaSST, AudioSet</td>
<td>121610</td>
<td>28240924</td>
<td>RF-regularized CNNs, PaSST transformer</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training teacher, training student, knowledge distillation, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Schmidt_FAU_task1_1</td>
<td>Schmidt2022</td>
<td>35</td>
<td>1.731</td>
<td>47.5</td>
<td></td>
<td></td>
<td>127943</td>
<td>15163468</td>
<td>CNN, SVM</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>pretraining, training, pruning, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_1</td>
<td>Singh2022</td>
<td>28</td>
<td>1.565</td>
<td>44.6</td>
<td>directly</td>
<td></td>
<td>13138</td>
<td>4129320</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
<td>keras/tensorflow</td>
<td>training (from scratch), pruning, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_2</td>
<td>Singh2022</td>
<td>31</td>
<td>1.606</td>
<td>44.3</td>
<td>directly</td>
<td></td>
<td>14886</td>
<td>5404520</td>
<td>CNN</td>
<td></td>
<td>maximum likelihood</td>
<td>keras/tensorflow</td>
<td>training (from scratch), pruning, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_3</td>
<td>Singh2022</td>
<td>23</td>
<td>1.492</td>
<td>45.9</td>
<td>directly</td>
<td></td>
<td>59570</td>
<td>18585480</td>
<td>CNN</td>
<td>5</td>
<td>average</td>
<td>keras/tensorflow</td>
<td>training (from scratch), pruning, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Singh_Surrey_task1_4</td>
<td>Singh2022</td>
<td>24</td>
<td>1.499</td>
<td>45.9</td>
<td>directly</td>
<td></td>
<td>60958</td>
<td>19831880</td>
<td>CNN</td>
<td>5</td>
<td>average</td>
<td>keras/tensorflow</td>
<td>training (from scratch), pruning, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_1</td>
<td>Sugahara2022</td>
<td>18</td>
<td>1.405</td>
<td>51.5</td>
<td></td>
<td></td>
<td>120229</td>
<td>26607000</td>
<td>MobileNet</td>
<td></td>
<td>weighted average</td>
<td>pytorch</td>
<td>training (from scratch), weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_2</td>
<td>Sugahara2022</td>
<td>15</td>
<td>1.389</td>
<td>51.6</td>
<td></td>
<td></td>
<td>120229</td>
<td>26607000</td>
<td>MobileNet</td>
<td></td>
<td>weighted average</td>
<td>pytorch</td>
<td>training (from scratch), weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_3</td>
<td>Sugahara2022</td>
<td>14</td>
<td>1.366</td>
<td>51.7</td>
<td></td>
<td></td>
<td>120229</td>
<td>26607000</td>
<td>MobileNet</td>
<td></td>
<td>weighted average</td>
<td>pytorch</td>
<td>training (from scratch), weight quantization</td>
</tr>
<tr>
<td></td>
<td>Sugahara_RION_task1_4</td>
<td>Sugahara2022</td>
<td>16</td>
<td>1.397</td>
<td>52.7</td>
<td></td>
<td></td>
<td>123346</td>
<td>26610000</td>
<td>MobileNet</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training (from scratch), weight quantization</td>
</tr>
<tr>
<td></td>
<td>Yu_XIAOMI_task1_1</td>
<td>Yu2022</td>
<td>21</td>
<td>1.456</td>
<td>46.2</td>
<td>embeddings</td>
<td></td>
<td>6306</td>
<td>16081000</td>
<td>CNN</td>
<td></td>
<td></td>
<td>keras/tensorflow</td>
<td>pretraining, training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_1</td>
<td>Zaragoza_Paredes2022</td>
<td>44</td>
<td>2.709</td>
<td>43.8</td>
<td></td>
<td></td>
<td>28320</td>
<td>28570080</td>
<td>CNN</td>
<td></td>
<td></td>
<td>keras/tensorflow</td>
<td>training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Zaragoza-Paredes_UPV_task1_2</td>
<td>Zaragoza_Paredes2022</td>
<td>46</td>
<td>2.904</td>
<td>41.9</td>
<td></td>
<td></td>
<td>28320</td>
<td>28570080</td>
<td>CNN</td>
<td></td>
<td></td>
<td>keras/tensorflow</td>
<td>training, weight quantization</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_1</td>
<td>Shao2022</td>
<td>40</td>
<td>2.096</td>
<td>54.9</td>
<td></td>
<td></td>
<td>127160</td>
<td>28228320</td>
<td>Mini-SegNet</td>
<td>3</td>
<td></td>
<td>keras/tensorflow</td>
<td>training, pruning, quantization aware training, weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Zhang_THUEE_task1_2</td>
<td>Shao2022</td>
<td>48</td>
<td>3.068</td>
<td>54.4</td>
<td></td>
<td></td>
<td>126078</td>
<td>28098645</td>
<td>Mini-SegNet</td>
<td>2</td>
<td></td>
<td>keras/tensorflow</td>
<td>training, pruning, quantization aware training, weight quantization, knowledge distillation</td>
</tr>
<tr>
<td></td>
<td>Zou_PKU_task1_1</td>
<td>Xin2022</td>
<td>20</td>
<td>1.442</td>
<td>56.3</td>
<td>embeddings</td>
<td></td>
<td>75562</td>
<td>28823618</td>
<td>CNN</td>
<td></td>
<td></td>
<td>pytorch</td>
<td>training, weight quantization</td>
</tr>
</tbody>
</table>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2022/technical_reports_task1.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Anastcio2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Anastcio2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Ai4edgept Submission to DCASE 2022 Low Complexity Acoustic Scene Classification Task1
       </h4>
<p style="text-align:left">
        Ricardo Anastácio<sup>1</sup>, Luís Ferreira<sup>2</sup>, Figueiredo Mónica<sup>1,3</sup> and Conde Bento Luís<sup>1,4</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>electronic engineering, Politécnico de Leiria, Leiria, Portugal, <sup>2</sup>University of Coimbra, Coimbra, Portugal, <sup>3</sup>Instituto de Telecomunicações, Portugal, <sup>4</sup>Institute of Systems and Robotics, Coimbra, Portugal
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">AI4EDGE_IPL_task1_1</span> <span class="label label-primary">AI4EDGE_IPL_task1_2</span> <span class="label label-primary">AI4EDGE_IPL_task1_3</span> <span class="label label-primary">AI4EDGE_IPL_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Anastcio2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Anastcio2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Anastcio2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_AI4EDGE_58_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Anastcio2022" class="panel-collapse collapse" id="collapse-Anastcio2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Ai4edgept Submission to DCASE 2022 Low Complexity Acoustic Scene Classification Task1
      </h4>
<p style="text-align:left">
<small>
        Ricardo Anastácio<sup>1</sup>, Luís Ferreira<sup>2</sup>, Figueiredo Mónica<sup>1,3</sup> and Conde Bento Luís<sup>1,4</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>electronic engineering, Politécnico de Leiria, Leiria, Portugal, <sup>2</sup>University of Coimbra, Coimbra, Portugal, <sup>3</sup>Instituto de Telecomunicações, Portugal, <sup>4</sup>Institute of Systems and Robotics, Coimbra, Portugal
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report details the submission to task1 of DCASE2022 competition. The task aims to classify acoustic scenes using devices with low computational power and memory. We propose two ensemble models for scene classification. The first model clusters classes into 2 groups, each of a two-network ensemble being responsible for intra-group discrimination, i.e. discriminating between the classes that are most related in the confusion matrix. The second model implements a canonical one-versus-all ten-network ensemble architecture followed by knowledge distillation, i.e. the ensemble model is used as the teacher network. The student is an optimised version of the DCASE2022 baseline architecture. In both models we resort to three different data pre-processing techniques: audio downsample; mel-spectrogram tuning; and data augmentation. We’ve used the DCASE2022 baseline for all networks - two-network ensemble, ten-network ensemble and student network - on which we have conducted an architecture’s hyperparameter search to identify the best performing architecture, while being compliant with DCASE2022 performance metrics. Results revealed that data pre-processing and knowledge distillation techniques improve overall performance. Nevertheless, a simple two-network ensemble without knowledge distillation, maintains the MACS and parameters size low, while achieving similar results.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         8kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         pitch shifting, time stretching, mixup, time masking, frequency masking
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, ensemble
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Anastcio2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_AI4EDGE_58_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Anastcio2022label" class="modal fade" id="bibtex-Anastcio2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexAnastcio2022label">
        Ai4edgept Submission to DCASE 2022 Low Complexity Acoustic Scene Classification Task1
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Anastcio2022,
    Author = "Anastácio, Ricardo and Ferreira, Luís and Mónica, Figueiredo and Luís, Conde Bento",
    title = "Ai4edgept Submission to {DCASE} 2022 Low Complexity Acoustic Scene Classification Task1",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This report details the submission to task1 of DCASE2022 competition. The task aims to classify acoustic scenes using devices with low computational power and memory. We propose two ensemble models for scene classification. The first model clusters classes into 2 groups, each of a two-network ensemble being responsible for intra-group discrimination, i.e. discriminating between the classes that are most related in the confusion matrix. The second model implements a canonical one-versus-all ten-network ensemble architecture followed by knowledge distillation, i.e. the ensemble model is used as the teacher network. The student is an optimised version of the DCASE2022 baseline architecture. In both models we resort to three different data pre-processing techniques: audio downsample; mel-spectrogram tuning; and data augmentation. We’ve used the DCASE2022 baseline for all networks - two-network ensemble, ten-network ensemble and student network - on which we have conducted an architecture’s hyperparameter search to identify the best performing architecture, while being compliant with DCASE2022 performance metrics. Results revealed that data pre-processing and knowledge distillation techniques improve overall performance. Nevertheless, a simple two-network ensemble without knowledge distillation, maintains the MACS and parameters size low, while achieving similar results."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cai2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Cai2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity Model Based on Depthwise Separable CNN for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Yiqiang Cai<sup>1</sup>, He Tang<sup>1</sup>, Chenyang Zhu<sup>2</sup>, Shengchen Li<sup>1</sup> and Xi Shao<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>School of Advanced Technology, Xi'an Jiaotong-Liverpool University, Suzhou, China, <sup>2</sup>School of Artificial Intelligence and Computing Sciences, Jiangnan University, Wuxi, China, <sup>3</sup>College of Tellecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cai_XJTLU_task1_1</span> <span class="label label-primary">Cai_XJTLU_task1_2</span> <span class="label label-primary">Cai_XJTLU_task1_3</span> <span class="label label-primary">Cai_XJTLU_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cai2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cai2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cai2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Cai_111_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cai2022" class="panel-collapse collapse" id="collapse-Cai2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity Model Based on Depthwise Separable CNN for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Yiqiang Cai<sup>1</sup>, He Tang<sup>1</sup>, Chenyang Zhu<sup>2</sup>, Shengchen Li<sup>1</sup> and Xi Shao<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>School of Advanced Technology, Xi'an Jiaotong-Liverpool University, Suzhou, China, <sup>2</sup>School of Artificial Intelligence and Computing Sciences, Jiangnan University, Wuxi, China, <sup>3</sup>College of Tellecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The task1 of DCASE 2022 put forward higher requirements for system complexity and the new datasets also brought greater challenges. We tried to reproduce several models in previous years, but did not get a good performance. Therefore, we introduced the depthwise separable CNN method to the baseline architecture, which successfully reduces the complexity and improves the accuracy. We also used three methods of data augmentation, mixup, pitch shifting and stretching to further improve the results.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, pitch shifting, spectrum correction
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cai2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Cai_111_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cai2022label" class="modal fade" id="bibtex-Cai2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCai2022label">
        Low-Complexity Model Based on Depthwise Separable CNN for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cai2022,
    Author = "Cai, Yiqiang and Tang, He and Zhu, Chenyang and Li, Shengchen and Shao, Xi",
    title = "Low-Complexity Model Based on Depthwise Separable {CNN} for Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "The task1 of DCASE 2022 put forward higher requirements for system complexity and the new datasets also brought greater challenges. We tried to reproduce several models in previous years, but did not get a good performance. Therefore, we introduced the depthwise separable CNN method to the baseline architecture, which successfully reduces the complexity and improves the accuracy. We also used three methods of data augmentation, mixup, pitch shifting and stretching to further improve the results."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Cao2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Cao2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity Acoustic Scene Classification Using Broadcasted ResNet and Data Augmentation
       </h4>
<p style="text-align:left">
        Wenchang Cao, Yanxiong Li, Qisheng Huang and Mingle Liu
       </p>
<p style="text-align:left">
<em>
         School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Cao_SCUT_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Cao2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Cao2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Cao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Cao_59_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Cao2022" class="panel-collapse collapse" id="collapse-Cao2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity Acoustic Scene Classification Using Broadcasted ResNet and Data Augmentation
      </h4>
<p style="text-align:left">
<small>
        Wenchang Cao, Yanxiong Li, Qisheng Huang and Mingle Liu
       </small>
<br/>
<small>
<em>
         School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic scene classification (ASC) is a task to classify each input audio recording into one class of pre-given acoustic scenes. As an important task in Detection and Classification of Acoustic Scenes and Events (DCASE), ASC has attracted a lot of attention from researchers in the community of audio and acoustic signal processing in recent years [1]-[4]. In the work of this report, we focus on the task of low-complexity ASC with multiple devices, namely, Task 1 of the DCASE2022 challenge [5]. In this task, a low-complexity model is required to classify audio recordings recorded by multiple devices (real and simulated). In the proposed ASC method, the BC-ResNet-Mod [6] is used as the backbone of our model whose training strategy is the Cross-Gradient Training (CGT) [7]. In addition, some data augmentation techniques are adopted for further improving the performance of the proposed method. The size of our model is 125.33 KB after model compression, which is lower than the size limit of 128 KB. Evaluated on the development dataset, our system obtains classification accuracy of 51.1%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, time stretching,pitch shifting,spectrum correction
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         BC-ResNet, CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Cao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Cao_59_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Cao2022label" class="modal fade" id="bibtex-Cao2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexCao2022label">
        Low-Complexity Acoustic Scene Classification Using Broadcasted ResNet and Data Augmentation
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Cao2022,
    Author = "Cao, Wenchang and Li, Yanxiong and Huang, Qisheng and Liu, Mingle",
    title = "Low-Complexity Acoustic Scene Classification Using Broadcasted {ResNet} and Data Augmentation",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "Acoustic scene classification (ASC) is a task to classify each input audio recording into one class of pre-given acoustic scenes. As an important task in Detection and Classification of Acoustic Scenes and Events (DCASE), ASC has attracted a lot of attention from researchers in the community of audio and acoustic signal processing in recent years [1]-[4]. In the work of this report, we focus on the task of low-complexity ASC with multiple devices, namely, Task 1 of the DCASE2022 challenge [5]. In this task, a low-complexity model is required to classify audio recordings recorded by multiple devices (real and simulated). In the proposed ASC method, the BC-ResNet-Mod [6] is used as the backbone of our model whose training strategy is the Cross-Gradient Training (CGT) [7]. In addition, some data augmentation techniques are adopted for further improving the performance of the proposed method. The size of our model is 125.33 KB after model compression, which is lower than the size limit of 128 KB. Evaluated on the development dataset, our system obtains classification accuracy of 51.1\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Dong2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Dong2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Fhr_mobilenet
       </h4>
<p style="text-align:left">
        Hongxia Dong<sup>1</sup>, Lin Zhang<sup>1</sup>, Xichang Cai<sup>1</sup>, Menglong Wu<sup>1</sup>, Ziling Qiao<sup>1</sup>, Yanggang Gan<sup>2</sup> and Juan Wu<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Electronic and Communication Engineering, North China University of Technology, Beijing, China, <sup>2</sup>Electronic and Communication Engineering, North China University Of Technology, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Dong_NCUT_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Dong2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Dong2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Dong2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Dong_16_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Dong2022" class="panel-collapse collapse" id="collapse-Dong2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Fhr_mobilenet
      </h4>
<p style="text-align:left">
<small>
        Hongxia Dong<sup>1</sup>, Lin Zhang<sup>1</sup>, Xichang Cai<sup>1</sup>, Menglong Wu<sup>1</sup>, Ziling Qiao<sup>1</sup>, Yanggang Gan<sup>2</sup> and Juan Wu<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Electronic and Communication Engineering, North China University of Technology, Beijing, China, <sup>2</sup>Electronic and Communication Engineering, North China University Of Technology, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our submission for Task1 of DCASE2022 challenge. We calculated 128 log-mel energies under the original sampling rate of 44.1KHz for each time slice by taking 2048 FFT points with 50% overlap. Additionally, deltas and deltadeltas were calculated from the log Mel spectrogram and stacked into the channel axis. The resulting spectrograms were of size 128 frequency bins, 43 time samples and 3 channels with each representing log-mel spectrograms, its delta features and its delta-delta features respectively. Then, the three channel feature map is fed into the mobilenet-based frequency high-resolution network. Finally, after 1 × 1 convolution and global average pooling, the classification results are obtained through softmax output. The classification accuracy of our proposed model is 53.9% with a loss value of 1.378. The number of parameters of the model is 70.608K, where each parameter is represented using int8 and the MACs are 28.461M.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies,delta and delta-delta
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         FHR_Mobilenet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Dong2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Dong_16_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Dong2022label" class="modal fade" id="bibtex-Dong2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexDong2022label">
        Acoustic Scene Classification Based on Fhr_mobilenet
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Dong2022,
    Author = "Dong, Hongxia and Zhang, Lin and Cai, Xichang and Wu, Menglong and Qiao, Ziling and Gan, Yanggang and Wu, Juan",
    title = "Acoustic Scene Classification Based on Fhr\_mobilenet",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes our submission for Task1 of DCASE2022 challenge. We calculated 128 log-mel energies under the original sampling rate of 44.1KHz for each time slice by taking 2048 FFT points with 50\% overlap. Additionally, deltas and deltadeltas were calculated from the log Mel spectrogram and stacked into the channel axis. The resulting spectrograms were of size 128 frequency bins, 43 time samples and 3 channels with each representing log-mel spectrograms, its delta features and its delta-delta features respectively. Then, the three channel feature map is fed into the mobilenet-based frequency high-resolution network. Finally, after 1 × 1 convolution and global average pooling, the classification results are obtained through softmax output. The classification accuracy of our proposed model is 53.9\% with a loss value of 1.378. The number of parameters of the model is 70.608K, where each parameter is represented using int8 and the MACs are 28.461M."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Hou2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Hou2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity for DCASE 2022 Task 1A Challenge
       </h4>
<p style="text-align:left">
        YuanBo Hou
       </p>
<p style="text-align:left">
<em>
         Telecommunications Engineering, xidian university, Xi'an, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Houyb_XDU_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Hou2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Hou2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Hou2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Houyb_1_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Hou2022').collapse('show');window.location.hash='#Hou2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Hou2022" class="panel-collapse collapse" id="collapse-Hou2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity for DCASE 2022 Task 1A Challenge
      </h4>
<p style="text-align:left">
<small>
        YuanBo Hou
       </small>
<br/>
<small>
<em>
         Telecommunications Engineering, xidian university, Xi'an, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the systems for task1/subtask A of the DCASE 2022 challenge. In order to reduce the number of model parameters and improve accuracy. In this work, I use a simple neural networks with causal convolution and bottleneck structure. The log-mel spectrograms are extracted to train the acoustic scene classification model. The mix-up and Specaugmentation are used to augment the acoustic features. My system achieves higher classification accuracies and lower log loss in the development dataset than baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Hou2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Houyb_1_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="" style="text-decoration:none;border-bottom:0;padding-bottom:6px" target="_blank" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Hou2022label" class="modal fade" id="bibtex-Hou2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHou2022label">
        Low-Complexity for DCASE 2022 Task 1A Challenge
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Hou2022,
    Author = "Hou, YuanBo",
    title = "Low-Complexity for {DCASE} 2022 Task {1A} Challenge",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes the systems for task1/subtask A of the DCASE 2022 challenge. In order to reduce the number of model parameters and improve accuracy. In this work, I use a simple neural networks with causal convolution and bottleneck structure. The log-mel spectrograms are extracted to train the acoustic scene classification model. The mix-up and Specaugmentation are used to augment the acoustic features. My system achieves higher classification accuracies and lower log loss in the development dataset than baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Kim2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Kim2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Kt Submission for the DCASE 2022 Challenge: Modernized Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        TaeSoo Kim, GaHui Lee and JaeHan Park
       </p>
<p style="text-align:left">
<em>
         AI2XL, KT Corporation, Seoul, South Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Park_KT_task1_1</span> <span class="label label-primary">Park_KT_task1_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Kim2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Kim2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Kim2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Park_15_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Kim2022" class="panel-collapse collapse" id="collapse-Kim2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Kt Submission for the DCASE 2022 Challenge: Modernized Convolutional Neural Networks for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        TaeSoo Kim, GaHui Lee and JaeHan Park
       </small>
<br/>
<small>
<em>
         AI2XL, KT Corporation, Seoul, South Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical reports, we present our team’s submission for DCASE 2022 TASK1 which is the low complexity Acoustic Scene Classification (ASC). We gradually modernized a neural network architecture design starting from the baseline model and discover several key components that contribute to the performance. To meet constraints of the model complexity, the number of parameters and the number of MACs are considered while applying each designs. As a result, our model achieves 1.2593 log-loss and 54.03% accuracy on the development set, while having less than 114k of total parameters (including the zero-valued) and 30 million MACs.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Kim2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Park_15_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Kim2022label" class="modal fade" id="bibtex-Kim2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKim2022label">
        Kt Submission for the DCASE 2022 Challenge: Modernized Convolutional Neural Networks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Kim2022,
    Author = "Kim, TaeSoo and Lee, GaHui and Park, JaeHan",
    title = "Kt Submission for the {DCASE} 2022 Challenge: Modernized Convolutional Neural Networks for Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical reports, we present our team’s submission for DCASE 2022 TASK1 which is the low complexity Acoustic Scene Classification (ASC). We gradually modernized a neural network architecture design starting from the baseline model and discover several key components that contribute to the performance. To meet constraints of the model complexity, the number of parameters and the number of MACs are considered while applying each designs. As a result, our model achieves 1.2593 log-loss and 54.03\% accuracy on the development set, while having less than 114k of total parameters (including the zero-valued) and 30 million MACs."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Lee2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Hyu Submission for the DCASE 2022: Efficient Fine-Tuning Method Using Device-Aware Data-Random-Drop for Device-Imbalanced Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Joo-Hyun Lee, Jeong-Hwan Choi, Pil Moo Byun and Joon-Hyuk Chang
       </p>
<p style="text-align:left">
<em>
         Electronic Engineering, Hanyang University, Seoul, Republic of Korea
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Chang_HYU_task1_1</span> <span class="label label-primary">Chang_HYU_task1_2</span> <span class="label label-primary">Chang_HYU_task1_3</span> <span class="label label-primary">Chang_HYU_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lee2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Chang_130_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee2022" class="panel-collapse collapse" id="collapse-Lee2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Hyu Submission for the DCASE 2022: Efficient Fine-Tuning Method Using Device-Aware Data-Random-Drop for Device-Imbalanced Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Joo-Hyun Lee, Jeong-Hwan Choi, Pil Moo Byun and Joon-Hyuk Chang
       </small>
<br/>
<small>
<em>
         Electronic Engineering, Hanyang University, Seoul, Republic of Korea
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This paper address the Hanyang University team submission for the DCASE 2022 Challenge Low-Complexity Acoustic Scene Classification task. The task aims to design a generalized audio scene classification system for various devices under low complexity and short input time conditions. We followed two strategies to achieve our goal: improving the model structure for short segmented audio and adopting transfer learning methods that are generalizable to unknown devices. Based on the BC-ResNet, which showed the best performance in DCASE 2021 challenge, we incorporated the method proposed in the field of short-duration speaker verification to secure high accuracy. In addition, we proposed a novel finetuning method using device-aware data-random-drop to get a generalized model across multiple devices. Most of the training dataset is data recorded with a specific device. We devised a fine tuning method that gradually excludes data recorded with a specific device from mini-batch during training, and this method improves generalization performance. Following the official protocol of cross validation setup from the TAU Urban Acoustic Scenes 2022 Mobile development dataset, we achieve 70.1% accuracy and 0.835 multi class cross-entropy loss, respectively.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, SpecAugment, time masking, frequency masking, temporal shuffle
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, BC-Res2Net
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         categorical cross entropy
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Chang_130_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee2022label" class="modal fade" id="bibtex-Lee2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee2022label">
        Hyu Submission for the DCASE 2022: Efficient Fine-Tuning Method Using Device-Aware Data-Random-Drop for Device-Imbalanced Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lee2022,
    Author = "Lee, Joo-Hyun and Choi, Jeong-Hwan and Byun, Pil Moo and Chang, Joon-Hyuk",
    title = "Hyu Submission for the {DCASE} 2022: Efficient Fine-Tuning Method Using Device-Aware Data-Random-Drop for Device-Imbalanced Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This paper address the Hanyang University team submission for the DCASE 2022 Challenge Low-Complexity Acoustic Scene Classification task. The task aims to design a generalized audio scene classification system for various devices under low complexity and short input time conditions. We followed two strategies to achieve our goal: improving the model structure for short segmented audio and adopting transfer learning methods that are generalizable to unknown devices. Based on the BC-ResNet, which showed the best performance in DCASE 2021 challenge, we incorporated the method proposed in the field of short-duration speaker verification to secure high accuracy. In addition, we proposed a novel finetuning method using device-aware data-random-drop to get a generalized model across multiple devices. Most of the training dataset is data recorded with a specific device. We devised a fine tuning method that gradually excludes data recorded with a specific device from mini-batch during training, and this method improves generalization performance. Following the official protocol of cross validation setup from the TAU Urban Acoustic Scenes 2022 Mobile development dataset, we achieve 70.1\% accuracy and 0.835 multi class cross-entropy loss, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Liang2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Liang2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity Acoustic Scene Classification Based on Residual Net
       </h4>
<p style="text-align:left">
        Jiangnan Liang, Cheng Zeng, Chuang Shi, Le Zhang, Yisen Zhou, Yuehong Li, Yanyu Zhou and Tianqi Tan
       </p>
<p style="text-align:left">
<em>
         University of Electronic Science and Technology of China, Chengdu, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Liang_UESTC_task1_1</span> <span class="label label-primary">Liang_UESTC_task1_2</span> <span class="label label-primary">Liang_UESTC_task1_3</span> <span class="label label-primary">Liang_UESTC_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Liang2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Liang2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Liang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Liang_64_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Liang2022" class="panel-collapse collapse" id="collapse-Liang2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity Acoustic Scene Classification Based on Residual Net
      </h4>
<p style="text-align:left">
<small>
        Jiangnan Liang, Cheng Zeng, Chuang Shi, Le Zhang, Yisen Zhou, Yuehong Li, Yanyu Zhou and Tianqi Tan
       </small>
<br/>
<small>
<em>
         University of Electronic Science and Technology of China, Chengdu, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the submitted systems for task 1 of the DCASE 2022 challenge. The log-mel energies, delta features and delta-delta features were extracted to train the model. We adopted a total of eight data augmentation methods. BC-ResNet and MobileNetV2 were used as training model. We used knowledge distillation and quantization to compress the model. Our systems achieved lower log loss and higher accuracy in the development dataset than the baseline system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         time masking, frequency masking, time warping, mixup; time masking, frequency masking, time warping, frequency warping, mixup; noise addition, pitch shifting, speed changing, time masking, mixup
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         BC-ResNet; MobileNetV2
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Liang2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Liang_64_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Liang2022label" class="modal fade" id="bibtex-Liang2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLiang2022label">
        Low-Complexity Acoustic Scene Classification Based on Residual Net
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Liang2022,
    Author = "Liang, Jiangnan and Zeng, Cheng and Shi, Chuang and Zhang, Le and Zhou, Yisen and Li, Yuehong and Zhou, Yanyu and Tan, Tianqi",
    title = "Low-Complexity Acoustic Scene Classification Based on Residual Net",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes the submitted systems for task 1 of the DCASE 2022 challenge. The log-mel energies, delta features and delta-delta features were extracted to train the model. We adopted a total of eight data augmentation methods. BC-ResNet and MobileNetV2 were used as training model. We used knowledge distillation and quantization to compress the model. Our systems achieved lower log loss and higher accuracy in the development dataset than the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Morocutti2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Morocutti2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Receptive Field Regularized CNNs with Traditional Audio Augmentations
       </h4>
<p style="text-align:left">
        Tobias Morocutti and Diaaeldin Shalaby
       </p>
<p style="text-align:left">
<em>
         Johannes Kepler University, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Morocutti_JKU_task1_1</span> <span class="label label-primary">Morocutti_JKU_task1_2</span> <span class="label label-primary">Morocutti_JKU_task1_3</span> <span class="label label-primary">Morocutti_JKU_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Morocutti2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Morocutti2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Morocutti2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Morocutti_96_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Morocutti2022" class="panel-collapse collapse" id="collapse-Morocutti2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Receptive Field Regularized CNNs with Traditional Audio Augmentations
      </h4>
<p style="text-align:left">
<small>
        Tobias Morocutti and Diaaeldin Shalaby
       </small>
<br/>
<small>
<em>
         Johannes Kepler University, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our system for Task 1 (Low-Complexity Acoustic Scene Classification) of the DCASE2022 Challenge. Due to the limited allowed complexity of the model to submit, we use a teacher-student approach. The teacher is a Receptive Field (RF) regularized CNN model and the student is a simpler 5-layer CNN with batch normalization, dropout and maxpool layers. In addition, some data augmentation techniques, such as adding gaussian noise, shifting, pitch shifting and time stretching are adopted for expanding the diversity of the dataset. Our system achieves an accuracy of 53.4% and a multiclass cross-entropy (log loss) of 1.279 on the development dataset. The student model has 21,930 parameters and a Multiply accumulate count of 9.775 million.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         22.05kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, pitch shifting, time stretching, shifting, adding gaussian noise
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         mel-spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         ensemble, CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Morocutti2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Morocutti_96_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Morocutti2022label" class="modal fade" id="bibtex-Morocutti2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexMorocutti2022label">
        Receptive Field Regularized CNNs with Traditional Audio Augmentations
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Morocutti2022,
    Author = "Morocutti, Tobias and Shalaby, Diaaeldin",
    title = "Receptive Field Regularized {CNNs} with Traditional Audio Augmentations",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes our system for Task 1 (Low-Complexity Acoustic Scene Classification) of the DCASE2022 Challenge. Due to the limited allowed complexity of the model to submit, we use a teacher-student approach. The teacher is a Receptive Field (RF) regularized CNN model and the student is a simpler 5-layer CNN with batch normalization, dropout and maxpool layers. In addition, some data augmentation techniques, such as adding gaussian noise, shifting, pitch shifting and time stretching are adopted for expanding the diversity of the dataset. Our system achieves an accuracy of 53.4\% and a multiclass cross-entropy (log loss) of 1.279 on the development dataset. The student model has 21,930 parameters and a Multiply accumulate count of 9.775 million."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Olisaemeka2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Olisaemeka2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Submission to DCASE 2022 Task 1: Depthwise Separable Convolutions for Low-Complexity Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Chukwuebuka Olisaemeka and Lakshmi Babu Saheer
       </p>
<p style="text-align:left">
<em>
         Computing Sciences, Anglia Ruskin University, Cambridge, United Kingdom
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Olisaemeka_ARU_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Olisaemeka2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Olisaemeka2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Olisaemeka2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Olisaemeka_92_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Olisaemeka2022" class="panel-collapse collapse" id="collapse-Olisaemeka2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Submission to DCASE 2022 Task 1: Depthwise Separable Convolutions for Low-Complexity Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Chukwuebuka Olisaemeka and Lakshmi Babu Saheer
       </small>
<br/>
<small>
<em>
         Computing Sciences, Anglia Ruskin University, Cambridge, United Kingdom
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the details of the TASK1 submission to the DCASE2022 challenge. The aim of this task is to design an acoustic scene classification system that targets devices with low memory and computational allowance. The task also aims to build systems that can generalize across multiple devices. To achieve this objective, a model using Depthwise Separable Convolutional layers is proposed, which reduces the number of parameters and computations required compared to the normal convolutional layers. This work further proposes the use of dilated kernels, which increase the receptive field of the convolutional layer without increasing the number of parameters to be learned. Finally, quantization is applied to reduce the model complexity. The proposed system achieves an average test accuracy of 39% and log loss of 1.878 on TAU Urban Acoustic Scenes 2022 Mobile, development dataset with a parameter count of 96.473k and 3.284 MMACs.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Olisaemeka2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Olisaemeka_92_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Olisaemeka2022label" class="modal fade" id="bibtex-Olisaemeka2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexOlisaemeka2022label">
        Submission to DCASE 2022 Task 1: Depthwise Separable Convolutions for Low-Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Olisaemeka2022,
    Author = "Olisaemeka, Chukwuebuka and Babu Saheer, Lakshmi",
    title = "Submission to {DCASE} 2022 Task 1: Depthwise Separable Convolutions for Low-Complexity Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes the details of the TASK1 submission to the DCASE2022 challenge. The aim of this task is to design an acoustic scene classification system that targets devices with low memory and computational allowance. The task also aims to build systems that can generalize across multiple devices. To achieve this objective, a model using Depthwise Separable Convolutional layers is proposed, which reduces the number of parameters and computations required compared to the normal convolutional layers. This work further proposes the use of dilated kernels, which increase the receptive field of the convolutional layer without increasing the number of parameters to be learned. Finally, quantization is applied to reduce the model complexity. The proposed system achieves an average test accuracy of 39\% and log loss of 1.878 on TAU Urban Acoustic Scenes 2022 Mobile, development dataset with a parameter count of 96.473k and 3.284 MMACs."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Pham2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Pham2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity Deep Learning Frameworks for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Lam Pham<sup>1</sup>, Ngo Dat<sup>2</sup>, Anahid Naghibzadeh-Jalali<sup>1</sup> and Alexander Schindler<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Center for Digital Safety &amp; Security, Austrian Institute of Technology, Vienna, Austria, <sup>2</sup>School of computer science and electronic engineering, Essex University, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">AIT_Essex_task1_1</span> <span class="label label-primary">AIT_Essex_task1_2</span> <span class="label label-primary">AIT_Essex_task1_3</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Pham2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Pham2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Pham2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_AIT_5_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Pham2022" class="panel-collapse collapse" id="collapse-Pham2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity Deep Learning Frameworks for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Lam Pham<sup>1</sup>, Ngo Dat<sup>2</sup>, Anahid Naghibzadeh-Jalali<sup>1</sup> and Alexander Schindler<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Center for Digital Safety &amp; Security, Austrian Institute of Technology, Vienna, Austria, <sup>2</sup>School of computer science and electronic engineering, Essex University, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we presents low-complexity deep learning frameworks for acoustic scene classification (ASC). The proposed frameworks can be separated into four main steps: Front-end spectrogram extraction, online data augmentation, back-end classification, and late fusion of predicted probabilities. In particular, we initially transform audio recordings into Mel, Gammatone, and CQT spectrograms. Next, data augmentation methods of Random Cropping, Specaugment, and Mixup are then applied on spectrograms. Augmented spectrograms are then fed into deep learning based classifiers. Finally, probabilities which obtained from three individual classifiers, which are trained with three type of spectrograms independently, are fused to achieve the best performance. Our experiments, which are conducted on DCASE 2022 Task 1 Development dataset, achieve low-complexity frameworks and the best classification accuracy of 60.1%, improving DCASE baseline by 17.2%.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, random cropping, SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         CQT, Gammatonegram, Mel
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         late fusion of predicted probabilities
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Pham2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_AIT_5_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Pham2022label" class="modal fade" id="bibtex-Pham2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPham2022label">
        Low-Complexity Deep Learning Frameworks for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Pham2022,
    Author = "Pham, Lam and Dat, Ngo and Naghibzadeh-Jalali, Anahid and Schindler, Alexander",
    title = "Low-Complexity Deep Learning Frameworks for Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this report, we presents low-complexity deep learning frameworks for acoustic scene classification (ASC). The proposed frameworks can be separated into four main steps: Front-end spectrogram extraction, online data augmentation, back-end classification, and late fusion of predicted probabilities. In particular, we initially transform audio recordings into Mel, Gammatone, and CQT spectrograms. Next, data augmentation methods of Random Cropping, Specaugment, and Mixup are then applied on spectrograms. Augmented spectrograms are then fed into deep learning based classifiers. Finally, probabilities which obtained from three individual classifiers, which are trained with three type of spectrograms independently, are fused to achieve the best performance. Our experiments, which are conducted on DCASE 2022 Task 1 Development dataset, achieve low-complexity frameworks and the best classification accuracy of 60.1\%, improving DCASE baseline by 17.2\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schmid2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Schmid2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        CP-JKU Submission to Dcase22: Distilling Knowledge for Low-Complexity Convolutional Neural Networks From a Patchout Audio Transformer
       </h4>
<p style="text-align:left">
        Florian Schmid<sup>1,2</sup>, Shahed Masoudian<sup>2</sup>, Khaled Koutini<sup>2</sup> and Gerhard Widmer<sup>1,2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Computational Perception (CP), Johannes Kepler University (JKU) Linz, Linz, Austria, <sup>2</sup>LIT Artificial Intelligence Lab, Johannes Kepler University (JKU) Linz, Linz, Austria
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Schmid_CPJKU_task1_1</span> <span class="label label-primary">Schmid_CPJKU_task1_2</span> <span class="label label-primary">Schmid_CPJKU_task1_3</span> <span class="label label-primary">Schmid_CPJKU_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
<span class="label label-success">
         Judges’ award
        </span>
</p>
<button aria-controls="collapse-Schmid2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schmid2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schmid2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Schmid_77_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Schmid2022').collapse('show');window.location.hash='#Schmid2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schmid2022" class="panel-collapse collapse" id="collapse-Schmid2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       CP-JKU Submission to Dcase22: Distilling Knowledge for Low-Complexity Convolutional Neural Networks From a Patchout Audio Transformer
      </h4>
<p style="text-align:left">
<small>
        Florian Schmid<sup>1,2</sup>, Shahed Masoudian<sup>2</sup>, Khaled Koutini<sup>2</sup> and Gerhard Widmer<sup>1,2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Computational Perception (CP), Johannes Kepler University (JKU) Linz, Linz, Austria, <sup>2</sup>LIT Artificial Intelligence Lab, Johannes Kepler University (JKU) Linz, Linz, Austria
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe the CP-JKU team’s submission for Task 1 Low-Complexity Acoustic Scene Classification of the DCASE 22 challenge [1]. We use Knowledge Distillation to teach low-complexity CNN student models from Patchout Spectrogram Transformer (PaSST) models. We use the pre-trained PaSST models on Audioset and fine-tune them on the TAU Urban Acoustic Scenes 2022 Mobile development dataset. We experiment with using an ensemble of teachers, different receptive fields of the student models, and mixing frequency-wise statistics of spectrograms to enhance generalization to unseen devices. Finally, the student models are quantized in order to perform inference computations using 8 bit integers, simulating the low-complexity constraints of edge devices.
      </p>
<p>
<strong>
        Awards:
       </strong>
       Judges’ award
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         32.0kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, mixstyle, pitch shifting; mixstyle, pitch shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         RF-regularized CNNs, PaSST transformer
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schmid2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Schmid_77_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/CPJKU/cpjku_dcase22" style="text-decoration:none;border-bottom:0;padding-bottom:6px" target="_blank" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schmid2022label" class="modal fade" id="bibtex-Schmid2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchmid2022label">
        CP-JKU Submission to Dcase22: Distilling Knowledge for Low-Complexity Convolutional Neural Networks From a Patchout Audio Transformer
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Schmid2022,
    Author = "Schmid, Florian and Masoudian, Shahed and Koutini, Khaled and Widmer, Gerhard",
    title = "{CP-JKU} Submission to Dcase22: Distilling Knowledge for Low-Complexity Convolutional Neural Networks From a Patchout Audio Transformer",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "In this technical report, we describe the CP-JKU team’s submission for Task 1 Low-Complexity Acoustic Scene Classification of the DCASE 22 challenge [1]. We use Knowledge Distillation to teach low-complexity CNN student models from Patchout Spectrogram Transformer (PaSST) models. We use the pre-trained PaSST models on Audioset and fine-tune them on the TAU Urban Acoustic Scenes 2022 Mobile development dataset. We experiment with using an ensemble of teachers, different receptive fields of the student models, and mixing frequency-wise statistics of spectrograms to enhance generalization to unseen devices. Finally, the student models are quantized in order to perform inference computations using 8 bit integers, simulating the low-complexity constraints of edge devices."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Schmidt2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Schmidt2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Structured Filter Pruning and Feature Selection for Low Complexity Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Lorenz Schmidt, Beran Kiliç and Nils Peters
       </p>
<p style="text-align:left">
<em>
         International Audio Laboratories, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Schmidt_FAU_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Schmidt2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Schmidt2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Schmidt2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Schmidt_93_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Schmidt2022" class="panel-collapse collapse" id="collapse-Schmidt2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Structured Filter Pruning and Feature Selection for Low Complexity Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Lorenz Schmidt, Beran Kiliç and Nils Peters
       </small>
<br/>
<small>
<em>
         International Audio Laboratories, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       The DCASE challenge track 1 provides a dataset for Acoustic Scene Classification (ASC), a popular problem in machine learning. This years challenge shortens the provided audio clips to 1 sec, adds a Multiply-Accumulate operations (MAC) constrain and additionally counts all parameters of the model. We tackle the problem by using three approaches: First we use a linear model with global moments of the spectrogram, getting into reach of the baseline; then we use feature selection to reduce generalization gap and MACs; and finally, structured filter pruning to bring the number of parameters below the parameter constraint. Using the evaluation split of the development dataset, our result shows an increase to 49.1% overall accuracy compared to the baseline system with 42.9% accuracy.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         16kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, rolling, SpecAugment
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN, SVM
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Schmidt2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Schmidt_93_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Schmidt2022label" class="modal fade" id="bibtex-Schmidt2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSchmidt2022label">
        Structured Filter Pruning and Feature Selection for Low Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Schmidt2022,
    Author = "Schmidt, Lorenz and Kiliç, Beran and Peters, Nils",
    title = "Structured Filter Pruning and Feature Selection for Low Complexity Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "The DCASE challenge track 1 provides a dataset for Acoustic Scene Classification (ASC), a popular problem in machine learning. This years challenge shortens the provided audio clips to 1 sec, adds a Multiply-Accumulate operations (MAC) constrain and additionally counts all parameters of the model. We tackle the problem by using three approaches: First we use a linear model with global moments of the spectrogram, getting into reach of the baseline; then we use feature selection to reduce generalization gap and MACs; and finally, structured filter pruning to bring the number of parameters below the parameter constraint. Using the evaluation split of the development dataset, our result shows an increase to 49.1\% overall accuracy compared to the baseline system with 42.9\% accuracy."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Shao2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Shao2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Mini-Segnet for Low-Complexity Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Yun-Fei Shao<sup>1</sup>, Xuan Zhang<sup>2</sup>, Ge-Ge Bing<sup>1</sup>, Ke-Meng Zhao<sup>1</sup>, Jun-Jie Xu<sup>2</sup>, Yong Ma<sup>2</sup> and Wei-Qiang Zhang<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Department of Electronic Engineering, Tsinghua University, Beijing, China, <sup>2</sup>School of Lingustic Sciences and Arts, Jiangsu Normal University, Xuzhou, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_THUEE_task1_1</span> <span class="label label-primary">Zhang_THUEE_task1_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Shao2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Shao2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Shao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zhang_34_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Shao2022" class="panel-collapse collapse" id="collapse-Shao2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Mini-Segnet for Low-Complexity Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Yun-Fei Shao<sup>1</sup>, Xuan Zhang<sup>2</sup>, Ge-Ge Bing<sup>1</sup>, Ke-Meng Zhao<sup>1</sup>, Jun-Jie Xu<sup>2</sup>, Yong Ma<sup>2</sup> and Wei-Qiang Zhang<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Department of Electronic Engineering, Tsinghua University, Beijing, China, <sup>2</sup>School of Lingustic Sciences and Arts, Jiangsu Normal University, Xuzhou, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report details the architecture we used to address task 1 of the DCASE2022 challenge. The goal of the task is to design an audio scene classification system for device-imbalanced datasets under the constraints of model complexity. Our architecture is based on SegNet, adding an instance normalization layer to normalize the activations of the previous layer at each step. Log-mel spectrograms, delta features, and delta-delta features are extracted to train the acoustic scene classification model. A total of 6 data augmentations are applied as follows: mixup, time and frequency domain masking, image augmentation, auto level, pix2pix, and random crop. We apply three model compression schemes: pruning, quantization, and knowledge distillation to reduce model complexity. The proposed system achieves higher classification accuracies and lower log loss than the baseline system. After model compression, our model achieves an average accuracy of 54.11% within the 127.2 K parameters size, 8-bit quantization, and MMACs less than 30 M.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, ImageDataGenerator, temporal crop, Auto levels, pix2pix
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         Mini-SegNet
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Shao2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zhang_34_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Shao2022label" class="modal fade" id="bibtex-Shao2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexShao2022label">
        Mini-Segnet for Low-Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Shao2022,
    Author = "Shao, Yun-Fei and Zhang, Xuan and Bing, Ge-Ge and Zhao, Ke-Meng and Xu, Jun-Jie and Ma, Yong and Zhang, Wei-Qiang",
    title = "Mini-Segnet for Low-Complexity Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This report details the architecture we used to address task 1 of the DCASE2022 challenge. The goal of the task is to design an audio scene classification system for device-imbalanced datasets under the constraints of model complexity. Our architecture is based on SegNet, adding an instance normalization layer to normalize the activations of the previous layer at each step. Log-mel spectrograms, delta features, and delta-delta features are extracted to train the acoustic scene classification model. A total of 6 data augmentations are applied as follows: mixup, time and frequency domain masking, image augmentation, auto level, pix2pix, and random crop. We apply three model compression schemes: pruning, quantization, and knowledge distillation to reduce model complexity. The proposed system achieves higher classification accuracies and lower log loss than the baseline system. After model compression, our model achieves an average accuracy of 54.11\% within the 127.2 K parameters size, 8-bit quantization, and MMACs less than 30 M."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Singh2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Singh2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity CNNs for Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Arshdeep Singh, James A King, Xubo Liu, Wenwu Wang and Mark D. Plumbley
       </p>
<p style="text-align:left">
<em>
         CVSSP, University of Surrey, Guildford, UK
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Singh_Surrey_task1_1</span> <span class="label label-primary">Singh_Surrey_task1_2</span> <span class="label label-primary">Singh_Surrey_task1_3</span> <span class="label label-primary">Singh_Surrey_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Singh2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Singh2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Singh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Singh_83_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Singh2022').collapse('show');window.location.hash='#Singh2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Singh2022" class="panel-collapse collapse" id="collapse-Singh2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity CNNs for Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Arshdeep Singh, James A King, Xubo Liu, Wenwu Wang and Mark D. Plumbley
       </small>
<br/>
<small>
<em>
         CVSSP, University of Surrey, Guildford, UK
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the SurreyAudioTeam22’s submission for DCASE 2022 ASC Task 1, Low-Complexity Acoustic Scene Classification (ASC). The task has two rules, (a) the ASC framework should have maximum 128K parameters, and (b) there should be a maximum of 30 millions multiply-accumulate operations (MACs) per inference. In this report, we present lowcomplexity systems for ASC that follow the rules intended for the task
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         maximum likelihood; average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Singh2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Singh_83_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Arshdeep-Singh-Boparai/DCASE2022_Challenge" style="text-decoration:none;border-bottom:0;padding-bottom:6px" target="_blank" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Singh2022label" class="modal fade" id="bibtex-Singh2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSingh2022label">
        Low-Complexity CNNs for Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Singh2022,
    Author = "Singh, Arshdeep and King, James A and Liu, Xubo and Wang, Wenwu and D. Plumbley, Mark",
    title = "Low-Complexity {CNNs} for Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes the SurreyAudioTeam22’s submission for DCASE 2022 ASC Task 1, Low-Complexity Acoustic Scene Classification (ASC). The task has two rules, (a) the ASC framework should have maximum 128K parameters, and (b) there should be a maximum of 30 millions multiply-accumulate operations (MACs) per inference. In this report, we present lowcomplexity systems for ASC that follow the rules intended for the task"
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sugahara2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Sugahara2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Self-Ensemble with Multi-Task Learning for Low-Complexity Acoustic Scene Classification
       </h4>
<p style="text-align:left">
        Reiko Sugahara, Ryo Sato, Masatoshi Osawa, Yuuki Yuno and Chiho Haruta
       </p>
<p style="text-align:left">
<em>
         RION CO., LTD., Tokyo, Japan
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Sugahara_RION_task1_1</span> <span class="label label-primary">Sugahara_RION_task1_2</span> <span class="label label-primary">Sugahara_RION_task1_3</span> <span class="label label-primary">Sugahara_RION_task1_4</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sugahara2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sugahara2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sugahara2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Sugahara_107_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Sugahara2022').collapse('show');window.location.hash='#Sugahara2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sugahara2022" class="panel-collapse collapse" id="collapse-Sugahara2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Self-Ensemble with Multi-Task Learning for Low-Complexity Acoustic Scene Classification
      </h4>
<p style="text-align:left">
<small>
        Reiko Sugahara, Ryo Sato, Masatoshi Osawa, Yuuki Yuno and Chiho Haruta
       </small>
<br/>
<small>
<em>
         RION CO., LTD., Tokyo, Japan
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes a procedure for Task 1 in Detection and Classification of Acoustic Scenes and Events (DCASE) 2022. The proposed method adopts MobileNet-based models with log-mel energies and deltas as inputs. The accuracy was improved by self-ensemble with multi-task learning. Data augmentations, e.g., mixup, SpecAugment, and spectrum modulation, were applied to prevent overfitting. To meet system complexity requirements, we adopted depth-separable convolution and quantization aware training. The model contains 120,505 parameters and requires 26.607 million multiply-and-accumulate operations. Consequently, the proposed system achieved a 56.5% accuracy and a log-loss of 1.179 based on the development data.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         mixup, SpecAugment, spectrum modulation
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, deltas
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         MobileNet
        </td>
</tr>
<tr>
<td class="col-md-3">
         Decision making
        </td>
<td>
         weighted average
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sugahara2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Sugahara_107_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Rion-Dev/DCASE2022-Task1-Rion" style="text-decoration:none;border-bottom:0;padding-bottom:6px" target="_blank" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sugahara2022label" class="modal fade" id="bibtex-Sugahara2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSugahara2022label">
        Self-Ensemble with Multi-Task Learning for Low-Complexity Acoustic Scene Classification
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sugahara2022,
    Author = "Sugahara, Reiko and Sato, Ryo and Osawa, Masatoshi and Yuno, Yuuki and Haruta, Chiho",
    title = "Self-Ensemble with Multi-Task Learning for Low-Complexity Acoustic Scene Classification",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes a procedure for Task 1 in Detection and Classification of Acoustic Scenes and Events (DCASE) 2022. The proposed method adopts MobileNet-based models with log-mel energies and deltas as inputs. The accuracy was improved by self-ensemble with multi-task learning. Data augmentations, e.g., mixup, SpecAugment, and spectrum modulation, were applied to prevent overfitting. To meet system complexity requirements, we adopted depth-separable convolution and quantization aware training. The model contains 120,505 parameters and requires 26.607 million multiply-and-accumulate operations. Consequently, the proposed system achieved a 56.5\% accuracy and a log-loss of 1.179 based on the development data."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Xin2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Xin2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Low-Complexity Acoustic Scene Classification with Mismatch-Devices Using Separable Convolutions and Coordinate Attention
       </h4>
<p style="text-align:left">
        Yifei Xin<sup>1</sup>, Yuexian Zou<sup>1</sup>, Fan Cui<sup>2</sup> and Yujun Wang<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Peking University, Shenzhen, China, <sup>2</sup>Xiaomi Corporation, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zou_PKU_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Xin2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Xin2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Xin2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zou_76_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Xin2022" class="panel-collapse collapse" id="collapse-Xin2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Low-Complexity Acoustic Scene Classification with Mismatch-Devices Using Separable Convolutions and Coordinate Attention
      </h4>
<p style="text-align:left">
<small>
        Yifei Xin<sup>1</sup>, Yuexian Zou<sup>1</sup>, Fan Cui<sup>2</sup> and Yujun Wang<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Peking University, Shenzhen, China, <sup>2</sup>Xiaomi Corporation, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report details the architecture we used to address Task 1 of the of DCASE2022 challenge. Our architecture is based on 4 layer convolutional neural network taking as input a log-mel spectrogram. The complexity of this network is controlled by using separable convolutions in the channel, time and frequency dimensions. Moreover, we introduce a novel attention mechanism by embedding positional information into channel attention, which we call coordinate attention to improve the accuracy of a CNN-based framework. Besides, we use SpecAugment++, time shifting and test time augmentations to further improve the performance of the system.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Data augmentation
        </td>
<td>
         SpecAugment++, time shifting
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         spectrogram
        </td>
</tr>
<tr>
<td class="col-md-3">
         Embeddings
        </td>
<td>
         CNN6
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Xin2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zou_76_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Xin2022label" class="modal fade" id="bibtex-Xin2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexXin2022label">
        Low-Complexity Acoustic Scene Classification with Mismatch-Devices Using Separable Convolutions and Coordinate Attention
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Xin2022,
    Author = "Xin, Yifei and Zou, Yuexian and Cui, Fan and Wang, Yujun",
    title = "Low-Complexity Acoustic Scene Classification with Mismatch-Devices Using Separable Convolutions and Coordinate Attention",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This report details the architecture we used to address Task 1 of the of DCASE2022 challenge. Our architecture is based on 4 layer convolutional neural network taking as input a log-mel spectrogram. The complexity of this network is controlled by using separable convolutions in the channel, time and frequency dimensions. Moreover, we introduce a novel attention mechanism by embedding positional information into channel attention, which we call coordinate attention to improve the accuracy of a CNN-based framework. Besides, we use SpecAugment++, time shifting and test time augmentations to further improve the performance of the system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yu2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Yu2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        Acoustic Scene Classification Based on Feature Fusion and Dilated-Convolution
       </h4>
<p style="text-align:left">
        Junfei Yu, Runyu Shi, Tianrui He and Kaibin Guo
       </p>
<p style="text-align:left">
<em>
         Mobile Phone, Xiaomi, Beijing, China
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yu_XIAOMI_task1_1</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yu2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yu2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Yu_48_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yu2022" class="panel-collapse collapse" id="collapse-Yu2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       Acoustic Scene Classification Based on Feature Fusion and Dilated-Convolution
      </h4>
<p style="text-align:left">
<small>
        Junfei Yu, Runyu Shi, Tianrui He and Kaibin Guo
       </small>
<br/>
<small>
<em>
         Mobile Phone, Xiaomi, Beijing, China
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our submission for Task 1 of the DCASE Challenge 2022. The goal of task 1 is to classify the recorded audios for acoustic scene classification using an int8 quantized model that does not exceed 128KB in size. In our submission, a variety of timefrequency features are extracted and fused to be the input of the deep learning network. As the backbone of the network, the dilated-convolution is applied for embedding of various input features. Furthermore, we make use of multiple time-frequency data augmentation on the original data to increase the diversity of the data. After the network training is completed, the variable type of the weight data is converted into INT8. This INT8 model achieves a log loss of 1.305 and an accuracy of 51.7% on the standard test set of the TAU Urban Acoustic Scenes 2022 Mobile development dataset.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies, spectral entropy, spectral flatness
        </td>
</tr>
<tr>
<td class="col-md-3">
         Embeddings
        </td>
<td>
         dilated-CNN
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yu2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Yu_48_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yu2022label" class="modal fade" id="bibtex-Yu2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYu2022label">
        Acoustic Scene Classification Based on Feature Fusion and Dilated-Convolution
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yu2022,
    Author = "Yu, Junfei and Shi, Runyu and He, Tianrui and Guo, Kaibin",
    title = "Acoustic Scene Classification Based on Feature Fusion and Dilated-Convolution",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "This technical report describes our submission for Task 1 of the DCASE Challenge 2022. The goal of task 1 is to classify the recorded audios for acoustic scene classification using an int8 quantized model that does not exceed 128KB in size. In our submission, a variety of timefrequency features are extracted and fused to be the input of the deep learning network. As the backbone of the network, the dilated-convolution is applied for embedding of various input features. Furthermore, we make use of multiple time-frequency data augmentation on the original data to increase the diversity of the data. After the network training is completed, the variable type of the weight data is converted into INT8. This INT8 model achieves a log loss of 1.305 and an accuracy of 51.7\% on the standard test set of the TAU Urban Acoustic Scenes 2022 Mobile development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zaragoza_Paredes2022" style="box-shadow: none">
<div class="panel-heading" id="heading-Zaragoza_Paredes2022" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2022: Comparative Analysis of CNNs for Acoustic Scene Classification Under Low-Complexity Considerations
       </h4>
<p style="text-align:left">
        Josep Zaragoza Paredes<sup>1</sup>, Javier Naranjo Alcázar<sup>2</sup>, Valery Naranjo Ornedo<sup>1</sup> and Pedro Zuccarello<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>ETSIT, Universitat Politècnica de València, Valencia, Spain, <sup>2</sup>R+D, Instituto Tecnológico de Informática, Valencia, Spain
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zaragoza-Paredes_UPV_task1_1</span> <span class="label label-primary">Zaragoza-Paredes_UPV_task1_2</span> <span class="clearfix"></span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zaragoza_Paredes2022" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zaragoza_Paredes2022" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zaragoza_Paredes2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zaragoza-Paredes_26_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Zaragoza_Paredes2022').collapse('show');window.location.hash='#Zaragoza_Paredes2022';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Source code">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zaragoza_Paredes2022" class="panel-collapse collapse" id="collapse-Zaragoza_Paredes2022" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2022: Comparative Analysis of CNNs for Acoustic Scene Classification Under Low-Complexity Considerations
      </h4>
<p style="text-align:left">
<small>
        Josep Zaragoza Paredes<sup>1</sup>, Javier Naranjo Alcázar<sup>2</sup>, Valery Naranjo Ornedo<sup>1</sup> and Pedro Zuccarello<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>ETSIT, Universitat Politècnica de València, Valencia, Spain, <sup>2</sup>R+D, Instituto Tecnológico de Informática, Valencia, Spain
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Acoustic scene classification is an automatic listening problem that aims to assign an audio recording to a pre-defined scene based on its audio data. Over the years (and in past editions of the DCASE) this problem has often been solved with techniques known as ensembles (use of several machine learning models to combine their predictions in the inference phase). While these solutions can show performance in terms of accuracy, they can be very expensive in terms of computational capacity, making it impossible to deploy them in IoT devices. Due to the drift in this field of study, this task has two limitations in terms of model complexity. It should be noted that there is also the added complexity of mismatching devices (the audios provided are recorded by different sources of information). This technical report makes a comparative study of two different network architectures: conventional CNN and Convmixer. Although both networks exceed the baseline required by the competition, the conventional CNN shows a higher performance, exceeding the baseline by 8 percentage points. Solutions based on Conv-mixer architectures show worse performance although they are much lighter solutions.
      </p>
<h5>
<strong>
        System characteristics
       </strong>
</h5>
<table class="table table-condensed">
<tr>
<td class="col-md-3">
         Sampling rate
        </td>
<td>
         44.1kHz
        </td>
</tr>
<tr>
<td class="col-md-3">
         Features
        </td>
<td>
         log-mel energies
        </td>
</tr>
<tr>
<td class="col-md-3">
         Classifier
        </td>
<td>
         CNN
        </td>
</tr>
</table>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zaragoza_Paredes2022" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2022/technical_reports/DCASE2022_Zaragoza-Paredes_26_t1.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/JNaranjo-Alcazar/acoustic_scene_dcase2022.git" style="text-decoration:none;border-bottom:0;padding-bottom:6px" target="_blank" title="Source code">
<i class="fa fa-file-code-o">
</i>
        Source code
       </a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zaragoza_Paredes2022label" class="modal fade" id="bibtex-Zaragoza_Paredes2022" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         ×
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZaragoza_Paredes2022label">
        DCASE 2022: Comparative Analysis of CNNs for Acoustic Scene Classification Under Low-Complexity Considerations
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zaragoza_Paredes2022,
    Author = "Zaragoza Paredes, Josep and Naranjo Alcázar, Javier and Naranjo Ornedo, Valery and Zuccarello, Pedro",
    title = "{DCASE} 2022: Comparative Analysis of {CNNs} for Acoustic Scene Classification Under Low-Complexity Considerations",
    institution = "DCASE2022 Challenge",
    year = "2022",
    month = "June",
    abstract = "Acoustic scene classification is an automatic listening problem that aims to assign an audio recording to a pre-defined scene based on its audio data. Over the years (and in past editions of the DCASE) this problem has often been solved with techniques known as ensembles (use of several machine learning models to combine their predictions in the inference phase). While these solutions can show performance in terms of accuracy, they can be very expensive in terms of computational capacity, making it impossible to deploy them in IoT devices. Due to the drift in this field of study, this task has two limitations in terms of model complexity. It should be noted that there is also the added complexity of mismatching devices (the audios provided are recorded by different sources of information). This technical report makes a comparative study of two different network architectures: conventional CNN and Convmixer. Although both networks exceed the baseline required by the competition, the conventional CNN shows a higher performance, exceeding the baseline by 8 percentage points. Solutions based on Conv-mixer architectures show worse performance although they are much lighter solutions."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
    $(document).ready(function() {
        var hash = window.location.hash.substr(1);
        var anchor = window.location.hash;

        var shiftWindow = function() {
            var hash = window.location.hash.substr(1);
            if($('#collapse-'+hash).length){
                scrollBy(0, -100);
            }
        };
        window.addEventListener("hashchange", shiftWindow);

        if (window.location.hash){
            window.scrollTo(0, 0);
            history.replaceState(null, document.title, "#");
            $('#collapse-'+hash).collapse('show');
            setTimeout(function(){
                window.location.hash = anchor;
                shiftWindow();
            }, 2000);
        }
    });
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>