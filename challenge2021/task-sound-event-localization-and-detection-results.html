<!DOCTYPE html><html lang="en">
<head>
    <title>Sound Event Localization and Detection with Directional Interference - DCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/favicon.ico" rel="icon">
<link rel="canonical" href="/challenge2021/task-sound-event-localization-and-detection-results">
        <meta name="author" content="DCASE" />
        <meta name="description" content="Task description The Sound Event Localization and Detection (SELD) task deals with methods that detect the temporal onset and offset of sound events when active, classify the type of the event from a known set of sound classes, and further localize the events in space when active. The focus of â€¦" />
    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="/theme/css/btex.min.css">
    <link rel="stylesheet" href="/theme/css/datatable.bundle.min.css">
    <link rel="stylesheet" href="/theme/css/btoc.min.css">
    <link rel="stylesheet" href="/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="/custom.css" type="text/css"/>
    <script type="text/javascript" src="/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="/images/logos/dcase/dcase2024_logo.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse"><ul class="nav navbar-nav" id="menuitem-list-main"><li class="" data-toggle="tooltip" data-placement="bottom" title="Frontpage">
        <a href="/"><img class="img img-responsive" src="/images/logos/dcase/dcase2024_logo.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Workshop">
        <a href="/workshop2024/"><img class="img img-responsive" src="/images/logos/dcase/workshop.png"/></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="DCASE2024 Challenge">
        <a href="/challenge2024/"><img class="img img-responsive" src="/images/logos/dcase/challenge.png"/></a>
    </li></ul><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-calendar fa-1x fa-fw"></i>&nbsp;Editions&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/events"><i class="fa fa-list fa-fw"></i>&nbsp;Overview</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2023/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2023 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2023/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2023 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2022/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2022 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2022/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2022 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2021/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2021 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2021/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2021 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2020/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2020 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2020/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2020 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2019/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2019 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2019/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2019 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2018/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2018 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2018/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2018 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2017/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2017 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2017/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2017 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/workshop2016/"><i class="fa fa-file-text fa-fw text-warning"></i>&nbsp;2016 Workshop</a>
    </li>
            <li class="">
        <a href="/challenge2016/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2016 Challenge</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="">
        <a href="/challenge2013/"><i class="fa fa-bar-chart fa-fw text-success"></i>&nbsp;2013 Challenge</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-users fa-1x fa-fw"></i>&nbsp;Community&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Information about the community">
        <a href="/community_info"><i class="fa fa-info-circle fa-fw"></i>&nbsp;DCASE Community</a>
    </li>
            <li class="divider" role="presentation"></li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Venues to publish DCASE related work">
        <a href="/publishing"><i class="fa fa-file fa-fw"></i>&nbsp;Publishing</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="Curated List of Open Datasets for DCASE Related Research">
        <a href="https://dcase-repo.github.io/dcase_datalist/" target="_blank" ><i class="fa fa-database fa-fw"></i>&nbsp;Datasets</a>
    </li>
            <li class="" data-toggle="tooltip" data-placement="bottom" title="A collection of tools">
        <a href="/tools"><i class="fa fa-gears fa-fw"></i>&nbsp;Tools</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="News">
        <a href="/news"><i class="fa fa-newspaper-o fa-1x fa-fw"></i>&nbsp;News</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for DCASE Community">
        <a href="https://groups.google.com/forum/#!forum/dcase-discussions" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Invitation link to Slack workspace for DCASE Community">
        <a href="https://join.slack.com/t/dcase/shared_invite/zt-12zfa5kw0-dD41gVaPU3EZTCAw1mHTCA" target="_blank" ><i class="fa fa-slack fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Twitter for DCASE Challenges">
        <a href="https://twitter.com/DCASE_Challenge" target="_blank" ><i class="fa fa-twitter fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/DCASE-REPO" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Contact us">
        <a href="/contact-us"><i class="fa fa-envelope fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Challenge2021</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge introduction">
        <a href="/challenge2021/"><i class="fa fa-home"></i>&nbsp;Introduction</a>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-acoustic-scene-classification" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-scene text-primary"></i>&nbsp;Task1&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" dropdown-header ">
        <strong>Results</strong>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-a"><i class="fa fa-bar-chart"></i>&nbsp;Subtask A</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-acoustic-scene-classification-results-b"><i class="fa fa-bar-chart"></i>&nbsp;Subtask B</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-large-scale text-success"></i>&nbsp;Task2&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-unsupervised-detection-of-anomalous-sounds-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group  active">
        <a href="/challenge2021/task-sound-event-localization-and-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-localization text-warning"></i>&nbsp;Task3&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-localization-and-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class=" active">
        <a href="/challenge2021/task-sound-event-localization-and-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-domestic text-info"></i>&nbsp;Task4&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-bird text-danger"></i>&nbsp;Task5&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-few-shot-bioacoustic-event-detection-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="btn-group ">
        <a href="/challenge2021/task-automatic-audio-captioning" class="dropdown-toggle" data-toggle="dropdown"><i class="fa dc-captioning text-task1"></i>&nbsp;Task6&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning"><i class="fa fa-info-circle"></i>&nbsp;Description</a>
    </li>
            <li class="">
        <a href="/challenge2021/task-automatic-audio-captioning-results"><i class="fa fa-bar-chart"></i>&nbsp;Results</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Challenge rules">
        <a href="/challenge2021/rules"><i class="fa fa-list-alt"></i>&nbsp;Rules</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Submission instructions">
        <a href="/challenge2021/submission"><i class="fa fa-upload"></i>&nbsp;Submission</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metropol-sevilla-03.jpg);box-shadow: 0px 1000px rgba(18, 52, 18, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-bottom:20px;">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-left">
                    <span class="fa-stack fa-5x sr-fade-logo"><i class="fa fa-square fa-stack-2x text-warning"></i><i class="fa dc-localization fa-stack-1x fa-inverse"></i><strong class="fa-stack-1x dcase-icon-top-text dcase-icon-top-text-sm">Localization</strong><span class="fa-stack-1x dcase-icon-bottom-text">Task 3</span></span><img src="../images/logos/dcase/dcase2021_logo.png" class="sr-fade-logo" style="display:block;margin:0 auto;padding-top:15px;"></img>
                    </div><h1 class="bold">Sound Event Localization and Detection with Directional Interference</h1><hr class="small right bold">
                        <span class="subheading subheading-secondary">Challenge results</span></div>
            </div>
        </div>
    </div>
<span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span></header><div class="container-fluid">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#task-description">Task description</a></li>
<li><a href="#teams-ranking">Teams ranking</a></li>
<li><a href="#systems-ranking">Systems ranking</a></li>
<li><a href="#system-characteristics">System characteristics</a></li>
<li><a href="#technical-reports">Technical reports</a></li>
</ul>
</div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h1 id="task-description">Task description</h1>
<p>The Sound Event Localization and Detection (SELD) task deals with methods that detect the temporal onset and offset of sound events when active, classify the type of the event from a known set of sound classes, and further localize the events in space when active. </p>
<p>The focus of the current SELD task is to build systems that are able to handle event polyphony while being robust to ambient noise and reverberation in different acoustic environments/rooms, under static and dynamic spatial conditions (i.e. with moving sources). Additionally, the systems should be robust to interfering noise and events that are localized but do not belong to the targets classes, and their spatiotemporal activity is unknown during training. The task provides two datasets, development and evaluation, recorded in a total of 13 different acoustics environments. 
Among the two datasets, only the development dataset provides the reference labels. The participants are expected to build and validate systems using the development dataset, report results on a predefined development set split, and finally test their system on the unseen evaluation dataset.</p>
<p>More details on the task setup and evaluation can be found in the <a class="btn btn-primary" href="/challenge2021/task-sound-event-localization-and-detection" style="">task description page.</a></p>
<h1 id="teams-ranking">Teams ranking</h1>
<p>The SELD task received 37 submissions in total from 13 teams across the world. Results from one of the teams were retracted on their request. The following table includes only the best performing system per submitting team.</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="anchor" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="submission_rank" data-scatter-y="er_20" data-show-chart="true" data-show-pagination-switch="false" data-show-rank="true" data-sort-name="team_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="4">Submission Information</th>
<th class="sep-left-cell" colspan="5">Evaluation dataset</th>
<th class="sep-left-cell" colspan="4">Development dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
Submission name
</th>
<th class="sep-left-cell" data-field="corresponding_author" data-sortable="false">
Corresponding<br/> author
</th>
<th class="sm-cell" data-field="corresponding_affiliation" data-sortable="false">
Affiliation
</th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
Technical<br/>Report
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="submission_rank" data-sortable="true" data-value-type="int">
Best official <br/>system rank
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="er_20" data-reversed="true" data-sortable="true" data-value-type="float2">
Error Rate <br/>(20Â°)
</th>
<th class="text-center" data-chartable="true" data-field="f_20" data-sortable="true" data-value-type="float1-percentage">
F-score <br/>(20Â°)
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="le" data-reversed="true" data-sortable="true" data-value-type="float1">
Localization <br/>error (Â°)
</th>
<th class="text-center" data-chartable="true" data-field="lr" data-sortable="true" data-value-type="float1-percentage">
Localization <br/>recall
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_er_20" data-reversed="true" data-sortable="true" data-value-type="float2">
Error Rate <br/>(20Â°)
</th>
<th class="text-center" data-chartable="true" data-field="dev_f_20" data-sortable="true" data-value-type="float1-percentage">
F-score <br/>(20Â°)
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_le" data-reversed="true" data-sortable="true" data-value-type="float1">
Localization <br/>error (Â°)
</th>
<th class="text-center" data-chartable="true" data-field="dev_lr" data-sortable="true" data-value-type="float1-percentage">
Loalization <br/>recall
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Shimada_SONY_task3_3</td>
<td>Kazuki Shimada</td>
<td>Sony Group Corporation</td>
<td>Shimada_SONY_task3_report</td>
<td>1</td>
<td>0.32</td>
<td>79.1</td>
<td>8.5</td>
<td>82.8</td>
<td>0.43</td>
<td>69.9</td>
<td>11.1</td>
<td>73.2</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Thi Ngoc Tho Nguyen</td>
<td>Nanyang Technological University</td>
<td>Nguyen_NTU_task3_report</td>
<td>5</td>
<td>0.32</td>
<td>78.3</td>
<td>10.0</td>
<td>78.3</td>
<td>0.37</td>
<td>73.7</td>
<td>11.2</td>
<td>74.1</td>
</tr>
<tr>
<td></td>
<td>Parrish_JHU_task3_2</td>
<td>Nathan Parrish</td>
<td>Johns Hopkins University</td>
<td>Parrish_JHU_task3_report</td>
<td>9</td>
<td>0.39</td>
<td>73.8</td>
<td>12.8</td>
<td>76.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_SGU_task3_1</td>
<td>Sang-Hoon Lee</td>
<td>Sogang University</td>
<td>Lee_SGU_task3_report</td>
<td>10</td>
<td>0.40</td>
<td>72.9</td>
<td>13.2</td>
<td>76.5</td>
<td>0.46</td>
<td>60.9</td>
<td>14.4</td>
<td>73.3</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_4</td>
<td>Sooyoung Park</td>
<td>Electronics and Telecommunications Research Institute</td>
<td>Park_ETRI_task3_report</td>
<td>14</td>
<td>0.46</td>
<td>67.8</td>
<td>12.8</td>
<td>72.3</td>
<td>0.44</td>
<td>69.6</td>
<td>13.7</td>
<td>74.2</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task3_1</td>
<td>Zihao Li</td>
<td>University of Chinese Academy of Sciences</td>
<td>Zhang_UCAS_task3_report</td>
<td>20</td>
<td>0.46</td>
<td>64.7</td>
<td>12.8</td>
<td>61.9</td>
<td>0.46</td>
<td>63.2</td>
<td>13.9</td>
<td>62.9</td>
</tr>
<tr>
<td></td>
<td>Ko_SKKU_task3_4</td>
<td>Jonghwan Ko</td>
<td>Sungkyunkwan University</td>
<td>Ko_SKKU_task3_report</td>
<td>24</td>
<td>0.58</td>
<td>60.3</td>
<td>15.1</td>
<td>70.7</td>
<td>0.38</td>
<td>73.4</td>
<td>14.7</td>
<td>81.0</td>
</tr>
<tr>
<td></td>
<td>Huang_Aalto_task3_1</td>
<td>Huang Daolang</td>
<td>Aalto University</td>
<td>Huang_Aalto_task3_report</td>
<td>28</td>
<td>0.57</td>
<td>52.3</td>
<td>18.5</td>
<td>58.5</td>
<td>0.71</td>
<td>36.8</td>
<td>23.3</td>
<td>66.8</td>
</tr>
<tr>
<td></td>
<td>Yalta_HIT_task3_1</td>
<td>Nelson Yalta</td>
<td>Hitachi, Ltd.</td>
<td>Yalta_HIT_task3_report</td>
<td>29</td>
<td>0.72</td>
<td>52.5</td>
<td>20.1</td>
<td>71.1</td>
<td>0.60</td>
<td>54.0</td>
<td>20.3</td>
<td>65.3</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task3_2</td>
<td>Javier Naranjo-Alcazar</td>
<td>Instituto TecnolÃ³gico de InformÃ¡tica</td>
<td>Naranjo-Alcazar_UV_task3_report</td>
<td>31</td>
<td>0.68</td>
<td>37.7</td>
<td>25.3</td>
<td>53.9</td>
<td>0.71</td>
<td>31.9</td>
<td>27.6</td>
<td>46.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Politis_TAU_task3_foa</td>
<td>Archontis Politis</td>
<td>Tampere University</td>
<td>Politis_TUNI_task3_report</td>
<td>31</td>
<td>0.67</td>
<td>37.2</td>
<td>23.9</td>
<td>45.8</td>
<td>0.73</td>
<td>30.7</td>
<td>24.5</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>Bai_NWPU_task3_2</td>
<td>Jisheng Bai</td>
<td>LianFeng Acoustic Technologies Co., Ltd.</td>
<td>Bai_NWPU_task3_report</td>
<td>35</td>
<td>0.79</td>
<td>16.4</td>
<td>66.5</td>
<td>35.5</td>
<td>0.76</td>
<td>20.7</td>
<td>40.1</td>
<td>30.8</td>
</tr>
<tr>
<td></td>
<td>Sun_AIAL-XJU_task3_2</td>
<td>Xinghao Sun</td>
<td>Xinjiang University</td>
<td>Sun_AIAL-XJU_task3_report</td>
<td>37</td>
<td>0.95</td>
<td>2.7</td>
<td>84.5</td>
<td>17.4</td>
<td>0.52</td>
<td>55.3</td>
<td>19.1</td>
<td>60.9</td>
</tr>
</tbody>
</table>
<h1 id="systems-ranking">Systems ranking</h1>
<p>Performance of all the submitted systems on the evaluation and the development datasets</p>
<table class="datatable table table-hover table-condensed" data-bar-hline="true" data-bar-horizontal-indicator-linewidth="2" data-bar-show-horizontal-indicator="true" data-bar-show-vertical-indicator="true" data-bar-show-xaxis="false" data-bar-tooltip-position="top" data-bar-vertical-indicator-linewidth="2" data-chart-default-mode="scatter" data-chart-modes="bar,scatter" data-id-field="anchor" data-pagination="false" data-rank-mode="grouped_muted" data-row-highlighting="true" data-scatter-x="submission_rank" data-scatter-y="er_20" data-show-chart="true" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="submission_rank" data-sort-order="asc">
<thead>
<tr>
<th class="sep-right-cell" data-rank="true" rowspan="2">Rank</th>
<th class="sep-left-cell" colspan="2">Submission Information</th>
<th class="sep-left-cell" colspan="5">Evaluation dataset</th>
<th class="sep-left-cell" colspan="4">Development dataset</th>
</tr>
<tr>
<th data-field="anchor" data-sortable="true">
Submission name
</th>
<th class="text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
Technical<br/>Report
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="submission_rank" data-sortable="true" data-value-type="int">
Official <br/>rank
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="er_20" data-reversed="true" data-sortable="true" data-value-type="float2">
Error Rate <br/>(20Â°)
</th>
<th class="text-center" data-chartable="true" data-field="f_20" data-sortable="true" data-value-type="float1-percentage">
F-score  <br/>(20Â°)
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="le" data-reversed="true" data-sortable="true" data-value-type="float1">
Localization <br/>error (Â°)
</th>
<th class="text-center" data-chartable="true" data-field="lr" data-sortable="true" data-value-type="float1-percentage">
Localization <br/>recall
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_er_20" data-reversed="true" data-sortable="true" data-value-type="float2">
Error Rate <br/>(20Â°)
</th>
<th class="text-center" data-chartable="true" data-field="dev_f_20" data-sortable="true" data-value-type="float1-percentage">
F-score <br/>(20Â°)
</th>
<th class="sep-left-cell text-center" data-chartable="true" data-field="dev_le" data-reversed="true" data-sortable="true" data-value-type="float1">
Localization <br/>error (Â°)
</th>
<th class="text-center" data-chartable="true" data-field="dev_lr" data-sortable="true" data-value-type="float1-percentage">
Localization <br/>recall
</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Shimada_SONY_task3_3</td>
<td>Shimada_SONY_task3_report</td>
<td>1</td>
<td>0.32</td>
<td>79.1</td>
<td>8.5</td>
<td>82.8</td>
<td>0.43</td>
<td>69.9</td>
<td>11.1</td>
<td>73.2</td>
</tr>
<tr>
<td></td>
<td>Shimada_SONY_task3_2</td>
<td>Shimada_SONY_task3_report</td>
<td>1</td>
<td>0.30</td>
<td>79.4</td>
<td>8.2</td>
<td>79.0</td>
<td>0.41</td>
<td>69.6</td>
<td>10.7</td>
<td>68.6</td>
</tr>
<tr>
<td></td>
<td>Shimada_SONY_task3_4</td>
<td>Shimada_SONY_task3_report</td>
<td>3</td>
<td>0.31</td>
<td>79.0</td>
<td>8.1</td>
<td>78.2</td>
<td>0.41</td>
<td>70.0</td>
<td>10.3</td>
<td>68.7</td>
</tr>
<tr>
<td></td>
<td>Shimada_SONY_task3_1</td>
<td>Shimada_SONY_task3_report</td>
<td>4</td>
<td>0.33</td>
<td>79.0</td>
<td>8.5</td>
<td>82.6</td>
<td>0.43</td>
<td>69.6</td>
<td>11.3</td>
<td>73.2</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_3</td>
<td>Nguyen_NTU_task3_report</td>
<td>5</td>
<td>0.32</td>
<td>78.3</td>
<td>10.0</td>
<td>78.3</td>
<td>0.37</td>
<td>73.7</td>
<td>11.2</td>
<td>74.1</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_1</td>
<td>Nguyen_NTU_task3_report</td>
<td>6</td>
<td>0.33</td>
<td>78.0</td>
<td>10.1</td>
<td>79.1</td>
<td>0.38</td>
<td>74.0</td>
<td>11.4</td>
<td>75.6</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_2</td>
<td>Nguyen_NTU_task3_report</td>
<td>7</td>
<td>0.33</td>
<td>78.0</td>
<td>10.2</td>
<td>78.7</td>
<td>0.38</td>
<td>73.8</td>
<td>11.2</td>
<td>75.0</td>
</tr>
<tr>
<td></td>
<td>Nguyen_NTU_task3_4</td>
<td>Nguyen_NTU_task3_report</td>
<td>8</td>
<td>0.35</td>
<td>77.2</td>
<td>10.3</td>
<td>80.3</td>
<td>0.39</td>
<td>74.1</td>
<td>12.1</td>
<td>77.9</td>
</tr>
<tr>
<td></td>
<td>Parrish_JHU_task3_2</td>
<td>Parrish_JHU_task3_report</td>
<td>9</td>
<td>0.39</td>
<td>73.8</td>
<td>12.8</td>
<td>76.8</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_SGU_task3_1</td>
<td>Lee_SGU_task3_report</td>
<td>10</td>
<td>0.40</td>
<td>72.9</td>
<td>13.2</td>
<td>76.5</td>
<td>0.46</td>
<td>60.9</td>
<td>14.4</td>
<td>73.3</td>
</tr>
<tr>
<td></td>
<td>Parrish_JHU_task3_1</td>
<td>Parrish_JHU_task3_report</td>
<td>11</td>
<td>0.41</td>
<td>72.6</td>
<td>13.5</td>
<td>76.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Lee_SGU_task3_4</td>
<td>Lee_SGU_task3_report</td>
<td>12</td>
<td>0.44</td>
<td>70.7</td>
<td>13.2</td>
<td>74.9</td>
<td>0.48</td>
<td>59.9</td>
<td>14.3</td>
<td>72.2</td>
</tr>
<tr>
<td></td>
<td>Lee_SGU_task3_2</td>
<td>Lee_SGU_task3_report</td>
<td>13</td>
<td>0.44</td>
<td>69.9</td>
<td>14.9</td>
<td>75.8</td>
<td>0.48</td>
<td>59.9</td>
<td>15.5</td>
<td>73.5</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_4</td>
<td>Park_ETRI_task3_report</td>
<td>14</td>
<td>0.46</td>
<td>67.8</td>
<td>12.8</td>
<td>72.3</td>
<td>0.44</td>
<td>69.6</td>
<td>13.7</td>
<td>74.2</td>
</tr>
<tr>
<td></td>
<td>Lee_SGU_task3_3</td>
<td>Lee_SGU_task3_report</td>
<td>15</td>
<td>0.45</td>
<td>68.9</td>
<td>15.3</td>
<td>75.2</td>
<td>0.49</td>
<td>59.4</td>
<td>15.1</td>
<td>73.4</td>
</tr>
<tr>
<td></td>
<td>Parrish_JHU_task3_3</td>
<td>Parrish_JHU_task3_report</td>
<td>15</td>
<td>0.45</td>
<td>68.8</td>
<td>14.8</td>
<td>75.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_2</td>
<td>Park_ETRI_task3_report</td>
<td>17</td>
<td>0.47</td>
<td>67.5</td>
<td>12.9</td>
<td>72.7</td>
<td>0.44</td>
<td>69.6</td>
<td>13.7</td>
<td>74.2</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_3</td>
<td>Park_ETRI_task3_report</td>
<td>17</td>
<td>0.46</td>
<td>67.5</td>
<td>12.8</td>
<td>72.1</td>
<td>0.44</td>
<td>69.6</td>
<td>13.7</td>
<td>74.2</td>
</tr>
<tr>
<td></td>
<td>Park_ETRI_task3_1</td>
<td>Park_ETRI_task3_report</td>
<td>19</td>
<td>0.48</td>
<td>65.9</td>
<td>12.5</td>
<td>68.9</td>
<td>0.44</td>
<td>69.6</td>
<td>13.7</td>
<td>74.2</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task3_1</td>
<td>Zhang_UCAS_task3_report</td>
<td>20</td>
<td>0.46</td>
<td>64.7</td>
<td>12.8</td>
<td>61.9</td>
<td>0.46</td>
<td>63.2</td>
<td>13.9</td>
<td>62.9</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task3_3</td>
<td>Zhang_UCAS_task3_report</td>
<td>21</td>
<td>0.48</td>
<td>64.0</td>
<td>12.8</td>
<td>58.9</td>
<td>0.47</td>
<td>61.3</td>
<td>14.0</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task3_4</td>
<td>Zhang_UCAS_task3_report</td>
<td>22</td>
<td>0.47</td>
<td>64.6</td>
<td>14.4</td>
<td>66.0</td>
<td>0.50</td>
<td>60.3</td>
<td>18.1</td>
<td>68.8</td>
</tr>
<tr>
<td></td>
<td>Zhang_UCAS_task3_2</td>
<td>Zhang_UCAS_task3_report</td>
<td>23</td>
<td>0.48</td>
<td>63.3</td>
<td>14.3</td>
<td>64.5</td>
<td>0.49</td>
<td>60.4</td>
<td>16.0</td>
<td>64.0</td>
</tr>
<tr>
<td></td>
<td>Ko_SKKU_task3_4</td>
<td>Ko_SKKU_task3_report</td>
<td>24</td>
<td>0.58</td>
<td>60.3</td>
<td>15.1</td>
<td>70.7</td>
<td>0.38</td>
<td>73.4</td>
<td>14.7</td>
<td>81.0</td>
</tr>
<tr>
<td></td>
<td>Ko_SKKU_task3_1</td>
<td>Ko_SKKU_task3_report</td>
<td>25</td>
<td>0.64</td>
<td>59.0</td>
<td>15.6</td>
<td>73.0</td>
<td>0.39</td>
<td>73.2</td>
<td>14.9</td>
<td>81.3</td>
</tr>
<tr>
<td></td>
<td>Parrish_JHU_task3_4</td>
<td>Parrish_JHU_task3_report</td>
<td>25</td>
<td>0.50</td>
<td>62.9</td>
<td>15.4</td>
<td>67.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Ko_SKKU_task3_2</td>
<td>Ko_SKKU_task3_report</td>
<td>27</td>
<td>0.67</td>
<td>58.1</td>
<td>15.8</td>
<td>73.5</td>
<td>0.45</td>
<td>70.8</td>
<td>15.2</td>
<td>81.8</td>
</tr>
<tr>
<td></td>
<td>Huang_Aalto_task3_1</td>
<td>Huang_Aalto_task3_report</td>
<td>28</td>
<td>0.57</td>
<td>52.3</td>
<td>18.5</td>
<td>58.5</td>
<td>0.71</td>
<td>36.8</td>
<td>23.3</td>
<td>66.8</td>
</tr>
<tr>
<td></td>
<td>Yalta_HIT_task3_1</td>
<td>Yalta_HIT_task3_report</td>
<td>29</td>
<td>0.72</td>
<td>52.5</td>
<td>20.1</td>
<td>71.1</td>
<td>0.60</td>
<td>54.0</td>
<td>20.3</td>
<td>65.3</td>
</tr>
<tr>
<td></td>
<td>Huang_Aalto_task3_2</td>
<td>Huang_Aalto_task3_report</td>
<td>30</td>
<td>0.59</td>
<td>49.8</td>
<td>19.5</td>
<td>57.3</td>
<td>0.71</td>
<td>35.9</td>
<td>24.5</td>
<td>67.4</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task3_2</td>
<td>Naranjo-Alcazar_UV_task3_report</td>
<td>31</td>
<td>0.68</td>
<td>37.7</td>
<td>25.3</td>
<td>53.9</td>
<td>0.71</td>
<td>31.9</td>
<td>27.6</td>
<td>46.6</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Politis_TAU_task3_foa</td>
<td>Politis_TUNI_task3_report</td>
<td>31</td>
<td>0.67</td>
<td>37.2</td>
<td>23.9</td>
<td>45.8</td>
<td>0.73</td>
<td>30.7</td>
<td>24.5</td>
<td>44.8</td>
</tr>
<tr>
<td></td>
<td>Naranjo-Alcazar_UV_task3_1</td>
<td>Naranjo-Alcazar_UV_task3_report</td>
<td>33</td>
<td>0.67</td>
<td>36.8</td>
<td>30.1</td>
<td>48.7</td>
<td>0.72</td>
<td>30.2</td>
<td>29.4</td>
<td>42.5</td>
</tr>
<tr class="info" data-hline="true">
<td></td>
<td>Politis_TAU_task3_mic</td>
<td>Politis_TUNI_task3_report</td>
<td>34</td>
<td>0.73</td>
<td>27.1</td>
<td>30.8</td>
<td>40.6</td>
<td>0.75</td>
<td>23.4</td>
<td>30.6</td>
<td>37.8</td>
</tr>
<tr>
<td></td>
<td>Bai_NWPU_task3_2</td>
<td>Bai_NWPU_task3_report</td>
<td>35</td>
<td>0.79</td>
<td>16.4</td>
<td>66.5</td>
<td>35.5</td>
<td>0.76</td>
<td>20.7</td>
<td>40.1</td>
<td>30.8</td>
</tr>
<tr>
<td></td>
<td>Bai_NWPU_task3_1</td>
<td>Bai_NWPU_task3_report</td>
<td>36</td>
<td>0.81</td>
<td>15.0</td>
<td>69.6</td>
<td>37.5</td>
<td>0.76</td>
<td>20.7</td>
<td>40.1</td>
<td>30.8</td>
</tr>
<tr>
<td></td>
<td>Sun_AIAL-XJU_task3_2</td>
<td>Sun_AIAL-XJU_task3_report</td>
<td>37</td>
<td>0.95</td>
<td>2.7</td>
<td>84.5</td>
<td>17.4</td>
<td>0.52</td>
<td>55.3</td>
<td>19.1</td>
<td>60.9</td>
</tr>
<tr>
<td></td>
<td>Sun_AIAL-XJU_task3_1</td>
<td>Sun_AIAL-XJU_task3_report</td>
<td>38</td>
<td>0.97</td>
<td>1.4</td>
<td>84.4</td>
<td>10.6</td>
<td>0.57</td>
<td>52.6</td>
<td>19.6</td>
<td>58.1</td>
</tr>
</tbody>
</table>
<h1 id="system-characteristics">System characteristics</h1>
<p>Summary of the submitted systems characteristics.</p>
<table class="datatable table table-hover table-condensed" data-chart-tooltip-fields="code" data-filter-control="true" data-filter-show-clear="true" data-id-field="anchor" data-pagination="true" data-rank-mode="grouped_muted" data-show-bar-chart-xaxis="false" data-show-chart="false" data-show-pagination-switch="true" data-show-rank="true" data-sort-name="submission_rank" data-sort-order="asc" data-striped="true" data-tag-mode="column">
<thead>
<tr>
<th data-field="submission_rank" data-sortable="true" data-value-type="int">
Rank
</th>
<th class="sm-cell" data-field="anchor" data-sortable="true">
Submission<br/>name
</th>
<th class="sep-left-cell text-center" data-field="bibtex_key" data-sortable="false" data-value-type="anchor">
Technical<br/>Report
</th>
<th class="sep-left-cell text-center narrow-col" data-field="model" data-filter-control="select" data-filter-strict-search="true" data-sortable="false" data-tag="true">
Model
</th>
<th class="sep-left-cell text-center narrow-col" data-axis-scale="log10_unit" data-field="model_params" data-sortable="true" data-value-type="numeric-unit">
Model<br/>params
</th>
<th class="sep-left-cell text-center narrow-col" data-field="input_format" data-filter-control="select" data-filter-strict-search="true" data-sortable="false" data-tag="true">
Audio<br/>format
</th>
<th class="sep-left-cell text-center narrow-col" data-field="input_feature" data-filter-control="select" data-filter-strict-search="true" data-sortable="false" data-tag="true">
Acoustic<br/>features
</th>
<th class="sep-left-cell text-center narrow-col" data-field="augmentation" data-filter-control="select" data-filter-strict-search="true" data-sortable="false" data-tag="true">
Data <br/>augmentation
</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Shimada_SONY_task3_3</td>
<td>Shimada_SONY_task3_report</td>
<td>RD3Net, RD3Net with TFRNN, EINV2 with D3block, ensemble</td>
<td>42647804</td>
<td>Ambisonic</td>
<td>magnitude spectra, PCEN spectra, IPD, cosIPD, sinIPD</td>
<td>EMDA, rotation, SpecAugment, impulse response simulation</td>
</tr>
<tr>
<td>1</td>
<td>Shimada_SONY_task3_2</td>
<td>Shimada_SONY_task3_report</td>
<td>RD3Net, RD3Net with TFRNN, EINV2 with D3block, ensemble</td>
<td>42647804</td>
<td>Ambisonic</td>
<td>magnitude spectra, PCEN spectra, IPD, cosIPD, sinIPD</td>
<td>EMDA, rotation, SpecAugment, impulse response simulation</td>
</tr>
<tr>
<td>3</td>
<td>Shimada_SONY_task3_4</td>
<td>Shimada_SONY_task3_report</td>
<td>RD3Net, RD3Net with TFRNN, EINV2 with D3block, ensemble</td>
<td>73578962</td>
<td>Ambisonic</td>
<td>magnitude spectra, PCEN spectra, IPD, cosIPD, sinIPD</td>
<td>EMDA, rotation, SpecAugment, impulse response simulation</td>
</tr>
<tr>
<td>4</td>
<td>Shimada_SONY_task3_1</td>
<td>Shimada_SONY_task3_report</td>
<td>RD3Net, RD3Net with TFRNN, EINV2 with D3block, ensemble</td>
<td>42647804</td>
<td>Ambisonic</td>
<td>magnitude spectra, PCEN spectra, IPD, cosIPD, sinIPD</td>
<td>EMDA, rotation, SpecAugment, impulse response simulation</td>
</tr>
<tr>
<td>5</td>
<td>Nguyen_NTU_task3_3</td>
<td>Nguyen_NTU_task3_report</td>
<td>CRNN, ensemble</td>
<td>107800000</td>
<td>Ambisonic</td>
<td>eigenvector-augmented log spectra, mel spectra</td>
<td>mixup, frequency shifting, random cutout, SpecAugment, channel swapping</td>
</tr>
<tr>
<td>6</td>
<td>Nguyen_NTU_task3_1</td>
<td>Nguyen_NTU_task3_report</td>
<td>CRNN, MHSA, ensemble</td>
<td>112200000</td>
<td>Ambisonic</td>
<td>eigenvector-augmented log spectra, mel spectra</td>
<td>mixup, frequency shifting, random cutout, SpecAugment, channel swapping</td>
</tr>
<tr>
<td>7</td>
<td>Nguyen_NTU_task3_2</td>
<td>Nguyen_NTU_task3_report</td>
<td>CRNN, MHSA, ensemble</td>
<td>83900000</td>
<td>Ambisonic</td>
<td>eigenvector-augmented log spectra, mel spectra</td>
<td>mixup, frequency shifting, random cutout, SpecAugment, channel swapping</td>
</tr>
<tr>
<td>8</td>
<td>Nguyen_NTU_task3_4</td>
<td>Nguyen_NTU_task3_report</td>
<td>CRNN, MHSA, ensemble</td>
<td>112200000</td>
<td>Ambisonic</td>
<td>eigenvector-augmented log spectra, mel spectra</td>
<td>mixup, frequency shifting, random cutout, SpecAugment, channel swapping</td>
</tr>
<tr>
<td>9</td>
<td>Parrish_JHU_task3_2</td>
<td>Parrish_JHU_task3_report</td>
<td>CNN, MHSA</td>
<td>222000000</td>
<td>Ambisonic</td>
<td>mel spectra, constant-Q spectra, intensity vector</td>
<td>rotation, wav mixing, frequency masking, time masking</td>
</tr>
<tr>
<td>10</td>
<td>Lee_SGU_task3_1</td>
<td>Lee_SGU_task3_report</td>
<td>CNN, Transformer</td>
<td>27797491</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>rotation, mixup, SpecAugment</td>
</tr>
<tr>
<td>11</td>
<td>Parrish_JHU_task3_1</td>
<td>Parrish_JHU_task3_report</td>
<td>CNN, MHSA</td>
<td>237000000</td>
<td>Ambisonic</td>
<td>mel spectra, constant-Q spectra, intensity vector</td>
<td>rotation, wav mixing, frequency masking, time masking</td>
</tr>
<tr>
<td>12</td>
<td>Lee_SGU_task3_4</td>
<td>Lee_SGU_task3_report</td>
<td>CNN, Transformer</td>
<td>19389427</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>rotation, mixup, SpecAugment</td>
</tr>
<tr>
<td>13</td>
<td>Lee_SGU_task3_2</td>
<td>Lee_SGU_task3_report</td>
<td>CNN, Transformer</td>
<td>26221567</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>rotation, mixup, SpecAugment</td>
</tr>
<tr>
<td>14</td>
<td>Park_ETRI_task3_4</td>
<td>Park_ETRI_task3_report</td>
<td>Transformer, ensemble</td>
<td>601251180</td>
<td>Both</td>
<td>mel spectra, intensity vector</td>
<td>rotation, time stretching</td>
</tr>
<tr>
<td>15</td>
<td>Lee_SGU_task3_3</td>
<td>Lee_SGU_task3_report</td>
<td>CNN, Transformer</td>
<td>27798106</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>rotation, mixup, SpecAugment</td>
</tr>
<tr>
<td>15</td>
<td>Parrish_JHU_task3_3</td>
<td>Parrish_JHU_task3_report</td>
<td>CNN, MHSA</td>
<td>73000000</td>
<td>Ambisonic</td>
<td>constant-Q spectra, intensity vector</td>
<td>rotation, wav mixing, frequency masking, time masking</td>
</tr>
<tr>
<td>17</td>
<td>Park_ETRI_task3_2</td>
<td>Park_ETRI_task3_report</td>
<td>Transformer</td>
<td>172641840</td>
<td>Both</td>
<td>mel spectra, intensity vector</td>
<td>rotation</td>
</tr>
<tr>
<td>17</td>
<td>Park_ETRI_task3_3</td>
<td>Park_ETRI_task3_report</td>
<td>Transformer, ensemble</td>
<td>429807444</td>
<td>Both</td>
<td>mel spectra, intensity vector</td>
<td>rotation</td>
</tr>
<tr>
<td>19</td>
<td>Park_ETRI_task3_1</td>
<td>Park_ETRI_task3_report</td>
<td>Transformer</td>
<td>172641840</td>
<td>Both</td>
<td>mel spectra, intensity vector</td>
<td>rotation</td>
</tr>
<tr>
<td>20</td>
<td>Zhang_UCAS_task3_1</td>
<td>Zhang_UCAS_task3_report</td>
<td>CNN, Conformer, ensemble</td>
<td>88684456</td>
<td>Both</td>
<td>mel spectra, intensity vector, GCC-PHAT</td>
<td>ACS, TFM</td>
</tr>
<tr>
<td>21</td>
<td>Zhang_UCAS_task3_3</td>
<td>Zhang_UCAS_task3_report</td>
<td>CNN, Conformer,ResNet,SEResNet,ensemble</td>
<td>79306528</td>
<td>Both</td>
<td>mel spectra, intensity vector, GCC-PHAT</td>
<td>ACS, TFM</td>
</tr>
<tr>
<td>22</td>
<td>Zhang_UCAS_task3_4</td>
<td>Zhang_UCAS_task3_report</td>
<td>CNN, Conformer</td>
<td>12834944</td>
<td>Both</td>
<td>mel spectra, intensity vector, GCC-PHAT</td>
<td>ACS, TFM, SP</td>
</tr>
<tr>
<td>23</td>
<td>Zhang_UCAS_task3_2</td>
<td>Zhang_UCAS_task3_report</td>
<td>CNN, Conformer</td>
<td>12669172</td>
<td>Both</td>
<td>mel spectra, intensity vector, GCC-PHAT</td>
<td>ACS, TFM</td>
</tr>
<tr>
<td>24</td>
<td>Ko_SKKU_task3_4</td>
<td>Ko_SKKU_task3_report</td>
<td>CNN, RNN, Transformer, ensemble</td>
<td>12360000</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>domain spatial augmentation, SpecAugment</td>
</tr>
<tr>
<td>25</td>
<td>Ko_SKKU_task3_1</td>
<td>Ko_SKKU_task3_report</td>
<td>CNN, RNN, Transformer, ensemble</td>
<td>9506544</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>domain spatial augmentation, SpecAugment</td>
</tr>
<tr>
<td>25</td>
<td>Parrish_JHU_task3_4</td>
<td>Parrish_JHU_task3_report</td>
<td>CNN, MHSA</td>
<td>21000000</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>rotation, wav mixing, frequency masking, time masking</td>
</tr>
<tr>
<td>27</td>
<td>Ko_SKKU_task3_2</td>
<td>Ko_SKKU_task3_report</td>
<td>CNN, RNN, Transformer, ensemble</td>
<td>9506544</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>domain spatial augmentation, SpecAugment</td>
</tr>
<tr>
<td>28</td>
<td>Huang_Aalto_task3_1</td>
<td>Huang_Aalto_task3_report</td>
<td>CNN, Transformer</td>
<td>2214052</td>
<td>Both</td>
<td>raw waveform</td>
<td>rotation, time masking, random audio equalization</td>
</tr>
<tr>
<td>29</td>
<td>Yalta_HIT_task3_1</td>
<td>Yalta_HIT_task3_report</td>
<td>Transformer</td>
<td>4100000</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>SpecAugment</td>
</tr>
<tr>
<td>30</td>
<td>Huang_Aalto_task3_2</td>
<td>Huang_Aalto_task3_report</td>
<td>CNN, Transformer</td>
<td>2251684</td>
<td>Both</td>
<td>raw waveform</td>
<td>rotation, time masking, random audio equalization</td>
</tr>
<tr>
<td>31</td>
<td>Naranjo-Alcazar_UV_task3_2</td>
<td>Naranjo-Alcazar_UV_task3_report</td>
<td>CRNN</td>
<td>590100</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>None</td>
</tr>
<tr data-hline="true">
<td>31</td>
<td>Politis_TAU_task3_foa</td>
<td>Politis_TUNI_task3_report</td>
<td>CRNN</td>
<td>494000</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>None</td>
</tr>
<tr>
<td>33</td>
<td>Naranjo-Alcazar_UV_task3_1</td>
<td>Naranjo-Alcazar_UV_task3_report</td>
<td>CRNN</td>
<td>592020</td>
<td>Microphone Array</td>
<td>mel spectra, GCC-PHAT</td>
<td>None</td>
</tr>
<tr data-hline="true">
<td>34</td>
<td>Politis_TAU_task3_mic</td>
<td>Politis_TUNI_task3_report</td>
<td>CRNN</td>
<td>494000</td>
<td>Microphone Array</td>
<td>mel spectra, GCC-PHAT</td>
<td>None</td>
</tr>
<tr>
<td>35</td>
<td>Bai_NWPU_task3_2</td>
<td>Bai_NWPU_task3_report</td>
<td>CRNN</td>
<td>116118</td>
<td>Microphone Array</td>
<td>mel spectra, GCC-PHAT</td>
<td>random segment augmentation</td>
</tr>
<tr>
<td>36</td>
<td>Bai_NWPU_task3_1</td>
<td>Bai_NWPU_task3_report</td>
<td>CRNN</td>
<td>24506660</td>
<td>Microphone Array</td>
<td>mel spectra, GCC-PHAT</td>
<td>random segment augmentation</td>
</tr>
<tr>
<td>37</td>
<td>Sun_AIAL-XJU_task3_2</td>
<td>Sun_AIAL-XJU_task3_report</td>
<td>CRNN</td>
<td>1338286</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>None</td>
</tr>
<tr>
<td>38</td>
<td>Sun_AIAL-XJU_task3_1</td>
<td>Sun_AIAL-XJU_task3_report</td>
<td>CRNN</td>
<td>1338286</td>
<td>Ambisonic</td>
<td>mel spectra, intensity vector</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><br/>
<br/></p>
<h1 id="technical-reports">Technical reports</h1>
<div class="btex" data-source="content/data/challenge2021/technical_reports_task3.bib" data-stats="true">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div aria-multiselectable="true" class="panel-group" id="accordion" role="tablist">
<div class="panel publication-item" id="Bai_NWPU_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Bai_NWPU_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2021 TASK 3: SELD SYSTEM BASED ON RESNET AND RANDOM SEGMENT AUGMENTATION
       </h4>
<p style="text-align:left">
        Jisheng Bai<sup>1</sup>, Zijun Pu<sup>2</sup>, Jianfeng Chen<sup>3</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>LianFeng Acoustic Technologies Co. Ltd., <sup>2</sup>Kunming University of Science and Technology, <sup>3</sup>Northwestern Polytechnical University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Bai_NWPU_task3_2</span> <span class="label label-primary">Bai_NWPU_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Bai_NWPU_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Bai_NWPU_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Bai_NWPU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Bai_126_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Bai_NWPU_task3_report" class="panel-collapse collapse" id="collapse-Bai_NWPU_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2021 TASK 3: SELD SYSTEM BASED ON RESNET AND RANDOM SEGMENT AUGMENTATION
      </h4>
<p style="text-align:left">
<small>
        Jisheng Bai<sup>1</sup>, Zijun Pu<sup>2</sup>, Jianfeng Chen<sup>3</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>LianFeng Acoustic Technologies Co. Ltd., <sup>2</sup>Kunming University of Science and Technology, <sup>3</sup>Northwestern Polytechnical University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our system proposed for Sound Event Localization \&amp; Detection (SELD) task of DCASE 2021 challenge [1]. In our approach, Resnet architectures are used in the task as main network for SELD, and GRU is used after the Resnet for catching temporal relationship of acoustic features. Moreover, a data augmentation method called random segment augmentation is adopted during training. Firstly, the original sound recordings which only contain single event sound are cut into 100ms length pieces. Secondly, all the sound pieces are shuffled and randomly combined for generating new recordings. Finally, our proposed system is evaluated on the development dataset of task3 and it achieve better performance than baseline.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Bai_NWPU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Bai_126_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Bai_NWPU_task3_reportlabel" class="modal fade" id="bibtex-Bai_NWPU_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexBai_NWPU_task3_reportlabel">
        DCASE 2021 TASK 3: SELD SYSTEM BASED ON RESNET AND RANDOM SEGMENT AUGMENTATION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Bai_NWPU_task3_report,
    Author = "Bai, Jisheng and Pu, Zijun and Chen, Jianfeng",
    title = "DCASE 2021 TASK 3: SELD SYSTEM BASED ON RESNET AND RANDOM SEGMENT AUGMENTATION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This technical report describes our system proposed for Sound Event Localization \\&amp; Detection (SELD) task of DCASE 2021 challenge [1]. In our approach, Resnet architectures are used in the task as main network for SELD, and GRU is used after the Resnet for catching temporal relationship of acoustic features. Moreover, a data augmentation method called random segment augmentation is adopted during training. Firstly, the original sound recordings which only contain single event sound are cut into 100ms length pieces. Secondly, all the sound pieces are shuffled and randomly combined for generating new recordings. Finally, our proposed system is evaluated on the development dataset of task3 and it achieve better performance than baseline."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Huang_Aalto_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Huang_Aalto_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SSELDNET: A FULLY END-TO-END SAMPLE-LEVEL FRAMEWORK FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Huang Daolang, Perez Ricardo
       </p>
<p style="text-align:left">
<em>
         Aalto University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Huang_Aalto_task3_2</span> <span class="label label-primary">Huang_Aalto_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Huang_Aalto_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Huang_Aalto_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Huang_Aalto_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Huang_24_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Huang_Aalto_task3_report" class="panel-collapse collapse" id="collapse-Huang_Aalto_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SSELDNET: A FULLY END-TO-END SAMPLE-LEVEL FRAMEWORK FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Huang Daolang, Perez Ricardo
       </small>
<br/>
<small>
<em>
         Aalto University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event localization and detection (SELD) is a multi-task learn- ing problem that aims to detect different audio events and estimate their corresponding locations. All of the previously proposed SELD systems were based on human-extracted features such as Mel-spectrograms to make the prediction, which required specific prior knowledge in acoustics. In this report, we investigate the possibility to apply representation learning directly to the raw audio and propose an end-to-end sample-level SELD framework. To improve generalization, we applied three data augmentation tricks: sound field rotation, time masking and random audio equalization. The proposed system is evaluated on the TAU-NIGENS Spatial Sound Events 2021 development dataset. The experimental results will be submitted to DCASE 2021 challenge task 3.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Huang_Aalto_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Huang_24_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Huang_Aalto_task3_reportlabel" class="modal fade" id="bibtex-Huang_Aalto_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexHuang_Aalto_task3_reportlabel">
        SSELDNET: A FULLY END-TO-END SAMPLE-LEVEL FRAMEWORK FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Huang_Aalto_task3_report,
    Author = "Daolang, Huang and Ricardo, Perez",
    title = "SSELDNET: A FULLY END-TO-END SAMPLE-LEVEL FRAMEWORK FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "Sound event localization and detection (SELD) is a multi-task learn- ing problem that aims to detect different audio events and estimate their corresponding locations. All of the previously proposed SELD systems were based on human-extracted features such as Mel-spectrograms to make the prediction, which required specific prior knowledge in acoustics. In this report, we investigate the possibility to apply representation learning directly to the raw audio and propose an end-to-end sample-level SELD framework. To improve generalization, we applied three data augmentation tricks: sound field rotation, time masking and random audio equalization. The proposed system is evaluated on the TAU-NIGENS Spatial Sound Events 2021 development dataset. The experimental results will be submitted to DCASE 2021 challenge task 3."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Ko_SKKU_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Ko_SKKU_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A COMBINATION OF VARIOUS NEURAL NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Daniel Rho<sup>1</sup>, Seungjin Lee<sup>1</sup>, JinHyeock Park<sup>1</sup>, Taesoo Kim<sup>1</sup>, Jiho Chang<sup>2</sup>, Jonghwan Ko<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Sungkyunkwan University, <sup>2</sup>Korea Research Institute of Standards and Science
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Ko_SKKU_task3_4</span> <span class="label label-primary">Ko_SKKU_task3_2</span> <span class="label label-primary">Ko_SKKU_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Ko_SKKU_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Ko_SKKU_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Ko_SKKU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Ko_127_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Ko_SKKU_task3_report').collapse('show');window.location.hash='#Ko_SKKU_task3_report';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Ko_SKKU_task3_report" class="panel-collapse collapse" id="collapse-Ko_SKKU_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A COMBINATION OF VARIOUS NEURAL NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Daniel Rho<sup>1</sup>, Seungjin Lee<sup>1</sup>, JinHyeock Park<sup>1</sup>, Taesoo Kim<sup>1</sup>, Jiho Chang<sup>2</sup>, Jonghwan Ko<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Sungkyunkwan University, <sup>2</sup>Korea Research Institute of Standards and Science
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes our approach to the DCASE 2021 task 3: Sound Event Localization and Detection (SELD). We pro- pose a network architecture, a combination of various network layers, which can yield the optimal performance for the SELD task. Furthermore, we propose which augmentation techniques to use to boost the performance of our proposed model with a limited train dataset. In order to further improve the performance, several techniques were applied at training and post-processing stages, such as adaptive gradient clipping, ensemble techniques, and class-wise dynamic thresholds. Evaluation results on the development dataset showed that the proposed approach outperformed the existing base- line model of the task.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Ko_SKKU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Ko_127_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/IRIS-AUDIO/SELD" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Ko_SKKU_task3_reportlabel" class="modal fade" id="bibtex-Ko_SKKU_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexKo_SKKU_task3_reportlabel">
        A COMBINATION OF VARIOUS NEURAL NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Ko_SKKU_task3_report,
    Author = "Rho, Daniel and Lee, Seungjin and Park, JinHyeock and Kim, Taesoo and Chang, Jiho and Ko, Jonghwan",
    title = "A COMBINATION OF VARIOUS NEURAL NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This technical report describes our approach to the DCASE 2021 task 3: Sound Event Localization and Detection (SELD). We pro- pose a network architecture, a combination of various network layers, which can yield the optimal performance for the SELD task. Furthermore, we propose which augmentation techniques to use to boost the performance of our proposed model with a limited train dataset. In order to further improve the performance, several techniques were applied at training and post-processing stages, such as adaptive gradient clipping, ensemble techniques, and class-wise dynamic thresholds. Evaluation results on the development dataset showed that the proposed approach outperformed the existing base- line model of the task."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Lee_SGU_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Lee_SGU_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT LOCALIZATION AND DETECTION USING CROSS-MODAL ATTENTION AND PARAMETER SHARING FOR DCASE2021 CHALLENGE
       </h4>
<p style="text-align:left">
        Sang-Hoon Lee, Jung-Wook Hwang, Sang-Buem Seo, Hyung-Min Park
       </p>
<p style="text-align:left">
<em>
         Sogang University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Lee_SGU_task3_3</span> <span class="label label-primary">Lee_SGU_task3_4</span> <span class="label label-primary">Lee_SGU_task3_1</span> <span class="label label-primary">Lee_SGU_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Lee_SGU_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Lee_SGU_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Lee_SGU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Lee_92_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Lee_SGU_task3_report" class="panel-collapse collapse" id="collapse-Lee_SGU_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT LOCALIZATION AND DETECTION USING CROSS-MODAL ATTENTION AND PARAMETER SHARING FOR DCASE2021 CHALLENGE
      </h4>
<p style="text-align:left">
<small>
        Sang-Hoon Lee, Jung-Wook Hwang, Sang-Buem Seo, Hyung-Min Park
       </small>
<br/>
<small>
<em>
         Sogang University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our model for DCASE2021 Challenge Task3: Sound Event Localization and Detection (SELD) with Directional Interference. The model learns sound event detection (SED) and direction-of-arrival (DoA) at once by multi-task learn- ing for the SELD task. When learning the model, general features for both SED and DoA prediction are extracted by using the parameter-sharing strategy at the feature level of SED and DoA. In addition, the output is estimated by adding an attention layer based on cross-modal attention (CMA) in the transformer decoder so that the system can efficiently learn associations between SED and DoA features. Furthermore, three different prediction rules are presented for fully connected (FC) networks to provide SED and DoA results. Experiment has been conducted on the TAU- NIGENS Spatial Sound Events 2021 dataset, and to produce more learning data, the data was augmented by the mixup to sum up weighted audio clips and the channel rotation to change the location information of the sound source by rotating input channels, in addition to SpecAugment. Experimental results showed that our method provided significantly improved performance than the baseline method.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Lee_SGU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Lee_92_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Lee_SGU_task3_reportlabel" class="modal fade" id="bibtex-Lee_SGU_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexLee_SGU_task3_reportlabel">
        SOUND EVENT LOCALIZATION AND DETECTION USING CROSS-MODAL ATTENTION AND PARAMETER SHARING FOR DCASE2021 CHALLENGE
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Lee_SGU_task3_report,
    Author = "Lee, Sang-Hoon and Hwang, Jung-Wook and Seo, Sang-Buem and Park, Hyung-Min",
    title = "SOUND EVENT LOCALIZATION AND DETECTION USING CROSS-MODAL ATTENTION AND PARAMETER SHARING FOR DCASE2021 CHALLENGE",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "In this report, we present our model for DCASE2021 Challenge Task3: Sound Event Localization and Detection (SELD) with Directional Interference. The model learns sound event detection (SED) and direction-of-arrival (DoA) at once by multi-task learn- ing for the SELD task. When learning the model, general features for both SED and DoA prediction are extracted by using the parameter-sharing strategy at the feature level of SED and DoA. In addition, the output is estimated by adding an attention layer based on cross-modal attention (CMA) in the transformer decoder so that the system can efficiently learn associations between SED and DoA features. Furthermore, three different prediction rules are presented for fully connected (FC) networks to provide SED and DoA results. Experiment has been conducted on the TAU- NIGENS Spatial Sound Events 2021 dataset, and to produce more learning data, the data was augmented by the mixup to sum up weighted audio clips and the channel rotation to change the location information of the sound source by rotating input channels, in addition to SpecAugment. Experimental results showed that our method provided significantly improved performance than the baseline method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Naranjo-Alcazar_UV_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Naranjo-Alcazar_UV_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT LOCALIZATION AND DETECTION USING SQUEEZE-EXCITATION RESIDUAL CNNS
       </h4>
<p style="text-align:left">
        Javier Naranjo-Alcazar<sup>1,2</sup>, Sergi Perez-Castanos<sup>1</sup>, Maximo Cobos<sup>1</sup>, Francesc J. Ferri<sup>1</sup>, Pedro Zuccarello<sup>2</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Universitat de Valencia, <sup>2</sup>Instituto TecnolÃ³gico de InformÃ¡tica
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Naranjo-Alcazar_UV_task3_2</span>, <span class="label label-primary">Naranjo-Alcazar_UV_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Naranjo-Alcazar_UV_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Naranjo-Alcazar_UV_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Naranjo-Alcazar_UV_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Naranjo-Alcazar_33_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Naranjo-Alcazar_UV_task3_report').collapse('show');window.location.hash='#Naranjo-Alcazar_UV_task3_report';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Naranjo-Alcazar_UV_task3_report" class="panel-collapse collapse" id="collapse-Naranjo-Alcazar_UV_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT LOCALIZATION AND DETECTION USING SQUEEZE-EXCITATION RESIDUAL CNNS
      </h4>
<p style="text-align:left">
<small>
        Javier Naranjo-Alcazar<sup>1,2</sup>, Sergi Perez-Castanos<sup>1</sup>, Maximo Cobos<sup>1</sup>, Francesc J. Ferri<sup>1</sup>, Pedro Zuccarello<sup>2</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Universitat de Valencia, <sup>2</sup>Instituto TecnolÃ³gico de InformÃ¡tica
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event localisation and detection (SELD) is a problem in the field of automatic listening that aims at the temporal detection and localisation (direction of arrival estimation) of sound events within an audio clip, usually of long duration. Due to the amount of data present in the datasets related to this problem, solutions based on deep learning have positioned themselves at the top of the state of the art. Most solutions are based on 2D representations of the audio (different spectrograms) that are processed by a convolutional-recurrent network. The motivation of this submission is to study the squeeze-excitation technique in the convolutional part of the network and how it improves the performance of the system. This study is based on the one carried out by the same team last year. This year, it has been decided to study how this technique improves each of the datasets (last year only the MIC dataset was studied). This modification shows an improvement in the performance of the system compared to the baseline using MIC dataset.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Naranjo-Alcazar_UV_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Naranjo-Alcazar_33_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Machine-Listeners-Valencia/seld-dcase2021" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Naranjo-Alcazar_UV_task3_reportlabel" class="modal fade" id="bibtex-Naranjo-Alcazar_UV_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNaranjo-Alcazar_UV_task3_reportlabel">
        SOUND EVENT LOCALIZATION AND DETECTION USING SQUEEZE-EXCITATION RESIDUAL CNNS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Naranjo-Alcazar_UV_task3_report,
    Author = "Naranjo-Alcazar, Javier and Perez-Castanos, Sergi and Cobos, Maximo and Ferri, Francesc J. and Zuccarello, Pedro",
    title = "SOUND EVENT LOCALIZATION AND DETECTION USING SQUEEZE-EXCITATION RESIDUAL CNNS",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "Sound event localisation and detection (SELD) is a problem in the field of automatic listening that aims at the temporal detection and localisation (direction of arrival estimation) of sound events within an audio clip, usually of long duration. Due to the amount of data present in the datasets related to this problem, solutions based on deep learning have positioned themselves at the top of the state of the art. Most solutions are based on 2D representations of the audio (different spectrograms) that are processed by a convolutional-recurrent network. The motivation of this submission is to study the squeeze-excitation technique in the convolutional part of the network and how it improves the performance of the system. This study is based on the one carried out by the same team last year. This year, it has been decided to study how this technique improves each of the datasets (last year only the MIC dataset was studied). This modification shows an improvement in the performance of the system compared to the baseline using MIC dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Nguyen_NTU_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Nguyen_NTU_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DCASE 2021 TASK 3: SPECTROTEMPORALLY-ALIGNED FEATURES FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Thi Ngoc Tho Nguyen<sup>1</sup>, Karn Watcharasupat<sup>1</sup>, Ngoc Khanh Nguyen<sup>1</sup>, Douglas L. Jones<sup>2</sup>, Woon Seng Gan<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Nanyang Technological University, <sup>2</sup>University of Illinois Urbana-Champaign
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Nguyen_NTU_task3_1</span> <span class="label label-primary">Nguyen_NTU_task3_3</span> <span class="label label-primary">Nguyen_NTU_task3_2</span> <span class="label label-primary">Nguyen_NTU_task3_4</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Nguyen_NTU_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Nguyen_NTU_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Nguyen_NTU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Nguyen_115_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Nguyen_NTU_task3_report').collapse('show');window.location.hash='#Nguyen_NTU_task3_report';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Nguyen_NTU_task3_report" class="panel-collapse collapse" id="collapse-Nguyen_NTU_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DCASE 2021 TASK 3: SPECTROTEMPORALLY-ALIGNED FEATURES FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Thi Ngoc Tho Nguyen<sup>1</sup>, Karn Watcharasupat<sup>1</sup>, Ngoc Khanh Nguyen<sup>1</sup>, Douglas L. Jones<sup>2</sup>, Woon Seng Gan<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Nanyang Technological University, <sup>2</sup>University of Illinois Urbana-Champaign
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       Sound event localization and detection consists of two subtasks which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different sound classes, direction-of-arrival estimation uses magnitude or phase differences between micro- phones to estimate source directions. Therefore, it is often difficult to jointly optimize these two subtasks simultaneously. We pro- pose a novel feature called spatial cue-augmented log-spectrogram (SALSA) with exact time-frequency mapping between the signal power and the source direction-of-arrival. The feature includes multichannel log-spectrograms stacked along with the estimated direct- to-reverberant ratio and a normalized version of the principal eigenvector of the spatial covariance matrix at each time-frequency bin on the spectrograms. Experimental results on the DCASE 2021 dataset for sound event localization and detection with directional interference showed that the deep learning-based models trained on this new feature outperformed the DCASE challenge baseline by a large margin. We combined several models with slightly different architectures that were trained on the new feature to further improve the system performances for the DCASE sound event localization and detection challenge.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Nguyen_NTU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Nguyen_115_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/thomeou/SALSA" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Nguyen_NTU_task3_reportlabel" class="modal fade" id="bibtex-Nguyen_NTU_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexNguyen_NTU_task3_reportlabel">
        DCASE 2021 TASK 3: SPECTROTEMPORALLY-ALIGNED FEATURES FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Nguyen_NTU_task3_report,
    Author = "Nguyen, Thi Ngoc Tho and Watcharasupat, Karn and Nguyen, Ngoc Khanh and Jones, Douglas L. and Gan, Woon Seng",
    title = "DCASE 2021 TASK 3: SPECTROTEMPORALLY-ALIGNED FEATURES FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "Sound event localization and detection consists of two subtasks which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different sound classes, direction-of-arrival estimation uses magnitude or phase differences between micro- phones to estimate source directions. Therefore, it is often difficult to jointly optimize these two subtasks simultaneously. We pro- pose a novel feature called spatial cue-augmented log-spectrogram (SALSA) with exact time-frequency mapping between the signal power and the source direction-of-arrival. The feature includes multichannel log-spectrograms stacked along with the estimated direct- to-reverberant ratio and a normalized version of the principal eigenvector of the spatial covariance matrix at each time-frequency bin on the spectrograms. Experimental results on the DCASE 2021 dataset for sound event localization and detection with directional interference showed that the deep learning-based models trained on this new feature outperformed the DCASE challenge baseline by a large margin. We combined several models with slightly different architectures that were trained on the new feature to further improve the system performances for the DCASE sound event localization and detection challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Park_ETRI_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Park_ETRI_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SELF-ATTENTION MECHANISM FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Sooyoung Park, Youngho Jeong, Taejin Lee
       </p>
<p style="text-align:left">
<em>
         Electronics and Telecommunications Research Institute
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Park_ETRI_task3_1</span> <span class="label label-primary">Park_ETRI_task3_4</span> <span class="label label-primary">Park_ETRI_task3_3</span> <span class="label label-primary">Park_ETRI_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Park_ETRI_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Park_ETRI_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Park_ETRI_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Park_106_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Park_ETRI_task3_report" class="panel-collapse collapse" id="collapse-Park_ETRI_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SELF-ATTENTION MECHANISM FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Sooyoung Park, Youngho Jeong, Taejin Lee
       </small>
<br/>
<small>
<em>
         Electronics and Telecommunications Research Institute
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This technical report describes the system submitted to DCASE 2021 Task 3: Sound Event Localization and Detection (SELD) with Directional Interference. The goal of Task 3 is to classify poly- phonic events with temporal activity into a given class and detect their direction in the presence of hidden sound events. Our system uses a Transformer that utilizes the self-attention mechanism that is now successfully used in many fields. We propose an architecture called Many-to-Many Audio Spectrogram Transformer (M2M-AST) that uses a pure Transformer to reduce the dependency of CNNs and easily change the output resolution. Using the architecture for Sound Event Detection (SED) and Direction of Arrival Estimation (DOAE), which are small sub-problems that consist of SELD, we show that our system outperforms the baseline system.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Park_ETRI_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Park_106_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Park_ETRI_task3_reportlabel" class="modal fade" id="bibtex-Park_ETRI_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPark_ETRI_task3_reportlabel">
        SELF-ATTENTION MECHANISM FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Park_ETRI_task3_report,
    Author = "Park, Sooyoung and Jeong, Youngho and Lee, Taejin",
    title = "SELF-ATTENTION MECHANISM FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This technical report describes the system submitted to DCASE 2021 Task 3: Sound Event Localization and Detection (SELD) with Directional Interference. The goal of Task 3 is to classify poly- phonic events with temporal activity into a given class and detect their direction in the presence of hidden sound events. Our system uses a Transformer that utilizes the self-attention mechanism that is now successfully used in many fields. We propose an architecture called Many-to-Many Audio Spectrogram Transformer (M2M-AST) that uses a pure Transformer to reduce the dependency of CNNs and easily change the output resolution. Using the architecture for Sound Event Detection (SED) and Direction of Arrival Estimation (DOAE), which are small sub-problems that consist of SELD, we show that our system outperforms the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Parrish_JHU_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Parrish_JHU_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        MULTI-SCALE NETWORK FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Patrick Emmanuel, Nathan Parrish, Mark Horton
       </p>
<p style="text-align:left">
<em>
         Johns Hopkins University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Parrish_JHU_task3_3</span> <span class="label label-primary">Parrish_JHU_task3_1</span> <span class="label label-primary">Parrish_JHU_task3_4</span> <span class="label label-primary">Parrish_JHU_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Parrish_JHU_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Parrish_JHU_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Parrish_JHU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Parrish_95_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Parrish_JHU_task3_report" class="panel-collapse collapse" id="collapse-Parrish_JHU_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       MULTI-SCALE NETWORK FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Patrick Emmanuel, Nathan Parrish, Mark Horton
       </small>
<br/>
<small>
<em>
         Johns Hopkins University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes a multi-scale approach to the DCASE 2021 Sound Event Localization and Detection with Directional Interference task. The goal of this task is to detect, classify, and localize in time and space events from twelve sound event classes in vary- ing reverberant acoustic environments in the presence of interfering sources. We train a network that jointly performs detection, localization, and classification using multi-channel magnitude spectral data and intensity vectors derived from first order ambisonics time-series. We implement a network with successive blocks of multi- scale filters to discriminate and extract overlapping classes with different spectral characteristics. We also implement an output format and permutation invariant training loss that enable the network to detect, classify, and localize multiple instances of the same class simultaneously. Experiments show that the proposed network out- performs the CRNN baseline networks in classification and localization metrics.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Parrish_JHU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Parrish_95_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Parrish_JHU_task3_reportlabel" class="modal fade" id="bibtex-Parrish_JHU_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexParrish_JHU_task3_reportlabel">
        MULTI-SCALE NETWORK FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Parrish_JHU_task3_report,
    Author = "Emmanuel, Patrick and Parrish, Nathan and Horton, Mark",
    title = "MULTI-SCALE NETWORK FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This report describes a multi-scale approach to the DCASE 2021 Sound Event Localization and Detection with Directional Interference task. The goal of this task is to detect, classify, and localize in time and space events from twelve sound event classes in vary- ing reverberant acoustic environments in the presence of interfering sources. We train a network that jointly performs detection, localization, and classification using multi-channel magnitude spectral data and intensity vectors derived from first order ambisonics time-series. We implement a network with successive blocks of multi- scale filters to discriminate and extract overlapping classes with different spectral characteristics. We also implement an output format and permutation invariant training loss that enable the network to detect, classify, and localize multiple instances of the same class simultaneously. Experiments show that the proposed network out- performs the CRNN baseline networks in classification and localization metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Politis_TUNI_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Politis_TUNI_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        A DATASET OF DYNAMIC REVERBERANT SOUND SCENES WITH DIRECTIONAL INTERFERERS FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Archontis Politis<sup>1</sup>, Sharath Adavanne<sup>1</sup>, Daniel Krause<sup>1</sup>, Antoine Deleforge<sup>2</sup>, Prerak Srivastava<sup>2</sup>, Tuomas Virtanen<sup>1</sup>
</p>
<p style="text-align:left">
<em>
<sup>1</sup>Tampere University, <sup>2</sup>INRIA
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Politis_TUNI_task3_foa</span> <span class="label label-primary">Politis_TUNI_task3_mic</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Politis_TUNI_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Politis_TUNI_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Politis_TUNI_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="https://arxiv.org/pdf/2106.06999.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Politis_TUNI_task3_report').collapse('show');window.location.hash='#Politis_TUNI_task3_report';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Politis_TUNI_task3_report" class="panel-collapse collapse" id="collapse-Politis_TUNI_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       A DATASET OF DYNAMIC REVERBERANT SOUND SCENES WITH DIRECTIONAL INTERFERERS FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Archontis Politis<sup>1</sup>, Sharath Adavanne<sup>1</sup>, Daniel Krause<sup>1</sup>, Antoine Deleforge<sup>2</sup>, Prerak Srivastava<sup>2</sup>, Tuomas Virtanen<sup>1</sup>
</small>
<br/>
<small>
<em>
<sup>1</sup>Tampere University, <sup>2</sup>INRIA
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The dataset is based on emulation of real recordings of static or moving sound events under real conditions of reverberation and ambient noise, using spatial room impulse responses captured in a variety of rooms and delivered in two spatial formats. The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference of the new dataset is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation for SELD problems accompanies the dataset and is described herein. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Politis_TUNI_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="https://arxiv.org/pdf/2106.06999.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/sharathadavanne/seld-dcase2021" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Politis_TUNI_task3_reportlabel" class="modal fade" id="bibtex-Politis_TUNI_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexPolitis_TUNI_task3_reportlabel">
        A DATASET OF DYNAMIC REVERBERANT SOUND SCENES WITH DIRECTIONAL INTERFERERS FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Politis_TUNI_task3_report,
    Author = "Politis, Archontis and Adavanne, Sharath and Krause, Daniel and Deleforge, Antoine and Srivastava, Prerak and Virtanen, Tuomas",
    title = "A DATASET OF DYNAMIC REVERBERANT SOUND SCENES WITH DIRECTIONAL INTERFERERS FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The dataset is based on emulation of real recordings of static or moving sound events under real conditions of reverberation and ambient noise, using spatial room impulse responses captured in a variety of rooms and delivered in two spatial formats. The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference of the new dataset is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation for SELD problems accompanies the dataset and is described herein. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Shimada_SONY_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Shimada_SONY_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        ENSEMBLE OF ACCDOA- AND EINV2-BASED SYSTEMS WITH D3NETS AND IMPULSE RESPONSE SIMULATION FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Kazuki Shimada, Naoya Takahashi, Yuichiro Koyama, Shusuke Takahashi, Emiru Tsunoo, Masafumi Takahashi, Yuki Mitsufuji
       </p>
<p style="text-align:left">
<em>
         Sony Group Corporation
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Shimada_SONY_task3_4</span> <span class="label label-primary">Shimada_SONY_task3_1</span> <span class="label label-primary">Shimada_SONY_task3_3</span> <span class="label label-primary">Shimada_SONY_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Shimada_SONY_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Shimada_SONY_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Shimada_SONY_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Shimada_117_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Shimada_SONY_task3_report" class="panel-collapse collapse" id="collapse-Shimada_SONY_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       ENSEMBLE OF ACCDOA- AND EINV2-BASED SYSTEMS WITH D3NETS AND IMPULSE RESPONSE SIMULATION FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Kazuki Shimada, Naoya Takahashi, Yuichiro Koyama, Shusuke Takahashi, Emiru Tsunoo, Masafumi Takahashi, Yuki Mitsufuji
       </small>
<br/>
<small>
<em>
         Sony Group Corporation
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes our systems submitted to the DCASE2021 challenge task 3: sound event localization and detection (SELD) with directional interference. Our previous system based on activity-coupled Cartesian direction of arrival (ACCDOA) repre- sentation enables us to solve a SELD task with a single target. This ACCDOA-based system with efficient network architecture called RD3Net and data augmentation techniques outperformed state-of-the-art SELD systems in terms of localization and location- dependent detection. Using the ACCDOA-based system as a base, we perform model ensembles by averaging outputs of several sys- tems trained with different conditions such as input features, train- ing folds, and model architectures. We also use the event inde- pendent network v2 (EINV2)-based system to increase the diver- sity of the model ensembles. To generalize the models, we further propose impulse response simulation (IRS), which generates simu- lated multi-channel signals by convolving simulated room impulse responses (RIRs) with source signals extracted from the original dataset. Our systems significantly improved over the baseline sys- tem on the development dataset.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Shimada_SONY_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Shimada_117_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Shimada_SONY_task3_reportlabel" class="modal fade" id="bibtex-Shimada_SONY_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexShimada_SONY_task3_reportlabel">
        ENSEMBLE OF ACCDOA- AND EINV2-BASED SYSTEMS WITH D3NETS AND IMPULSE RESPONSE SIMULATION FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Shimada_SONY_task3_report,
    Author = "Shimada, Kazuki and Takahashi, Naoya and Koyama, Yuichiro and Takahashi, Shusuke and Tsunoo, Emiru and Takahashi, Masafumi and Mitsufuji, Yuki",
    title = "ENSEMBLE OF ACCDOA- AND EINV2-BASED SYSTEMS WITH D3NETS AND IMPULSE RESPONSE SIMULATION FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This report describes our systems submitted to the DCASE2021 challenge task 3: sound event localization and detection (SELD) with directional interference. Our previous system based on activity-coupled Cartesian direction of arrival (ACCDOA) repre- sentation enables us to solve a SELD task with a single target. This ACCDOA-based system with efficient network architecture called RD3Net and data augmentation techniques outperformed state-of-the-art SELD systems in terms of localization and location- dependent detection. Using the ACCDOA-based system as a base, we perform model ensembles by averaging outputs of several sys- tems trained with different conditions such as input features, train- ing folds, and model architectures. We also use the event inde- pendent network v2 (EINV2)-based system to increase the diver- sity of the model ensembles. To generalize the models, we further propose impulse response simulation (IRS), which generates simu- lated multi-channel signals by convolving simulated room impulse responses (RIRs) with source signals extracted from the original dataset. Our systems significantly improved over the baseline sys- tem on the development dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Sun_AIAL-XJU_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Sun_AIAL-XJU_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        SOUND EVENT LOCALIZATION AND DETECTION BASED ON CRNN USING ADAPTIVE HYBRID CONVOLUTION AND MULTI-SCALE FEATURE EXTRACTOR
       </h4>
<p style="text-align:left">
        Xinghao Sun, Xiujuan Zhu, Ying Hu
       </p>
<p style="text-align:left">
<em>
         Xinjiang University
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Sun_AIAL-XJU_task3_1</span> <span class="label label-primary">Sun_AIAL-XJU_task3_2</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Sun_AIAL-XJU_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Sun_AIAL-XJU_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Sun_AIAL-XJU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Sun_49_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
<a class="btn btn-xs btn-success" data-placement="bottom" href="" onclick="$('#collapse-Sun_AIAL-XJU_task3_report').collapse('show');window.location.hash='#Sun_AIAL-XJU_task3_report';return false;" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="">
<i class="fa fa-file-code-o">
</i>
         Code
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Sun_AIAL-XJU_task3_report" class="panel-collapse collapse" id="collapse-Sun_AIAL-XJU_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       SOUND EVENT LOCALIZATION AND DETECTION BASED ON CRNN USING ADAPTIVE HYBRID CONVOLUTION AND MULTI-SCALE FEATURE EXTRACTOR
      </h4>
<p style="text-align:left">
<small>
        Xinghao Sun, Xiujuan Zhu, Ying Hu
       </small>
<br/>
<small>
<em>
         Xinjiang University
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this report, we present our method for Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 challenge task 3: Sound Event Localization and Detection with Directional Interference (SELDDI). In this paper, we propose a method based on Adaptive Hybrid Convolution (AHConv) and multi-scale feature extractor. The square convolution shares the weight in each T-F bin of the fixed area in feature map, that is limited. In order to address this problem, we propose a AHConv mechanism instead of square convolution to obtain time and frequency dependencies. We also explored multi-scale feature extractor which can integrate information from very local to exponentially large receptive field within the block. In order to adaptive recalibrate the feature maps after convolutional operation, we designed an adaptive attention block which are largely embodied in the AHConv. On TAU-NIGENS Spatial Sound Events 2021 development dataset, our systems demonstrate a significant improvement over the baseline system. Only the first- order Ambisonics (FOA) dataset was considered in this experiment.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Sun_AIAL-XJU_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Sun_49_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/Codersun10755/DCASE2021-Sun_AIAL-XJU_task3" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="">
<i class="fa fa-file-code-o">
</i>
</a>
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Sun_AIAL-XJU_task3_reportlabel" class="modal fade" id="bibtex-Sun_AIAL-XJU_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexSun_AIAL-XJU_task3_reportlabel">
        SOUND EVENT LOCALIZATION AND DETECTION BASED ON CRNN USING ADAPTIVE HYBRID CONVOLUTION AND MULTI-SCALE FEATURE EXTRACTOR
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Sun_AIAL-XJU_task3_report,
    Author = "Sun, Xinghao and Zhu, Xiujuan and Hu, Ying",
    title = "SOUND EVENT LOCALIZATION AND DETECTION BASED ON CRNN USING ADAPTIVE HYBRID CONVOLUTION AND MULTI-SCALE FEATURE EXTRACTOR",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "In this report, we present our method for Detection and Classification of Acoustic Scenes and Events (DCASE) 2021 challenge task 3: Sound Event Localization and Detection with Directional Interference (SELDDI). In this paper, we propose a method based on Adaptive Hybrid Convolution (AHConv) and multi-scale feature extractor. The square convolution shares the weight in each T-F bin of the fixed area in feature map, that is limited. In order to address this problem, we propose a AHConv mechanism instead of square convolution to obtain time and frequency dependencies. We also explored multi-scale feature extractor which can integrate information from very local to exponentially large receptive field within the block. In order to adaptive recalibrate the feature maps after convolutional operation, we designed an adaptive attention block which are largely embodied in the AHConv. On TAU-NIGENS Spatial Sound Events 2021 development dataset, our systems demonstrate a significant improvement over the baseline system. Only the first- order Ambisonics (FOA) dataset was considered in this experiment."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Yalta_HIT_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Yalta_HIT_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        THE HITACHI DCASE 2021 TASK 3 SYSTEM: HANDLING DIRECTIVE INTERFERENCE WITH SELF ATTENTION LAYERS
       </h4>
<p style="text-align:left">
        Nelson Yalta, Takashi Sumiyoshi, Yohei Kawaguchi
       </p>
<p style="text-align:left">
<em>
         Hitachi Ltd.
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Yalta_HIT_task3_1</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Yalta_HIT_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Yalta_HIT_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Yalta_HIT_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yalta_29_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Yalta_HIT_task3_report" class="panel-collapse collapse" id="collapse-Yalta_HIT_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       THE HITACHI DCASE 2021 TASK 3 SYSTEM: HANDLING DIRECTIVE INTERFERENCE WITH SELF ATTENTION LAYERS
      </h4>
<p style="text-align:left">
<small>
        Nelson Yalta, Takashi Sumiyoshi, Yohei Kawaguchi
       </small>
<br/>
<small>
<em>
         Hitachi Ltd.
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       This report describes the Hitachi system for the DCASE 2021 Challenge - Task 3. Our proposal relies on a single-stage system that employs the transformer encoder (i.e., self-attention layers) as a core idea. We evaluate the effect of applying different transformer configurations to handle the directive interferences on the presence of multiple sound events. Additionally, the transformer employs residual connections to extract the features from the input streams. We trained the model using specaugment as data augmentation and performed threshold post-processing for each sound event. Em- ploying the first-order Ambisonic (FOA) signals, the transformer was trained using the activity-coupled Cartesian DOA vector (ACCDOA) representations. This unified training framework showed better performance than training the model for each sub-task independently.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Yalta_HIT_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Yalta_29_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Yalta_HIT_task3_reportlabel" class="modal fade" id="bibtex-Yalta_HIT_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexYalta_HIT_task3_reportlabel">
        THE HITACHI DCASE 2021 TASK 3 SYSTEM: HANDLING DIRECTIVE INTERFERENCE WITH SELF ATTENTION LAYERS
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Yalta_HIT_task3_report,
    Author = "Yalta, Nelson and Sumiyoshi, Takashi and Kawaguchi, Yohei",
    title = "THE HITACHI DCASE 2021 TASK 3 SYSTEM: HANDLING DIRECTIVE INTERFERENCE WITH SELF ATTENTION LAYERS",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "This report describes the Hitachi system for the DCASE 2021 Challenge - Task 3. Our proposal relies on a single-stage system that employs the transformer encoder (i.e., self-attention layers) as a core idea. We evaluate the effect of applying different transformer configurations to handle the directive interferences on the presence of multiple sound events. Additionally, the transformer employs residual connections to extract the features from the input streams. We trained the model using specaugment as data augmentation and performed threshold post-processing for each sound event. Em- ploying the first-order Ambisonic (FOA) signals, the transformer was trained using the activity-coupled Cartesian DOA vector (ACCDOA) representations. This unified training framework showed better performance than training the model for each sub-task independently."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
<div class="panel publication-item" id="Zhang_UCAS_task3_report" style="box-shadow: none">
<div class="panel-heading" id="heading-Zhang_UCAS_task3_report" role="tab">
<div class="row">
<div class="col-xs-9">
<h4>
        DATA AUGMENTATION AND CLASS-BASED ENSEMBLED CNN-CONFORMER NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
<p style="text-align:left">
        Yuxuan Zhang, Shuo Wang, Zihao Li, Kejian Guo, Shijin Chen, Yan Pang
       </p>
<p style="text-align:left">
<em>
         University of Chinese Academy of Sciences
        </em>
</p>
<p style="text-align:left">
<span class="label label-primary">Zhang_UCAS_task3_1</span> <span class="label label-primary">Zhang_UCAS_task3_4</span> <span class="label label-primary">Zhang_UCAS_task3_2</span> <span class="label label-primary">Zhang_UCAS_task3_3</span>
</p>
<p style="text-align:left">
</p>
<button aria-controls="collapse-Zhang_UCAS_task3_report" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#accordion" data-toggle="collapse" href="#collapse-Zhang_UCAS_task3_report" type="button">
<i class="fa fa-caret-down">
</i>
        Details...
       </button>
</div>
<div class="col-xs-3">
<div class="btn-group">
<button class="btn btn-xs btn-danger" data-target="#bibtex-Zhang_UCAS_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
         Bibtex
        </button>
<a class="btn btn-xs btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zhang_67_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:2px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
         PDF
        </a>
</div>
</div>
</div>
</div>
<div aria-labelledby="heading-Zhang_UCAS_task3_report" class="panel-collapse collapse" id="collapse-Zhang_UCAS_task3_report" role="tabpanel">
<div class="panel-body well well-sm">
<h4>
       DATA AUGMENTATION AND CLASS-BASED ENSEMBLED CNN-CONFORMER NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION
      </h4>
<p style="text-align:left">
<small>
        Yuxuan Zhang, Shuo Wang, Zihao Li, Kejian Guo, Shijin Chen, Yan Pang
       </small>
<br/>
<small>
<em>
         University of Chinese Academy of Sciences
        </em>
</small>
</p>
<h5>
<strong>
        Abstract
       </strong>
</h5>
<p class="text-justify">
       In this technical report, we describe the system participating in the DCASE 2021, Task 3: Sound Event Localization and Detection (SELD) challenge. We introduce Conformer block into the base- line system to make better use of temporal context information for the SELD task. To expand the official training dataset, we use Audio Channel Swapping (ACS), Speed Perturbation (SP), and Time- Frequency Masking (TFM) as augmentation techniques. In addition, we proposed a class-based ensemble method to attain a more robust sound event detection (SED) and sound source localization (SSL) estimation result for each sound event. After evaluating our best-proposed system on DCASE 2021 Challenge Task 3 Development Dataset, we approximately achieve 44\% and 37\% relative improvements on the SELD scores, respectively.
      </p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtex-Zhang_UCAS_task3_report" data-toggle="modal" type="button">
<i class="fa fa-file-text-o">
</i>
        Bibtex
       </button>
<a class="btn btn-sm btn-warning" data-placement="bottom" href="../documents/challenge2021/technical_reports/DCASE2021_Zhang_67_t3.pdf" rel="tooltip" style="text-decoration:none;border-bottom:0;padding-bottom:6px" title="Download pdf">
<i class="fa fa-file-pdf-o fa-1x">
</i>
        PDF
       </a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtex-Zhang_UCAS_task3_reportlabel" class="modal fade" id="bibtex-Zhang_UCAS_task3_report" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">
         Ã—
        </span>
<span class="sr-only">
         Close
        </span>
</button>
<h4 class="modal-title" id="bibtexZhang_UCAS_task3_reportlabel">
        DATA AUGMENTATION AND CLASS-BASED ENSEMBLED CNN-CONFORMER NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION
       </h4>
</div>
<div class="modal-body">
<pre>@techreport{Zhang_UCAS_task3_report,
    Author = "Zhang, Yuxuan and Wang, Shuo and Li, Zihao and Guo, Kejian and Chen, Shijin and Pang, Yan",
    title = "DATA AUGMENTATION AND CLASS-BASED ENSEMBLED CNN-CONFORMER NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "November",
    abstract = "In this technical report, we describe the system participating in the DCASE 2021, Task 3: Sound Event Localization and Detection (SELD) challenge. We introduce Conformer block into the base- line system to make better use of temporal context information for the SELD task. To expand the official training dataset, we use Audio Channel Swapping (ACS), Speed Perturbation (SP), and Time- Frequency Masking (TFM) as augmentation techniques. In addition, we proposed a class-based ensemble method to attain a more robust sound event detection (SED) and sound source localization (SSL) estimation result for each sound event. After evaluating our best-proposed system on DCASE 2021 Challenge Task 3 Development Dataset, we approximately achieve 44\\% and 37\\% relative improvements on the SELD scores, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">
        Close
       </button>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><br/></p>
<script>
(function($) {
$(document).ready(function() {
var hash = window.location.hash.substr(1);
var anchor = window.location.hash;

var shiftWindow = function() {
var hash = window.location.hash.substr(1);
if($('#collapse-'+hash).length){
scrollBy(0, -100);
}
};
window.addEventListener("hashchange", shiftWindow);

if (window.location.hash){
window.scrollTo(0, 0);
history.replaceState(null, document.title, "#");
$('#collapse-'+hash).collapse('show');
setTimeout(function(){
window.location.hash = anchor;
shiftWindow();
}, 2000);
}
});
})(jQuery);
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="/theme/js/respond.min.js"></script>
<script type="text/javascript" src="/theme/js/btex.min.js"></script>
<script type="text/javascript" src="/theme/js/datatable.bundle.min.js"></script>
<script type="text/javascript" src="/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>